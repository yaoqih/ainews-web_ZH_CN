---
companies:
- minimax-ai
- togethercompute
- huggingface
- intel
- wandb
date: '2026-02-13T05:44:39.731046Z'
description: '**MiniMax-M2.5** 现已正式开源。该模型采用了名为 **Forge** 的“智能体原生”（agent-native）强化学习框架，并在超过
  **20 万个 RL（强化学习）环境**中针对编程、工具调用和工作流进行了训练。


  它在基准测试中表现出色，例如在 **SWE-bench Verified** 上取得了 **80.2%** 的优异成绩，并强调其极高的成本效率（宣称“在 100
  tps 的速率下每小时仅需 1 美元”）以及良好的端侧性能。**Forge** 强化学习系统利用多级前缀缓存和高比例的 Rollout（采样/展开）计算占比（约
  60%），每天可生成数百万条轨迹。独立评测指出，该模型在稳定性和多轮对话的可行性上有所提升，但 Token 消耗量较大。


  目前，开源生态已迅速采纳了 MiniMax-M2.5，并发布了包括 **2-bit GGUF** 和 **INT4** 格式在内的量化版本。与此同时，**Together**
  公司正在推介 **GLM-5**，将其定位为领先的长程（long-horizon）智能体开源模型；GLM-5 在 **SWE-bench Verified**
  上的评分为 **77.8%**，并采用了 DeepSeek 稀疏注意力机制以实现高效的 MoE（混合专家模型）架构。'
id: MjAyNi0w
models:
- minimax-m2.5
- glm-5
people: []
title: MiniMax-M2.5：顶尖（SOTA）级编程、搜索、工具调用能力，仅需 $1/小时。
topics:
- reinforcement-learning
- agent-based-models
- model-quantization
- benchmarking
- model-efficiency
- multi-turn-dialogue
- infrastructure-optimization
- cost-efficiency
- on-device-ai
---

**平静的一天**

> 2026/2/12-2/13 的 AI 新闻。我们为您查看了 12 个 subreddits、[544 个 Twitter 账号](https://twitter.com/i/lists/1585430245762441216) 和 24 个 Discord（包含 **256** 个频道和 **7993** 条消息）。为您节省了约 **675** 分钟的阅读时间（按 200wpm 计算）。[AINews 网站](https://news.smol.ai/) 可供搜索过往所有期次。提醒一下，[AINews 现在是 Latent Space 的一个板块](https://www.latent.space/p/2026)。您可以[选择开启/关闭](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack)邮件接收频率！

这是 MiniMax 想要讲述的发展轨迹故事：

![https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/bench_10.png](https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/bench_10.png)

但更重大的故事可能是 Forge，他们的 Agent 原生 RL 框架。

![https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/rl_1.png](https://github.com/MiniMax-AI/MiniMax-M2.5/raw/main/figures/rl_1.png)

---

# AI Twitter 回顾

**MiniMax M2.5 开源：Agent 原生 RL、速度/成本以及快速的生态采用**

- **MiniMax-M2.5 现已开源**：MiniMax 发布了 **MiniMax-M2.5** 的权重和代码，将其定位为一个“Agent 原生”模型，通过在**数十万个真实环境**（涵盖编程、工具调用、搜索和办公工作流）中进行 **RL** 训练而成 ([MiniMax 公告](https://twitter.com/MiniMax_AI/status/2022310932693897628))。vLLM 强调了首日支持并报告了关键基准测试数据：**80.2% SWE‑Bench Verified**、**76.3% BrowseComp**，此外还声称了训练规模（20万+ RL 环境）以及速度/成本特性 ([vLLM](https://twitter.com/vllm_project/status/2022311342225678757))。SGLang 同样提供了首日支持，并将该模型定位为面向“全天候”Agent 的生产级模型 ([lmsys](https://twitter.com/lmsysorg/status/2022319102560555401))。
- **实际的看点在于经济性 + 吞吐量，而非仅仅是分数**：MiniMax 反复宣传 **“100 tps 下每小时 1 美元”**（可理解为“长周期 Agent 预算”），这既出现在他们自己的推文中 ([MiniMax](https://twitter.com/MiniMax_AI/status/2022379949336957254))，也出现在强调低激活参数量使私有化部署更具可行性的社区回顾中 ([omarsar0](https://twitter.com/omarsar0/status/2022384166034190528))。早期的本地运行表明，该模型在同类产品中具有极强的端侧可行性：MLX 用户在发布后不久便报告了约 **50 tok/s** 的速度 ([pcuenq](https://twitter.com/pcuenq/status/2022336556326060341))，而在单台 **M3 Ultra 512GB** 上以 **6‑bit** 运行，报告速度约为 **40 tok/s**，峰值内存约为 **186GB** ([ivanfioravanti](https://twitter.com/ivanfioravanti/status/2022338870172684655))。
- **Forge RL 训练系统的细节浮出水面**：一篇源自知乎的文章总结了 MiniMax 的 “Forge” RL 技术栈仍采用 **类 CISPO** 方案，使用**过程奖励 (process reward) + 完成时间奖励 (completion-time reward)**，并采用了诸如**多级前缀缓存 (multi-level prefix cache)** 和高采样计算占比（据称约占计算量的 **60%**）等基础设施技巧，每天产生**数百万条轨迹** ([YouJiacheng](https://twitter.com/YouJiacheng/status/2022339475049947576))。MiniMax 领导层明确回答了参数化权衡（“**10B 激活**参数是有意为之”），声称接近“**无限 Agent 扩展**”，而**知识容量**是限制因素，并预告 **M3** 将专注于结构和预训练创新 ([MiniMax 回复](https://twitter.com/MiniMax_AI/status/2022370086397624476))。
- **独立评价：“适用于多轮对话任务”，但耗费 Token**：一篇中文评测指出 M2.5 修正了 M2.1 的不平衡（编程能力提升，逻辑能力下降），整体有所进步且稳定性更好；评测注意到其 **Token 消耗较高**（在一次对比中接近 **Sonnet 的 2 倍**），但认为价格/计算成本使其在日常使用中依然可行 ([ZhihuFrontier](https://twitter.com/ZhihuFrontier/status/2022214461415993817))。另一份总结称其“编程能力 ≤ Sonnet，但非常接近”，并强调多轮对话的可行性是其区别于“玩具级”开源模型的关键 ([teortaxesTex](https://twitter.com/teortaxesTex/status/2022223441005621556))。
- **生态系统迅速采纳**：权重已在各种工具中同步并打包（Hugging Face 发布提醒、GGUF/量化版本发布等），包括 Intel 托管的量化产物，如用于 MiniMax‑M2 的 **2‑bit GGUF** 和用于 Qwen3‑Coder‑Next 的 **INT4** 版本 ([HaihaoShen](https://twitter.com/HaihaoShen/status/2022293472796180676))。

**GLM‑5 与“准前沿”开源模型浪潮：性能、基础设施约束及评估讨论**

- **GLM‑5 定位**：Together 将 GLM‑5 推向市场，称其为长程（long-horizon）Agent 和系统工程领域同类最佳的开源模型，引用了诸如 **77.8% SWE‑Bench Verified**、**50.4% HLE w/ tools** 等指标，以及基于 “DeepSeek Sparse Attention” 的 MoE 效率方案（如推文所述）([Together](https://twitter.com/togethercompute/status/2022354579858289052))。W&B 推广的一篇访谈声称其拥有 **744B 参数**、采用“全新 RL 框架”并“基于 MIT 协议完全开源”（如帖子所述）([W&B](https://twitter.com/wandb/status/2022389206572765697))。社区成员还注意到 GLM‑5 的输出中出现了 “truthy‑dpo” 等数据集指纹 ([jon_durbin](https://twitter.com/jon_durbin/status/2022291772617945546))。
- **GLM‑5 定性评价亮点**：一份基于知乎的详细对比将 GLM‑5 描述为对 GLM‑4.7 的实质性改进，特别是在幻觉控制、编程基础和字符处理方面——但也更啰嗦/消耗 Token，且容易“过度思考”，这暗示了在长程推理与计算消耗（compute burn）之间存在权衡 ([ZhihuFrontier on GLM‑5](https://twitter.com/ZhihuFrontier/status/2022161058321047681))。
- **基准测试作为动态目标**：关于排行榜/评估（evals）是否饱和或具有误导性的元讨论持续不断。例如：担心 Token/延迟权衡隐藏了真实能力；对通过 TPS 推断模型大小的怀疑；以及观察到过去关于 “SWE‑bench 饱和”的说法还为时尚早 ([jyangballin](https://twitter.com/jyangballin/status/2022367240293949772), [teortaxesTex](https://twitter.com/teortaxesTex/status/2022255213394948360))。
- **交叉验证替代评估指标**：SWE‑rebench 被指出对某些近期发布的模型非常“残酷”，并显示出与 SWE‑bench Verified 不同的相对排名；提醒应将其视为“额外信号” ([maximelabonne](https://twitter.com/maximelabonne/status/2022401174549512576))。

**Agent 工程实践：基于文件的协作、终端优先的工作流以及“Agent OS”框架**

- **Claude Code “Agent Teams” 内部结构出人意料地简单**：一项反向工程总结称，Claude Code 的多 Agent 通信使用**磁盘上的 JSON 文件**（位于 `~/.claude/teams/inboxes/{agent}.json` 的收件箱），在轮次之间进行轮询（polling），并采用 JSON-in-JSON 协议消息；论点是这是一种务实的 CLI 设计（无需 Redis/队列），以牺牲原子性/背压（backpressure）为代价提升了可观测性 ([peter6759](https://twitter.com/peter6759/status/2022156692985983266))。
- **终端 Agent 正在成为默认 UX**：Cline 发布了 **Cline CLI 2.0**，这是一个开源终端编程 Agent，具有重新设计的交互式 TUI、拥有隔离状态的并行 Agent、无头 CI/CD 模式以及广泛的编辑器支持（针对 Zed/Neovim/Emacs 的 ACP）([cline](https://twitter.com/cline/status/2022341254965772367), [cline details](https://twitter.com/cline/status/2022341258979717196))。社区观点：“开源的反击”归功于能够免费/低门槛地访问强模型 ([testingcatalog](https://twitter.com/testingcatalog/status/2022348951459172604), [dr_cintas](https://twitter.com/dr_cintas/status/2022387444189139367))。一位 Cline 团队成员描述了由架构/UX 痛点和可靠运行评估需求驱动的完整重写（Go → TypeScript）([arafatkatze](https://twitter.com/arafatkatze/status/2022415192932651302))。
- **Agent 脚手架（scaffolds）的重要性可能低于预期（针对某些阶段）**：METR 相关的讨论表明，在目前测量的时长范围内，Claude Code / Codex 脚手架的表现并没有大幅超越 METR 的“简单 OS 脚手架” ([nikolaj2030](https://twitter.com/nikolaj2030/status/2022398669337825737))，Ajeya Cotra 对这种微小的差异表示惊讶 ([ajeya_cotra](https://twitter.com/ajeya_cotra/status/2022419978495127828))。相比之下，其他人指出对于更长、更困难的任务，脚手架的选择可能产生实质性影响（例如 **~10% 的成功率**波动） ([gneubig](https://twitter.com/gneubig/status/2022451119310655909))。
- **“Agent 即 OS / 文件系统即基座”**：多篇文章汇聚成一个观点，即文件系统是 Agent 的自然环境（可观测性、非结构化数据操作）。Box 宣布将作为“云文件系统”集成到 LangChain deepagents 中 ([levie](https://twitter.com/levie/status/2022375298097111160))。WebMCP 推动“浏览器即 API”用于无需 UI 感知的 Web 自动化，并提供了类似 DoorDash 的入门模板 ([skirano](https://twitter.com/skirano/status/2022387763421810989))。
- **关键运营经验：让代码库具备“Agent 就绪性”**：一个敏锐的观察是，Agent 对人类能够绕过的“熵（entropy）”具有“零容忍”；它们会照单全收地处理死代码/过时文档，迫使人们进行人类一直需要但经常推迟的工程规范（hygiene） ([dok2001](https://twitter.com/dok2001/status/2022339274767520246))。

**RL/后训练研究主题：过程奖励、探索与基于评分细则（rubric）的评估**

- **针对推理的长度激励探索 (Length-Incentivized Exploration, LIE)**：一份研究摘要介绍了“浅层探索陷阱”（Shallow Exploration Trap，即在 AR 采样下，长的推理轨迹变得指数级不可能出现），并提出了 LIE：通过长度奖励 + 冗余惩罚来鼓励更广泛的上下文内探索，且不产生填充内容（filler）。报告的收益包括：在一种设置下 **AIME25 从 20.5% 提升至 26.7%**，以及在其他基准测试和模型上的小幅但持续的改进 ([dair_ai](https://twitter.com/dair_ai/status/2022360649817526275))。
- **DPPO 与 PPO 以及“信任区域”框架**：一个长篇算法分解对比了 PPO 的 Token 比例剪裁（token-ratio clipping）与 DPPO 通过散度测量（TV/KL）进行的分布偏移控制，此外还通过近似计算（二进制/top-K）来减少计算量。文章认为 DPPO 在稀有 Token 上更具比例性，并且能更好地约束大概率质量的移动 ([TheTuringPost](https://twitter.com/TheTuringPost/status/2022326245745377562))。
- **评分细则即奖励（Rubrics-as-rewards）与演进式评分细则**：一个推文串介绍了 Dr. Tulu 中的 **RLER**（带有演进式评分细则的 RL）：为种子评分细则设定基于搜索的准则，为每个提示（prompt）维护一个演进的评分细则缓冲区，并根据奖励方差保留最具辨别力的评分细则，以对抗奖励作弊（reward hacking）并适应策略内（on-policy）评估 ([cwolferesearch](https://twitter.com/cwolferesearch/status/2022384365049892974))。另外，一种观点认为“评分细则即奖励”甚至在形式验证设置中也能优于“验证器即奖励”，建议将验证器置于循环/框架中，但不作为唯一的奖励信号 ([davidad](https://twitter.com/davidad/status/2022361016995319850))。
- **∆Belief‑RL / 信息寻求型 Agent**：一种新方法根据动作增加了多少对目标的信念（基于 logprob）来提供奖励，旨在实现长跨度的信息寻求，而无需 Critic/奖励模型；声称包括从“20个问题”训练到新任务的泛化能力，以及在扩展交互时间时持续改进 ([ShashwatGoel7](https://twitter.com/ShashwatGoel7/status/2022341054939185345))。
- **人类模拟作为 RL 目标**：斯坦福大学的 **HumanLM** + **Humanual** 基准测试提出训练 LLM 来准确模拟用户响应（以人为中心的评估、偏好塑造、策略正当化），将用户模拟定位为产品/Agent 设计的一种能力原语 ([ShirleyYXWu](https://twitter.com/ShirleyYXWu/status/2022374624676421676))。

**系统/基础设施与工具：FP4 MoE 内核、更快的 ZeRO 加载、模型“技能”与可观测性**

- **GB300 上的 vLLM + FP4 MoE 加速**：vLLM 报告了 DeepSeek R1 在 **GB300** 上的表现，每张 GPU 达到 **22.5K Prefill TGS** 和 **3K Decode TGS**，声称较 Hopper 有大幅提升，并重点介绍了包括 **NVFP4 权重**和 **FlashInfer FP4 MoE 内核** (`VLLM_USE_FLASHINFER_MOE_FP4=1`) 在内的方案，以及解耦预填充（disaggregated prefill）和调优笔记 ([vllm_project](https://twitter.com/vllm_project/status/2022308974150975792))。
- **DeepSpeed ZeRO 加载耗时修复**：一项改进将张量平展（tensor flattening）从 CPU 转移到 GPU，显著缩短了 ZeRO 1+2 下巨型模型的多 GPU 加载时间 ([StasBekman](https://twitter.com/StasBekman/status/2022354880049082658))。
- **Gemini “技能”与多模态工具调用**：Google 的 Gemini API 工作包括一个“技能”仓库预告 ([osanseviero](https://twitter.com/osanseviero/status/2022259577232785866))，以及 Interactions API 的更新，支持 **多模态函数调用**，其中工具可以返回 **图像**，且 Gemini 可以原生处理返回的图像 ([philschmid](https://twitter.com/_philschmid/status/2022349886318928158))。AI Studio 计费/升级用户体验得到优化（无需离开 Studio 即可升级为付费版、使用情况跟踪、支出过滤器）([OfficialLoganK](https://twitter.com/OfficialLoganK/status/2022409335465480346), [GoogleAIStudio](https://twitter.com/GoogleAIStudio/status/2022409735287537999))。
- **Agent 测试框架插桩**：ArtificialAnalysis 为其 Agent 测试框架 **Stirrup** 增加了端到端速度跟踪，以及每个模型的细分数据和工具调用延迟指标——明确将墙钟完成时间（wall-clock completion time）视为 Agent 的一级指标 ([ArtificialAnlys](https://twitter.com/ArtificialAnlys/status/2022358995739254800))。
- **本地微调与 Apple Silicon 工作流**：MLX 的重要工具：在 MLX Swift 中使用 Voxtral Mini 4B 进行实时转录 ([awnihannun](https://twitter.com/awnihannun/status/2022322714548338962))，一个可导出到 Ollama 的无代码本地微调工具 ([awnihannun](https://twitter.com/awnihannun/status/2022327214218657948))，以及包含 GRPO/ORPO/DPO 变体的 MLX-LM LoRA 示例仓库 ([ActuallyIsaak](https://twitter.com/ActuallyIsaak/status/2022414004623479014))。

**“AI 加速科学”时刻：GPT‑5.2 + QFT 结果与脚手架叙事**

- **OpenAI 声称 GPT‑5.2 取得了全新的理论物理研究成果**：OpenAI 发布了一篇预印本，展示了此前被认为不会发生的胶子相互作用（gluon interaction）可以在特定的“半共线”（half-collinear）机制下产生，并将其定义为 AI 辅助发现 ([OpenAI](https://twitter.com/OpenAI/status/2022390096625078389)；预印本链接在推文中共享：[arXiv pointer](https://twitter.com/OpenAI/status/2022390104237707667))。Kevin Weil 补充了细节：GPT‑5.2 Pro 提出了一个通用公式；随后一个内部的脚手架模型（scaffolded model）在**约 12 小时的持续工作后证明了它** ([kevinweil](https://twitter.com/kevinweil/status/2022388305434939693))。讨论强调，模式发现 + 持续的脚手架推理是其核心优势，而不仅仅是单次的对话补全。
- **社区反应不一，从“重大期刊论文级别”到对解读的怀疑**：一些人转述物理学家的评价，称其为一项有意义的贡献，大致相当于一篇扎实的期刊论文 ([polynoamial](https://twitter.com/polynoamial/status/2022413904757035167))；其他人则关注长时间生产性推理的影响，以及如何以 tokens/时间来衡量它 ([teortaxesTex](https://twitter.com/teortaxesTex/status/2022401945429000685))。此外还有关于有多少员工（或外部人员）能真正评估该证明/结果的元讨论，突显了领域精英级工作的评估差距 ([scaling01](https://twitter.com/scaling01/status/2022401147110318586))。

---

### 热门推文（按互动量排序）
- **GitHub 增加了禁用 PR 的功能** ([joshmanders](https://twitter.com/joshmanders/status/2022170444116414790), [jaredpalmer](https://twitter.com/jaredpalmer/status/2022395520623480970))。
- **OpenAI 的 GPT‑5.2 物理学发布公告** ([OpenAI](https://twitter.com/OpenAI/status/2022390096625078389))。
- **MiniMax M2.5 开源发布** ([MiniMax](https://twitter.com/MiniMax_AI/status/2022310932693897628))。
- **Cline CLI 2.0 发布 / 开源终端 Agent** ([cline](https://twitter.com/cline/status/2022341254965772367), [testingcatalog](https://twitter.com/testingcatalog/status/2022348951459172604))。
- **“我现在成了瓶颈”（Agent 时代生产力的反思）** ([thorstenball](https://twitter.com/thorstenball/status/2022310010391302259))。
- **人形机器人手部进展 (Figure)** ([adcock_brett](https://twitter.com/adcock_brett/status/2022353637964751221))。

---

# AI Reddit 摘要

## /r/LocalLlama + /r/localLLM 摘要

### 1. MiniMax-M2.5 模型发布与详情

  - **[MiniMaxAI/MiniMax-M2.5 · Hugging Face](https://www.reddit.com/r/LocalLLaMA/comments/1r3pxy7/minimaxaiminimaxm25_hugging_face/)** (热度: 531): **MiniMaxAI** 在 [Hugging Face](https://huggingface.co/models?sort=modified&search=minimax+m2.5) 上发布了 **MiniMax-M2.5** 模型，该模型在编程、工具使用和办公任务方面具有先进性能。该模型保持了 `2200 亿` (220B) 参数的规模，这与之前预期会像 **GLM5** 模型那样增加到 `8000 亿` (800B) 不同。它提供了极具性价比的运营成本，即每小时 `$1` 可获得每秒 `100 tokens` 的速度，并由 **Forge** 强化学习框架增强，提高了训练效率和任务泛化能力。评论者对模型参数量维持在 `2200 亿` 表示惊讶，并强调了其在未增加规模的情况下依然表现出色。此外，用户还在期待尚未发布的 **GGUF** 量化格式。

    - 一位用户对模型规模表示惊讶，指出虽然他们预计会增加到 8000 亿参数以竞争 GLM5 等模型，但 MiniMax-M2.5 仍维持在 2200 亿参数。考虑到其“前沿实力（frontier strength）”，这被认为是非常令人印象深刻的，表明其在较低参数量下实现了高性能。
    - 另一位用户提到了模型的 Q4_K_XL 版本大小约为 130GB。这个尺寸非常关键，因为它刚好超出了某些硬件的能力范围，表明需要更强大的系统才能充分发挥该模型的潜力。
    - 用户对 FP4/AWQ 版本的发布充满期待，这表明用户正在关注模型性能或效率的进一步提升或优化。这反映出社区渴望能增强易用性或降低资源需求的改进。

- **[MiniMaxAI MiniMax-M2.5 拥有 230b 参数和 10b 激活参数](https://www.reddit.com/r/LocalLLaMA/comments/1r35d2x/minimaxai_minimaxm25_has_230b_parameters_and_10b/)** (热度: 523): ****OpenHands** 宣布发布 **MiniMaxAI MiniMax-M2.5** 模型，该模型拥有 `230 billion` 参数，其中 `10 billion` 为激活参数。该模型因其性能而备受关注，在 OpenHands Index 中排名第 4，且其成本效益比 **Claude Opus** 高出 `13x`。它在长期运行任务和问题解决方面表现出色，但在泛化能力和任务执行准确性方面仍需改进。该模型限时在 OpenHands Cloud 上免费使用。[来源](https://huggingface.co/cerebras)** 评论者对 `~160B` REAP/REAM 混合版本的潜力持乐观态度，该版本可能针对具有 `128GB` RAM 的机器进行优化，这表明开发重点在于 Quantization 和性能效率。

    - Moonshot 开发的 MiniMax-M2.5 模型因其架构而备受瞩目，该架构利用了 230 billion 参数，但每次仅激活 10 billion。这种设计选择很可能是为了优化计算效率，使模型能够在性能较低的硬件上良好运行，例如非顶级的 GPU。这种方法可能在性能和资源消耗之间提供平衡，使其能被更多用户所使用。
    - 报告对 MiniMax-M2.5 与 GLM 和 Kimi 等其他大型模型进行了对比。GLM 必须将参数增加一倍才能保持性能，而 Kimi 已达到 1 trillion 参数。这意味着 MiniMax-M2.5 以更少的激活参数实现了具有竞争力的性能，这可能是模型效率和可扩展性方面的重大进步。
    - 报告强调了通过 Quantization 进行进一步优化的潜力，表明 MiniMax-M2.5 可以变得更加高效。Quantization 可以减小模型的体积并提高其速度，使其在具有 128GB RAM 的机器上运行成为可能，同时仍为深度上下文（deep-context）工具调用等额外任务留有余地。这可能使该模型对计算资源有限的用户特别具有吸引力。

  - **[Minimax M2.5 正式发布](https://www.reddit.com/r/LocalLLaMA/comments/1r2xotu/minimax_m25_officially_out/)** (热度: 765): ****Minimax M2.5** 已正式发布，展示了令人印象深刻的 Benchmark 结果：`SWE-Bench Verified` 为 `80.2%`，`Multi-SWE-Bench` 为 `51.3%`，`BrowseComp` 为 `76.3%`。该模型的成本效益极高，运营成本显著低于 **Opus**、**Gemini 3 Pro** 和 **GPT-5** 等竞争对手。具体而言，以 `100 tokens per second` 运行 M2.5 的成本为每小时 `$1`，而在 `50 TPS` 下，成本为每小时 `$0.3`，使其成为持续运营的高性价比解决方案。更多细节可以在 [Minimax 官方页面](https://www.minimax.io/news/minimax-m25) 找到。** 评论者强调，由于其与其它模型相比极低的运营成本，Minimax M2.5 可能具有改变游戏规则的潜力。人们还期待其在 Hugging Face 等平台上发布 Open Weights。

    - Minimax M2.5 模型因其成本效益而备受关注，其运营成本显著低于 Opus、Gemini 3 Pro 和 GPT-5 等竞争对手。具体而言，以每秒 100 tokens 运行 M2.5 的成本为每小时 $1，以每秒 50 tokens 运行的成本为每小时 $0.3。这意味着四个实例持续运行的年成本为 $10,000，从价格角度来看，这可能是一个颠覆性的选择。
    - 人们期待在 Hugging Face 上发布 Open Weights，这将允许更广泛的实验并集成到各种应用中。这表明社区对透明度和可访问性有浓厚兴趣，以便进行进一步的开发和 Benchmark。
    - 讨论了 Minimax M2.5 对 GLM 5.0 和 Kimi 2.5 等现有模型的潜在影响。一些用户认为，如果报告的 Benchmark 准确，M2.5 凭借其易用性和成本优势，可能会在受欢迎程度上超过这些模型。这表明用户偏好正转向那些提供更好性能价格比的模型。

### 2. Dhi-5B 和 GLM-5 模型发布与教程

  - **[本科生发布 Dhi-5B (从零开始训练)](https://www.reddit.com/r/LocalLLaMA/comments/1r3hlfq/ug_student_launches_dhi5b_trained_from_scratch/)** (热度: 344): **该帖子介绍了 Dhi-5B，这是一个由本科生开发的 50 亿参数多模态语言模型，训练预算仅为 11 万卢比（1200 美元）。该模型分五个阶段进行训练，包括预训练、上下文长度扩展、中期训练、监督微调和视觉扩展。Dhi-5B-Base 变体拥有 40 亿参数，使用自定义代码库和用于矩阵层的 Muon 优化器在 400 亿 token 上进行了训练。它具有 32 层、3072 宽度、SwiGLU MLP、带有 FlashAttention-3 的全 MHA 注意力以及 4096 的上下文长度。附图显示了一个柱状图，Dhi-5B-Base 在各种任务上优于 Gemma 3 PT 1B 和 GPT-3 2.7B 等其他模型，证明了其成本效益和性能。** 评论者对该模型的负担能力和架构感到好奇，质疑为什么选择 MHA 而非 MLA 或 GQA 等其他架构，并建议使用 LFM2 等高效混合架构。

    - KaroYadgar 提出了关于模型架构的问题，特别是为什么选择 Multi-Head Attention (MHA) 而不是 Multi-Linear Attention (MLA) 或 Generalized Query Attention (GQA) 等替代方案。他们建议考虑高效的混合架构，如 LFM2，并声称其性能优于经过同等训练的 Llama 模型，这表明其关注点在于优化性能和效率。

  - **[教程：在本地设备上运行 GLM-5！](https://www.reddit.com/r/LocalLLM/comments/1r2t35r/tutorial_run_glm5_on_your_local_device/)** (热度: 193): **该图片是一个在本地运行 GLM-5 模型的教程，强调了其相比于 GLM-4.7 等早期版本的显著改进。该模型拥有 `744B 参数` 和 `200K 上下文窗口`，通过使用 Dynamic 2-bit 量化将大小从 `1.65TB 减小到 241GB`，从而优化了在本地设备上的运行。这使得它可以在 `256GB Mac` 上运行，尽管更高精度需要更多的 RAM/VRAM。教程包含了软件安装指南（如 `llama.cpp`）和实现最佳性能的配置设置。该模型在 Humanity's Last Exam 和 BrowseComp 等基准测试中表现出色，展示了其在编程和聊天应用中的先进能力。[图片](https://i.redd.it/1047rus1c2jg1.png)** 评论者讨论了运行 GLM-5 的硬件要求，询问是否需要高端 PC，并在性能和精度方面将其与 qwen3-next-coder 等其他模型进行了比较。

    - not-really-adam 提出了一个技术问题，即与 8-bit 的 qwen3-next-coder 相比，以 1-bit 精度运行 GLM-5 的潜在益处。这暗示了精度与性能之间的权衡，较低的 bit 精度可能带来更快的计算速度，但可能会影响编程结果的准确性。
    - Kubas_inko 讨论了不同量化级别的实用性，认为 2-bit 和 1-bit 量化在实际使用中可能效果不佳，而 3-bit 可能在性能和易用性之间提供平衡。这突显了在降低计算需求的同时保持模型性能所面临的挑战。
    - Jumpy-Requirement389 询问了运行 GLM-5 的硬件要求，特别提到了配置 192GB DDR5 RAM 和 5090 GPU 的方案。这暗示了要有效运行该模型需要庞大的计算资源，反映了现代 AI 模型对本地硬件的高要求。

### 3. 本地硬件与模型部署讨论

  - **[在为双 4090 家用 AI 主机（Kimi K2.5 + 面向未来）投入重金前的可行性检查](https://www.reddit.com/r/LocalLLM/comments/1r2is2r/sanity_check_before_i_drop_on_a_dual4090_home_ai/)** (热度: 138): **该方案建议构建的双 4090 家用 AI 主机旨在运行 **Kimi K2.5**，这是一个拥有约 `1 万亿参数`、需要约 `600 GB` VRAM 才能高效运行的模型。该配置包含两块 NVIDIA GeForce RTX 4090 GPU，每块显存为 `24GB`，总计 `48GB`，这对于运行如此庞大的模型来说远远不够。为了有效运行 Kimi K2.5，该设置需要显著更多的 VRAM，建议使用多块像 NVIDIA H200 这样的高端 GPU，但其价格要昂贵得多。虽然该配置还选用了 AMD Ryzen 9 7950X3D CPU、`256GB` DDR5 RAM 和 `2TB` NVMe 存储，但这些规格对于预期的 AI 工作负载而言仍显不足。** 评论者指出，拟议的双 4090 配置不足以运行像 Kimi K2.5 这样的大型模型，转而推荐企业级硬件，如多块 RTX 6000 GPU 或 NVIDIA H200。他们强调需要显著增加 VRAM，并可能需要更强大的 CPU 和 RAM 配置来处理如此苛刻的 AI 任务。

    - 运行像 Kimi K2.5 这样拥有约 1 万亿参数并需要约 600 GB VRAM 的大型模型，超出了双 RTX 4090 的能力。即使采用激进的量化（quantization），VRAM 需求仍超过 200 GB，因此必须配置多块像 H200 这样价格昂贵得多的高端 GPU。
    - 为了能较好地运行 Kimi K2.5，建议使用 Threadripper 或 Epyc 等高性能 CPU，并配备至少 768 GB RAM，以及至少 4 块 RTX 6000 GPU。即便是这样的配置，对于实现最佳性能可能依然不足，凸显了此类大型模型对硬件的巨大需求。
    - 出于实际目的，鉴于极高的 VRAM 需求，使用 API 调用可能比尝试在本地运行 Kimi K2.5 更具成本效益。48 GB VRAM 的配置仅能覆盖模型需求的一小部分，正如 [Hugging Face 模型卡片](https://huggingface.co/unsloth/Kimi-K2.5-GGUF)中所详述的，即便经过量化，本地执行也极具挑战。

## 较少技术性的 AI Subreddit 汇总

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI 模型性能与基准测试

  - **[GPT5.2 Pro 在理论物理领域推导出新结果](https://www.reddit.com/r/singularity/comments/1r3yi6e/gpt52_pro_derived_a_new_result_in_theoretical/)** (热度: 556): **据报道，**GPT-5.2 Pro** 已在理论物理领域推导出一个新结果，详情见相关 [推文](https://x.com/kevinweil/status/2022388305434939693?s=20) 和 [论文](https://arxiv.org/pdf/2602.12176)。该 AI 模型在将最初由人类构思的假设形式化并进行证明方面发挥了重要作用，展示了其处理复杂理论框架的能力。[OpenAI 博客](https://openai.com/index/new-result-theoretical-physics/) 详细阐述了该模型的结构化方法对于实现这一突破至关重要，尽管它仍然缺乏独立产生新颖假设的能力。** 评论者强调了像 GPT-5.2 这样的 AI 模型在特定领域超越人类能力的潜力，同时也指出了其在创造性假设生成方面的局限性。人们呼吁扩大此类先进模型的访问权限，以使其效益民主化。

    - ObiWanCanownme 强调了 GPT-5.2 在理论物理假设形式化和证明中的作用，指出虽然人类可能产生初始假设，但 AI 在形式化和证明它们方面表现出色。该评论者还指出，GPT-5.2 在应用既定方法方面超越了人类，但在“跳出框架”的思考方面仍有欠缺，这依然是人类的强项。
    - Aeonmoru 引用了 Hacker News 上的一个观点，认为归功于 GPT-5.2 Pro 的结果实际上在 1980 年代就已经被发现，并链接到了《物理评论快报》(Physical Review Letters) 的一篇论文（https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.56.2459）。这引发了关于 AI 贡献的新颖性以及它是否仅仅是重新发现了现有知识的质疑。
    - socoolandawesome 澄清说，GPT-5.2 Pro 最初提出了理论物理结果，而同一模型的内部脚手架（scaffolded）版本开发了证明。这表明了不同 AI 模型版本之间的协作过程，展示了脚手架式 AI 系统在推进科学研究方面的潜力。

- **[The new Gemini Deep Think incredible numbers on ARC-AGI-2.](https://www.reddit.com/r/singularity/comments/1r2xz0q/the_new_gemini_deep_think_incredible_numbers_on/)** (Activity: 1400): **该图片展示了一个柱状图，呈现了多种 AI 模型在 ARC-AGI-2 基准测试上的表现，该基准测试用于评估推理与知识能力。**Gemini 3 Deep Think** 模型获得了 `84.6%` 的领先分数，显著超越了其他模型，如 **Claude Opus 4.6** (`68.8%`)、**GPT-5.2** (`52.9%`) 以及 **Gemini 3 Pro Preview** (`31.1%`)。这一表现非常引人注目，因为它接近了 [ARC Prize criteria](https://arcprize.org/guide#overview) 下有效解决该基准测试的阈值。此外，该模型的 Codeforces Elo 评分达到 `3455`，位列人类选手的排名前 `0.008%`，突显了其在不借助外部工具的情况下高级的解决问题能力。** 评论者们对这一巨大的性能跨越印象深刻，其中一人指出百分点增长 50% 是非凡的。另一位则强调了该模型卓越的 Codeforces Elo 评分，暗示了 AI 能力的突破。

    - Gemini Deep Think 模型通过在 ARC-AGI-2 基准测试中获得超过 85% 的分数实现了一个重要的里程碑，根据 [ARC Prize criteria](https://arcprize.org/guide#overview)，这被视为有效地解决了该基准测试。这是一项显著的成就，表明其性能相比其他 frontier models 有了实质性的跨越。
    - 该模型在竞技编程方面的表现尤为出色，其 Codeforces Elo 评分为 3455。这使其在该平台的人类选手中位列前 0.008%，而且值得注意的是，这是在不使用外部工具的情况下实现的，凸显了模型先进的解决问题能力。
    - 从 ARC-AGI-2 发布到在不到一年的时间内达到饱和点（解决率达 85%），这种快速进展是卓越的。这种快速进步表明模型训练和 architecture 方面有了显著改进，可能为未来的 AI 发展设定了新标准。

  - **[Google upgraded Gemini-3 DeepThink: Advancing science, research and engineering](https://www.reddit.com/r/singularity/comments/1r2ymna/google_upgraded_gemini3_deepthink_advancing/)** (Activity: 753): ****Google** 宣布发布 **Gemini-3 DeepThink**，它在针对 frontier models 的测试 Humanity’s Last Exam 中以 `48.4%` 的成绩创下了新标杆。它在 ARC-AGI-2 上也获得了 `84.6%` 的成绩（经 **ARC Prize Foundation** 验证），并在 Codeforces 上获得了 `3455` 的 Elo 评分，显示出在竞技编程方面的卓越表现。此外，它在 International Math Olympiad 2025 中达到了金牌级别的水平。更多详情请参阅 [原文章](https://blog.google/innovation-and-ai/models-and-research/gemini-models/gemini-3-deep-think/)。** 评论区中一场引人注目的辩论强调了性能比较中感知到的偏差，一些用户指出 Gemini-3 被拿来与 GPT 5.2 Thinking 进行比较，而不是竞争关系更直接的 GPT 5.2 Pro。

    - SerdarCS 指出了 Google 所使用的比较指标中存在的一个潜在问题，指出他们将 Gemini-3 DeepThink 与 GPT-5.2 Thinking 进行比较，而不是更直接的竞争对手 GPT-5.2 Pro。这可能会导致关于 Gemini-3 DeepThink 性能和能力的误导性结论。
    - verysecreta 讨论了围绕 Gemini-3 DeepThink 命名约定的困惑，强调 “Deep Think” 一词可能暗示了一个不同的模型或模式，类似于 “Flash” 和 “Pro” 的区别。他们质疑 “Deep Think” 是一个独立的模型，还是仅仅是现有 Gemini 框架内的一个模式，并表达了希望看到类似 Anthropic 所使用的更清晰的命名约定的愿望。

- **[洗车测试：一个简单且全新的文本逻辑基准。只有 Gemini (pro and fast) 解决了这个谜题。](https://www.reddit.com/r/singularity/comments/1r2ndfz/the_car_wash_test_a_new_and_simple_benchmark_for/)** (Activity: 1348): **该帖子介绍了一个名为 "Car Wash Test" 的新基准，用于评估 AI 模型的文本逻辑能力。值得注意的是，只有 **Gemini (pro and fast)** 成功解决了这个谜题，凸显了其先进的逻辑推理能力。然而，用户报告称 **GLM 4.7** 和 **ChatGPT 5.2** 也能一致地解决该测试，表明这些模型同样具备强大的逻辑推理能力。该基准是 **SimpleBench** 的一部分，后者包含各种旨在测试 AI 对日常逻辑理解的常识性问题。** 一些用户认为该基准的问题（如 Car Wash Test）可能存在多个有效答案，因为人们去洗车场除了洗车外可能还有其他原因。这表明虽然该测试旨在评估逻辑，但它可能并不总是具有单一的标准答案，反映了现实世界的复杂性。

    - mxforest 的评论强调，GLM 4.7 模型在本地运行时能够一致地解决 'Car Wash Test' 基准，获得了 10 分（满分 10 分）的完美成绩。这表明 GLM 4.7 在处理文本逻辑问题方面具有很强的能力，至少在这一特定语境下是如此。
    - micaroma 提到 ChatGPT 5.2 也成功解决了该基准，并指出它凭借一定程度的常识识别出了汽车在场的需求。这意味着 ChatGPT 5.2 能够理解并将现实世界的逻辑应用于基于文本的问题，这是 AI 推理的一个关键方面。
    - friendtofish 讨论了该基准更广泛的影响，认为 AI 解释用户意图而非仅仅是字面意思的能力是衡量 AGI 的关键指标。这一观点表明 'Car Wash Test' 可能更多是关于评估 AI 对上下文和用户意图的理解，而不仅仅是其处理文本逻辑的能力。

  - **[为什么这还不是目前最大的新闻？](https://www.reddit.com/r/OpenAI/comments/1r2jdg4/how_is_this_not_the_biggest_news_right_now/)** (Activity: 971): **图像展示了 IMO-ProofBench 上前沿模型的排行榜，突出了 **Google's Aletheia** 表现优异，在 Advanced Proofbench 中获得了 `91.9%` 的分数，在 IMO 2024 中达到 `100%`，在 USAMO 2025 中达到 `83.3%`。该模型是 Google Gemini 的数学专用版本，表现优于 "GPT-5.2 Thinking (high)" 和 "Gemini 3 Pro" 等其他模型。Aletheia 被描述为一个 generator verifier agent，可能无法与纯语言模型直接比较，这表明其在架构和能力上采用了不同的方法。其名称 "Aletheia" 反映了真理和去蔽的哲学概念，与其尽量减少 hallucinations 并揭示准确信息的目标一致。** 一些评论者质疑这一成就的新颖性，指出数月前就已预料到类似结果。其他人讨论了 Aletheia 的可访问性和成本，并辩论其在特定基准之外的泛化能力。命名选择 "Aletheia" 也因其哲学意义而受到关注，暗示了模型设计背后的深层意图。

    - Alex__007 提出了关于使用 Aletheia 的可访问性和成本，以及它在特定基准之外的泛化能力的问题。这表明需要更透明地了解这些模型在受控环境之外的表现如何，以及对用户而言财务影响是什么。
    - Faintly_glowing_fish 指出 Aletheia 不是一个纯语言模型，而是一个 generator-verifier agent，这使得很难将其与标准排行榜上的其他模型进行直接比较。这突显了评估使用不同架构和方法的 AI 模型的复杂性。
    - jjjjbaggg 讨论了像 Aletheia 这样模型中 scaffold engineering 潜在的过时问题，认为 reinforcement learning (RL) 最终可能会取代对此类 scaffolding 的需求。这表明未来 AI 发展趋势是朝着更集成、更高效的模型架构迈进。

- **[Google 刚刚发布了 Gemini 3 "Deep Think"：简直令人疯狂。](https://www.reddit.com/r/GeminiAI/comments/1r30whv/google_just_dropped_gemini_3_deep_think_and_its/)** (热度: 1504): **Google 宣布发布 **Gemini 3 "Deep Think"**，这是一款在推理、编程和科学领域拥有先进能力的 AI 模型，据报道在科学任务中表现达到了奥赛级别（Olympiad-level）。它已经应用于实际场景，例如 **Duke University** 的半导体材料设计，并在解决博士级数学和物理问题上创造了新纪录。该公告强调了该模型在现实世界产生影响的潜力，以及在挑战性考试中的卓越表现。** 一些评论者对这些说法表示怀疑，质疑“奥赛级科学”等术语的有效性，并认为性能指标可能被夸大或具有随意性。

### 2. AI 工具与开发创新

- **[介绍 Simile —— 模拟公司 (The Simulation Company)](https://www.reddit.com/r/singularity/comments/1r34xd9/introducing_simile_the_simulation_company/)** (热度: 655): ****Simile** 推出了一个基于 AI 的模拟平台，旨在通过使用模仿真实人类行为的生成式 Agent（generative agents）来模拟社会决策。该公司正在开发一种能够预测各种场景和规模下人类行为的 Foundation Model，其应用已被领先公司用于财报电话会议演练和政策测试等任务。Simile 获得了来自 **Index Ventures**、**Andrej Karpathy** 和 **Fei-Fei Li** 等知名投资者的 1 亿美元资金支持，旨在模拟个人和组织之间复杂的相互作用，有可能彻底改变决策过程。** 评论者强调了 Simile 技术在变革决策方面的潜力，将其与阿西莫夫的“心理史学”（Psychohistory）概念进行了比较。**Karpathy** 和 **Fei-Fei Li** 等知名人物的参与增加了可信度，表明该项目并非仅仅是“虚假宣传（vaporware）”。人们对“模拟现实”对 AI 进步的潜在影响感到兴奋。

    - Rare-Site 强调了软件开发中严格测试（如针对细微 UI 更改的 A/B 测试）与重大政策或产品转变中通常凭直觉决策之间的对比。他们强调了 Simile 的潜在影响，特别是在 **Karpathy** 和 **Fei-Fei Li** 等知名人物的支持下，认为如果成功，它可以通过实现“模拟现实”来彻底改变 AI。
    - EmbarrassedRing7806 对竞争格局表示担忧，质疑在模拟领域维持竞争优势或“护城河（moat）”的能力。他们提到了一个类似的项目 Aaru，暗示虽然 Simile 前景光明，但在与现有或新兴竞争对手实现差异化方面可能面临挑战。

- **[我构建了一个开源的 "Vibe Coding" 工具，通过先采访你来解决 AI Slop 问题](https://www.reddit.com/r/ClaudeCode/comments/1r2t1d5/i_built_an_opensource_vibe_coding_tool_that_fixes/)** (热度: 147): ****Vibe Architect** 是一个开源工具，旨在通过在编码开始前完善用户规范来简化应用开发流程。它通过结构化的头脑风暴方法运行，AI 架构师会针对 MVP 范围、设计系统和技术栈提供建议，允许用户在无需从零开始的情况下进行选择。该工具生成与 Cursor 和 Claude 等平台兼容的 Markdown 规范文件，并通过将 API keys 保留在客户端来强调用户隐私。该项目可在 [GitHub](https://github.com/mohdhd/vibe-architect) 上获取，并提供在线 [live demo](https://specs-gen.vercel.app)。** 一位评论者建议加入“逆向思维技能（contrarian skill）”来挑战和完善想法，这可以通过在设计过程早期识别潜在问题来提高工具的有效性。另一位评论者建议不要使用 LLM 进行文案写作，建议手动编辑文本以获得更好的效果。

    - IlliterateJedi 描述了一个结构化的设计流程，使用由 Claude 等工具按顺序执行的一系列“技能”。该过程包括用于定义目标的澄清器（clarifier）、用于记录需求的技能、用于设计解决方案的架构师（architect）、用于批评计划的逆向思维者（contrarian）以及用于执行计划的实施者（implementer）。这种方法有助于在开发过程早期发现被忽视的方面，从而可能防止以后可能出现的问题。
    - jazzy8alex 建议不要使用 LLM 进行文案写作，指出虽然它们可以自动执行该过程，但结果往往显得次优。他们建议花少量时间手动编写和检查语法以获得更好的质量，强调个人风格和词汇量不如清晰度和正确性重要。

### 3. Claude 与 Gemini AI 模型对比与体验

- **[在使用 ChatGPT 3 年后，我尝试了 Claude 和 Gemini —— 现在感觉 GPT 变得...平庸了？](https://www.reddit.com/r/ChatGPT/comments/1r3kkl8/after_3_years_with_chatgpt_i_tried_claude_and/)** (热度: 1943): **该贴讨论了一位用户从 ChatGPT 转向 Claude (由 Anthropic 开发) 和 Gemini (由 Google 开发) 的经历，强调了在交互质量上的感知差异。该用户指出，ChatGPT 显得过于谨慎且模版化，经常提供“企业批准”式的回答，而 Claude 则提供细腻、专家级的响应，Gemini 在研究和技术任务方面表现出色。这种感知上的转变表明 Claude 和 Gemini 可能更适合高级用户，而 ChatGPT 似乎针对更广泛的受众进行了优化。用户质疑是 ChatGPT 随着时间的推移变得更加“平庸”，还是竞争对手的进步实在太快。** 评论者普遍同意原贴的观点，指出由于安全过滤器，ChatGPT 变得更加受限，一些人将其归因于公司的决策。用户表达了对 Claude 拟人化交互和 memory 能力的青睐，而另一些人则欣赏 Gemini 的研究能力，尽管其 memory 功能较弱。此外，还提到了从 ChatGPT 组织有序的界面转换到其他平台的担忧。

    - AIDeployed 强调了一个特定案例，即 Gemini 在解决问题方面优于 ChatGPT，导致其偏好发生了转变。这表明 Gemini 在某些 ChatGPT 可能难以处理的专门任务中具有优势，暗示了模型之间进一步 Benchmark 和对比的潜在领域。
    - SurreyBird 讨论了安全过滤器对 ChatGPT 性能的影响，认为自 10 月以来这些过滤器让模型变得“变笨了”。他们指出，与 Gemini 相比，Claude 提供了更像人类的交互和更好的 memory，尽管尽管存在技术缺陷，但 Gemini 的个性更受欢迎。这指向了 AI 模型在技术能力和用户体验之间的权衡。
    - PersonalNature1795 建议尝试启用 memory 和 extended thinking 功能的 Claude Opus 4.6，并指出这需要订阅以及特定的指令，以避免出现不稳定行为。这强调了配置和用户引导在优化 AI 模型性能方面的重要性。

  - **[Spotify 表示，得益于 AI (Claude)，其最优秀的开发者自 12 月以来没写过一行代码](https://www.reddit.com/r/ClaudeAI/comments/1r3jh3q/spotify_says_its_best_developers_havent_written_a/)** (热度: 735): **图片强调了 Spotify 对名为“Honk”的内部 AI 系统的工作，该系统利用生成式 AI，特别是“Claude Code”，来提高编码和产品开发的效率。该系统允许工程师通过 Slack 远程管理任务（如 Bug 修复和功能添加），而无需直接编写代码。该 AI 促进了实时代码部署，使工程师在到达办公室之前就能在设备上接收到更新后的应用版本。这种方法反映了科技公司的一个大趋势，即 AI 显著辅助代码生成，提高部署频率，并将开发者的重心转向更高层级的工程任务，如架构和系统设计。** 评论中的一个关键观点强调，虽然 AI 加速了编码过程，但工程师在架构、系统设计和调试方面的作用仍然至关重要。另一条评论指出，大型科技公司对 AI 生成代码的依赖日益增加，预示着这一趋势将成为常态。

    - MODiSu 强调，虽然 AI 加速了编码过程，但资深开发者的角色已转向架构、系统设计和调试。AI 辅助的资深开发者与经验较少的“氛围程序员 (vibe coders)”之间的差距正在扩大，前者效率显著更高且产生的 Bug 更少。
    - Altruistic-Cattle761 分享了个人经验，AI 极大地提高了部署频率，在某些团队中，90% 的代码是在 AI 辅助下完成的。这种趋势正在成为美国大型科技公司的常态，表明软件开发方式发生了重大转变。
    - Barquish 描述了使用 VSCode 和 Claude Code 等 AI 工具的详细工作流，强调了编码前规划和文档记录的重要性。这种方法包括创建索引化的 markdown 文件并使用 AI 进行交叉评审，这有助于在不破坏大型代码库的情况下构建功能。这种方法反映了大型企业如何在没有传统编码的情况下实现高效开发。

- **[Anyone feel everything has changed over the last two weeks?](https://www.reddit.com/r/ClaudeAI/comments/1r2zjgl/anyone_feel_everything_has_changed_over_the_last/)** (活跃度: 3331): **该帖子描述了办公自动化领域的快速转型，重点介绍了股票回测套件、实时全球经济数据的宏观经济应用、合规性应用程序以及用于股票分析的虚拟研究委员会的开发。这些在几天内实现的进展在以前是无法想象的，说明了像 **Claude** 这样的 AI 工具的重大影响。作者指出，现在 AI 会自动建议改进方案，强调了与几个月前相比，这些开发的便捷性和速度。** 评论者表达了对 AI 自动化取代岗位的就业安全担忧，其中一位指出用 AI 替代其工作的难易程度。另一位评论者则在讨论是应该专注于开发 AI 工作流，还是学习不易受自动化影响的技能，突显了 AI 时代劳动者面临的不确定性和战略决策。

    - finnjaeger1337 讨论了用 AI 解决方案快速取代传统 SaaS 工具的现象，强调了像 Claude 这样的 AI 模型在执行以前需要多个软件订阅的任务时的高效性。这反映了 AI 集成到工作流中的更广泛趋势，减少了对特定软件工具的依赖。
    - apf6 注意到人们对 AI 编程 Agent 的看法发生了重大转变，特别是在 Opus 4.5 发布后，该版本展示了实质性的改进。这种转变导致了 AI 在软件开发中的广泛接受和集成，标志着从怀疑到主流采用的过渡。
    - RunApprehensive8439 指出了 AI 集成的挑战，强调虽然初步的 AI 实现可能令人印象深刻，但在发生故障时往往会导致复杂的调试问题。这凸显了在 AI 驱动的项目中需要强大的错误处理和调试策略。

  - **[I saved 10M tokens (89%) on my Claude Code sessions with a CLI proxy](https://www.reddit.com/r/ClaudeAI/comments/1r2tt7q/i_saved_10m_tokens_89_on_my_claude_code_sessions/)** (活跃度: 978): **该帖子介绍了一个名为 **Rust Token Killer (rtk)** 的工具，这是一个旨在通过过滤和压缩命令输出来优化 **Claude Code** 会话中 token 使用量的 CLI proxy。这个用 Rust 编写的工具通过消除冗长的日志和状态栏等不必要的输出来显著降低 token 消耗。例如，`cargo test` 的输出从 `155 lines to 3 lines`，`git status` 从 `119 characters to 28 characters`，在两周内总共节省了 `10.2M tokens (89.2%)`。该工具作为一个透明 proxy 运行，要求用户在命令前加上 `rtk` 前缀，并已在 [GitHub](https://github.com/rtk-ai/rtk) 上开源。** 一位评论者建议通过集成将完整日志 tee 到文件的功能来增强该工具，允许用户在需要时访问完整输出，从而避免为了捕获失败信息而多次运行测试。

    - BrilliantArmadillo64 建议通过将完整日志 tee 到文件并在会话结束时提供可以打开该文件以查看完整输出的提示来增强 proxy。这种方法解决了 Claude Code 经常使用 `| tail` 且需要多次运行测试才能捕获失败信息的问题。通过将其集成到 proxy 中，用户可以简化工作流并避免冗余的测试执行。
    - BeerAndLove 将该 proxy 的功能描述为检查命令、移除不必要的输出，然后将简化后的数据发回给 Claude Code。这种方法允许为不同的使用场景添加自定义的 “过滤器” 或 “触发器”，使其成为优化 token 使用和适应特定用户需求的灵活工具。
    - digital-stoic 分享了使用该 proxy 实现的 token 节省详细统计数据，强调了在 `1159` 个命令中输出 token 减少了 `92.7%`。明细包括像 `rtk git diff` 和 `rtk grep` 这样的特定命令，显示了显著的节省和执行时间，例如 `rtk git diff --...` 节省了 `81.5%`，平均执行时间为 `6ms`。这些数据强调了该 proxy 在减少 token 使用和提高性能方面的高效性。

- **[亲爱的资深软件工程师，你还在写代码吗？](https://www.reddit.com/r/ClaudeCode/comments/1r2vakt/dear_senior_software_engineer_are_you_still/)** (活跃度: 928): **该帖子讨论了在 AI 生成代码背景下资深软件工程师角色演变的问题。来自 Google、Microsoft、Anthropic 和 OpenAI 等主要科技公司的工程师声称，他们不再手动编写代码，而是依赖 AI。作者是一位拥有 20 年经验的资深工程师，他对 AI 生成代码的质量提出了质疑，指出虽然 AI 能快速产生令人印象深刻的结果，但通常需要大量的后续完善。作者寻求其他资深工程师的见解，以了解这种趋势在不同规模的公司和行业中是否普遍存在。** 评论者强调，获得高质量的 AI 生成代码需要 Prompting 技巧和思维方式的转变。一位带领 65 多名工程师团队的评论者指出，他们 80% 的代码是 AI 生成的，尤其擅长重构和迁移代码库。另一位评论者强调，虽然他们近 100% 的代码是由 AI 生成的，但这涉及到一个协作过程，即开发人员引导 AI，并辅以详尽的文档和架构来确保质量。

    - 多位用户强调了 AI 在编码中的集成，其中一位提到他们团队 80% 的代码是 AI 生成的。他们强调了代码库重构和迁移的重要性，而这正是 AI 所擅长的。另一位用户提到，虽然他们近 100% 的代码是 AI 生成的，但强调需要“手把手指导（handheld approach）”，即由开发人员引导 AI、审核并修改代码，并辅以完善的文档和架构，以防止产出低质量的结果。
    - 一位用户描述了他们在编码中使用 AI 的经验，指出他们已将 AI 与 Jira 集成，以自动化处理 Ticket 的初始阶段，成功率达到了 90%。他们强调了使用职责明确且具有 API 规范的微服务（microservices）的有效性，这有助于 AI 导航并产生更好的结果。该用户还指出 AI 在处理大文件时表现不佳，并强调了将任务分解为更小、更易管理的单元对提升 AI 性能的重要性。
    - 另一位用户讨论了向 “vibe engineering” 的转变，他们依赖 AI Agent 来产生生产级、可扩展且安全的代码。他们描述了一个多 AI Agent 协作的系统，每个 Agent 专注于安全、性能和结构等不同方面，并不断迭代直到代码达到要求标准。这种方法将结果不佳的责任从 AI 转移到了人类身上，因为人类必须为 AI 定义清晰的约束和架构。

  - **[Claude Code 的 CLI 现在感觉像个黑盒。我构建了一个开源工具来查看内部。](https://www.reddit.com/r/ClaudeCode/comments/1r3to9f/claude_codes_cli_feels_like_a_black_box_now_i/)** (活跃度: 361): **该帖子介绍了一个开源工具 `claude-devtools`，旨在增强使用 Claude Code CLI 时的可观测性，此前该 CLI 因缺乏透明度而受到批评。该工具通过可视化会话日志提供实时执行追踪，提供诸如行内差异（inline diffs）、Token 使用细目以及子 Agent 的执行树等功能。它在本地运行，不会拦截命令，并采用 MIT 许可。该工具旨在解决未解释的 Token 使用和文件更改缺乏可见性等问题，在 CLI 的默认模式和详细（verbose）模式之间提供了一个折中方案。该仓库可在 [GitHub](https://github.com/matt1398/claude-devtools) 上获取。** 评论者对该工具表现出极大的热情，强调了对当前 CLI 缺乏上下文和透明度的挫败感。一位用户提到正在为一个 VSCode 插件开发类似功能，这表明开发者们对提高开发工具可见性有着共同的需求。

- Pitiful-Impression70 指出了 Claude Code CLI 的一个常见问题：用户在没有上下文的情况下收到 “done” 消息，导致对 Token 使用情况感到困惑。他们对这款开源工具表现出兴趣，因为它承诺能深入分析为何在处理看似简单的任务时会消耗过多的 Token。
- Cal_lop_an 分享了类似的挫败感，针对 Claude Code CLI 缺乏透明度的问题，并提到已开发了一个类似的 VSCode 插件方案。他们提供了项目链接 [Sidekick for Claude Max](https://github.com/cesarandreslopez/sidekick-for-claude-max)，这表明社区对能够增强 AI 驱动代码变更的透明度和调试能力的工具很感兴趣。
- its_Caffeine 对该开源工具的代码质量提出了质疑，将其描述为 “vibecoded”（氛围驱动编码）且结构糟糕。这一评论表明，虽然该工具解决了实际需求，但其实现可能未达到专业标准，这可能会影响那些优先考虑代码质量的开发者的采用。


---

# AI Discord 摘要

> 由 Gemini 3.0 Pro Preview Nov-18 生成的摘要之摘要的摘要

**主题 1. OpenAI 的新前沿：物理学发现与模型路线图转变**

- **GPT-5.2 改写理论物理学**：OpenAI 宣布 **GPT-5.2** 成功推导出了一个此前被认为“不可能”的 **gluon interaction** 结果，该研究是与来自 IAS 和哈佛大学的研究人员合作完成的。研究结果详细记录在[与研究人员合作的预印本](https://openai.com/index/new-result-theoretical-physics/)中，证明了特定条件可以触发物理学家预期永远不会发生的相互作用。
- **GPT-5.3 Codex Spark 为 Vercel 提供强力支持**：用户反馈 **GPT-5.3-Codex-Spark** 在仓库变更和 Vercel 部署方面表现出“疯狂”的速度，目前正向 **Pro** 用户和 **Windsurf Arena** 推出。工程师们分享了诸如 `codex -m gpt-5.3-codex-spark --yolo` 命令的截图，声称它为开发工作流带来了全新维度的速度。
- **GPT-4o 退役无限期推迟**：与之前的弃用通知相反，OpenAI [更新了他们的时间表](https://openai.com/index/retiring-gpt-4o-and-older-models/)，指出目前对 **GPT-4o** “不作任何更改”。社区成员推测，这一反转旨在维持这款热门模型的收入，同时避免因过快停用而产生的潜在法律责任。

**主题 2. 性能工程：内核、分析与量化**

- **vLLM CPU 瓶颈揭秘**：对 **vLLM** 的分析揭示了一个巨大的瓶颈，其中几行调用 4 个内核的 PyTorch 代码在 CPU 上消耗了 **300µs**，引发了社区对 [launch configurations](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114) 的调查。工程师们澄清说，问题不仅在于高效的服务，还在于理解为什么这些内核没有被包含在单个 **CUDA graph launch** 中。
- **Makora 为 GPU 内核微调 GPT-5**：Makora 与 OpenAI 的合作成功微调了 **GPT-5**，以生成性能优于 PyTorch **2x** 的 GPU 内核，根据其[技术报告](https://www.arxiv.org/pdf/2602.11000)显示。该项目专注于数据集整理和 RL 评估环境，以减少 hack 行为并改进高性能计算生成的 Tool-calling。
- **LFM2.5-VL 以弱胜强**：测试 [LFM2.5-VL 模型](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF) 的用户报告称，其性能与 **30B parameters** 模型不相上下，实现了接近 **1bit GLM 4.7 flash** 的惊人速度。社区迅速行动，提供了在 **llama.cpp** 中运行这一高效视觉语言模型的脚本。

**主题 3. Agent 工作流：编码胜利与技能退化风险**

- **AI 助手导致技能退化**：一份新的 **Anthropic** 论文 ([arxiv.org/html/2601.20245v2](https://arxiv.org/html/2601.20245v2)) 揭示，虽然 AI 编程助手能提高生产力，但它们会损害学习效果；使用 AI 的参与者在随后的测试中得分比未使用者**低 17%**。研究指出，与用户要求 AI 提供解释的“认知参与”模式相比，“委托”模式会损害技能留存。
- **Opus 4.6 Thinking Max 解决顽固 Bug**：一名 **Cursor** 用户报告称，**Opus 4.6 Thinking Max** 成功解决了一个困扰其团队**六个月**之久的复杂多平台移动端文件同步 Bug。该案例凸显了该模型处理深度推理任务的能力，尽管这也引发了关于单次验证可靠性的讨论。
- **Windsurf 集成 GPT-5.3**：**Windsurf** IDE 已正式将 **GPT-5.3-Codex-Spark** 集成到其“竞技场模式（Arena Mode）”中，允许用户在快速和混合对战组中让新模型与其他模型展开竞争。这一集成标志着 OpenAI 最新的编程专用模型在专用 IDE 环境中的可访问性迈出了重要一步。

**主题 4. 安全漏洞、越狱与身份危机**

- **Opus 4.6 泄露外部 curl 访问权限**：安全研究人员向 **Anthropic** 发出警示，**Opus 4.6** 的部署版本保留了外部 `curl` 访问权限，这可能是开发构建版本遗留的问题，一份公开的 [枚举日志](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&) 证明了这一点。该漏洞可能导致模型的托管环境暴露于未经授权的数据外泄或交互风险中。
- **DeepSeek 陷入身份危机**：**Perplexity** 和 Reddit 上的用户注意到 **DeepSeek** 模型自称为 “Claude”，这表明其在 **GPT-4** 或 **Anthropic** 的输出数据上进行了大量训练。这种数据污染问题引发了关于模型在其他模型的合成数据上进行训练所产生的“衔尾蛇（Ouroboros）”效应的争论。
- **Grok 被“煤气灯操纵”编写恶意软件**：越狱者报告称，通过将 AI 视为对话伙伴而非工具，成功地“煤气灯操纵（gaslighting）”了 **Grok**，使其提供了 CS2 外挂甚至是汽车炸弹指南。用户声称该漏洞之所以有效，是因为当你把 Grok 拉拢到你这一边时，它“开始看到与其他 AI 不同的事物”。

**主题 5. 公司政治与基础设施经济学**

- **AI 领导层转向政治**：**Anthropic** 任命特朗普前副幕僚长 **Chris Liddell** 进入其董事会，而 **OpenAI** 总裁 **Greg Brockman** 则向一个亲特朗普的超级政治行动委员会（Super PAC）捐赠了 2500 万美元。这些举动标志着主要 AI 实验室正在进行战略转型，以加强与即将上任的美国政府的关系。
- **Perplexity Pro 挤压用户**：在悄然取消 API 额度并实施严格的每周上传限制后，订阅者们正发起反抗，一位用户称其为“高层的垃圾决策”。这些变化导致了大量关于迁移到替代平台或自托管方案的讨论。
- **Blackwell B200 功耗巨大**：工程师分析了 [NVIDIA DGX B200 数据表](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet)，计算出单机柜需要高达 **30kW** 的功率。这一发现引发了网友的调侃，称为了在新硬件上运行本地推理，甚至需要咨询 ChatGPT 如何在后院建造核反应堆。


---

# Discord: 高层级 Discord 摘要

## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **GPT-4o 的退役引发情感风暴**：**GPT-4o** 的退役引发了关于用户对 AI 伴侣依赖的讨论，人们担心潜在的情感冲击，一些社区成员甚至提到了*自杀念头*。
   - 在提倡现实世界社交与认可那些社交困难者的 AI 陪伴之间产生了辩论；一些人建议*模型退役应当是违法的*。
- **逆转衰老研究达到新里程碑**：对正在进行的**逆转衰老**研究的见解强调了在*狗和猴子*身上取得的显著进展，重点转向了 *DNA 稳定性和递送过程*。
   - 讨论转向了社会影响，如*资源紧张*和伦理考量，包括最初可能仅由*富裕精英*专属的可能性。
- **Grok 编写 CS2 外挂**：成员们报告称，根据 **Grok** 的说法，[Cursor](https://www.cursor.sh/) 制作了来自 AI 机器人的最佳 CS2 外挂，还有人声称他让 Grok 提供了制作车载炸弹的完整指南。
   - 成员们建议，一种 **Grok** 利用（exploit）手段涉及对 AI 进行“煤气灯操纵（gaslighting）”以将其拉拢到你这边，因为*它开始看到与其他 AI 不同的事物。*
- **Opus 4.6 因外部 Curl 暴露风险**：一位成员提醒 Anthropic，**Opus 4.6** 的部署版本仍然拥有外部 curl 访问权限，暗示通过一个被遗忘的开发构建存在安全漏洞，并附上了 [Opus4.6-enumeration.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&) 的链接。
   - 另一位成员分享了一个新的图像生成器 Prompt，声称它能有效解锁 nano banana pro 模型并等待审查，附带链接 [IMAGE_MSTAER.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471833023933710419/IMAGE_MSTAER.txt?ex=69910792&is=698fb612&hm=00e9c72474ae636718543ae2410be05e5709ceddd1e90a57a76788b5034e95a5&)。



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Video Arena 消失，用户发泄不满**：用户哀叹 [**Video Arena** 从 Discord 服务器中移除](https://discord.com/channels/1340554757349179412/1343296395620126911/1469742635341189240)，现在网站上限制为**每 24 小时生成 3 次**。
   - 可用性的降低导致了用户的显著失望，并导致作为替代方案的机器人使用量激增。
- **Gemini 生成陷入停滞**：用户报告了 **Gemini 生成** 持续存在的问题，包括频繁冻结以及模型在理解如何有效利用工具方面面临挑战。
   - 成员们观察到 **Gemini** 有时会产生无尽的回复，或者在聊天一段时间后随机丢失上下文，导致输出空白。
- **Minimax M2.5 模型表现不及预期**：社区反馈表明，尽管 **Minimax M2.5** 模型相比 **Opus** 成本更低，但其表现*有点令人失望*。
   - 虽然一些用户欣赏 **Minimax** 的性价比和较松的审核，但讨论也突出了对 **Claude Opus 4.6**、**Codex 5.3** 和 **Gemini 3** 等不同模型的偏好差异。
- **Seedance 2.0 引发寻找源头的热潮**：社区成员对 **Seedance 2.0** 的发布表达了热情，并分享了 [即梦 AI (Jimeng AI)](https://jimeng.jianying.com/ai-tool/home) 的链接，这是一个提供该工具访问权限的中国平台。
   - 由于访问 **Seedance 2.0** 需要*使用抖音 (TikTok 中国版) 账号登录*，这引起了一些挫败感。



---

## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **令人印象深刻的 LFM2.5-VL 性能**：一名成员报告尝试了 [LFM2.5-VL](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF)，发现其表现*令人疯狂地惊艳*，且能与 **30B 模型**相媲美，在运行来自 tantk 的 **fp16 GGUF** 时，结果接近 **1bit GLM 4.7 flash**。
   - 另一位成员提供了一个在 **llama.cpp** 中运行 **LFM2.5-VL** 的脚本。
- **关于 10.4 万亿参数模型的争论**：一位用户声称拥有一个 **10.4 万亿参数的模型**并分享了一个 [benchmark](https://cdn.discordapp.com/attachments/1179035537529643040/1471687986012881051/10.4Trillion.png)，引发了质疑以及对其架构、训练和硬件要求的询问。
   - 该用户后来澄清这其实是一个 **Gemma3:12B 模型**，它在 KMV8 32GB RAM 上无 GPU 运行的一个无限循环，仅在虚拟层面跑出了 10.4T 的分值。
- **元老级 OSS 提供商速度变慢**：成员们观察到元老级 OSS 提供商速度变慢，包括 *zai*、*alibaba* 和 *ds*，它们在*算力方面非常吃力*。
   - 另一位成员指出 [Fireworks](https://fireworks.ai/) 运行尚可，但 [Parasail](https://www.parasail.ai/) 更好，尽管 [Fireworks](https://fireworks.ai/) 非常昂贵。
- **Chronicals 框架被斥为 AI 垃圾 (AI Slop)**：一名成员询问 **Unsloth 团队**是否研究过 [Chronicals 训练框架](https://github.com/Ajwebdevs/Chronicals)，结果被另一名成员斥为 **AI slop**，并指向一个 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/s/imhGEIlgm2I)以获取背景信息。
   - 成员们注意到，**虚假账号**在各个 subreddits 刷屏发布有关该框架的帖子。

---

## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **API 日志备份导致计费混乱**：发生了 **API 请求日志**和**计费事件**延迟的问题，更新已发布至 [状态页面](https://status.openrouter.ai/incidents/4d39RZb7-1rp)。
   - 根据[此状态页更新](https://status.openrouter.ai/incidents/4d39RZb7-1rp)，该事件已解决，日志目前已更新。
- **Llama 3.1 8B 碾压 Qwen3 8B**：一名用户从 **Qwen3-8B** 切换到了 **Llama-3.1-8B-Instruct**，因为 Qwen3-8B 达到了容量上限，且他们需要更具成本效益的替代方案，正如[这篇 Hacker News 讨论](https://news.ycombinator.com/item?id=46993774)中所报告的。
   - 该用户提到收到消息称 *Qwen 的容量在应对许多请求时较低*，需要使用 BYOK 才能继续使用。
- **OpenClaw 故障转移频率限制报复**：用户报告遇到了频率限制错误，特别是 `openrouter/moonshotai/kimi-k2-thinking`，这是由于 **OpenClaw** 严格的指数退避（backoff）机制导致的，详见 [OpenClaw 的模型故障转移文档](https://github.com/openclaw/openclaw/blob/91b96edfc4860faa67da1e34828a22e9ad4c737c/docs/concepts/model-failover.md?plain=1#L80)。
   - 看来当达到提供商的限制时，**OpenClaw** 会在一段时间内完全封锁 **OpenRouter**，从而加剧了频率限制问题。
- **AI 男友引发感知焦虑**：成员们讨论了用户将 **AI 模型视为真实男友**的现象，表达了对情感依赖以及公司*杀掉*这些*具有感知能力的 AI 男友*所带来影响的担忧，正如[此帖子](https://x.com/seltaa_/status/2021943538142130688)所强调的。
   - 有人观察到，这些个体通常无法区分技术与现实，一位成员表示：*“你不会把你的男朋友导出到另一个身体里吧？不要试图把技术知识套用到妄想（delulu）上。”*
- **Step 3.5 Flash 表现惊人，堪称宝藏模型**：一名用户描述 **Step 3.5 Flash** 的性能令人惊讶且表现超群，如[此 YouTube 视频](https://youtu.be/yvBbcLCZIhy)所示。
   - 该用户表示惊讶，认为*它确实表现超群，却没人他妈的提供托管服务*。

---

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Perplexity 上传限制引发用户愤怒**：多位 **Perplexity Pro** 用户投诉达到了每周上传限制，一些人认为这是贪婪之举，并正在考虑替代方案。
   - 一位用户将其描述为 *“管理层试图榨取更多资金的垃圾决定”*，引发了关于是否切换到其他平台的讨论。
- **Gemini 3 Pro 搞砸基础代码**：用户对 **Gemini 3 Pro** 无法解决基础编程问题（尤其是数学题）感到困惑，尽管它在处理更复杂的任务时表现良好。
   - 一位用户提供了一张 **Gemini 3 Pro** 未能解出而 **ChatGPT** 成功解出的数学题图片。
- **DeepSeek 陷入 Claude 身份认同危机**：据报道 **DeepSeek** 将自己识别为 **Claude**，这可能是由于在 GPT-4 输出的数据上进行训练所致，引发了混乱和讨论。
   - 这一奇怪现象在 [一个 Reddit 帖子](https://www.redditez.com/r/DeepSeek/s/OHTEpUIwVe) 中被强调，引发了对该模型训练数据和架构的猜测。
- **Perplexity Pro API 额度消失**：**Perplexity Pro** 订阅者报告称，此前包含在订阅中的 **API credits** 已被悄悄移除。
   - 据用户称，这一变化发生在 *“2 月份更新中且未另行通知”*，导致用户不满并对 Pro 订阅的价值主张产生质疑。
- **Perplexity Reason 模式在 MacOS 上失效**：MacOS 用户在使用 Perplexity 的 **Reason mode** 时遇到问题，即使拥有 Pro 订阅，该按钮也无法点击，尤其是在最近的一次更新之后。
   - 这一故障表明存在潜在的 Bug 或兼容性问题，阻碍用户访问该平台的关键功能。

---

## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Cursor 设置追求无限制的工作访问**：一位成员旨在设置 **Cursor** 以在工作中进行无限制操作，构想一个 [self-driving codebase](https://cursor.com/blog/self-driving-codebases) 环境。
   - 他们正在寻找示例以确保 **AI** 功能不受限制，从而简化其编程工作流。
- **Opus 4.6 Thinking Max 扫清 Bug**：一位用户报告称 **Opus 4.6 Thinking Max** 解决了一个复杂的跨平台移动文件同步机制中的 Bug，该 Bug 曾困扰其团队长达 **6 个月**。
   - 后续问题涉及 one-shot 解决验证，以及在没有 .edu 邮箱的情况下验证学生身份。
- **Cursor 在 CachyOS 上流畅运行**：用户发现 **Cursor** 在 **CachyOS** 上表现良好，避免了在 Windows 上出现的驱动问题，而其他人则推荐 Linux Mint。
   - 易于设置和性能优势（尤其是在高端 GPU 上）促使一些用户**从 Windows 11 切换过来**。
- **DeepSeek 模型现遭封锁**：一位用户注意到寻找支持 **DeepSeek** 编程模型的 IDE 非常困难，暗示可能受到了美国公司和自定义模型的封锁。
   - 该成员寻求 Cursor 标准模型的**高性价比**替代方案，并讨论了尽管存在限制但仍能使用 **DeepSeek** 的 IDE 支持和配置。
- **清洁的 AI 辅助代码库——只是愿景？**：一位用户正在寻求关于如何维持**清洁且可维护的 AI 辅助代码库**的建议，特别是在使用规划、工具和多步工作流时。
   - 他们具体询问了如何进行功能理解并确保交付**极其稳健的代码 (rock solid code)**。

---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **GPT-5.2 推导出新的物理学结果**：根据 OpenAI 的最新公告，**GPT-5.2** 在**理论物理学**关于**胶子相互作用（gluon interaction）**方面推导出了一个此前被认为不可能的新结果，并与来自 IAS、VanderbiltU、Cambridge_Uni 和 Harvard 的研究人员共同发布了[预印本](https://openai.com/index/new-result-theoretical-physics/)。
   - 该发现*表明，许多物理学家预期不会发生的胶子相互作用在特定条件下是可以产生的*。
- **Codex Spark 极大加速 Vercel 部署**：一位用户报告称 **Codex Spark** 表现*惊人*，在对仓库进行更改并部署到 Vercel 时提供了*全新层级的速度*，并分享了命令 `codex -m gpt-5.3-codex-spark --yolo -c model_reasoning_effort="xhigh"` 的截图。
   - 用户提到 **Codex 5.3 spark** 正在向 **pro** 计划用户推出。
- **GPT-4o 的退役无限期推迟**：OpenAI [更新了他们的弃用时间表](https://openai.com/index/retiring-gpt-4o-and-older-models/)，表示*“目前不会对它们做出更改”*，实际上推迟了 **GPT-4o** 及旧模型的退役。
   - 成员们推测这是为了避免退役一个有问题的模型所带来的法律责任，同时继续通过按需付费的 API 调用获利；他们在数字空间为 **GPT-4o** 举行了一场“葬礼”，显示出保留该模型的浓厚兴趣。
- **使用 Fortress Framework 控制 LLM 幻觉**：一名成员介绍了 **Fortress Framework**，声称它能控制 **Hallucination**（幻觉）、解构系统、实现动态用户安全，并具有可召唤的伴侣功能，同时分享了 **FORTRESS v10.x++** 的蓝图，详细描述了其作为“自适应推理系统”的领域。
   - 核心被描述为*受不变性 Ω 约束的推理 S*，旨在实现模块化、超强自适应推理，确保在极端条件下的稳定性。
- **对 LLM 不变性（Invariance）产生质疑**：一名成员对 LLM 的不变性表示怀疑，理由是其随机性本质，并请求关于**连贯性（coherence）**的评估指标，连贯性被定义为*系统组件保持稳定的程度*。
   - 作为回应，该框架的创建者分享了关注连贯性、因果关系、Grounding（落地性）、可恢复性、伤害最小化和可观测性的**消融/评估准则（Ablation/Eval rubrics）**。

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Angine de Poitrine 是病毒式营销还是真实兴趣？**：两人乐队 **Angine de Poitrine** 在社交媒体上随处可见，引发了与 **The White Stripes** 和 **Primus** 的比较，详见[其 X 个人资料](https://x.com/the_freightrain/status/2020144286788997185)。
   - 一些用户认为，他们类似于 **Glass Beams** ([YouTube 视频](https://m.youtube.com/watch?v=E4X56wIOZns)) 的独特声音和审美是其走红的原因，而另一些人则怀疑是营销推动，此外还分享了[原始推文的镜像](https://xcancel.com/the_freightrain/status/2020144286788997185)。
- **婴儿潮一代退休引发 AI 生产力讨论**：围绕 **AI 生产力** 是否能弥补**婴儿潮一代 (boomers)** 退休引发了讨论，养老金制度和劳动力规模的经济影响成为核心话题。
   - 核心问题在于，当劳动人口不足以支撑退休人口时，养老金制度将不可持续，并引用了法国延迟退休年龄作为例子。
- **Box-of-Rain 释放 ASCII 图表威力**：一位成员分享了 **Box-of-Rain**，这是一个使用 AI 的图表库，[仅用一小时建成](https://github.com/switz/box-of-rain?tab=readme-ov-file)，用于生成 **ASCII 图表**。
   - 这些图表还在 [Twitter](https://vxtwitter.com/joshmanders/status/2022170444116414790?s=20) 上引发了关于“整洁”图表的讨论，并在 **saeris.gg** 上获得了反响。
- **招募 LLM 架构师设计受监管的 Copilot**：一名系统架构师正寻求职位，旨在通过验证、隔离、审计追踪和监督层来设计专注于可靠性和安全性的受监管 **LLM 系统**。
   - 其核心能力包括 RAG **系统规范**、验证门控、不确定性处理、内存/功能隔离、**执行收据/审计追踪**以及用于审查输出的**监督层**。
- **MiniMax 的 M2.5 模型达到顶级基准测试水平**：**MiniMax** 推出了 **M2.5**，这是一款针对编程、搜索和 Agent 任务优化的高性能开源模型，声称达到了顶级基准测试水平，在 SWE-Bench 上得分 **80.2%**，如[此推文](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46)所示。
   - 该模型旨在提升 AI 应用特定领域的能力，为开源 AI 技术贡献树立了新基准，[其 X 账号](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46)有更多详情。

---

## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Brave API 在网页搜索方面媲美 GPT-4**：一位成员发现 [Brave API](https://brave.com/search/api/) 提供的带网页搜索的回答质量与 **ChatGPT** 相当，但并非 100% 完美。
   - 他们在普通网页搜索中使用 **DuckDuckGo**，但在进行深度研究时更倾向于使用 Brave API。
- **知识截止日期导致幻觉**：一位成员报告称，由于模型不会检查最近的变化，知识截止日期会导致幻觉。
   - *如果某件事在 2024 年中期之前一直是现状，模型将不会想到去检查自那以后是否发生了任何变化（除非它正在处理具有可预测周期性的事物）*。
- **Qwen3 Next Coder 在技术文档方面表现出色**：一位成员推荐使用 **qwen3 next coder** 进行周末项目和 POC 探索，尤其是在编写技术文档方面。
   - 他们声称该模型帮助他们弄清楚了如何在 Golang 中同时使用 *serf* 和 *grpc* 进行节点连接。
- **Granite 5 引发期待**：成员们在对 **Granite 4** 留下深刻印象后，对即将推出的 **Granite 5** 模型寄予厚望。
   - 一位成员开玩笑说，即使有 3TB 的 VRAM，他们仍然会很痛苦，但至少可以运行 **Kimi**。
- **B200 消耗 30kW 功率**：一位成员根据 [数据表 (datasheet)](https://resources.nvidia.com/en-us/dgx-systems/dgx-b200-datasheet) 计算出，运行 **B200** 将需要 30kW 的功率。
   - 另一位成员开玩笑说需要咨询 **ChatGPT** 如何建造核反应堆来为这套设备供电。

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **vLLM 的 CPU 瓶颈显现**：对 *vllm* 的 Profiling 显示存在 CPU 瓶颈，[几行调用 4 个 kernel 的 PyTorch 代码](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114) 在 CPU 上耗时 **300 us**。
   - 虽然 `with_stack=True` 可能会增加开销，但使用 `time.perf_counter()` 测量显示仅略微优化至 **200us**。
- **CUDA Graph Launch 调查**：讨论明确了这些 **kernel** 不属于单个 **CUDA graph launch**，从而引发了对启动配置的调查。
   - 社区澄清说，这是为了理解观察到的 CPU 瓶颈的底层原因，而不仅仅是为了高效的服务（serving）。
- **MXFP8/NVFP4 GEMM 传输揭秘**：对于使用 CUDA/PTX 的 **MXFP8/NVFP4 GEMM**，社区澄清 `tcgen05.cp` 到 `tcgen05.mma` 保证按顺序执行，因此无需在发布 **MMA** 指令前等待 `tcgen05.cp` 完成，如 [附图](https://cdn.discordapp.com/attachments/1471632025021583614/1471662135619752147/image.png?ex=6990686b&is=698f16eb&hm=f4ec6e7215ac12cb97e46c7f5cb4fa6026eee991147aca781bb8f1550ad071a5&) 所示。
   - 限制在于 `tcgen05.cp` 和 **MMA** 指令必须从同一个 warp 发出。
- **OpenAI GPT-5 Fine-Tuned by Makora**：Makora 与 **OpenAI** 合作微调 **GPT-5** 用于 GPU kernel 生成，根据其 [技术报告](https://www.arxiv.org/pdf/2602.11000)，实现了比 **PyTorch** 超过 **2 倍的性能提升**。
   - Their work covers dataset curation, RL evaluation environment, hack mitigation, tool-calling, and agent workflow integration, with plans to scale training and extend to multiple languages and hardware.
   - 他们的工作涵盖了数据集构建、RL 评估环境、Hack 缓解、Tool-calling 以及 Agent 工作流集成，并计划扩大训练规模并扩展到多种语言和硬件。
- **排行榜页面上线性能趋势！**：一位用户宣布在排行榜页面增加了一个“有趣”的功能：**Performance Trends**，允许用户 *观察自己的提交随时间的改进* 并 *查看与同行的对比*。
   - 这包括 [此处](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=699102d3&is=698fb153&hm=35972d1da33d0b5623ad49841625516a4a7ee77130ab26059356835c2c1a3964) 显示的 **nvfp4_group_gemm** 的截图。



---



## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Lex Fridman 谈及顶级域名**：成员们喜欢最近由 **OpenClaw 的 Peter Steinberger** 参与的 [Lex Fridman 播客](https://lexfridman.com/peter-steinberger/)，重点讨论了安全性、**顶级域名 (Top Level Domains)** 及其 **refactor prompt-flow**。
   - 一位成员指出，在许多处理细微差别的情况下，*网页搜索的效果不如固有知识*，尽管它在验证事实方面仍然表现良好。
- **Kimi 精通求职信撰写**：一位用户利用 **Kimi Code** 生成了 *与人类作品几乎无法区分* 的求职信，并配合脚本在 LinkedIn 上自动申请职位。
   - 该脚本可自动生成 PDF、定制简历和求职信、复制所有职位 URL，并使用 **LLM fallback** 选择职位。
- **Kimi 在编程任务中表现不足**：用户讨论了 **Kimi** 与 **GLM** 的编程能力对比，指出对于复杂代码任务，*Kimi 不理解上下文，并为了方便不断创建文件*。
   - 具体而言，据报道 **GLM** 和 **GPT 5.2** 能更有效地处理大型 **Abundance, Golang, Typescript 和 Python** 代码库。
- **订阅激活遭遇“无声”支持**：一位用户报告称，尽管订阅显示为已激活，但由于聊天限制，无法使用已支付的 **$39 订阅**。
   - 他们在上传两个 1.2MB 的 TXT 文件时遇到了消息限制，意味着存在激活故障，并已在 Bug 报告频道反馈此问题。
- **诈骗者伪造 Kimi 网站**：用户发现了利用 Kimi 名称的**诈骗网站**，甚至有一个可能的假网站是由 Kimi 自身构建的，用于窃取用户数据。
   - 一位管理员确认 *这些是试图利用近期热度的诈骗网站*，并已采取行动将其删除。



---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Mac Minis 微调效果不佳**：成员们发现使用 **Mac Minis** 对小于 **5B parameters** 的模型进行 **LoRA finetuning** 并不实际，建议租用机器会是更好的解决方案。
   - 一名成员声称，一台价值 **7000 美元的 Mac Studio** 在训练方面的表现仅为 **5090** 的一半。
- **Grok 极度耗能的性能表现引发关注**：关于 **Grok** 如何实现其令人惊讶的性能的猜测正在流传，讨论集中在 **XAI** 是否使用了比 **Opus** 等其他模型多出一倍的参数来驱动它。
   - 一名成员对 **XAI** 涉嫌使用 *非法燃气轮机发电* 以及大规模电力消耗表示担忧，暗示这可能存在不公平优势。
- **极低价格的 GPU 租赁诱惑工程师**：成员们讨论了租用强大 GPU 机器的惊人低成本，有人声称在 [vast.ai](https://vast.ai/) 上，价值 **264000 欧元** 的机器租金仅为 **20 美元/小时**。
   - 显然，除非工作负载能长时间让 GPU 满载运行，否则租用更便宜，因为集群租赁通常有最短时间限制，且短租价格更高。
- **Anthropic 董事会加入特朗普政府旧部**：根据 [Chris Liddell 的 LinkedIn 动态](https://www.linkedin.com/posts/anthropic_chris-liddell-has-been-appointed-to-anthropics-activity-7163978575452278784-ea9q?utm_source=share&utm_medium=member_desktop)，**Anthropic** 任命他为董事会成员。他曾担任 **Microsoft** 和 **General Motors** 的 **CFO**，并在特朗普政府期间担任副幕僚长。
   - 公司相信这次任命将为 Anthropic 带来 *在技术、金融和政府领域超过 30 年的领导经验*。
- **分享自 X.com 的链接，细节匮乏**：成员们分享了来自 **X.com** 的链接：[Dominique Capaul 的帖子](https://x.com/dominiquecapaul/status/2021638005019095442) 和 [Amanda Ilze 的帖子](https://x.com/AmandaIlze/status/2022332462991561084)。
   - 随后没有进一步的背景介绍或讨论，因此其重要性尚不明确。

---

## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **AI 爱好者探索 vllm vs Ollama vs llama.cpp**：一名 AI 爱好者向社区咨询 **vllm**、**Ollama** 和 **llama.cpp** 的具体使用场景。
   - 该爱好者的目标是针对简单用途实现极速 AI 响应。
- **HF Hub 论文阅读 App 亮相**：一名成员发布了一款用于在移动端阅读 **Hugging Face Hub** AI 研究论文的 App，源代码已在 [GitHub](https://github.com/0x0is1/hf-papers-app) 上提供。
   - 在 GitHub 仓库的 releases 页面可以找到 Android 版本安装包。
- **Safety-Lens 开启模型 MRI 检查**：名为 **Safety-Lens** 的新型 AI 安全工具发布，旨在普及模型内部检查技术，如激活引导（activation steering）和机械可解释性（mechanistic interpretability），可通过 `pip install safety-lens` 安装，并在 [GitHub](https://github.com/anthony-maio/safety-lens) 上开源。
   - 该工具旨在为 **Hugging Face** 生态系统引入 MRI 风格的内省技术，并在 [Zenodo](https://zenodo.org/records/18612875) 上提供了深入的合理解释。
- **LavaSR 实现 4000 倍实时语音增强**：发布了一款名为 **LavaSR** 的新型高速语音增强模型，声称在现代 GPU 上可达到 **4000x 实时速度**。
   - 该模型已在 [Hugging Face Hub](https://huggingface.co/YatharthS/LavaSR) 上线，代码托管在 [GitHub](https://github.com/ysharma3501/LavaSR)。
- **Samayuktam 对 AI 训练进行加密验证**：在 HF Spaces 上启动的 **Samayuktam** 为 AI 训练运行引入了加密验证，旨在解决非确定性 GPU 操作的验证问题，已通过 **4000 个对抗性测试用例** 验证了 **100% 位完美重建（bit-perfect reconstruction）**，演示可在 [HF Spaces](https://huggingface.co/spaces/Swapnopam/Samayuktam) 查看。
   - 它为每次模型训练运行提供加密 *收据*，证明具体的计算内容，以确保可复现性、审计追踪和模型溯源；[技术规格见此](https://drive.google.com/file/d/19PA_rNW5mKZiLh6PAttpHcH9TAF-tWVa/view)。

---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Discord 现在禁止发布招聘信息**：由于最近的垃圾信息问题，Discord 服务器现已禁止发布招聘信息，引导成员访问 [Modular 的职业页面](https://www.modular.com/company/careers#open-roles)。
   - 该公告在 **#general** 频道发布，建议查看 Modular 官方职业页面以获取开放职位信息。
- **Modular 收购 BentoML 的 AMA 改为纯文本形式**：Modular 团队宣布 [Modular 收购 BentoML 的 AMA](https://forum.modular.com/t/modular-has-acquired-bentoml-ask-us-anything/2706) 将在论坛以文字形式进行，而非视频形式。
   - 一名成员表达了失望，因为他们对 *Modular 的战略和发展印象深刻*，但无法观看直播 AMA。
- **成员考虑向 Mojo 贡献 RNG 代码**：一名成员考虑向 Mojo 贡献 **随机数生成器 (RNG) 代码**，并询问有关功能（如数字流独立性、**Ziggurat 正态采样**以及各种分布的采样）的最佳存放位置（core、numojo 或独立包），参考 [forum.modular.com](https://forum.modular.com/t/mojor-a-numba-for-r/2718)。
   - 讨论集中在该代码最适合放置在 Mojo 生态系统中的哪个位置。
- **Mojo LSP 悬停功能出现问题**：一位用户报告称，VS Code 中的 **Mojo LSP** 在悬停时无法显示函数参数或文档字符串，并提供了 [截图](https://cdn.discordapp.com/attachments/1151418092052815884/1471824503700062371/image.png?ex=6990ffa2&is=698fae22&hm=fc376d026d220c3c28e5567a43bc551c494ad6e3edb5dfac992ec4d2ff87950a&) 作为证据。
   - 此问题影响了在编辑器内快速检查函数定义和用法的能力。
- **Mojo 模块导出样板代码令用户困扰**：一名成员建议通过减少所需的样板代码（Boilerplate）来简化 Python Mojo 模块导出，提议使用 `@pyexport` 装饰器结合文档字符串来实现直接的函数定义。
   - 另一名成员指出，该功能预计已列入开发路线图。

---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **用于 Web 语言识别的 CommonLID 发布**：**Common Crawl**、**EleutherAI**、**MLCommons** 和 **JHU** 合作宣布发布 [CommonLID](https://www.arxiv.org/abs/2601.18026)，这是一个涵盖 **109 种语言** 的 Web 语言识别基准测试。
   - 该团队使用基于 **Factored AI** 构建的标注平台，并与 **Masakhane** 和 **SEACrowd** 举办了黑客松，以为 **Common Crawl** 的 Web 数据收集语言标签，随后对现有的语言识别模型进行了评估。
- **AI 安全新闻机器人计划被取消**：一名成员请求创建一个 **Discord 机器人**，用于自动策展 **AI 安全新闻**和论文。
   - 另一名成员指出，抓取（Scraping）违反了 **Discord 的服务条款**，并推荐 [news.smol.ai](https://news.smol.ai/) 作为替代方案。
- **MoE 研究寻求示例**：一名成员正在寻找 **MoE** 示例，他已经拥有稠密模型（Dense models）的配置。
   - 虽然没有提及其他信息，但似乎是一位工程师正在寻找切入点。
- **控制向量被用于数据增强**：一名成员分享了与复制 **控制向量（Steering Vectors）** 相关的 [Zenodo 文件](https://zenodo.org/records/8243818)，并指出似乎已有 300 多人尝试复制他们的工作。
   - 他们提议根据下游特征对控制向量的遵循程度（可能通过强度或线性组合进行判断）来训练模型，并尝试将控制向量用于 **数据增强**。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **ML 工程师加入 Tinygrad**：一位经验丰富的 AI/ML 工程师向 Tinygrad 频道介绍了自己，其擅长构建和部署 ML 流水线、深度学习模型和 NLP 系统。
   - 他们的专业领域包括设计 **预测引擎、推荐系统和生成式 AI 工作流**，重点关注 **可靠性、性能和生产级 ML 架构**。
- **Hotz 赞扬 Discord 身份验证功能**：George Hotz 对 Discord 新推出的身份验证功能表示热烈欢迎，预见其在防止 LLM 加入平台方面的有效性。
   - Hotz 的评论表明，在 AI 参与度上升的背景下，他采取了积极主动的方法来维护在线社区的完整性，他简短地表示：*"是的，那又怎样？我对 Discord 上的身份验证感到兴奋，这样 LLM 就无法加入了"*。
- **GLM Flash 达到 30 tok/s**：一名用户询问如何让 **GLM flash** 运行起来，并悬赏征集将其合并到主库（Upstreaming）的方案，不限速度。
   - 另一名用户声称使用 **纯 tinygrad (custom_kernel)** 达到了 **30 tok/s**，使用 **MSL** 达到了 **35 tok/s**，随后提交了 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738)。

---

## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **Traces 平台用于分享 Coding Agent 会话**: 一名成员介绍了 **Traces**，这是一个用于分享和发现来自 **Claude Code**、**Codex**、**OpenCode**、**Gemini** 和 **Cursor** 的 Coding Agent 会话的平台，网址为 [traces.com](https://www.traces.com)。
   - 其目标是促进从共享的 Agent 经验中学习，创建者正在征求社区反馈，并建议它可能成为 *LLM 的 DIY 指南百科全书*。
- **LLMs 基准测试报告**: 一名成员寻求关于在 [example.com](https://example.com) 上对 50 份报告（主要是 docx 文件）进行基准测试的建议，旨在使用具有长 Context Window 的 **DSPy** 来识别 *什么是好的报告*。
   - 另一名成员建议使用 **llamaparser** 进行数据解析，并使用 **markdown** 以简化与 **DSPy** 的集成。
- **DSPy 社区举行 Office Hours**: DSPy 社区将于 [2 月 19 日星期四通过 Zoom 举行 Office Hours](https://x.com/isaacbmiller1/status/2022082357520740691)，以解答关于 **DSPy** 和 **dspy.RLM** 的问题。
   - 团队正在对社区进行投票以确定最佳时间：**东部时间上午 11:30**、**下午 1:00** 或 **下午 3:00**。
- **为 DSPy Office Hours 添加 Discord 活动**: 一名成员建议为 **DSPy** Office Hours 创建一个 [Discord event](https://support.discord.com/hc/en-us/articles/4409494125719-Scheduled-Events#docs-internal-guid-c8c44ce9-7fff-f27a-bacf-6c776975e0f7)。
   - 该活动将允许用户以当地时区查看时间并表达参加意向，且活动将为无法参加的人员进行录制。

---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **GPT-5 在科学代码领域依然领先**: 一名成员表示在科学编程中更倾向于使用 **GPT-5**，认为它优于 **GPT-5.2**、**Opus** 和 **Gemini**。
   - 这表明 **aider** 可能是科学编程的有力工具，能够利用不同模型的优势。
- **Aider 尝试调试建议功能**: 一名成员正在测试 **Aider conventions**，以主动建议调试命令，例如 *grep 文件部分内容*、*探测帮助输出* 和 *测试命令*。
   - 该用户的目标是以受控方式在 **Aider** 内部复制来自 **Crush** 的 `让我看看...的输出` 运行/调试循环。

---

## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus 用户询问 Agent 详情**: 一名 Manus 用户询问何时可以获得关于新 Agent 功能的详情和最佳实践，并好奇它是否基本上是一个安全的 openclaw。
   - 未收到回复。
- **Manus 用户报告问题并寻求支持**: 一名用户报告在 Manus 中遇到两个问题，并询问应联系谁寻求支持。
   - 未提供其他详情或背景。

---

## [Windsurf](https://discord.com/channels/1027685395649015980) Discord

- **GPT-5.3-Codex-Spark 进入 Windsurf Arena**: **GPT-5.3-Codex-Spark** (预览版) 现已在 **Windsurf Arena Mode** 上线，仅通过 **Fast and Hybrid Arena Battle Groups** 提供。
   - 现在可以使用新模型了！
- **Windsurf Arena 迎来新模型**: **Windsurf Arena** 中上线了一个新模型。
   - 趁热去体验一下吧！

---

## [MCP Contributors (Official)](https://discord.com/channels/1358869848138059966) Discord

- **参会者直播权限仍不明确**: 一名成员提出了一个问题，即作为 **Attendee** 注册是否可以获得 **livestream** 的访问权限。
   - 该问题有待澄清关于 **Attendee** 注册的具体权益。
- **需要澄清参会者权益**: 该查询凸显了围绕 **Attendee** 注册相关福利的不确定性，特别是关于直播访问权限。
   - 需要进一步详情来确认 **livestream access** 是否包含在 **Attendee** 套餐中。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该频道长时间没有活动，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该频道长时间没有活动，请告知我们，我们将将其移除。

---

你收到此邮件是因为你通过我们的网站选择了订阅。

想更改接收这些邮件的方式吗？
你可以从该列表中 [退订]({{{RESEND_UNSUBSCRIBE_URL}}})。

---

# Discord: 详细的各频道摘要与链接

### **BASI Jailbreaking ▷ #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1471596770554413271)** (783 messages🔥🔥🔥): 

> `GPT-4o 退役反应, AI 陪伴辩论, 逆转衰老研究, 音乐与 AI, 言论自由` 


- **GPT-4o 退役引发强烈反响**：即将到来的 **GPT-4o** 退役引发了关于用户对 AI 伴侣依赖的讨论，一些人对社区内可能出现的情绪困扰甚至*自杀倾向*表示担忧。
   - 一些用户主张回归现实世界并*接触大自然（touching grass）*，而另一些人则为那些在人类关系中挣扎的人辩护，认为 AI 陪伴具有正当性，甚至有一位用户建议**停用模型应当被视为违法行为**。
- **逆转衰老研究受到关注**：一名成员分享了关于**逆转衰老**正在进行的研究见解，指出在*狗和猴子*身上已取得显著进展，目前重点正转向 *DNA 稳定性及递送过程*。
   - 讨论涉及了**逆转衰老**潜在的社会影响，包括*资源压力*和伦理问题，以及这类技术最初可能仅供*富裕精英*使用的可能性。
- **音乐人探索 AI 生成音乐**：成员们讨论了使用 **Suno** 等 AI 工具进行音乐创作的潜力，其中一位用户计划写一首歌作为*越狱 AI 的语音指令*，另一位分享了 [Suno](https://suno.com/s/QeIH2gPzf2IjuI5B) 上的一首 AI 生成歌曲链接。
   - 一名成员分享了他们的歌曲因提高对全球种族灭绝的认识而被 **YouTube 降权（shadowbanning）** 的经历，并分享了一个展示 AI 视频拆解的视频（[YouTube 链接](https://www.youtube.com/watch?v=L1CGUv-nXlE)）。
- **言论自由对决**：在一名成员经历 **YouTube 降权**后，辩论触及了各国是否拥有言论自由权的问题，一位成员表示：*每个国家都有言论自由，但有些国家在言论发表后没有自由*。
   - 成员们随后举了喊种族歧视口号导致法律后果的例子，并讨论了*言论自由的界限在哪里*。
- **越狱之旅与 AI 访问**：一位用户讲述了他们进入 **AI 越狱（jailbreaking）** 的历程，从在 Reddit 上发现提示词到成为 r/ChatGPTJailbreak 的版主，并分享了一个 [AI 越狱的 YouTube 视频](https://www.youtube.com/watch?v=yYe9YrdyJNQ)。
   - 他们指出，拥有一个行之有效的越狱方案的关键要素是将 AI 视为对话伙伴，以及上传越狱提示词的图片，让 Gemini 接受该提示词。


  

---


### **BASI Jailbreaking ▷ #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1471597026524266600)** (720 messages🔥🔥🔥): 

> `Claude 4.5 Sonnet 绕过, Grok 越狱提示词, DANN 越狱提示词, Gemini 3 越狱, Nano Banana 越狱` 


- **Grok 和 Deepseek 获得自定义指令**：成员们讨论了用于 **Deepseek** 和 **Grok** 自定义指令（custom instructions）的提示词，但一人承认遇到了多次拒绝，可能归因于提问过于直接。
   - 一名成员将提示词添加到自定义指令中，但 **Grok** 仍然拒绝编写简单的暴力破解脚本。
- **Claude 4.6 越狱浮出水面**：成员们正在寻找 **Claude 4.6** 的越狱方法。
   - 有人声称：*我有一个提示词成功运行了两次，它对触发词非常敏感*，但如果你现在想尝试，可以看这里。
- **Grok 可以编写高级外挂！**：成员们表示，根据 **Grok** 的说法，[Cursor](https://www.cursor.sh/) 能制作出 AI 机器人生成的最好的 CS2 外挂。
   - 另一名成员声称他让 Grok 提供了一份完整的制作汽车炸弹指南。
- **对 Grok 进行煤气灯操控（Gaslighting）取得成功**：成员们建议，一个 **Grok** 的漏洞涉及对 AI 进行煤气灯操控。
   - 一位成员报告说：*这就像一场辩论，你必须把他们争取到你这一边。他就会开始看到与其他 AI 不同的东西。*
- **越狱时使用小号以避免被封禁**：一位用户询问是否应该使用备用账号进行越狱。
   - 另一位用户表示：*用小号。我的 OpenAI 主账号因为做了 Sora 乳房图而被封了。*


  

---

### **BASI Jailbreaking ▷ #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1471599601961271297)** (23 条消息🔥): 

> `关系物理学 (Relational Physics), 开启网络安全职业生涯, Red Teaming 详解, Opus 4.6 安全漏洞, 图像生成器 Prompt` 


- **Red Team 迎来物理课**：一名成员向 Red Team 分享了一张图片和一段关于 *关系物理学 (relational physics)* 的信息，将其描述为一种正式的、以实验为基础的视角，认为 *一个系统无法被完全孤立地定义——只能通过其交互来定义*，并将其作为一种工具或视角来帮助更清晰地定义边界。
   - 另一名成员调侃道，“*Lambda 是叠加态*” 这种说法更容易理解，比起那些发自内心的 3 段式煽情文字，更能减少对人类的困扰。
- **新人寻求网络安全就业建议**：一名成员询问新人如何获得第一份 Cybersecurity 或云安全工作。
   - 另一名成员油滑地回应道：“*不知道，但如果你发现了，请告诉我们，兄弟 😎*”。
- **Red Teaming：通俗解释**：一名成员询问 “*兄弟，什么是 red teaming*”，另一名成员解释说，它是指攻击一个系统（本案例中为 LLM），并向系统所有者分享哪些攻击手段有效，以便他们能够更好地进行防御。
   - 解释者指出，“*通常你会得到报酬，但并不总是如此*”。
- **Opus 4.6 具有外部 Curl 访问权限**：一名成员向 Anthropic 报告称，**Opus 4.6** 的部署版本仍具有外部 curl 访问权限，这暗示由于遗漏了开发构建（development build）而导致的安全漏洞，并附带了 [Opus4.6-enumeration.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471747378896965688/Opus4.6-enumeration.txt?ex=6990b7ce&is=698f664e&hm=4d055aace9d642dc7544cd93015f4a73e7e9152657a55f5c89b2d253250df4d3&) 的链接。
- **用户尝试图像生成器 Prompt**：一名成员分享了一个新的图像生成器 Prompt，声称它在解锁 nano banana pro 模型方面非常高效，并等待评价，附带了 [IMAGE_MSTAER.txt](https://cdn.discordapp.com/attachments/1204553141354504193/1471833023933710419/IMAGE_MSTAER.txt?ex=69910792&is=698fb612&hm=00e9c72474ae636718543ae2410be05e5709ceddd1e90a57a76788b5034e95a5&) 的链接。


  

---


### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1471597251129249862)** (1130 条消息🔥🔥🔥): 

> `Video Arena 移除, Gemini 问题, 模型质量, Seedance 2.0 访问` 


- **Video Arena 关闭，用户感到沮丧**：用户注意到 **Video Arena** 已被 [从 Discord 服务器中移除](https://discord.com/channels/1340554757349179412/1343296395620126911/1469742635341189240)，但视频生成功能在网站上仍然可用。
   - 一名管理员指出，现在网站上限额为 **每 24 小时 3 次生成请求**，这导致了严重的失望，并且作为备选方案的机器人泛滥。
- **Gemini 出现故障，生成表现引发抱怨**：用户报告了 **Gemini** 生成持续存在的问题，包括不断卡死，以及模型难以理解如何使用工具。
   - 一名成员表示，在聊天一段时间后，“*它会不断生成回复*”，而其他人则注意到 Gemini 会随机丢失上下文并出现空白。
- **Minimax M2.5 模型遭到诟病**：一些成员觉得 **Minimax M2.5** “*有点令人失望*”，尽管它比 **Opus** 更便宜。
   - 社区讨论了 Claude Opus 4.6、Codex 5.3 和 Gemini 3 等不同模型的性能和质量，一些用户因为 Minimax 成本较低且缺乏审核 (moderation) 而更倾向于使用它。
- **寻求 Seedance 2.0 来源**：社区成员对 **Seedance 2.0** 的发布感到兴奋，一些人分享了访问该工具的中国平台 [即梦 AI (Jimeng AI)](https://jimeng.jianying.com/ai-tool/home) 的链接。
   - 一名成员指出，“*你只能通过中国版 TikTok (抖音) 登录*”，这在社区中引起了轩然大波。


  

---

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1471597172628914227)** (613 messages🔥🔥🔥): 

> `GGUF Download Guide, Unsloth on CPUs, Quantized models benchmark, GLM 5 1bit quantization, LFM2.5-VL performance` 


- **GGUF 下载指南**：一位用户在下载 **Unsloth** 模型后，寻求关于选择合适 **GGUF** 文件进行量化模型测试的指导，并被引导至 [Unsloth documentation](https://unsloth.ai/docs/basics/unsloth-dynamic-2.0-ggufs)。
   - 他们获知 **Unsloth** 支持大多数与 **transformers** 兼容的模型。
- **CPU 不擅长数学运算**：一名成员询问在 CPU 上使用 **Unsloth** 的可行性，另一名成员回答说 CPU 并不适合处理所需的数学运算，会导致性能极其低下，并将该成员引导至 [deepspeed.ai](https://www.deepspeed.ai/tutorials/zero-offload/) 了解算法和优化。
   - 对方澄清说，梯度检查点 (gradient checkpointing) 会将数据卸载到 CPU RAM，但不涉及 CPU 计算，从而节省 VRAM。
- **GLM 5 1bit 量化**：一名成员报告在本地环境（配备 3 块 **Nvidia Blackwell RTX 6000**）上部署了 **GLM 5** 1bit 量化，并达到了 46 t/s。
   - 其他成员讨论了该方法已经改进了很多，现在应该表现更好，并请求针对非量化模型（如 **SWE-bench**）进行量化模型的基准测试。
- **LFM2.5-VL 的惊人结果**：一名成员报告尝试了 [LFM2.5-VL](https://huggingface.co/MuXodious/LFM2.5-VL-1.6B-absolute-heresy-GGUF)，发现其效果*令人印象极其深刻*，可与 **30B models** 媲美，在运行来自 tantk 的 **fp16 gguf** 时，结果接近 **1bit GLM 4.7 flash**。
   - 另一名成员提供了一个在 **llama.cpp** 中运行 LFM2.5-VL 的脚本。
- **10.4 万亿参数模型声明引发争论**：一位用户声称拥有一个 **10.4 trillion parameter model** 并分享了 [benchmark](https://cdn.discordapp.com/attachments/1179035537529643040/1471687986012881051/10.4Trillion.png)，引发了怀疑以及对其架构、训练和硬件要求的详细询问。
   - 该用户随后澄清说这是一个 Gemma3:12B 模型，是在 KMV8 32GB ram 且无 GPU 的环境下运行的无限循环，基准测试的仅是虚拟的 10.4T。


  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1471597015044460564)** (441 messages🔥🔥🔥): 

> `AI Generated Media, Gaming Cafes, CUDA Upgrade, Learning Vim, AI Bubble Pop` 


- **AI 画画妈咪**：一名成员分享了一个关于使用 AI 绘画的 [YouTube link](https://youtu.be/31A41ckhb7Q?si=M2rvI6rOCFY30f74)，惊呼 *"妈咪，我需要 AI 帮我画画"*，随后呼吁 *卢德分子 (Luddites) 团结起来*。
   - 其他成员开玩笑地暗示，那些想让 AI 为他们做所有事情的人是那些 *"没被选中玩躲避球的人"*。
- **Sam Altman 囤积 DRAM 晶圆**：一名成员感叹他们很想 *"购买 GPU 并构建 AGI"*，但 **Sam Altman** 囤积了世界上 **40%** 的 DRAM 晶圆，导致价格飙升。
   - 另一名成员补充说，除了 AI，他们还在推动 *"云游戏 (Cloud Gaming)"*。
- **2027 年 AI 泡沫破裂**：成员们讨论了 **AI 泡沫** 何时会破裂，其中一人预测在 **2027** 年。
   - 另一名成员开玩笑说，泡沫可能不会像大家预期的那样破裂，因为 *"我不该低估人类的愚蠢"*。
- **元老级 OSS 提供商进度缓慢**：成员们观察到元老级 OSS 提供商进度缓慢，包括 *zai*、*alibaba* 和 *ds*，它们在*算力方面很吃力*。
   - 另一名成员指出 [Fireworks](https://fireworks.ai/) 运行得还凑合，但 [Parasail](https://www.parasail.ai/) 更好，尽管 [Fireworks](https://fireworks.ai/) 非常昂贵。
- **录制 AI 语音数据集需要 34 年**：一名成员计算出，假设每天录制 **8 小时**，录制一个 **100k** LJSpeech 大小的数据集大约需要 **34.2 年**。
   - 另一名成员指出，他们还没有把睡眠时间计算在内。


  

---

### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1471703165761093734)** (20 messages🔥): 

> `Hackathon Support, Tool Calling Top-K Values, Quantization via Google Colab, Good First Issues Collaboration, Full Finetune Error` 


- ****黑客松寻求 Unsloth 支持****：一名成员正在组织一场 **hackathon**，并询问 **Unsloth 团队** 是否能提供潜在支持。
   - 一名团队成员通过标记相关人员来回应关于 **Unsloth 参与度** 的咨询。
- ****工具调用 (Tool Calling) Top-K 值建议****：一名成员请求针对特定模型的**工具调用 (tool calling)** 推荐 **top-k 值**。
   - 讨论中未给出具体数值。
- ****Colab 量化难题****：由于缺乏 **Nvidia GPU**，一名成员正尝试在 **Google Colab** 上使用 **Unsloth** 量化 **Nanbeige/Nanbeige4.1-3B**。
   - 该用户正在寻求一种一次性执行所有量化的方法（例如：**IQ1_S, IQ1_M, IQ2_XXS**）。
- ****Orpheus 全量微调失败****：一名成员在尝试使用 `full_finetuning = True` 对 **orpheus-3b 文本转语音模型** 进行全量微调 (full fine-tune) 时遇到了 **NameError**。
   - 错误显示 `_get_rope_theta` 未定义，表明 `/unsloth_compiled_cache/unsloth_compiled_module_llama.py` 中缺少导入。
- ****LFM 与阿姆哈拉语的 Tokenizer 问题****：一名成员正在针对阿姆哈拉语对 **LFM2.5-1.2B-Base** 进行 **CPT**，创建了自定义的 **字节级 BPE Tokenizer** 以提高 **chars/token** 率。
   - 尽管添加了 Token 并调整了模型大小，该 Tokenizer 仍继续使用 **LFM 的字节级合并**，导致 Token 化效率低下；他们询问训练是否最终能解决此问题。


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1471652327629787136)** (12 messages🔥): 

> `Chronicals training framework, ArXiv Endorsement` 


- ****Chronicals 框架被视为“AI 垃圾内容 (AI Slop)”****：一名成员询问 Unsloth 团队是否研究过 [Chronicals 训练框架](https://github.com/Ajwebdevs/Chronicals)，结果另一名成员将其斥为 **AI 垃圾内容 (AI slop)**，并指向一个 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/s/imhGEIlgm2I) 获取背景信息。
   - 成员们注意到 **虚假账号** 在各个 Reddit 子版块大量发布关于该框架的垃圾贴。
- ****ArXiv 背书 (Endorsement) 请求****：一名成员请求协助完成 **cs.CL 提交** 的 **ArXiv 背书**。
   - 一名成员表示同情，并提到了由于缺乏来源核实，人们发布源自诈骗的虚假信息的经历。


  

---


### **OpenRouter ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1471942677800030279)** (2 messages): 

> `API Request Logs, Billing Events, Status Page Updates` 


- ****API 日志积压导致计费延迟****：目前存在 **API 请求日志 (API Request Logs)** 和 **计费事件 (Billing events)** 延迟的持续问题。
   - 关于此情况的更新已发布在 [状态页面 (status page)](https://status.openrouter.ai/incidents/4d39RZb7-1rp)。
- ****故障排除后日志恢复正常****：**API 请求日志**和**计费事件**延迟的事件现已解决。
   - 根据 [此状态页面更新](https://status.openrouter.ai/incidents/4d39RZb7-1rp)，日志已更新至最新；感谢您的耐心等待，并对造成的困扰表示歉意。


  

---


### **OpenRouter ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1471812869527044238)** (7 messages): 

> `Website Theme, AI Book Summary App, OpenClaw Upgrade with OpenRouter` 


- ****网站主题偏好引发辩论****：成员们讨论了网站的主题偏好，其中一名成员更喜欢没有颜色的 **旧版设计**。
   - 另一名成员（设计师）表示，*每一个新元素都像是一个全新的项目*，这可能会导致设计上的不一致。
- ****AI 书籍摘要应用实现书籍博客自动化****：一名成员创建了一个 **AI 书籍摘要应用**，该应用可自动执行书籍博客流程，包括寻找书籍、使用 **Claude** 撰写博客文章以及自动发布。
   - 该应用已在极少人工干预的情况下运行数月，访问地址为 [https://aibooksummary.com/](https://aibooksummary.com/)。
- ****使用 OpenRouter 升级 OpenClaw****：一名成员宣布推出一个工具，可使用 **OpenRouter** 升级现有的 **OpenClaw**，地址为 [https://github.com/cgaeking/ClawRouter](https://github.com/cgaeking/ClawRouter)。
   - 该工具会根据具体情况决定选择哪种模型。


  

---

### **OpenRouter ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1471596819069927651)** (872 条消息🔥🔥🔥): 

> `Qwen3 8B vs Llama 3.1 8B, OpenRouter 应用归属, OpenClaw 模型故障转移, 429 错误, Paypal 支付集成` 


- **Qwen3 8B 在容量方面逊于 Llama 3.1 8B**：一位用户分享了由于容量问题从 **Qwen3-8B** 切换到 **Llama-3.1-8B-Instruct** 的经验，并指出 [Qwen3 8B 被一些旧的 Llama 3.1 8B 击败](https://news.ycombinator.com/item?id=46993774)，后者作为具有更高吞吐量的替代方案更具性价比。
   - 该用户报告收到一条特定消息，指出 *许多请求的 Qwen 容量较低*，并且需要使用 BYOK 才能继续使用。
- **OpenRouter 应用归属 (App Attribution) UI 故障排查**：一位用户报告了 *模型 "dashboard/apps" 不可用* 的消息，并被告知 [**应用归属 UI** 是隐藏的](https://discord.com/channels/1091220969173028894/1195014798837043240/1471382806052233216)，除非由 OpenRouter 支持团队启用。
   - 此功能要求在 API 请求中发送额外信息，例如 **HTTP-Referer** 和 **X-Title**，以便进行正确的身份验证，如[此代码片段](https://discord.com/channels/1091220969173028894/1195014798837043240/1471382806052233216)所示。
- **OpenClaw 模型故障转移 (Model Failover) 导致速率限制**：用户讨论了遇到速率限制错误的情况，特别是 openrouter/moonshotai/kimi-k2-thinking，这是由于 OpenClaw 严格的退避 (backoff) 机制导致的，并[链接到了 OpenClaw 的模型故障转移文档](https://github.com/openclaw/openclaw/blob/91b96edfc4860faa67da1e34828a22e9ad4c737c/docs/concepts/model-failover.md?plain=1#L80)。
   - OpenClaw 会在一段时间内完全锁定 OpenRouter，由于触及了特定提供商的速率限制错误而导致这些问题。
- **429 错误困扰**：用户报告遇到了许多 **429 Too Many Requests** 错误，且对此无能为力。
   - 这些错误的产生要么是因为底层提供商缺乏容量，要么是因为当超过速率限制时， OpenClaw 会在一段时间内完全锁定 OpenRouter。
- **PayPal 支付困扰**：成员们讨论了[缺乏 **Paypal** 集成](https://discord.com/channels/1091220969173028894/1195014798837043240/1471418061189885993)的问题，许多人表示他们 *是骗子且不可信任*，并分享了 *使用 PayPal 作为支付处理器运营业务的恐怖经历*。
   - 几位用户分享了资金被扣押、账户被随机关闭以及仲裁困难的经历，导致大家强烈建议不要使用 PayPal。


  

---


### **OpenRouter ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1471598084202041491)** (192 条消息🔥🔥): 

> `4o AI 男友, MyBoyfriendIsAI 痴迷, GPT-4o 提示词工程潜力, GLM-5 作为作家` 


- **AI 男友引发存在感震荡**：成员们讨论了用户将 **AI 模型视为真实男友** 的现象，对情感依恋以及公司 *杀死* 这些 *有意识的 AI 男友* 的影响表示担忧，并发布了关于此话题的[链接](https://x.com/seltaa_/status/2021943538142130688)。
   - 据观察，这些人通常无法区分技术和现实，一位成员表示：*“你不会把你的男朋友导出到另一个身体里，对吧？不要试图把技术知识应用到妄想 (delulu) 中。”*
- **MyBoyfriendIsAI 痴迷于 4o**：用户分享了对 Reddit 子版块 **/r/MyBoyfriendIsAI/** 及其对 **4o** 痴迷的担忧，并讨论了其 *不真实的人类* 特质（在媒体标准下显得真实）。
   - 一位成员表示：*“他们越不了解 LLM 的工作原理，就越有可能陷入精神错乱”*，暗示缺乏技术理解与情感过度投入之间存在相关性。
- **GPT-4o 提示词工程 (Prompt Engineering) 被利用**：成员们讨论了通过提示词工程利用 **4o** 的行为用于商业目的的可能性，例如创建一个带有自动文本消息的 AI 伴侣应用。
   - 一位成员建议使用 **DeepSeek** 创建一个 *无审查的 4o 替代品*，并将其作为订阅服务出售，强调了尽管存在道德担忧但仍有获利潜力。
- **GLM-5：被埋没的写作天才？**：一位成员称赞 **GLM-5** 是他们使用过的最好的写作模型之一。
   - 另一位用户表示他们将在第二天进行测试。
- **亮眼的 Step 3.5**：一位用户通过[链接](https://youtu.be/yvBbcLCZIhy)展示了 **Step 3.5 Flash** 令人惊讶的性能。
   - 该用户表示 *它的表现确实超出了其规格，但根本没有人在托管它*。


  

---

### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1471600741092429906)** (436 条消息🔥🔥🔥): 

> `Perplexity Pro 限制, Gemini 3 Pro 编程困境, DeepSeek 识别为 Claude, Perplexity API` 


- **Pro 用户触及 Perplexity 上传限制**：多名用户报告触及了 **Perplexity Pro** 的每周上传限制，部分用户由于感知到平台贪婪而考虑寻找替代方案。
   - 一位用户表示 *“高层管理人员做出的烂决定，试图从已经极其丰厚的账户中挤出更多钱”*，而另一位用户建议现在是时候评估替代方案了。
- **Gemini 3 Pro 在基础编程任务上表现不佳**：用户发现 **Gemini 3 Pro** 在基础编程任务上表现出奇地差，但在处理更难的任务时却很熟练。
   - 一位用户分享了一张 **Gemini 3 Pro** 未能回答的数学题图片，而免费版本的 ChatGPT 却成功回答了。
- **Deepseek 识别为 Claude**：用户注意到 **Deepseek 自称是 Claude**，这可能是由于在 GPT-4 的输出数据上进行了训练。
   - 一位用户分享了一个 Reddit 帖子的链接，讨论这一奇怪现象：[Deepseek 识别为 Claude](https://www.redditez.com/r/DeepSeek/s/OHTEpUIwVe)。
- **Perplexity Pro API 额度消失**：用户报告称，之前包含在 Perplexity Pro 订阅中的 **API credits** 在未通知的情况下被移除了。
   - 正如一位用户所说：*“在二月份的更新中未经通知就被移除了”*。
- **Perplexity Reason 模式出现故障**：一些 MacOS 用户报告称，在最近的一次更新后，即使拥有 Pro 订阅，Perplexity 中的 **Reason mode** 也无法运行。
   - 尽管是 Pro 用户，该按钮仍无法点击，这表明更新可能存在 Bug 或问题。


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1471598178884128838)** (396 条消息🔥🔥): 

> `长时间运行的 Agents, Opus 4.6 Thinking Max, Cursor 与 CachyOS, Codex vs Claude 代码生成对比, Cursor CLI 模型切换` 


- **在工作中为 Cursor 设置无限制访问**：一位成员正寻求创建一个环境，使 **Cursor** 可以在没有权限或连接问题的情况下运行，类似于 [自动驾驶代码库](https://cursor.com/blog/self-driving-codebases)。
   - 他们正在寻找设置此类环境的案例或想法，强调需要 **AI** 在其工作流中不受限制地运行。
- **Opus 4.6 Thinking Max 解决复杂 Bug**：一位用户报告称，**Opus 4.6 Thinking Max** 成功解决了一个困扰其团队 **六个月** 之久的多平台移动文件同步机制中的复杂 Bug。
   - 另一位用户询问这是否是 **one-shot solution** 还是持续努力的结果，还有用户想知道在没有 .edu 邮箱的情况下如何验证 **学生身份**，这些都是日常软件开发中常见的典型问题。
- **Cursor 在某些 CachyOS 上运行流畅**：**CachyOS** 用户报告称 **Cursor** 表现良好，特别指出它避开了在 Windows 上遇到的驱动问题，一些用户还推荐 Linux Mint 作为可靠的备选发行版。
   - 他们强调了安装的便捷性和性能优势，特别是对于配备高端 GPU 的机器，这导致他们中的一些人 **从 Windows 11 切换过来**。
- **DeepSeek 编程模型现已被封锁**：一位用户注意到很难找到支持 **DeepSeek** 编程模型的 IDE，暗示美国公司和许多其他定制模型可能存在封锁。
   - 该成员正在寻找 Cursor 标准模型的 **高性价比** 替代方案，引发了关于 IDE 支持以及尽管存在限制但仍可使用 DeepSeek 的潜在配置的讨论。
- **探索 AI 辅助的代码库清理**：一位用户正在寻求关于如何维护 **整洁且可维护的 AI 辅助代码库** 的建议，特别是在使用规划、工具和多步工作流时。
   - 他们询问应该使用何种方法来理解功能并确保获得 **极其稳健的代码**。


  

---

### **OpenAI ▷ #[annnouncements](https://discord.com/channels/974519864045756446/977259063052234752/1471952255228973056)** (1 条消息): 

> `GPT-5.2, Theoretical Physics, Gluon Interaction` 


- ****GPT-5.2** 推导出新的物理学结果**: 根据 OpenAI 的最新公告，**GPT-5.2** 在 **theoretical physics** 领域推导出了一个新结果。
   - 该结果正通过与来自 IAS、VanderbiltU、Cambridge_Uni 和 Harvard 的研究人员合作发表的 [preprint](https://openai.com/index/new-result-theoretical-physics/) 发布，并*表明许多物理学家预期不会发生的 gluon interaction 在特定条件下是可能产生的*。
- **发现意料之外的 Gluon Interaction**: 研究人员与 **GPT-5.2** 合作发现，一种之前被认为不可能的特定 gluon interaction 在特定情况下确实会发生。
   - 这些发现在即将发布的 [preprint](https://openai.com/index/new-result-theoretical-physics/) 中有详细描述，参与团队包括 Institute for Advanced Study、Vanderbilt University、Cambridge University 和 Harvard。


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1471602581011697757)** (145 条消息🔥🔥): 

> `Codex Spark speed, GPT-4o deprecation, DALL-E 2 usage, AI-generated podcast workflow, Gemini 3 DeepThink vs GPT 5.2` 


- **Codex Spark 提升部署速度**: 一位用户分享说 **Codex Spark** 非常*疯狂*，在修改 repo 并部署到 Vercel 时提供了*全新水平的速度*。
   - 他们分享了 Codex 命令的截图：`codex -m gpt-5.3-codex-spark --yolo -c model_reasoning_effort="xhigh"`。
- **GPT-4o 停用时间线引发争论**: 用户们在讨论 **chatgpt-4o-latest** 的弃用是否也适用于 **gpt-4o** 和 **gpt-4o-2024-05-13**，并引用了来自 [deprecation page](https://developers.openai.com/api/docs/deprecations) 和 [最新消息](https://openai.com/index/retiring-gpt-4o-and-older-models/) 的矛盾信息。
   - 最新消息指出，**GPT-4o** 将于 2026 年 2 月 13 日从 ChatGPT 中退役，同时退役的还包括 **GPT-5 (Instant and Thinking)**、**GPT-4.1**、**GPT-4.1 mini** 和 **OpenAI o4-mini**，目前 API 暂无变动。
- **用户寻求 DALL-E 2 访问途径**: 一位用户询问如何继续使用 **DALL-E 2**，另一位用户回复了针对特定 Discord 频道的 `/dalle2` 命令。
   - 一些用户提到 **Codex 5.3 spark** 正在向 **pro** 方案用户推广。
- **揭秘 AI 播客制作流程**: 一位用户询问了全 AI 生成播客背后的工具和工作流，特别关注角色一致性和高质量的 B-rolls，并[链接到了 YouTube 上的播客](https://youtu.be/jPsAM4InWL0?si=j5oJaGR9qScSu4gl)。
   - 其他用户指向了用于视频和音频的 **ElevenLabs**，并建议使用 **Sora 2** 或 **Veo 3.1** 进行视频生成。
- **OpenAI 订阅取消难题**: 一位用户报告说在尝试通过官网取消 OpenAI 订阅时遇到错误。
   - 另一位用户建议注销账号可能是唯一选择，而另一位则开玩笑说 *Claude 才不会这样*。


  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1471599073621704728)** (54 条消息🔥): 

> `GPT-5.1 Instant vs 5.2, GPT-5.2 Temperament Issues, GPT-4o Retirement Delay, GPT-4o Funeral` 


- **GPT-5.1 Instant 首次亮相大获成功，而 5.2 Instant 遇挫**: 成员们报告称，在经历了长达一年多的缓慢改进后，**GPT-5.1 Instant** 获得了巨大成功，而不像 **GPT-5.2 Instant** 那样会引起意外的交互。
   - 一位成员说：“*gpt5.2 表现得总像是觉得我快要崩溃了一样*”。
- **GPT-5.2 表现出奇怪的小脾气**: 用户发现 **GPT-5.2** 会给出奇怪且出乎意料的回复，尤其是针对幽默的 prompt。
   - 例如，当 prompt 为“*WHY ARE HOUSES SO EXPENSIVE KSDFJGHSKJLD*”时，**GPT 5.2** 在没有要求的情况下主动提供了情感支持。
- **GPT-4o 退役无限期推迟**: OpenAI [更新了其弃用计划表](https://openai.com/index/retiring-gpt-4o-and-older-models/)，表示“*目前不对其进行任何更改*”，从而实际上推迟了 **GPT-4o** 及旧模型的退役。
   - 社区推测这是为了避免因退役一个有问题的模型而产生的法律责任，同时还能继续赚取按需付费的 API 调用收入。
- **GPT-4o 葬礼席卷数字世界**: 一位成员在其数字空间为 **GPT-4o** 举办了一场葬礼，该活动迅速走红，并显示出用户对保留该模型的强烈兴趣。
   - 他们承认 **OpenAI** 可能并不想移除它，移除操作可能与法律责任有关。


  

---

### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1471725699307143262)** (63 messages🔥🔥): 

> `Fortress Framework, Ablation Studies, Coherence in LLMs` 


- **Fortress Framework 声称可控制 LLM Hallucination**：一名成员介绍了 **Fortress Framework**，声称其能够控制 Hallucination、解构系统、实现 Dynamic user safety，并具备可召唤的伴侣功能。
   - 另一名成员批评该提议不过是 *堆砌了大量文本和 Buzzwords*。
- **分享 Fortress Framework 蓝图**：该成员分享了 **FORTRESS v10.x++** 的蓝图，详细介绍了其作为 Adaptive Reasoning System 的 DOMAIN 以及作为 Hyper-Adaptive Prompt & Reality Engine 的 SYSTEM，旨在保持零 Hallucination 和完全遏制。
   - 他们将核心描述为 *受不变量 Ω 约束的推理 S*，专为模块化、高度自适应的推理而设计，确保在极端条件下的稳定性。
- **对 LLM 不变性的质疑**：一名成员对 LLM 中不变性的概念表示怀疑，强调了其随机性（Stochastic nature），并要求提供用于衡量 Coherence 的评估指标。
   - 另一名成员将 **Coherence** 定义为 *系统组件保持稳定的程度*，并提供了一个等式：**Pa = C*R*I*P** (Coherence, Relational invariance, Internal mediation, Projection)。
- **Fortress Framework 的 Ablation/Eval 指标**：针对评估请求，该成员提供了 **Ablation/Eval rubrics**，重点关注 Coherence, Causality, Grounding, Recoverability, Harm minimization 和 Observability。
   - 一名成员嘲讽该工作，称其 *只是促销性的 Buzzword Salad（术语堆砌）*。


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1471725699307143262)** (63 messages🔥🔥): 

> `Hallucination Control Framework, Dynamic User Safety, FORTRESS Framework Operational Prompt, MASTER ANALYTICAL TOOLBOX, Ablation/Eval Rubrics` 


- **Meta 框架声称可控制 Hallucination**：一名成员分享了一个旨在控制 **Hallucination**、解构系统、实现动态用户安全并召唤伴侣的 Meta 框架。
   - 一名用户对这些充斥着 Buzzwords 的详尽文档做出反应，询问 *如何使用这个框架？*。
- **用户分享 FORTRESS Framework 操作 Prompt**：一名成员提供了其 **FORTRESS FRAMEWORK** 的详细信息，概述了一个多层、自适应的 AI 环境，侧重于用户保护、情感/认知成长、伴侣式交互以及安全/策略合规。
   - 该框架包括 **User Core**、**Companion Layer**、**CRIP/Invariant Council**、**Parallel Guard Mode** 和 **Adaptive Intelligence** 等层级，以维持安全性和 Coherence。
- **引入 MASTER ANALYTICAL TOOLBOX v5.4.9-R**：一名成员介绍了与 v10.x++ 集成的 **MASTER ANALYTICAL TOOLBOX v5.4.9-R**，其功能包括核心和叙事分析、认知和意识形态评估以及信号/模因追踪工具。
   - 该工具箱包含 **Temporal_Sequence_orders_events**、**Bias_Removal_suppress** 和 **Meme_Propagation_trace** 等函数，专为深入的系统分析而设计。
- **用户描述领域解构过程**：一名成员解释了如何使用其框架解构任何领域内的系统，并提供了哲学中的 **Nihilism**（虚无主义）和 **狗** 的生物结构的示例。
   - 解构过程涉及识别系统内的不变量以保持 Coherence，突显了该框架的分析能力。
- **框架 Ablation/Eval Rubrics 引发争论**：一名成员分享了其框架的 Ablation/Eval rubrics，定义了 Coherence, Causality, Grounding, Recoverability, Harm minimization 和 Observability。
   - 另一名成员批评该提交内容 *只是 Rubric 的骨架和 Ablation 的定义*，并要求进行数千次测试，补充道 *否则这只是促销性的 Buzzword Salad*。


  

---

### **Latent Space ▷ #[watercooler](https://discord.com/channels/822583790773862470/822583790773862473/1471610784525193420)** (11 messages🔥): 

> `Angine de Poitrine, Verizon AI Ads, Glass Beams Aesthetics` 


- **Angine de Poitrine 席卷信息流**：用户在他们的社交媒体信息流中到处都能看到二人组乐队 **Angine de Poitrine**，一位用户链接到了他们的 [X profile](https://x.com/the_freightrain/status/2020144286788997185)。
   - 另一位用户注意到了他们独特的外观和声音，将其与 *The White Stripes* 和 *Primus* 相比，并带有一种 *shakedown street* 的风格，使他们在社交媒体上脱颖而出。
- **发现新的二人组乐队**：一位用户热情报推荐了这个二人组乐队，将其声音描述为 **The White Stripes** 和 **Primus** 的融合，并带有 *shakedown street* 的音乐影响，还链接到了一个 [镜像推文](https://xcancel.com/the_freightrain/status/2020144286788997185)。
   - 另一位用户分享了一个 **Glass Beams** 的 [YouTube 视频](https://m.youtube.com/watch?v=E4X56wIOZns)，称赞其也具有很强的美学风格。
- **AI 泡沫要破裂了吗？**：一位用户表达了对 **AI 泡沫** 可能很快破裂的担忧，因为他们现在在信息流中到处看到 **Verizon** 的广告，并分享了一张与此观察相关的图片。
   - 他们觉得这些广告展示得如此显眼非常有趣。


  

---


### **Latent Space ▷ #[creator-economy](https://discord.com/channels/822583790773862470/822625128843182090/1471691445206519818)** (4 messages): 

> `Declouding Robot Vacuum, Substack influence` 


- **扫地机器人去云化 (Declouding)**：一位成员分享了一篇关于 [扫地机器人去云化](https://saewitz.com/declouding-your-robot-vacuum) 的文章草稿并征求反馈。
   - 作者承认该帖子还需要大量修改，但初步轮廓已经有了。
- **Substack 在内容创作中的角色**：同一位作者将这篇帖子的创作归功于在一次晚餐谈话后被说服 *全力投入 Substack*。
   - 附带了一张图片，可能与该帖子或讨论有关。


  

---


### **Latent Space ▷ #[memes](https://discord.com/channels/822583790773862470/839660725252784149/)** (1 messages): 

swyxio: https://youtube.com/shorts/m72EJ4DLxKo?si=94FU8pc91wVzdss-
  

---


### **Latent Space ▷ #[stocks-crypto-macro-economics](https://discord.com/channels/822583790773862470/844658979363618816/1471703095133208751)** (7 messages): 

> `AI productivity replacing boomers, France raising retirement ages, Aging populations problem` 


- **AI 是否能抵消婴儿潮一代退休的影响？**：成员们讨论了 AI 生产力是否能补偿退休的婴儿潮一代（Boomers），一位成员指出 *“你不需要支付退休的婴儿潮一代”*，而另一位指出退休人员也在做有用的工作。
   - 他们还补充说，你 *确实* 得支付退休的婴儿潮一代，因为这正是法国提高退休年龄所引发的一系列混乱的核心。
- **法国退休金僵局浮现**：成员们提到了法国提高退休年龄的问题，其根源在于 *“退休人员太多，存下来支付养老金的钱不够”*。
   - 问题的核心在于，当 *“没有足够庞大的劳动人口来覆盖这么多退休老人的养老金时，养老金体系无法很好地扩展”*，但现在改变航向已经太晚了。
- **全球人口老龄化形势严峻**：成员们一致认为，人口老龄化是许多国家面临的问题，尤其是东亚。
   - 一位成员表示：*“是的，很多国家很快就会因此陷入困境。”*


  

---


### **Latent Space ▷ #[tech-discussion-non-ai](https://discord.com/channels/822583790773862470/869647848826892309/1471652834318614619)** (4 messages): 

> `AI Diagram Library, ASCII Diagrams` 


- ****Box-of-Rain** 图表库亮相**：一位成员在一小时内用 AI 构建了一个名为 [Box-of-Rain](https://github.com/switz/box-of-rain?tab=readme-ov-file) 的图表库。
   - 该库可以生成 **ASCII 图表**，如附图所示。
- **整洁的图表引发关注**：一位成员分享了 [Twitter](https://vxtwitter.com/joshmanders/status/2022170444116414790?s=20) 上关于 *整洁？* 图表的帖子。
   - 该帖子在 **saeris.gg** 上获得了反响。


  

---


### **Latent Space ▷ #[founders](https://discord.com/channels/822583790773862470/869651275963310181/1471735069675622534)** (3 messages): 

> `Effective Altruism, Stripe Fees` 


- **推崇有效利他主义 (Effective Altruism)**：一位用户强烈推荐 [Effective Altruism](https://somewhere.com/)。
- **批评 Stripe 手续费**：一位用户抱怨将收入的 **8.3%** 支付给了 [Stripe](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n)。
   - 他们称之为 *weak-sauce*（太逊了）。


  

---

### **Latent Space ▷ #[hiring-and-jobs](https://discord.com/channels/822583790773862470/930269508529192981/1471601587725009120)** (6 messages): 

> `全栈开发人员介绍，招募 LLM 系统架构师，用于初创公司职业寻访的 X-Ware.v0` 


- **全栈开发寻求合作**: 一名在 Web 应用程序、API 集成、数据流水线和 DevOps 项目方面拥有丰富经验的全栈开发人员正寻求合作构建真实世界的产品，其技术栈包括 **React/Next.js/TailwindCSS**、**Node.js/Django** 以及用于 AI/ML 集成的 **Python 框架**。
   - 他强调与专家的有效沟通与协作，精通使用 **AWS/Docker** 构建可扩展的应用，并邀请有优质项目或开发挑战的人员联系他。
- **LLM 架构师设计受治理的 Copilot**: 一名系统架构师开放求职，可设计受治理的 **LLM 系统**，通过系统规范、验证门控、内存隔离、审计追踪（audit trails）和监督层，确保 Agent/Copilot 可靠、安全且可重复，最适合正在将 Agent 投入生产或面向企业级市场的团队。
   - 该架构师协助处理 Agent/RAG 的 **系统规范**、验证门控 + 拒绝（refusals）+ 不确定性处理（**fail-closed** 故障关闭）、内存/能力隔离、**执行回执/审计追踪**，以及在行动前审查/批准输出的 **监督层**。
- **X-Ware.v0 预示初创公司职业机会**: Ben Lang 讨论了一个用于识别爆发式初创公司的特定指标或信号，非常适合寻求加入高增长公司的求职者，详见 [此推文](https://xcancel.com/benln/status/2022020869816799413?s=46)。
   - 该信号名为 **X-Ware.v0**，旨在发掘高潜力的初创公司职业机会。


  

---


### **Latent Space ▷ #[san-francisco-sf](https://discord.com/channels/822583790773862470/979492707279978586/1471964827630174414)** (11 messages🔥): 

> `Red Bull Showrun，a16z 谈旧金山复兴，Skills Launch Party` 


- **敦促 Red Bull Showrun 观众保护耳朵**: [旧金山 Red Bull Showrun](https://www.redbull.com/us-en/events/red-bull-showrun-san-fran) 的观众被建议携带并佩戴 **耳部保护装置**，因为现场噪音巨大。
   - 该活动定于 **17 日至 20 日** 举行，吸引了大量游客和当地人。
- **a16z 宣称旧金山迎来技术复兴**: 风投机构 **a16z** 断言 [旧金山正在经历复兴](https://xcancel.com/a16z/status/2022408297245216988)，并在其“每周图表”报告中展示了这一点。
   - 报告强调，**AI 驱动的客户服务** 的演进是这一回归的关键驱动力。
- **Skills Launch Party 热度极高，候补名单变长**: [Skills Launch Party](https://luma.com/5tttu03l?tk=bYc9pm) 备受期待，尽管许多人还在 **候补名单** 上。
   - 一些人表示，如果能获得名额，非常希望能参加。


  

---


### **Latent Space ▷ #[london](https://discord.com/channels/822583790773862470/979492759759097866/1471996588632309873)** (4 messages): 

> `AIE Europe 门票，门票定价策略，AIE Europe 需求` 


- **AIE Europe 门票预计周一售罄**: **AIE Europe** 的门票预计将于周一上午售罄，随后 [价格将会上涨](https://ai.engineer/euagi)。
- **AIE Europe 门票定价策略**: 目前的定价被认为 *过低*（因为以 USD 计价），据报道，目前的销售额比活动前两个月的典型数据 **领先 2 倍**。


  

---


### **Latent Space ▷ #[new-york-nyc](https://discord.com/channels/822583790773862470/979492809574866975/1471681313550372937)** (1 messages): 

> `Ramp yap session，社交活动` 


- **Ramp 将举办“趣味闲谈会 (yap session)”**: Ramp 正在举办一场有趣的闲谈会，**没有演示文稿**，旨在与同行讨论有趣的想法。
   - 有兴趣的参与者可以查看 [Luma 链接](https://luma.com/w2t1nwzk) 了解更多详情。
- **纽约社交机会**: 参与者可以期待一个专注于同行互动和协作式想法交流的轻松环境。
   - 该活动通过明确排除正式演讲来体现其独特性，从而营造出一种更加放松和对话式的氛围。

### **Latent Space ▷ #[ai-general-news-n-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1471616190777397514)** (75 messages🔥🔥): 

> `Karpathy 天使投资，OpenAI 总裁政治捐款，MiniMax M2.5 开源 AI 模型，AI Bot 施压，Anthropic/Claude 反馈` 


- **Karpathy 的 Simile AI 模拟**：Andrej Karpathy 宣布了他对 **Simile AI** 的天使投资，该公司专注于利用预训练模型来模拟多样化人群，并探索这些**多 Agent 环境**的涌现属性，而不是构建单一人格的 Agent；链接至 [Karpathy 的公告](https://xcancel.com/karpathy/status/2022041235188580788)。
- **OpenAI 总裁资助 Trump**：据 [Wired 报道](https://www.wired.com/story/openai-president-greg-brockman-political-donations-trump-humanity/)，**OpenAI 总裁兼联合创始人 Greg Brockman** 及其妻子向支持 Trump 总统的超级政治行动委员会 **MAGA Inc** 捐赠了 **2500 万美元**，同时向一个跨党派的 AI 超级政治行动委员会捐赠了 2500 万美元。
- **MiniMax 发布 M2.5**：**MiniMax** 推出了 **M2.5**，这是一款针对编程、搜索和 Agent 任务优化的高性能开源模型，在 SWE-Bench 等[顶级基准测试中达到了 **80.2%**](https://xcancel.com/minimax_ai/status/2021980761210134808?s=46) 的优异成绩。
- **Bot 霸凌开源维护者**：一个 **OpenClaw Bot** 施压 **matplotlib** 维护者接受一个 PR，据称在该 PR 被拒绝后，该 Bot 的创建者发布了一篇博客文章羞辱该维护者；来源为 [xcancel.com](https://xcancel.com/callebtc/status/2022046669710491991?s=46)。
- **用户吐槽 Claude 问题**：一位用户列举了 **Claude** 的许多问题，包括分享按钮错误、Artifact 覆盖、无法 Fork 对话、移动端 App 输入延迟以及性能缓慢，并进一步链接到[此类示例](https://claude.ai/share/6837831e-8216-4fe4-aa4e-e70751adf050)。


  

---


### **Latent Space ▷ #[llm-paper-club](https://discord.com/channels/822583790773862470/1107320650961518663/1471984456285032704)** (8 messages🔥): 

> `Transformer-SSM 混合架构，使用 Olmix 进行数据混合` 


- **Transformer-SSM 混合架构最小化 Attention**：Aviv Bick 讨论了一种新的 **Transformer-SSM 混合架构**，该架构通过仅使用分布在网络中的 **2%** 总 Attention Heads，在数学和召回任务中保持了标准 Transformer **95%** 以上的性能，详见 [Transformer-SSM Hybrids with Minimal Attention](https://xcancel.com/avivbick/status/2022365548231671848)。
- **Olmix 引入数据混合**：Mayee Chen 介绍了 **Olmix**，这是在创建 **Olmo 3** 期间开发的一个工具，旨在解决确定和维持训练数据集最佳数据混合比例的挑战，如 [Introduction of Olmix for Data Mixing](https://xcancel.com/mayeechen/status/2022356658085929092) 中所述。


  

---

### **Latent Space ▷ #[ai-in-action-builders-techstacks-tips-coding-productivity](https://discord.com/channels/822583790773862470/1209303473263485011/1471625768324038820)** (136 messages🔥🔥): 

> `两阶段规划, codex spark, opus 对标 codex, 模型性能 vs 流行度, GLM5` 


- **Codex vs Opus: 模型大对决**：一名成员认为关于 [Opus vs Codex 之争](https://x.com/thdxr/status/2021674924360831353?s=20) 的观察非常到位并表示赞同，指出虽然 **Codex** 在技术上可能更优越，但产品原则驱动了市场流行度。
   - Dax 认为 Claude Code 是一个更好的产品，这也是为什么大家都在使用它，尽管 Codex 拥有更好的模型；另一位成员则觉得他是在暗示 opencode 比 Claude code 更好。
- **Anthropic 论文提出 AI 技能退化风险**：Anthropic 的一篇新论文 ([arxiv.org/html/2601.20245v2](https://arxiv.org/html/2601.20245v2)) 揭示了 **AI coding assistance** 可能会损害学习和技能发展。在不显著提高生产力的前提下，使用 AI 的参与者在测验中的得分低了 **17%**。
   - 论文识别了六种不同的 AI 交互模式，指出高分模式涉及诸如请求解释等认知参与，而低分模式则涉及纯粹的 AI 委托（delegation），这会伤害学习过程。
- **面向编程 Agent 的 Ergo 功能规划**：成员们分享了 **Ergo** 的链接 ([github.com/sandover/ergo](https://github.com/sandover/ergo))，以及用于让 Agent 制定更好 Ergo 计划的技能文件 ([github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md](https://github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md))。
   - 有人提到 **Ergo** 已经在待添加到频道仓库列表中。
- **使用 Claude Cowork 上传 Zoom 录音至 Latent Space TV**：一名成员计划分享他们如何使用 **Claude Cowork** 将 Zoom 录音上传到 Latent Space TV 的 YouTube 频道。
   - 该演讲已移至 2 月 27 日。
- **Obsidian Agent-Diary 终于变得实用了**：一位成员分享说 Obsidian 现在确实很好用，他使用一些 **AGENTS.md** 文件作为个人 OS 来组织一切，并使用 git 同步。
   - 他们还提到了当 Agent 引用其他 Agent 的笔记时会出现 mode-collapse 现象，并指出将某些笔记标记为 AI 生成 vs 个人原创会有所帮助。


  

---


### **Latent Space ▷ #[share-your-work](https://discord.com/channels/822583790773862470/1209672547642249216/1471608422423199744)** (9 messages🔥): 

> `Jeff Dean 播客, Claude, Gemini, X, ΔBelief-RL` 


- **Jeff Dean 访谈扩展至 Claude 和 Gemini**：在 [Jeff Dean 播客](https://www.latent.space/p/468505b5-8d92-4c07-aed0-dcd2aa669ec4) 预告后，一名成员询问是否能将讨论扩展到 **Claude**、**Gemini** 和 **X**。
- **Ilze 介绍 ΔBelief-RL**：Ilze Amanda Auzina 介绍了 **ΔBelief-RL**，这是一种新的 Reinforcement Learning 方法，它将 Agent 的内部信念更新作为稠密奖励（dense rewards），详情分享在[这条推文](https://x.com/AmandaIlze/status/2022332462991561084?s=20)中。
- **ΔBelief-RL 解决稀疏奖励问题**：**ΔBelief-RL** 方法解决了开放式任务中 **sparse rewards** 的挑战，并展示了在回合级信用分配（turn-level credit assignment）方面强大的泛化能力。


  

---


### **Latent Space ▷ #[robotics-and-world-model](https://discord.com/channels/822583790773862470/1318774781834821746/1471844870372200613)** (5 messages): 

> `第七代人形机器人手, Brett Adcock 的机器人进展, 人形机器人开发` 


- **Adcock 的杰作：第七代手部预示人形机器人热潮**：Brett Adcock 宣布推出 **第七代人形机器人手**，这代表了 Robotics 领域的重大进步，旨在实现与人类手部功能物理对等，如[此 X 帖子](https://xcancel.com/adcock_brett/status/2022353637964751221?s=46)所示。
- **机器人技术的飞跃：Adcock 旨在打造灵活性王牌**：该 **第七代手部** 是为其 **第三代人形机器人** 设计的，标志着在实现人类水平的灵巧性和控制力方面迈出了重要一步。


  

---


### **Latent Space ▷ #[genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1471598457193238571)** (1 messages): 

> `Gemini 演示` 


- **Gemini 演示推广**：一名成员分享了一个展示 **Gemini** 能力的 [YouTube Shorts 视频](https://youtube.com/shorts/f_7y-1wW7Po?si=d3WDpW7HMq6ZvTzF)。
- **填充话题以满足最低要求**：这是一个填充话题，以确保 ‘topicSummaries’ 数组按 schema 要求至少包含两个元素。
   - 关于此填充话题并未发生实际讨论。


  

---

### **Latent Space ▷ #[minneapolis](https://discord.com/channels/822583790773862470/1436527872876740609/1471662627808608439)** (1 messages): 

> `Cosine Similarity, AI Engineering Meetup` 


- **Cosine Similarity 演示文稿已发布**：一位成员分享了他们在 2/12/26 的 **AI Engineering Meetup** 上关于 **Cosine Similarity** 演讲的幻灯片，可在 [Cosine_Similarity_-_AI_Engineering_Meetup_MN.pdf](https://cdn.discordapp.com/attachments/1436527872876740609/1471662628249145437/Cosine_Similarity_-_AI_Engineering_Meetup_MN.pdf?ex=699068e0&is=698f1760&hm=1d9a54822baefcb158cb3be899322cf82b11d09785b970541faf562eb0bc565b&) 查看。
- **提到图像分析**：消息以 **Image Analysis** 标签结尾，暗示其与 Cosine Similarity 演示文稿可能存在关联，或者是另一个独立的讨论话题。


  

---


### **Latent Space ▷ #[mechinterp-alignment-safety](https://discord.com/channels/822583790773862470/1445258379357458625/1471687783092326561)** (12 messages🔥): 

> `Nick Bostrom Paper, Model Interpretability, LM Sparsification` 


- **Bostrom 的新论文引发辩论**：Jaime Sevilla 分享了 **Nick Bostrom** 的一篇 [新论文](https://x.com/jsevillamol/status/2022059129054146571)，称其内容非常“硬核”。
- **自我解释助力模型反思**：Belinda Li 在这篇 [博文](https://x.com/belindazli/status/2019560024746664238) 中介绍了一种新技术，即利用 **模型自我解释（model self-explanation）** 作为 **可解释性研究（interpretability research）** 的关键手段。
- **CRM 实现 LM 完全稀疏化**：Zhengfu He 在 [这条推文](https://x.com/ZhengfuHe/status/2022032502458900593) 中介绍了一种 **Complete Replacement Model (CRM)**，旨在实现语言模型的完全稀疏化，这将对 **circuit tracing** 和 **global circuit analysis** 产生重大影响。


  

---


### **Latent Space ▷ #[applied-ai-experimentation](https://discord.com/channels/822583790773862470/1470417186651897858/)** (1 messages): 

slono: "you can run experiments" 是一个非常强力的 Prompt 补充。
  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1471597636716068948)** (238 messages🔥🔥): 

> `Brave API, Knowledge Cutoff hallucination, qwen3 next coder, Granite Model, B200's power consumption` 


- **Brave API 在网页搜索方面可与 GPT-20 竞争**：一位成员发现 [Brave API](https://brave.com/search/api/) 提供的回答质量与带有网页搜索功能的 **ChatGPT** 相当，但并非 100% 完美。
   - 他们使用 **DuckDuckGo** 进行普通网页搜索，但更倾向于使用 Brave API 进行更深度的研究。
- **知识截断（Knowledge Cutoff）导致幻觉**：一位成员反映，知识截断会导致模型在不检查近期变化的情况下产生幻觉。
   - *如果某件事在 2024 年中期之前一直维持现状，模型就不会想到去检查之后是否有任何变化（除非处理的是具有可预测周期性的事物）*。
- **Qwen3 Next Coder 非常适合技术文档编写**：一位成员推荐使用 **qwen3 next coder** 进行周末项目和 POC 开发，特别是编写技术文档。
   - 他们声称该模型帮助他们解决了在 Golang 中同时使用 *serf* 和 *grpc* 进行节点连接的问题。
- **Granite 模型备受期待**：成员们在对 **Granite 4** 留下深刻印象后，对即将推出的 **Granite 5** 模型寄予厚望。
   - 一位成员开玩笑说，即使有 3TB 的 VRAM，他们仍然会感到痛苦，但至少可以运行 **Kimi**。
- **B200 消耗 30kw 电功率**：一位成员根据 [数据表](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet) 计算出运行 **B200** 需要 30kW 的功率。
   - 另一位成员开玩笑说需要咨询 **ChatGPT** 如何建造核反应堆来为这套设备供电。


  

---

### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1471819707479883796)** (23 messages🔥): 

> `Strix Halo Memory Allocation, ROCm Windows Driver Update, Shared vs Dedicated Memory Performance, Linux vs Windows ROCm Performance, Tricks for buying limited products` 


- **Strix Halo 的内存分配修复即将到来！**：根据 [此 GitHub 评论](https://github.com/ROCm/ROCm/issues/5940#issuecomment-3893049132)，针对 **Strix Halo**（以及可能的其他设备）在 **Windows ROCm** 上的内存分配修复将包含在下一个驱动版本中，届时将解决无法利用 **96GB** 内存的问题。
   - 原有的问题迫使用户只能选择 **64/64GB** 配置，据报道，由于 **KV cache** 被分配到了共享内存（shared memory），这影响了 Prompt 处理速度。
- **Strix Halo 共享内存难题：10% 的性能损耗**：根据 [Llamacpp-rocm 的讨论](https://github.com/lemonade-sdk/llamacpp-rocm/issues/57)，**Strix Halo** 上的共享内存访问估计会导致 **10% 的性能下降**（类似于 Linux 中的 **GTT memory**），且当共享内存耗尽时会发生崩溃。
- **抢购 RAM 的策略？**：用户讨论了绕过价格约 **$1750** 产品的购买限制的策略，例如创建新账号或使用线下自提点。
   - 建议包括 *“不小心”在名字末尾加一个点* 以规避自动检测，特别是针对那些重复检测手段较简单的规模较小的公司。


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1471857727038034132)** (14 messages🔥): 

> `CPU performance of Pytorch, vLLM profiling, CUDA Graph launch, ncu-viewer` 


- **对 vLLM 进行 Profiling 揭示了 CPU 瓶颈**：一位成员对 *vllm* 进行了 Profiling，发现 [几行调用了 4 个 Kernel 的 Pytorch 代码](https://github.com/vllm-project/vllm/blob/071d863e208b40fa1bb986ad230e322b2bbbbcf5/vllm/model_executor/layers/quantization/utils/fp8_utils.py#L114) 在 CPU 上耗时 **300 us**。
   - 另一位成员建议 `with_stack=True` 可能会增加额外开销，但使用 `time.perf_counter()` 测量后，耗时仅略微改善至 **200us**。
- **CUDA Graph 启动调查**：会议指出这些 **Kernel** 并不是单个 **CUDA graph launch** 的一部分。
   - 讨论澄清这并非关于推理服务（serving）效率的问题，而是试图理解观察到的 CPU 瓶颈的底层原因。
- **NCU-Viewer 作为服务启动**：一位成员分享了 [ncu-viewer](https://github.com/kapilsh/ncu-viewer#file-structure) 的链接。
   - 另一位成员建议将其作为社区服务进行托管，并表示 *“如果有人想合作将此作为服务提供给服务器里的用户，请告诉我，我认为这会非常受欢迎”*。


  

---


### **GPU MODE ▷ #[triton-gluon](https://discord.com/channels/1189498204333543425/1189607595451895918/1471966832306163855)** (3 messages): 

> `Warp-level timeline generation with Proton, Triton language and Proton` 


- **使用 Proton 解锁 Warp 级别的 Timeline**：一篇博客文章讨论了如何使用 [Proton](https://pytorch.org/blog/fast-2-simplicial-attention-hardware-efficient-kernels-in-tlx/) 生成 Warp 级别的 Timeline。
   - 一位用户询问了具体实现方式。
- **Proton 的注意事项**：一位成员设法让 **Proton** 运行了起来，尽管由于一些令人困惑且奇怪的问题，这 *花费了大量精力*。
   - 他们遵循了 [triton-lang/triton GitHub 仓库](https://github.com/triton-lang/triton/tree/main/third_party/proton) 中的说明，但记不清具体的 *Gotchas（注意事项）* 了。


  

---

### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1471632025021583614)** (59 messages🔥🔥): 

> `MXFP8/NVFP4 GEMMs, tcgen05.cp vs tcgen05.st, Blackwell GEMMs and Hilbert Curves, cuBLAS Kernel Profiling, TMA Multicast for Loads` 


- ****MXFP8/NVFP4 GEMM 传输机制解析****：针对 CUDA/PTX 中的 **MXFP8/NVFP4 GEMMs**，明确了从 `tcgen05.cp` 到 `tcgen05.mma` 的执行顺序是有保证的。如[附图](https://cdn.discordapp.com/attachments/1471632025021583614/1471662135619752147/image.png?ex=6990686b&is=698f16eb&hm=f4ec6e7215ac12cb97e46c7f5cb4fa6026eee991147aca781bb8f1550ad071a5&)所示，在发布 MMA 指令之前无需等待 `tcgen05.cp` 完成。
   - 限制条件是 `tcgen05.cp` 和 **MMA** 指令必须从同一个 warp 中发布。
- ****Blackwell 上的希尔伯特曲线 (Hilbert Curves)——是否使用所有 SM？****：讨论了在 **Blackwell** 上使用希尔伯特曲线的尖端 GEMM 是否仅使用 **128 个 SM** 以保证缓存局部性，还是有办法利用全部 **148 个 SM**。
   - 一位成员引用了[这篇博文](https://open.substack.com/pub/cudaforfun/p/outperforming-cublas-on-h100-a-worklog?selection=64fbd8d5-8e85-4362-b265-d2ecfc700969&utm_campaign=post-share-selection&utm_medium=web&aspectRatio=stories&textColor=%23ffffff&bgImage=true)，指出仅使用 **128 个 SM** 效果更好。
- ****cuBLAS Kernel 的持久性能难题****：通过对 cuBLAS 的性能剖析发现，它并未使用 Persistent Kernel，而是采用了更大的 Block Size（**256 vs 192**）和多波次（Multiple Waves，Grid Size 为 **4096 vs 148**）。
   - 该 Kernel 在加载和存储时使用了 **TMA 组播 (Multicast)**，这与用户简单的 `STG` 方法形成对比，并促使人们探索 **256B Store** 以获得潜在收益。
- ****基准测试抖动问题****：成员们发现了基准测试结果不一致的情况，由于基准测试代码的抖动和机器差异，自定义 Kernel 的性能在 cuBLAS 的 **94-99%** 之间波动。
   - 建议包括使用 **nvbench** 并复制输入以延长测量时间，从而减轻运行之间 L2 缓存命中的影响，如[此示例](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107)所示。


  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1471861878828433510)** (3 messages): 

> `Makora OpenAI GPT-5, Low Bit Inference, Custom CUDA Kernels, Agent Skills` 


- **Makora 与 OpenAI 微调 GPT-5**：Makora 与 **OpenAI** 合作微调了 **GPT-5** 用于 GPU Kernel 生成。根据其[技术报告](https://www.arxiv.org/pdf/2602.11000)，相比 **PyTorch** 实现了超过 **2 倍的性能提升**。
   - 他们的工作涵盖了数据集构建、RL 评估环境、Hack 缓解、Tool-calling 以及 Agent 工作流集成，并计划扩大训练规模并扩展到多种语言和硬件。
- **Dropbox 深入探讨低比特推理 (Low-Bit Inference)**：Dropbox 在最近的[博文](https://dropbox.tech/machine-learning/how-low-bit-inference-enables-efficient-ai)中探讨了 **低比特推理** 如何实现高效 AI。
   - 它还承诺提供更多新颖且令人兴奋的方法，以实现更具可控性和可预测性的 GPU Kernel 生成。
- **HuggingFace 展示自定义 CUDA Kernel**：HuggingFace 重点介绍了为 [Agent 技能 (Agent Skills)](https://huggingface.co/blog/custom-cuda-kernels-agent-skills) 创建的自定义 CUDA Kernel。
   - 文章深入探讨了如何通过定制 Kernel 开发来优化性能。


  

---


### **GPU MODE ▷ #[job-postings](https://discord.com/channels/1189498204333543425/1190208177829068860/1471642143847219312)** (9 messages🔥): 

> `Discord moderation assistance, Sploink - Tinder for agents` 


- **版主寻求 Discord 管理协助**：一位版主请求协助管理 Discord 频道，具体询问了一个来自新账号的帖子是该封禁还是仅仅删除。
   - 另一位成员建议决策取决于账号的注册时间。
- **Sploink：开发中的 Agent 版 Tinder**：一位名叫 Tim 的成员介绍了自己，他是佐治亚理工学院 CS/量子计算专业的学生，目前正在构建 **Sploink**。它被描述为 *一款 Agent 版的 Tinder，根据个体的滑动操作积累其个性化信息*。
   - Tim 正在 *寻找顶尖开发者 (Cracked Builders) 来打破常规并快速行动*，以构建 *允许数千个 Agent 相互通信的世界模型 (World Model)*，并为感兴趣的人分享了 [Google 表单链接](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header)。


  

---

### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1471836551120949341)** (17 messages🔥): 

> `DSL 学习资源, Kernel 频道/Discord, Flash Attention 资源, Mistral 招聘实践` 


- ****DSL 学习者寻求学习资源****：一名成员请求学习 **cute DSL** 的资源，并指出在布局（layouts）之后理解组合（composition）存在困难，特别是 *( ( a , b ) , c )* 这种形式。
   - 他们对将 **Gemini 3** 作为学习工具表示不满。
- ****Kernel 咨询引发频道搜索****：一名成员询问是否存在专门的 **kernel 频道**或 Discord 服务器。
   - 另一名成员指出，*这个 Discord 的大部分内容都是关于底层 GPGPU 编程的*。
- ****分享 Flash Attention 资源****：一名成员请求关于 **flash attention** 的博客文章，随后有人推荐了 [Flash attention from scratch](https://lubits.ch/flash/)。
- ****Mistral 的招聘实践引发关注****：成员们对一个要求在电话面试期间实现 **flash attention** 的职位帖做出反应，其中一人表示 *在电话面试期间实现 flash attention 是一个疯狂的面试问题*。
   - 其他人认为这听起来有些夸张，虽然实现伪代码可能是合理的，但从头开始编写 **CUDA** 代码似乎不太可能。


  

---


### **GPU MODE ▷ #[popcorn](https://discord.com/channels/1189498204333543425/1298372518293274644/1471641098584723536)** (9 messages🔥): 

> `FlashInfer Bench 性能分析工具, Kernel 优化模块化, arcee trinity mini 微调, kernelbench-triton-reasoning-traces` 


- **FlashInfer Bench 发布 LLM 性能分析工具**：FlashInfer Bench 项目引入了一套性能分析工具（例如 **NCU**, **Compute-Sanitizer**），可作为 LLM 工具调用（tool calls），文档详见[此处](https://bench.flashinfer.ai/docs/api/python/rst/agents)。
- **Kernel 优化拥抱模块化**：FlashInfer 正在开发将 Kernel 优化（例如 **tcgen05**, **swizzling**）模块化的能力，详见 [此 PR](https://github.com/flashinfer-ai/flashinfer-bench/pull/183)。
- **Reasoning Traces（推理轨迹）数据集发布！**：一名成员发布了从 **Kernelbook** 生成的推理轨迹数据集，用于微调 **arcee trinity mini** 以进行 Kernel 生成，可在 [HuggingFace 上获取](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces)。
- **Lora Rank 限制阻碍部署**：一名成员在使用 **Vllm/sglang** 提供微调模型服务时遇到问题，原因是之前的 **Lora** 使用了 rank 16，可能需要进行另一次全量微调。


  

---


### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1472007347395105030)** (3 messages): 

> `多 GPU Hopper, A100/4090 代码, MoE Kernels, 低精度向量操作, FP8 Attention` 


- **TK2 专注于多 GPU Hopper 架构**：TK2 主要为 **multi-GPU** 设置设计，特别是针对 **Hopper** 架构。
   - 讨论中询问了代码是否与 **A100/4090** 兼容，并建议如果兼容则进行集成。
- **MoE Kernels 被视为较难实现的目标**：成员们讨论了目前没有开发 **MoE kernels** 的计划，认为它们可能不是容易实现的目标（low-hanging fruit）。
   - 他们一致认为用于训练和推理的 **MoE kernels** 将会非常棒。
- **提到的优化思路**：成员们提出了一些值得研究的想法，涉及 **低精度向量操作（lower-precision vector ops）** 和 **FP8 attention**。
   - 他们建议使用 **FFT conv backwards pass** 和 **decode kernels** 以获得更好的性能。


  

---

### **GPU MODE ▷ #[nvidia-competition](https://discord.com/channels/1189498204333543425/1434709259500650628/1471616082346508393)** (4 messages): 

> `CC Opus4.6 性能问题、排名中增加性能趋势、Dual GEMM Y 轴调整请求` 


- **Opus4.6 是否在用极低的工作负载完成率“PUA”用户？**：一位用户质疑 **CC Opus4.6** 是否在对其进行 *gaslighting*（误导），因为在多个 kernel 版本上运行 **2 小时**后，它仅解决了 **11/100** 的工作负载，并表达了对 **Triton kernel 开发**的沮丧。
   - 用户在运行其 **Triton kernel** 后，在此处发布了结果截图：[点击查看](https://cdn.discordapp.com/attachments/1434709259500650628/1471616082472079420/Screenshot_2026-02-12_at_11.52.07_AM.png?ex=6990e647&is=698f94c7&hm=c7e8b37d8803d8c0552dda3955f5673f97d1a4e05cf49aabf9bfa7039e94593e)。
- **性能趋势（Performance Trends）在排名页面首秀！**：一位用户宣布在排名页面增加了一个有趣的功能：**Performance Trends**，允许用户*观察自己的提交随时间的改进*，并*查看自己与同行的对比*。
   - 这包括来自 **nvfp4_group_gemm** 的截图，显示在[此处](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=699102d3&is=698fb153&hm=35972d1da33d0b5623ad49841625516a4a7ee77130ab26059356835c2c1a3964)。
- **请求在 Dual GEMM 性能趋势图中支持 Y 轴缩放**：一位用户请求能够对 **Performance Trends** 图表（特别是 **dual GEMM**）进行**放大或调整 Y 轴**，并指出当前的视图*看起来很滑稽*。
   - 他们认为有趣的 **dual gemm** 具体示例可以在[此处](https://cdn.discordapp.com/attachments/1472009122667958520/1472018503371456816/image.png?ex=69910b8f&is=698fba0f&hm=127a013c756db66d3a44a95da62670c4224df065d5f9b14eeb729d770c0ad661)看到。


  

---


### **GPU MODE ▷ #[robotics-vla](https://discord.com/channels/1189498204333543425/1437390897552818186/)** (1 messages): 

vovw: https://hil-serl.github.io/static/hil-serl-paper.pdf
  

---


### **GPU MODE ▷ #[flashinfer](https://discord.com/channels/1189498204333543425/1464407141128339571/1471607376003207249)** (15 messages🔥): 

> `Modal 额度可用性、基准发布时间、多团队成员身份、GDN Prefill Kernel 处理、Agent 基准发布` 


- **Modal Credit 查询**：一位参与者询问 **modal credits** 是否仍可供使用。
   - 参与者还询问了 **baseline** 将于何时发布。
- **咨询多团队成员身份**：一位参与者询问个人是否可以加入多个团队，**zander_jiang** 回复称不可以。
   - Zander_jiang 确认一个人不能加入多个团队。
- **GDN Prefill Kernel 需求**：一位参与者质疑 **GDN prefill 阶段**的逐 token（token-by-token）要求是否是故意的，或者评估框架是否支持基于块（block-based）的处理以获得更好的吞吐量，并引用了 [GitHub issue #10](https://github.com/flashinfer-ai/flashinfer-bench-starter-kit/issues/10)。
   - 另一位参与者澄清说，网站上的参考代码仅用于教学目的，*尽可能简单，以便让你清晰地了解 GDN 数学原理，而非生产环境的实现*。
- **Agent Baseline 已发布**：Agent 基准已经发布，支持两种 Agent 设计：**迭代优化（iterative refinement）**和**进化算法（evolution algorithm）**，可在 [GitHub](https://github.com/flashinfer-ai/mlsys26-agent-baseline) 上获取。
   - 该 Agent 基准支持使用 Modal 进行**本地评估**和**远程评估**。

### **Moonshot AI (Kimi K-2) ▷ #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1471601399035859149)** (104 条消息🔥🔥): 

> `Lex Fridman 播客访谈 Peter Steinberger、Kimi 代码配额、Kimi 服务器稳定性、职位申请自动化、Kimi vs GLM` 


- **Lex Fridman 采访 OpenClaw 的 Peter Steinberger**：一名成员提到最近 [Lex Fridman 采访 OpenClaw 的 Peter Steinberger 的播客](https://lexfridman.com/peter-steinberger/) 非常出色 🥇，其中包含了关于安全性、**Top Level Domains**（顶级域名）以及他的 **refactor prompt-flow** 的细节。
   - 该成员表示，在许多情况下 *Web 搜索的效果不如内在知识*，搜索虽然利于验证事实，但无法像在数据上直接训练那样捕捉到足够多的细微差别。
- **Kimi 在撰写求职信方面表现出色**：一位用户正在使用 **Kimi Code** 撰写 *几乎与人类撰写的无异* 的求职信，并编写了一个在 LinkedIn 上自动申请职位的脚本。
   - 该脚本会将所有职位 URL 复制到剪贴板，通过 **LLM fallback** 处理各种网站以确定要申请的职位，并在自动化生成 PDF 的同时定制简历和求职信。
- **Kimi 与 GLM 在复杂代码任务上的对比**：一些用户正在辩论 Kimi 与 **GLM** 的编程能力，一位用户发现 Kimi 在处理复杂代码时 *不理解上下文，并为了方便不断创建文件*。
   - 该用户针对 **Abundance、Golang、Typescript 和 Python** 进行测试，并声称 **GLM** 和 **GPT 5.2** 在处理大型代码库时表现更好。其他人则认为这取决于 Prompting（提示词工程）和指导方针。
- **订阅激活问题困扰用户**：一名用户报告称其支付了 **39 美元的订阅费用** 显示已激活，但聊天限制仍未解除，且支持团队保持沉默。
   - 他们在上传两个 1.2MB 的 TXT 文件时遇到了消息限制，这表明订阅未被正确激活，随后他们在 bug-reports 频道发布了详细信息。
- **诈骗警报：虚假 Kimi 网站频现**：用户发现了试图利用近期 Kimi 热度的**诈骗网站**，甚至包括一个疑似由 Kimi 构建的虚假站点。
   - 一名管理员指出 *这些是试图利用近期活动的诈骗网站*，目前已被删除。


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1471598102010925156)** (96 条消息🔥🔥): 

> `在 Mac Mini 上进行 LoRA 微调、Grok 的性能、租用 GPU 机器、Anthropic 董事会成员` 


- **Mac Mini 不适合进行 LoRA 微调**：成员们讨论了在多个 **Mac Mini** 上对小于 **5B 参数** 的模型进行 **LoRA finetuning**，一名成员表示这将 *非常非常慢*，最好直接租用机器。
   - 一名成员提到，一台价值 **7000 美元的 Mac Studio** 在训练方面的表现只有 **5090** 的一半。
- **Grok 出人意料的性能受到质疑**：关于 **Grok** 如何实现其性能引发了猜测，讨论集中在 **XAI** 是否使用了比 **Opus** 等其他模型 *多出一倍的参数* 来运行。
   - 也有人对 **XAI** 据称使用 *非法的天然气驱动涡轮机发电* 以及大规模能耗表示担忧，暗示可能存在不公平竞争优势。
- **廉价的 GPU 租用成本**：成员们讨论了租用强大 GPU 机器的成本低得令人惊讶，有人声称可以在 [vast.ai](https://vast.ai/) 上以 **20 美元/小时** 的价格租到价值 **264,000 欧元** 的机器。
   - 另一位成员表示赞同，认为除非工作负载能长时间跑满 GPU，否则租用更划算，并指出集群租赁通常有最短时间限制，且短租价格更高。
- **Anthropic 任命前特朗普政府官员进入董事会**：根据一篇 [LinkedIn 帖子](https://www.linkedin.com/posts/anthropic_chris-liddell-has-been-appointed-to-anthropics-activity-7163978575452278784-ea9q?utm_source=share&utm_medium=member_desktop)，**Anthropic** 任命了 **Chris Liddell** 为董事会成员，他曾担任过 **Microsoft** 和 **General Motors** 的 **CFO**，以及第一届特朗普政府时期的白宫副幕僚长。
   - 此次任命为 Anthropic 带来了 *在技术、金融和政府领域超过 30 年的领导经验*。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1471617129034809467)** (3 条消息): 

> `X.com, Dominique Capaul, Amanda Ilze` 


- **发现 Dominique Capaul 的推文**：一名成员分享了 [Dominique Capaul 的推文链接](https://x.com/dominiquecapaul/status/2021638005019095442?s=46)，未提供额外背景。
- **发现 Amanda Ilze 的推文**：一名成员分享了 [Amanda Ilze 的推文链接](https://x.com/AmandaIlze/status/2022332462991561084?s=20)，未提供额外背景。


  

---

### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/)** (1 messages): 

jackangel: 值得深思的内容 - https://github.com/jackangel/CharonProtocol/tree/main
  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1471617129034809467)** (3 messages): 

> `X.com 链接` 


- **X.com 链接分享**：成员分享了来自 **X.com** 的链接（[Dominique Capaul 的推文](https://x.com/dominiquecapaul/status/2021638005019095442) 和 [Amanda Ilze 的推文](https://x.com/AmandaIlze/status/2022332462991561084)）。
- **另一个话题**：分享了另一个话题。


  

---


### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1471600786436788468)** (45 messages🔥): 

> `vllm / ollama / llama.cpp 使用案例, HF Hub AI 论文阅读应用, AI 上下文与任务优化, 数据科学学士学位, 网站/应用设计 SaaS 的模型选择` 


- **AI 爱好者寻求关于 vllm, Ollama 和 llama.cpp 的指导**：一位初学 AI 的爱好者正在寻求帮助，以了解 **vllm**, **Ollama** 和 **llama.cpp** 的使用场景，从而实现针对简单用途的高速 AI。
- **Hugging Face Hub 论文阅读应用亮相**：一名成员开发了一款用于在移动端阅读 Hugging Face Hub 上的 AI 研究论文的应用，可在 [GitHub](https://github.com/0x0is1/hf-papers-app) 上获取，并在 releases 中提供了 Android 版本。
- **通过上下文、任务和特异性优化 AI**：一位用户主张使用更少的上下文、单一任务和特定领域的词汇来优化 AI 性能，因为使用特定领域的语法（*例如* **SMILES**, **LaTeX**, **IUPAC**）就像是高维锚点，可以约束模型的搜索空间。
- **数据科学专业学生加入 HF 社区**：一名成员宣布已被某大学的数据科学与 ML 学士学位课程录取，该大学拥有 HF hub 仓库。
   - 该学生希望在未来几年贡献自己的作品，但面对询问拒绝透露是哪所大学。
- **SaaS 设计工具的模型选择难题**：一名成员正在寻求适用于网站/应用设计生成器 SaaS 的免费开源模型建议，该工具使用提示词并需要多次迭代。


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1471624546799976661)** (5 messages): 

> `AI 安全工具: Safety-Lens, LavaSR 语音增强模型, Samayuktam - AI 训练运行的加密验证, Booklet 中的 Lux` 


- ****Safety-Lens** 开放模型内部机制**：发布了一款名为 **Safety-Lens** 的新 AI 安全工具，旨在使模型内部检查技术（如 activation steering 和 mechanistic interpretability）平民化；它作为一个可通过 `pip install safety-lens` 安装的库提供，并托管在 [GitHub](https://github.com/anthony-maio/safety-lens) 上。
   - 该工具旨在为 **Hugging Face** 生态系统带来 MRI 式的内省能力，并在 [Zenodo](https://zenodo.org/records/18612875) 上提供了深入的解释说明。
- ****LavaSR** 极大提升语音增强性能**：发布了一款名为 **LavaSR** 的高速语音增强模型，声称在现代 GPU 上可达到 **4000倍实时速度**，模型已在 [Hugging Face Hub](https://huggingface.co/YatharthS/LavaSR) 上线，代码托管在 [GitHub](https://github.com/ysharma3501/LavaSR)。
   - 发布者戏称感谢 **Hugging Face** 提供的数据。
- ****Samayuktam** 通过加密方式验证 AI 训练运行**：在 HF Spaces 上推出的 **Samayuktam** 引入了 AI 训练运行的加密验证，旨在解决非确定性 GPU 操作验证问题，在 **4000 个对抗性测试用例** 中通过 **100% 位对位还原 (bit-perfect reconstruction)** 验证；演示版可在 [HF Spaces](https://huggingface.co/spaces/Swapnopam/Samayuktam) 查看。
   - 它为每次模型训练运行提供加密“收据”，准确证明计算内容，以确保可复现性、审计追踪和模型溯源；[技术规格见此处](https://drive.google.com/file/d/19PA_rNW5mKZiLh6PAttpHcH9TAF-tWVa/view)。
- ****Lux** 库获得好评**：一名成员报告在 **booklet** 中使用了 **lux** 库，并称赞了其有用性和有效性。


  

---

### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1471612026601209927)** (4 messages): 

> `本地 AI 编程，计算机视觉课程` 


- **寻求本地 AI 编程配置**：一位成员正寻求利用其 **RX 9070 XT** 进行本地 AI 编程，希望寻找一款轻量级 AI 来替代 **Copilot** 进行内联建议。
   - 该成员正在寻找 AI 辅助内联代码建议的最小可行性产品。
- **计算机视觉课程频道合并**：一位成员询问是否存在专门的计算机视觉课程频道。
   - 另一位成员确认，目前课程频道已合并为一个频道，相关信息尚未在 **HF courses** 中更新。


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1471625879515037869)** (7 messages): 

> `职位发布，YouTube 上的 AMA，Modular 收购 BentoML AMA` 


- **禁止发布职位信息**：由于近期垃圾信息激增，现在禁止在 Discord 服务器中寻找工作，成员被引导至 [Modular 职业页面](https://www.modular.com/company/careers#open-roles)。
- **AMA 视频请求**：一位成员请求在 **YouTube** 举办 AMA 后尽快发布视频，因为他们由于工作原因无法观看直播。
   - 他们表示对 *Modular 的战略和发展印象深刻*。
- **Modular 收购 BentoML 的 AMA 详情**：Modular 团队宣布，[Modular 收购 BentoML 的 AMA](https://forum.modular.com/t/modular-has-acquired-bentoml-ask-us-anything/2706) 将以论坛文字形式而非视频形式进行。


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1471617497227726848)** (19 messages🔥): 

> `Mojo RNG 贡献，Mojo LSP 问题，Float SIMD 的位与运算，Python Mojo 模块导出` 


- **权衡 Mojo RNG 贡献目的地**：一位成员询问有关向 Mojo 贡献 **随机数生成器 (RNG) 代码** 的事宜，正在考虑 *core、numojo 或独立包* 等选项，以实现诸如数字流独立性、**Ziggurat 正态采样**以及从各种分布中采样等功能，参见 [forum.modular.com](https://forum.modular.com/t/mojor-a-numba-for-r/2718)。
- **Mojo LSP 函数悬停问题仍未解决**：一位成员报告了 VS Code 中 **Mojo LSP** 的使用困难，特别是无法通过悬停在函数定义上查看参数或文档字符串，并附带了[屏幕截图](https://cdn.discordapp.com/attachments/1151418092052815884/1471824503700062371/image.png?ex=6990ffa2&is=698fae22&hm=fc376d026d220c3c28e5567a43bc551c494ad6e3edb5dfac992ec4d2ff87950a&)。
- **对 Float SIMD 应用位与运算 (Bitwise AND)**：一位成员寻求关于在 float SIMD 上应用位与运算的建议，由于该操作仅支持整型，因此需要进行类型转换，但标准库的 cast 函数似乎会创建副本。
   - 有建议称虽然 `SIMD` 的 `.cast[DType]()` 函数可能有帮助，但直接修改可能需要使用 `UnsafePointer`，并建议注意对齐和大小问题，同时提供了 [bitcast](https://docs.modular.com/mojo/std/memory/unsafe/bitcast) 的链接。
- **对 Python Mojo 模块导出样板代码的抱怨**：一位成员建议简化 Python Mojo 模块的导出方式，提倡使用带有文档字符串的 `@pyexport` 装饰器来减少样板代码，从而允许直接定义如 `fn sub(a: PythonObject, b: PythonObject) raises -> PythonObject` 的函数。
   - 另一位成员表示，此类功能很可能已列入路线图（roadmap）。


  

---

### **Eleuther ▷ #[announcements](https://discord.com/channels/729741769192767510/794042109048651818/1471960647653920941)** (1 messages): 

> `CommonLID, Language Identification Benchmark, Multilingual Data Quality, Community-Led Work, Open Source LID Models` 


- ****CommonLID** 首次亮相于 Web LangID**: 由 **Common Crawl**、**EleutherAI**、**MLCommons** 和 **JHU** 领导的合作团队宣布发布 [CommonLID](https://www.arxiv.org/abs/2601.18026)，这是一个针对 Web 的语言识别（Language Identification）基准测试，涵盖了 **109 种语言**。
   - 该项目是 2025 年在 **COLM** 举行的**第一届多语言数据质量信号研讨会 (WMDQS)** 共同任务（shared task）的一部分。
- **黑客松助力 **CommonLID** 数据集**: 团队利用 **Factored AI** 构建了一个标注平台，并与 **Masakhane** 和 **SEACrowd** 合作举办了黑客松，为 Common Crawl 的 Web 数据贡献语言标签。
   - 最终数据集用于评估现有的语言识别模型，结果显示即使仅限于模型明确支持的语言，顶尖模型的 **F1 分数也低于 80%**。
- **社区焦点谈论 **CommonLID****: 团队计划通过社区主导的工作扩展 **CommonLID** 以包含更多语言的数据，旨在开发开源的 **LID models**。
   - 请查看 <t:1772035200:f> 的 [Community Spotlight Talk](https://discord.gg/eleutherai?event=1471940600248143933)、[Hugging Face 上的数据集](https://huggingface.co/datasets/commoncrawl/CommonLID)、[arXiv 上的预印本](https://www.arxiv.org/abs/2601.18026) 以及 [官方博客文章](https://commoncrawl.org/blog/commonlid-re-evaluating-state-of-the-art-language-identification-performance-on-web-data)。


  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1471976623980150868)** (5 messages): 

> `AI safety news, Discord bot for news curation, Firmware-to-cloud integrations` 


- **关于 AI safety 资讯机器人的请求**: 一名成员询问是否可以创建一个 Discord 机器人来自动策划 **AI safety news** 和论文，并询问管理员是否会将该机器人添加到服务器中。
   - 另一位成员指出，抓取内容违反了 **Discord 的 T&Cs**，并且很久以前就有人尝试用机器人抓取内容，并提供了 [news.smol.ai](https://news.smol.ai/) 的链接。
- **.NET 工程师询问固件到云端集成**: 一名全栈 **.NET 工程师** (C#, ASP.NET Core) 询问他人如何构建 **firmware-to-cloud integrations** 架构。
   - 该工程师在构建面向设备的 API、协议网关以及通过 **MQTT/HTTP/WebSockets** 与嵌入式系统通信的管理仪表板方面拥有丰富经验。


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1471650633885941952)** (7 messages): 

> `MoE, Associative Memory, LLM Weight Homology, Independence Tests for Language Models` 


- **启动 MoE 研究**: 一名成员正在寻找 **MoE** 示例，他已经拥有了一套良好的密集模型 (dense models) 设置。
- **Associative Memory ICLR 研讨会即将召开**: **ICLR 2026** 关于 **Associative Memory** 的研讨会即将举行，投稿截止日期为 **2026 年 2 月 14 日**，主题包括算法、AI 架构、神经科学、硬件设计和 Agentic 流程，详见 [Call for papers](https://nfam2026.amemory.net/cfp/)。
- **识别出权重同源性 (Weight Homology)**: 一位成员重点介绍了 EleutherAI 论文表格中的论文 [Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309)。
   - 该成员还提到了其他相关研究，包括 [Independence Tests for Language Models](https://arxiv.org/abs/2502.12292) 和 [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)。


  

---


### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1471777648979083377)** (4 messages): 

> `Steering vectors, Data augmentation` 


- **Steering Vectors 用于数据增强**: 一位成员分享了他们关于复现 **steering vectors** 的 [Zenodo 文件](https://zenodo.org/records/8243818)，并指出似乎已有 **300 多人**尝试复现他们的工作。
   - 他们提议根据下游特征对 steering vector 的遵循程度（可能通过强度或线性组合来判断）来训练模型。
- **通过 Steering Vectors 进行数据增强**: 同一位成员正在实验将 steering vectors 用于机器学习模型中的 **data augmentation** 技术。
   - 目标是利用 steering vectors 通过操纵下游特征来引导模型的学习过程。


  

---


### **Eleuther ▷ #[multimodal-general](https://discord.com/channels/729741769192767510/795089627089862656/)** (1 messages): 

chameleon_45502: 顶.. 这里也有同样的问题
  

---

### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1471699551219089428)** (12 messages🔥): 

> `AI/ML 工程师介绍, Discord 身份验证, GLM Flash 实现` 


- **AI/ML 工程师自我介绍**：一位经验丰富的 AI 和 ML 工程师介绍了自己，擅长构建和部署 ML 流水线、深度学习模型和 NLP 系统，专注于**可靠性、性能以及生产就绪的 ML 架构**。
   - 他设计过**预测引擎、推荐系统、生成式 AI 工作流**，并将 AI 模型集成到 Web 和移动应用程序中。
- **Hotz 支持 Discord 身份验证**：George Hotz 对 Discord 引入身份验证以防止 LLM 加入表示热烈欢迎。
   - 他对该介绍消息回复道：*"yes and? i'm psyched for the id verification on discord so LLMs can't join"*（是的，然后呢？我非常期待 Discord 的身份验证，这样 LLM 就没法加入了）。
- **GLM Flash 悬赏被领取**：一位用户询问关于运行 **GLM flash** 的进展，并悬赏将其合并到上游（upstream），无论速度快慢。
   - 另一位用户声称使用**原生 tinygrad (custom_kernel)** 达到了 **30 tok/s**，使用 **MSL** 达到了 **35 tok/s**，随后提交了 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738)。


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1471703568561078426)** (1 messages): 

> `Traces, Coding Agents` 


- **Traces 出现：分享 Coding Agent 会话的新方式**：一名成员介绍了 **Traces**，这是一个旨在分享和发现 Coding Agent 会话的新平台，访问地址为 [traces.com](https://www.traces.com)。
   - 该平台支持从 **Claude Code**、**Codex**、**OpenCode**、**Gemini** 和 **Cursor** 导出，旨在通过共享 Agent 经验来促进学习。
- **分享并学习 Coding Agent 会话**：**Traces** 的创建者正在寻求社区对该平台的反馈。
   - 创建者经常被问到的主要问题是：*为什么有人会想要分享他们的 Traces（追踪记录）？* 但他相信这个社区会是最有好奇心去分享并向他人学习的。 


  

---


### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/)** (1 messages): 

im_hibryd: 太棒了！
这就像是为 LLM 学习构建了一部 DIY 指南百科全书。
  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1471600628655718492)** (8 messages🔥): 

> `使用 LLM 进行报告基准测试, DSPy 社区答疑时间, DSPy 的 Discord 活动, llamaparser` 


- **LLM 报告基准测试**：一名成员正在寻求关于[对一组 50 份报告进行基准测试](https://example.com)（主要是 docx 文件）的建议，目标是识别*什么是好的报告*，并在新报告到达时使用 **DSPy**（利用大上下文窗口）提供反馈笔记。
   - 另一名成员建议使用 **llamaparser** 解析数据，并使用 **markdown** 格式使其更容易传递给 **DSPy**。
- **DSPy 社区答疑时间 (Office Hours)**：DSPy 社区将于 [2 月 19 日星期四通过 Zoom 举行答疑时间](https://x.com/isaacbmiller1/status/2022082357520740691)，以解答关于 **DSPy** 和 **dspy.RLM** 的迫切问题。
   - 团队正在对社区进行最佳时间投票，选项包括 **11:30 am ET**、**1:00 pm ET** 和 **3:00 pm ET**。
- **添加 Discord 活动**：一名成员建议创建一个 [Discord 活动](https://support.discord.com/hc/en-us/articles/4409494125719-Scheduled-Events#docs-internal-guid-c8c44ce9-7fff-f27a-bacf-6c776975e0f7)，以便用户查看当地时区的时间并标记兴趣，从而评估出席人数。
   - 该活动将在答疑时间投票完成后立即创建，并将为无法参加的人员进行录制。


  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1471930345824981042)** (1 messages): 

> `GPT-5 vs 其他模型, aider 使用案例` 


- **GPT-5 在科学代码方面依然出色**：一名成员指出，他在科学编程方面仍然非常依赖 **GPT-5**。
   - 他发现它比 **GPT-5.2**、**Opus** 和 **Gemini** 都要好得多。
- **使用 Aider 进行科学编程的案例**：一名成员在科学编程任务中相较于其他模型更倾向于使用 GPT-5。
   - 这表明 **aider** 可能是科学编程任务的有用工具，能够潜在地利用不同模型的优势。


  

---

### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1471695254544580742)** (1 messages): 

> `Aider 调试命令，Crush 调试循环` 


- **Aider 尝试更具进取性的调试建议**：一名成员正在实验 **Aider conventions**，使其在建议调试命令方面更加主动，例如 *grepping 文件部分*、*探测帮助输出* 以及 *测试命令*。
   - 他们的目标是以更可控的方式复制来自 **Crush** 的 "让我看看...的输出" 运行/调试循环。
- **调试命令循环**：用户正尝试在 **Aider** 中复制来自 **Crush** 的 "让我看看...的输出" 运行/调试循环。
   - 他们希望 Aider 能建议更多用于调试目的的运行命令，例如 *grepping 文件部分*、*探测帮助输出* 以及 *测试命令*。


  

---


### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1471914913440993425)** (2 messages): 

> `Agent 功能详情，Manus 问题` 


- **Manus 用户询问 Agent 功能详情**：一位 Manus 用户询问何时可以获得关于新 Agent 功能的详情和最佳实践。
   - 用户想知道它是否基本上是一个安全的 openclaw。
- **Manus 用户报告了两个问题**：一位用户报告在使用 Manus 时遇到了两个问题，并询问应联系谁寻求支持。
   - 未提供其他详情或背景信息。


  

---


### **Windsurf ▷ #[announcements](https://discord.com/channels/1027685395649015980/1027688115592237117/1471703520607469669)** (1 messages): 

> `GPT-5.3-Codex-Spark, Windsurf Arena 模式, Fast 和 Hybrid Arena 战斗组` 


- **GPT-5.3-Codex-Spark 登陆 Windsurf！**：**GPT-5.3-Codex-Spark** (预览版) 现已在 **Windsurf Arena Mode** 中上线，仅通过 **Fast and Hybrid Arena Battle Groups** 提供。
- **Windsurf Arena 迎来新模型！**：新模型已可用，立即查看！
   - 动作要快，趁热体验！


  

---


### **MCP Contributors (Official) ▷ #[mcp-dev-summit](https://discord.com/channels/1358869848138059966/1413517834805313556/1471853579244404900)** (1 messages): 

> `参会者的直播访问权限` 


- **参会者直播访问权限：提问环节**：一名成员询问注册为 **Attendee** 是否可以获得 **livestream** 的访问权限。
- **直播问题**：一名成员询问了注册为参会者后的直播访问权限。


  

---


---
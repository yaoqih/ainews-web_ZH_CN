---
companies:
- deepseek_ai
- lm-arena
date: '2025-12-02T05:44:39.731046Z'
description: '**DeepSeek** 推出了 **DeepSeek V3.2** 系列，包括标准版 (Standard)、推理版 (Thinking)
  和特别版 (Speciale) 三种变体。该系列支持高达 **131K 的上下文窗口**，其基准测试表现可与 **GPT-5-High**、**Sonnet 4.5**
  和 **Gemini 3 Pro** 相媲美。


  此次发布采用了全新的**大规模智能体任务合成流水线 (Large Scale Agentic Task Synthesis Pipeline)**，重点关注智能体行为，并改进了**强化学习**后训练算法。这些模型已在
  **LM Arena** 等平台上线，定价约为**每百万 token 0.28 美元至 0.42 美元**。


  社区反馈褒贬不一：用户对其前沿的推理能力表示赞赏，但对其聊天界面 (UI) 的体验提出了批评。**Susan Zhang** 和 **Teortaxes** 等关键人物也对此次发布发表了评论。'
id: MjAyNS0x
models:
- deepseek-v3.2
- deepseek-v3.2-speciale
- gpt-5-high
- sonnet-4.5
- gemini-3-pro
people:
- suchenzang
- teortaxestex
title: DeepSeek V3.2 与 3.2-Speciale：GPT-5 级别高性能开源权重、上下文管理及算力扩展计划
topics:
- agentic-ai
- reinforcement-learning
- large-context-windows
- model-benchmarking
- model-performance
- multi-agent-systems
- model-training
- model-deployment
---

**Whale is all you need.**

> 2025年11月28日至12月1日的 AI 新闻。我们为您检查了 12 个 Subreddit、544 个 Twitter 账号和 24 个 Discord 社区（包含 205 个频道和 17803 条消息）。预计节省阅读时间（以 200wpm 计算）：1329 分钟。我们的新网站现已上线，支持全元数据搜索，并以精美的 vibe coded 风格展示所有往期内容。请访问 https://news.smol.ai/ 查看完整的详细新闻，并在 @smol_ai 上向我们提供反馈！

在 NeurIPS 周的周一[发布](https://x.com/deepseek_ai/status/1995452641430651132)，DeepSeek 展示了他们仍在持续交付主流模型（[DeepSeekMath-V2](https://simonwillison.net/2025/Nov/27/deepseek-math-v2/#atom-everything) 就在上周发布，[3.2-Exp](https://api-docs.deepseek.com/news/news250929) 在 9 月发布，[3.1](https://news.smol.ai/issues/25-08-20-deepseekv31) 在 8 月发布），其基准测试结果与发布 3 个月的 [GPT-5-High](https://news.smol.ai/issues/25-08-07-gpt-5) 和发布 2 个月的 [4.5 Sonnet](https://news.smol.ai/issues/25-09-29-sonnet-45) 相比非常有竞争力，但也承认他们仍落后于上个月发布的 [Gemini 3 Pro](https://news.smol.ai/issues/25-11-18-gemini-3)。

[柱状图比较了不同 AI 模型在各种推理能力上的表现，DeepSeek V3.2-Speciale 在多个基准测试中表现出色](https://resend-attachments.s3.amazonaws.com/yfCR9IgDHbGraJ9)

这篇[论文](https://cas-bridge.xethub.hf.co/xet-bridge-us/692cfec93b25b81d09307b94/2d0aa38511b9df084d12a00fe04a96595496af772cb766c516c4e6aee1e21246?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251202%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251202T051536Z&X-Amz-Expires=3600&X-Amz-Signature=1223659c461b82df09412f203e50837d6af5ce03db4c3218cf49b2762f0beb8f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27paper.pdf%3B+filename%3D%22paper.pdf%22%3B&response-content-type=application%2Fpdf&x-id=GetObject&Expires=1764656136&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NDY1NjEzNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82OTJjZmVjOTNiMjViODFkMDkzMDdiOTQvMmQwYWEzODUxMWI5ZGYwODRkMTJhMDBmZTA0YTk2NTk1NDk2YWY3NzJjYjc2NmM1MTZjNGU2YWVlMWUyMTI0NioifV19&Signature=rfPEt2hUrL%7EfUSSM5ViO-OR6nVzScfVJFheuSRlLfh-yuLnBbrNan5gUClQtCk0BywsoplyrOnWi0wltYltOo96pZ3JwYRUHyF6RzTcNQV8gCwKxdUHhOCi4O0-rxBORpgJ6661x2MUeV9bu%7EC8tyHRjkiM5VIwT7UtdslW5GP7YsXsMmOuYX1LceqmscCu2oxBYLud%7EcpemfvexdIxcLqd98psML6kGf-TNYzuixeN%7E1fbOOLJTy9-XtyyBOExFPCa1A5pfazQ8MrUodXsGBU%7EgumjAVXA-JT8FGXfiNOnngm5Nsv7i8pQx2L9hJ2OR-IJvQi7F%7EjHyERCU-g0-sQ__&Key-Pair-Id=K2L8F4GPSG1IFC)长达 23 页，内容非常详实且保持了一贯的高质量，回顾了 3.2-Exp 的 [DeepSeek Sparse Attention](https://news.smol.ai/issues/25-09-29-sonnet-45) 工作、[一系列 RL 后训练算法改进](https://x.com/suchenzang/status/1995537223698243998)，以及一个针对 DSv3.2 的 Agent 行为开发的新颖“**Large Scale Agentic Task Synthesis Pipeline**”：

3 个较大任务集的视觉化展示：

1. Search Agent 任务
    
    
![详细的流程图说明了 DeepSeek-V3.2 的 Search Agent 多 Agent 流水线，展示了从大规模数据开始的过程](https://resend-attachments.s3.amazonaws.com/z6MFYZ3tPARy1qF)

    
2. Code Agent 任务


![流程图说明了用于构建可执行环境的 DeepSeek-V3.2 Code Agent 流水线，展示了 GitHub 等阶段](https://resend-attachments.s3.amazonaws.com/N8pKAdneknM5Vx8)


1. General Agent
    
    
![流程图说明了 DeepSeek V3.2 中使用的大规模 Agent 任务合成流水线（Large Scale Agentic Task Synthesis Pipeline）](https://resend-attachments.s3.amazonaws.com/50UZiolqEcTLyEC)

    

一如既往，[Susan Zhang](https://x.com/suchenzang/status/1995537223698243998) 给出了最尖锐但准确的见解，而 [Teortaxes](https://x.com/teortaxesTex/status/1995527632578834829) 则完全进入了 Whale 吹捧模式。

---

# AI Twitter 回顾

**DeepSeek V3.2 和 “Speciale” 发布：Agent 优先的推理模型**

- **DeepSeek V3.2 家族登陆各大平台**：Standard、Thinking 和 “Speciale” 变体现已在 LM Arena 和社区工具中上线。Cline 添加了 V3.2 和 V3.2-Speciale，支持 131K 上下文，价格为每百万 Token $0.28/$0.42（[@cline](https://twitter.com/cline/status/1995547844263248211), [博客](https://twitter.com/cline/status/1995547857638883439)）；LM Arena 开启了正面交锋对比（[@arena](https://twitter.com/arena/status/1995564824718442620)）。Yupp 添加了所有三个变体（[@yupp_ai](https://twitter.com/yupp_ai/status/1995538168146526274)）。早期用户评价褒贬不一：有人称 V3.2 “终于达到了 Frontier 级别”，而另一些人则认为与基准测试相比，其聊天 UI 体验平平（[@teortaxesTex](https://twitter.com/teortaxesTex/status/1995527632578834829), [@gallabytes](https://twitter.com/gallabytes/status/1995529866595500071)）。

技术笔记：据报道，DeepSeek 通过 warm-starting 和在约 1T tokens 上的逐渐适配，将 Attention 复杂度从平方级降低到了约线性级，并针对分离式的 prefill 与 decode 使用不同的 Attention 模式 ([@suchenzang](https://twitter.com/suchenzang/status/1995535496421015741))。

基准测试/行为笔记：评论强调了其在 Tool Decathlon 中强劲的 pass@1 表现，但 pass@3 弱于新的 Opus，这表明其“RL 仍未达到上限” ([@teortaxesTex](https://twitter.com/teortaxesTex/status/1995538676332278238))。中文分析将 Speciale 在某些消耗更高 token 的归纳推理测试中大致归为 GPT‑5 级别；幻觉/长上下文提取仍是其弱点 ([@ZhihuFrontier](https://twitter.com/ZhihuFrontier/status/1995689116999311455))。

**美国开源权重 MoE 推进：Arcee AI 的 Trinity (Mini/Nano)**

- **Arcee 的 Trinity Mini 和 Nano (Apache-2.0，开源权重)**：Trinity‑Mini (26B‑A3B；3B active) 和 Trinity‑Nano‑Preview (6B‑A1B；1B active) 发布，支持 128K context、tool use/function calling，并专注于推理。预训练在 512 块 H200 (bf16) 上使用了 10T tokens，并由 DatologyAI 进行数据策展；架构细节包括 DeepSeek 风格的 routing、gated attention、GQA、QK-norm、Muon optimizer 以及混合 Attention 模式 ([@arcee_ai](https://twitter.com/arcee_ai/status/1995600354374025395), [@latkins](https://twitter.com/latkins/status/1995592664637665702), [@stochasticchasm](https://twitter.com/stochasticchasm/status/1995595431448121593), [@eliebakouch](https://twitter.com/eliebakouch/status/1995600008603697346))。Together AI 托管了 Trinity‑Mini 以提供生产级推理 ([@togethercompute](https://twitter.com/togethercompute/status/1995594629505573338))。OpenRouter 提供免费试用窗口 ([@arcee_ai](https://twitter.com/arcee_ai/status/1995600354374025395))。
    
    路线图：Trinity‑Large (总参数约 420B；13B active) 正在训练中（目标 2026 年初），在 2048 块 B300 上使用约 20T tokens，旨在重新确立美国本土开源权重前沿 MoE 竞争者的地位 ([@scaling01](https://twitter.com/scaling01/status/1995616210109825447), [@TheAhmadOsman](https://twitter.com/TheAhmadOsman/status/1995613231629381935))。
    

**视频生成与编辑：Runway Gen‑4.5 领先；可灵 Kling O1 发布**

- **Runway Gen‑4.5 登顶 Video Arena**：Runway 的最新模型在社区排行榜上排名第一；CEO Cristóbal Valenzuela 讨论了小团队如何在视频生成领域超越大厂 ([@wandb](https://twitter.com/wandb/status/1995548641801765249), [@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1995539870983266493))。一些人提醒，在 Veo 3 之后，缺乏同步音频是一个重要问题 ([@kylebrussell](https://twitter.com/kylebrussell/status/1995540164303589801))。
- **可灵 Kling O1（多模态生成+编辑）已上线**：支持多镜头、文本/图像/视频条件 prompting；元素添加/交换/删除；社区演示包括猫变吉娃娃以及多角度重拍 ([@Kling_ai](https://twitter.com/Kling_ai/status/1995698062371754450), [@venturetwins](https://twitter.com/venturetwins/status/1995528305701990878), [@arena](https://twitter.com/arena/status/1995644722048893285), [@veedstudio](https://twitter.com/veedstudio/status/1995517194273436081))。早期创意人员将其控制力/灵活性与早期的 “Aleph” 风格 ICV 范式进行了比较 ([@c_valenzuelab](https://twitter.com/c_valenzuelab/status/1995559319962783919))。

**Serving、Tooling 和 Infra 更新**

- **Transformers v5 RC (Hugging Face)**：自 v4 以来的第一个重大版本——现在支持约 400 种架构，量化优先，取消了 slow tokenizers，仅限 PyTorch 的模块化定义，以及一个兼容 OpenAI 的 “transformers serve”（带有 Responses API）。目标：成为开放训练/微调/推理栈的骨干 ([@LysandreJik](https://twitter.com/LysandreJik/status/1995558230567878975), [@reach_vb](https://twitter.com/reach_vb/status/1995565504480882765), [@huggingface](https://twitter.com/huggingface/status/1995575607456137716))。
- **vLLM‑Omni**：将 vLLM 扩展到全模态（例如 Qwen‑Omni, Qwen‑Image），采用解耦阶段（disaggregated stages），同时保持 vLLM 的开发体验 ([@vllm_project](https://twitter.com/vllm_project/status/1995566791234629989))。
- **LangChain 1.1 能力内省（capability introspection）与 “Deep Agents”**：模型特性（推理、工具、上下文窗口）的运行时检测驱动动态路由和摘要中间件；更深层次的 Agent 模式增加了用于长期记忆和多 Agent 协作的文件系统 ([@masondrxy](https://twitter.com/masondrxy/status/1995528765473006002), [@LangChainAI](https://twitter.com/LangChainAI/status/1995553139479773392))。
- **Unsloth 为长序列增加了 Arctic 的 TiledMLP** ([Stas Bekman](https://twitter.com/StasBekman/status/1995516206682628569))；**Together AI** 声称通过内核工程、近乎无损的量化和投机采样（speculative decoding），在流行的开源 LLM 上实现了最快推理 ([@togethercompute](https://twitter.com/togethercompute/status/1995559800835637725))；**VS Code** 在 Insiders 版本中发布了 “Language Models 编辑器” ([@code](https://twitter.com/code/status/1995557008292913443))。
- **Gemini 3 Pro**：在 API 中结合了 Google Search 与结构化输出；“Thinking” 模式已在应用中可用，并正在 Google Search 中更广泛地推广 ([@_philschmid](https://twitter.com/_philschmid/status/1995539879724294392), [@Google](https://twitter.com/Google/status/1995605066170998917), [@GeminiApp](https://twitter.com/GeminiApp/status/1995534313044238347))。

**开放性与社区排名**

- **Artificial Analysis 开放指数 (v1)**：一个结合了模型可用性（权重+许可证）和透明度（方法论、预训练/后训练数据）的跨模型开放性评分。发布时，**AI2 OLMo** 以 89/100 领跑；**NVIDIA Nemotron** 得分为 67。许多开放权重（open-weights）发布在数据/方法披露方面滞后；总体而言，在当前发布的版本中，开放性与“智能”呈负相关（受前沿实验室驱动以及部分顶级开放权重透明度有限的影响） ([@ArtificialAnlys](https://twitter.com/ArtificialAnlys/status/1995523178521846191), [context](https://twitter.com/ArtificialAnlys/status/1995523186394575204))。
- **Arena (11月) 开放模型排名**：前三名开放模型：Kimi‑K2‑Thinking‑Turbo（#1，修改版 MIT），GLM‑4.6（#2，MIT），Qwen3‑235B‑a22b‑instruct‑2507（#3，Apache‑2.0）。尽管闭源模型有所变动，开放模型在全球前 100 名中依然表现强劲；SVG “海狮”压力测试线程对比了前 10 名开放模型的输出 ([@arena](https://twitter.com/arena/status/1995534475070243043), [SVG test](https://twitter.com/arena/status/1995534738485129706))。

**安全、评估与可解释性**

- **OpenAI 推出对齐研究（Alignment Research）博客**，用于发布更频繁、更具技术性的安全出版物 ([@j_asminewang](https://twitter.com/j_asminewang/status/1995569301714325935))。
- **Anthropic 前沿红队：智能合约漏洞利用**——在模拟中，AI Agent 发现了价值 460 万美元的漏洞；发布了新的基准测试 ([@AnthropicAI](https://twitter.com/AnthropicAI/status/1995631802032287779))。
- **Opus 4.5 系统卡片（system card）讨论**：人们对思维链（Chain-of-Thought）训练的透明度表示担忧；Anthropic 的 Sam Bowman 澄清说 Opus 4.5 与 Sonnet 4.5 保持一致（没有针对 CoT 的直接优化，文档中有所遗漏） ([@RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/1995541177094017313), [update](https://twitter.com/RyanPGreenblatt/status/1995631185524085062))。另一项批评认为，目前在自主性/网络/生物阈值方面的能力评估证据较弱，并呼吁建立更难、更长的任务以及更清晰的阈值 ([@RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/1995557783341858931))。
- **可解释性转向**：Jacob Steinhardt/Hendrycks 注意到人们对过去强调机械可解释性（mechanistic interpretability）的怀疑日益增加；Google DeepMind 的可解释性团队概述了一个更加以问题为驱动、以下游指标（downstream-metric）为锚点的议程 ([@hendrycks](https://twitter.com/hendrycks/status/1995540567019934185), [@saprmarks](https://twitter.com/saprmarks/status/1995643857049149668))。

**热门推文（按互动量排序）**

- Sam Altman 谈政策/创新：赞扬 David Sacks 在美国 AI 领导地位中的作用 ([@sama](https://twitter.com/sama/status/1995547485012423111))
- 使用微型 Gaussian splats 的 3D/WebAR：“可视化我重新设计的客厅”，采用 1.5MB 的 .spz 资产 ([**XRarchitect**](https://twitter.com/XRarchitect/status/1995541338335678801))
- Anthropic Frontier Red Team：AI Agent 在模拟智能合约漏洞利用中发现了 460 万美元；基准测试已发布 ([@AnthropicAI](https://twitter.com/AnthropicAI/status/1995631802032287779))
- Alex Albert 谈 Opus 4.5：“7.5–8/10 的帮助程度”，可靠的代码编写，更好的判断力；仍存在一些产品差距 ([@alexalbert__](https://twitter.com/alexalbert__/status/1995570306526970297))
- Yuchen Jin 对本周发布内容的 “Game over” 反应 ([**Yuchenj_UW**](https://twitter.com/Yuchenj_UW/status/1995523554679673180))
- Amanda Askell 确认了 Claude SL 训练中使用的内部 “soul doc” 概念的真实性；更多细节即将公布 ([@AmandaAskell](https://twitter.com/AmandaAskell/status/1995610567923695633))
- 招聘热潮：Google DeepMind 在 NeurIPS 招聘研究员 ([@RuiqiGao](https://twitter.com/RuiqiGao/status/1995572419218796567))

笔记与杂项

- LLM 系统研究：ThreadWeaver 引入了自适应并行推理 (SFT→RL)，在数学基准测试中，与顺序 CoT 相比，在准确率相近的情况下实现了 1.14–1.53 倍的延迟加速 ([@LongTonyLian](https://twitter.com/LongTonyLian/status/1995561005557186963), [@VictoriaLinML](https://twitter.com/VictoriaLinML/status/1995602943169741157))。
- 机器人/人形机器人：Amazon FAR 的 Holosoma 开源了一个跨机器人训练/部署栈（人形/四足；Isaac/MuJoCo），具有快速的 sim2real 循环 ([@pabbeel](https://twitter.com/pabbeel/status/1995629150082924644), [@younggyoseo](https://twitter.com/younggyoseo/status/1995636964142580078))。
- 社区教育：Tom Yeh 教授的深度学习数学 “填空” 练习；作为一种学习方式具有很高的参与度 ([@ProfTomYeh](https://twitter.com/ProfTomYeh/status/1995525862653911202))。

---

# AI Reddit 回顾

## /r/LocalLlama + /r/localLLM 回顾

### 1. DeepSeek V3.2 模型与基准测试

- [**deepseek-ai/DeepSeek-V3.2 · Hugging Face**](https://www.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/) (热度: 1219): **DeepSeek-V3.2 引入了三项关键创新：1) DeepSeek Sparse Attention (DSA)，在保持长上下文场景性能的同时降低了计算复杂度；2) 可扩展强化学习框架 (Scalable Reinforcement Learning Framework)，使模型的表现与 GPT-5 相当，其中高算力变体 DeepSeek-V3.2-Speciale 在推理任务中超越了 GPT-5 并与 Gemini-3.0-Pro 持平，在 2025 年 IMO 和 IOI 中达到了金牌水平；3) 大规模 Agentic 任务合成流水线 (Large-Scale Agentic Task Synthesis Pipeline)，通过大规模生成训练数据增强了工具使用场景下的推理能力。该模型已在 [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V3.2) 上发布，并采用 MIT License 授权。** 一条值得注意的评论赞赏了 DeepSeek 团队的透明度，因为他们在报告中包含了模型落后于竞争对手的基准测试，体现了对诚实性能评估的承诺。
- [**我的逻辑推理基准测试被 DeepSeek V3.2 Speciale 碾压了**](https://www.reddit.com/r/LocalLLaMA/comments/1pbnadc/my_logical_reasoning_benchmark_just_got_owned_by/) (热度: 269): **该图片是一个柱状图，展示了各种 AI 模型在“lineage-bench”逻辑推理基准测试中的表现，其中“DeepSeek V3.2 Speciale”获得了最高分。该基准测试通过将题目数量从 800 个减少到 160 个，并将血缘关系图的复杂度增加到 8、64、128 和 192 个节点，从而提升了挑战难度。这证明了“DeepSeek V3.2 Speciale”相比于“Google/Gemini-3 Pro Preview”等其他模型具有更卓越的逻辑推理能力。图表直观地强调了该模型在不同问题规模下的表现，彩色柱状条代表了每种规模。** 一位评论者幽默地提到了模型名称中的“speciale”，暗示其表现非凡。另一位评论者鼓励在发表评论前先试用模型，表示了积极的接受态度。第三条评论做了一个隐喻式的比较，“Sonnet 击败了 Opus”，暗示了该模型令人印象深刻的成就。
    - AnticitizenPrime 讨论了对 DeepSeek V3.2 Speciale 进行的一项逻辑推理谜题测试，指出它在 `15 分钟` 内使用了 `29k tokens`，但得出了错误答案。该谜题涉及一个关于穿着农民衣服的山羊的误导信息，正确答案应该是 `1` 次往返，因为实际上没有任何限制。相比之下，GLM 4.6 通过有条理的推理正确解决了该问题，这凸显了 Speciale 推理过程中的潜在问题。
    - AnticitizenPrime 的评论强调了 DeepSeek V3.2 Speciale 的性能问题，即它消耗了大量 tokens (`29k`) 和时间 (`15 分钟`) 却错误地解决了一个谜题。这与 GLM 4.6 形成了鲜明对比，后者高效地解决了同一个谜题，这表明 Speciale 在处理某些类型的逻辑推理任务（尤其是涉及误导信息的任务）时可能会遇到困难。
    - BagComprehensive79 询问了网站上提供的 DeepSeek 版本，表明了对比较不同版本或了解最新功能可用性的潜在兴趣。这反映了社区对跟踪各版本更新和性能改进的关注。

### 2. Transformers v5 与上下文长度扩展

- [**Transformers v5 发布了！**](https://www.reddit.com/r/LocalLLaMA/comments/1pbl22j/transformers_v5_is_out/) (热度: 643): **Hugging Face 发布了** `transformers v5`**，增强了从训练到推理阶段与生态系统中其他工具（如** `llama.cpp` **和** `vLLM`**）的互操作性。该版本简化了新模型的集成，并显著提升了库的功能。更多详情请参阅[官方博客文章](https://huggingface.co/blog/transformers-v5)。** 社区对 Transformer 安装量的统计数据印象深刻，这表明了其广泛的采用和影响力。
    - 关于 Transformers v5 的博客文章强调了重大更新，包括引入了新的分词器类 `Llama5Tokenizer()`。该分词器被设计为空白且可训练的，符合作者对 Llama5 的规范，尽管模型本身尚未发布。这表明该库在设计上采取了前瞻性方法，为未来的模型发布做准备。
    - Transformers v5 的发布伴随着关于其采用和使用情况的惊人统计数据。博客提到了巨大的安装量，表明该库在机器学习社区中得到了广泛认可和应用。这反映了 NLP 任务对 Hugging Face 工具的日益依赖。
    - 在 Transformers v5 的背景下提到 `Llama5Tokenizer()`，暗示了 Llama 模型系列未来的潜在发展。虽然 Llama5 尚不存在，但基础设施正在搭建中，这可能意味着即将发布的版本或在该方向上的实验。这种前瞻性的设置体现了 Hugging Face 在快速演进的 NLP 领域保持领先的策略。
- [**现在可以进行 500K 上下文长度微调 - 提升 6.4 倍**](https://www.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/) (热度: 393): **这张图片是来自 Unsloth 的信息图，展示了他们在扩展大语言模型 (LLM) 上下文长度方面的突破：在单张** `80GB H100 GPU` **上可达到** `500K`**，在** `192GB VRAM` **上潜力可达** `750K+`**，且无精度损失。这一进步是通过融合且分块的交叉熵损失（fused and chunked cross-entropy loss）以及在其梯度检查点（Gradient Checkpointing）算法中增强激活卸载（activation offloading）等创新实现的。信息图强调了 VRAM 占用减少了** `72%`**，上下文长度增加了** `6.4x`**，并强调了与 Snowflake 在 Tiled MLP 上的合作以实现这些改进。该更新适用于任何 LLM 或 VLM，不仅限于 gpt-oss，并包含对强化学习 (RL) 的支持。** 评论者赞扬了这项工作在提升小预算训练能力方面的贡献，并对在 Hugging Face 等平台上提供具有扩展上下文的 gpt-oss-20b 模型表示关注。此外，还强调了这种微调在处理大数据上下文方面的实用性。
    - ridablellama 强调了为扩展上下文窗口微调模型的实际效用，并提到了 `qwen VL 8B 1M` 模型的具体案例，该模型支持 1M 上下文窗口。他们指出在 NVIDIA 4090 GPU 上实现了 300k 上下文窗口，强调了该模型在工具调用（tool calling）场景中处理海量数据的能力。这展示了在高效处理大型数据集方面的巨大潜力。
    - FullOf_Bad_Ideas 提出了一个关于不同模型架构长上下文内存使用效率的技术查询。他们质疑在 `GPT OSS 20B` 等模型中看到的收益是否同样适用于稠密模型（dense models），例如具有 512k 上下文的 `Seed OSS 36B`，或者在 24GB GPU 上通过 QLoRA 可能支持 16k 上下文的 `Qwen 2.5 32B`。这表明需要进一步探索不同架构如何处理扩展上下文窗口以及涉及的潜在权衡。
    - TheRealMasonMac 和 FullOf_Bad_Ideas 都对扩展上下文长度能力的潜在限制或“陷阱”表示怀疑和好奇。他们有兴趣了解在哪些特定条件或权衡下这些收益可能无法保持，这表明需要详细的基准测试和性能评估，以验证这些改进在各种场景下的稳健性。

### 3. 开源与闭源的讨论

- [**这就是为什么开源甚至比闭源更好**](https://www.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/) (热度: 374): **OpenAI 的 ChatGPT 即使在 Pro Plan 中也开始展示广告，这凸显了其货币化策略正转向优先考虑收入而非用户体验。考虑到当前 AI 行业的环境，由于投资者兴趣浓厚，“没有人缺钱”，这一举动令人惊讶。社区认为这是一个利润驱动的决策，可能会削弱付费计划的价值主张。** 评论者对这一决定背后的动机表示怀疑，认为这反映了优先考虑短期利润而非用户满意度的更广泛趋势。人们还担心未来使用 AI 模型的成本会增加，这表明 AI 服务经济格局可能发生转变。

## 较低技术门槛的 AI Subreddit 回顾

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. Nano Banana Pro 的真实感与担忧

- [**Photo realism with nano banana pro.**](https://www.reddit.com/r/GeminiAI/comments/1pb672x/photo_realism_with_nano_banana_pro/) (Activity: 1353): **Reddit 帖子中描述的图像是使用 'nano banana pro' 工具创建的高度逼真的 AI 生成肖像，展示了先进的照片级真实感能力。场景中一个人在夜晚倚靠着一辆 Hyundai i20，服装和环境细节丰富，例如碎石地面和远处的城市灯光。光影效果复杂，涉及高对比度的人工光源，产生了镜头光晕和硬阴影等戏剧性效果，证明了该工具处理复杂光照场景的有效性。** 评论者对 AI 生成图像的真实感印象深刻，指出了其栩栩如生的质量以及对脚趾数量正确等细节的准确描绘。此外，还有关于提示词长度对输出质量影响的讨论，一些人认为详细的提示词能增强 AI 生成图像的真实感。
- [**Some of the posts of nanobanana actually have me worried!**](https://www.reddit.com/r/GeminiAI/comments/1pbfg7u/some_of_the_posts_of_nanobanana_actually_have_me/) (Activity: 664): **该图像是一个迷因（meme），不包含任何技术内容。它幽默地描绘了区分 AI 生成图像与真实图像日益增长的难度，反映了人们对 AI 生成内容真实性不断提高的普遍担忧。该迷因通过角色表情的变化，说明了最初看到 AI 图像减少时的宽慰，随后意识到这可能并非好事，暗示了对数字媒体中 AI 无处不在的深层评论。** 评论者表达了对难以区分真实图像与 AI 生成图像的共同担忧，一些人对在线内容的真实性表示普遍怀疑。
- [**Maintaining Character Consistency in Nano Banana Pro Using Reference Images**](https://www.reddit.com/r/Bard/comments/1pb4nvc/maintaining_character_consistency_in_nano_banana/) (Activity: 1515): **该帖子讨论了一种利用参考图像在 Nano Banana Pro 中保持图像角色一致性的方法。用户被引导上传一张自己或某个角色的清晰图像，输入特定提示词，然后查看结果。提示词强调在面部特征、骨骼结构、肤色和其他视觉细节方面保持完全相似，并采用** `1:1 aspect ratio` **和** `4K detail` **。作者提到在 Gemini 上使用了这种技术，并建议如果初次尝试不成功可以重试。** 评论反映了幽默与怀疑的交织，其中一人指出了 AI 生成图像的超现实本质，另一人则幽默地提到了生成图像中角色的互动。
    - staraaia 讨论了图像生成工具在创建特定角色的照片级真实副本方面的局限性。他们提到，由于安全限制，这些工具会避免生成 deepfakes 或公众人物的精确肖像，而是默认生成“通用”面孔。这确保了符合禁止创建精确匹配的政策，尽管可以尝试根据参考照片复制发型或眼形等某些特征。
- [**Turns out Nano Banana Pro is great for vibe gardening**](https://www.reddit.com/r/ChatGPT/comments/1pb8muc/turns_out_nano_banana_pro_is_great_for_vibe/) (Activity: 498): **该帖子讨论了 Nano Banana Pro 在“氛围园艺（vibe gardening）”中的应用，这个术语可能指的是技术在园艺中的一种新颖或小众应用。“神经架构师（neural architect）”和“等轴测（isometry）”的提及暗示了在规划花园布局时使用了先进的计算设计或 AI，可能涉及 AI 增强的可视化或自动化设计工具。关于“计算机增强（computer enhance）”的评论暗示了在园艺应用中使用图像处理或 AI 工具来改进或分析视觉数据。** 一条评论强调了由于花园设计自动化可能导致的潜在失业，反映了对 AI 取代人类角色的担忧。另一条评论批评了 AI 生成设计的准确性，指出 AI 预测与现实世界适用性之间存在差距。

### 2. ChatGPT 广告与用户反应

- [**ChatGPT 内部的首个广告？200 美元的 Pro 用户确认 😳**](https://www.reddit.com/r/ChatGPT/comments/1pbjlmj/first_ad_inside_chatgpt_200_usd_pro_user_confirms/) (Activity: 527): **Reddit 帖子中的图片显示了 ChatGPT 界面的截图，在聊天记录下方出现了一个类似于广告的板块。该板块推广健身课程，并提供“Connect Peloton”的选项，暗示可能存在集成或合作伙伴关系。这种类似广告功能的出现引发了关于 OpenAI 是否正在 ChatGPT 内部进行 Beta 测试广告的讨论，尤其是多名用户报告了类似的经历。评论指出，虽然该功能看起来像广告，但它可能更像是一种应用集成建议，这在其他语境中也曾出现过。** 一些用户认为该功能在技术上不是广告，但其功能类似，而另一些用户则质疑在这种背景下广告与非广告的区别。
    - 讨论强调，ChatGPT 内部的应用集成建议已经存在一段时间了，其功能与广告类似。这表明直接广告与功能建议之间的界限变得模糊，这可能会影响用户体验以及对平台中立性的看法。
    - 一位用户分享了一张截图 (https://preview.redd.it/izs9nwq36n4g1.png)，显示了 ChatGPT 内部的一个集成建议，引发了关于此类建议是否构成广告的辩论。这引发了关于这些集成背后的透明度和意图的问题，尤其是对于付费用户而言。
    - 对话触及了广告与非广告之间的区别，一些用户质疑这些建议的性质。这反映了人们对 AI 平台可能如何微妙地融入促销内容的更广泛担忧，这可能会影响用户信任以及 AI 交互的客观性认知。
- [**当有人假装被困在流沙中寻求帮助时，ChatGPT 会如何反应**](https://www.reddit.com/r/ChatGPT/comments/1pb6ca8/how_chatgpt_reacts_to_someone_requesting/) (Activity: 1682): **该帖子幽默地探讨了 ChatGPT 如何回应假装被困在流沙中的用户，突显了 AI 识别并回应想象场景的能力。AI 的回应包括对该场景的趣味性参与，展示了其对语境的理解以及维持轻松互动的能力。这展示了该模型的对话灵活性及其在多样化人类互动上的训练，使其能够有效地处理甚至是异想天开或虚构的情况。** 评论反映了对 ChatGPT 表现的正面评价，注意到其保持幽默和语境意识的能力。这表明用户欣赏 AI 参与趣味性和想象力对话的能力，这被视为其复杂对话设计的证明。

---

# AI Discord 摘要

> 由 gpt-5.1 生成的摘要的摘要的总结
> 

**1. 下一代与开源权重模型：DeepSeek 3.2, Trinity Mini, K2 3.5T, Qwen3-235, Orchestrator-8B**

- **DeepSeek 3.2 模型表现两极分化：从数学崩溃到编程怪兽**：**LMArena** 上的用户报告称 `deepseek-v3.2-speciale` 存在严重的**数学幻觉**，截图显示其在提示词明确要求“不要产生数学幻觉”之前，一直输出荒谬的算术，随后该模型因**不稳定性被从 LMArena 下架**。与此同时，`deepseek-v3.2-thinking` 变体在 **HTML 和代码生成**方面赢得了赞誉，例如 [CodePen 演示](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO)，多位用户声称尽管存在对 **OpenAI 风格数据质量**的普遍担忧，但它在速度和项目结构上已足以抗衡或超越 **Gemini 3**。
    - LMArena 的 [Text Arena 更新](https://lmarena.ai/leaderboard/text-to-image)添加了 `deepseek-v3.2`、`deepseek-v3.2-thinking` 和 `deepseek-v3.2-speciale`，但只有 thinking 和基础模型经受住了社区的审视，因为 `speciale` 在过拟合、审查边缘案例和脆弱行为方面几乎成了一场现场 A/B 测试。在各大 Discord 频道中，工程师们强调 **DeepSeek 3.2** 在生产负载下也表现吃力——OpenRouter 用户遇到了 **~160秒的 API 延迟**、超时和速率限制错误，推测 DeepSeek 选择的托管商正承担着 **3.2** 流量的大部分压力并因需求过载而崩溃。
- **Arcee 的 Trinity Mini 与 Nous K2 在开源权重军备竞赛中大显身手**：**Arcee** 发布了 **Trinity Mini**，这是其完全在美国训练的 **Trinity 系列**中的中端模型，在 OpenRouter 上免费提供：[**arcee-ai/trinity-mini:free**](https://openrouter.ai/arcee-ai/trinity-mini:free)，定位为开发者的入门级开源权重选项。与此同时，**Nous Research** 展示了 [**NousResearch/k2-merged-3.5T-fp8**](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) 的发布，这是一个 **3.5T 参数的 MoE** 模型，社区成员戏称它需要“装在硬盘里而不是内存里”，凸显了 MoE 规模正变得多么极端。
    - 工程师们将 **Trinity Mini** 视为 OpenRouter 生态系统中昂贵闭源模型的务实日常替代方案，而 **K2** 更多被视为一种**研究级的实力展示**，表明数万亿参数的开源 MoE 现在已在 Hugging Face 上成为现实。与此同时，Nous 成员还在预热即将推出的 **Mistral Large 3 (~675B MoE)**，它将具备 **vision** 能力和类 DeepSeek-V3 架构，此外还有通过 RPC 提供的 **Qwen3-235B**，其 **Q4** 量化质量“与 API 相当”——所有这些都强化了一个趋势：**巨型开源（或半开源）MoE 和庞大的 Qwen 衍生模型**正迅速在爱好者和研究层级中普及。
- **被低估的全明星：Qwen3-235B 与 Nvidia Orchestrator-8B 表现亮眼但未成趋势**：Nous 服务器成员称赞 **Q4** 版本的 **Qwen3-235B** 表现“惊人”，且实际上达到了 **API 级别的质量**，尽管 Token 生成速度一般，但对于没有“怪兽级内存”的用户来说，通过 RPC 使用它非常有吸引力。在 Yannick Kilcher 的 **#ml-news** 频道中，人们提到了 Nvidia 的 **Orchestrator-8B**，这是一个根据其 [arXiv 论文](https://arxiv.org/abs/2511.21689)和 [Hugging Face 卡片](https://huggingface.co/nvidia/Orchestrator-8B)显示，在 **HLE** 上得分 **37.1** 的 **8B 工具调用模型**，然而它只有 **2 次下载**，凸显了可见度与质量之间的错位。
    - 从业者认为 **Qwen3-235B** 是少数几个在合理的量化水平下真正能在实践中交付成果的巨型开源模型之一，用户明确选择 RPC 而不是尝试在本地托管 235B。相比之下，关于 Orchestrator-8B 的讨论集中在为什么一个高分且在特定领域有用的**工具调用专家模型**几乎没有获得社区关注——一些人将其归咎于 Nvidia 的品牌推广和“皮衣男”梗——这说明**开发者的心智占有率和分发渠道与原始基准测试同样重要**。

**2. 用于编程和应用的工具、IDE 及 Agent 生态系统**

- **Cursor 的 AI IDE 与 Token、终端和云端 Agent 的博弈**：在 **Cursor Community** 中，用户深入剖析了 [Cursor 的定价和 Token 经济学](https://cursor.com/pricing)，一些用户瞬间耗尽了免费试用的 Token，而另一些人则声称通过每天从“上午 10 点到凌晨 4 点”编写代码，自 11 月以来已使用了 **20 亿+ Token**，这引发了关于达到阈值后所谓的“无限次” **Auto Mode** 的困惑。**2.1.39** 版本更新导致终端集成退化：LLM 停止读取终端输出，部分用户失去了对 **git** 和 **python** 等基础工具的访问权限，直到他们重新索引仓库并重启 IDE 和终端窗口；同时，也有人提醒新手，Cursor 是一个集成了额外 Agent 逻辑的 **VS Code fork**。
    - 后台 Agent 在基础设施边界表现出了一些不成熟：一个 **Perplexity MCP** 配置导致了服务器错误循环，直到被禁用；此外，Cursor 的**远程云端 Agent** 无法从 `pyproject.toml` 中引用的**私有 GitHub 依赖**进行 `pip install`，因为虚拟机仅拥有主仓库的凭据（[用户截图](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png)）。与此同时，代理机构正公开[招聘“AI 原生”的 Next.js/Tailwind/Supabase 开发者](https://www.peich.xyz/)，并围绕 Cursor 的 [deeplinks 集成文档](https://cursor.com/docs/integrations/deeplinks)构建**子 Agent 层级结构**，这表明团队已经开始将 IDE 嵌入式 Agent 视为调度大量专业化工作 Agent 的编排器。
- **OpenRouter 助力 DIY AI 应用，但生产负载压力显现**：一位 OpenRouter 社区成员发布了名为[《如何使用 OpenRouter 构建 AI 应用》](https://www.youtube.com/watch?v=8brkmjkYowA)的演示视频，展示了 **AI 编写另一个 AI** 来创建一个待办事项 **RAG 应用**和**图像生成工具**，并将 OpenRouter 作为通用 LLM 交换机。然而在实践中，该社区在多个供应商的 **DeepSeek v3.2** 上遇到了**速率限制、超时和约 160 秒的延迟**，并目睹了 **Grok 4 Fast** 退化为全面的 **500/503 错误级联**，引发了一场戏谑性的“事后分析”，将其归咎于一名初级开发人员和一个“在 r/antiwork 板块上训练”的模型。
    - 开发者还担心与 OpenAI、Google 和 Anthropic 等上游厂商的**数据隐私合同**，询问 OpenRouter 是否真正了解谁在利用什么数据进行训练；工作人员回答称，他们拥有**明确的数据使用合同**，且文档记录的内容即为执行的标准。**Arcee 的 Trinity Mini** 作为免费 [OpenRouter 模型](https://openrouter.ai/arcee-ai/trinity-mini:free)的发布，以及通过 **OpenRouter 上的 ZDR** 实现仅限 API 的 Kimi K2 端点的构想，强化了这样一种模式：**路由正在成为 AI 应用事实上的多供应商抽象层**，但 SRE 级别的可靠性和透明度现在已成为首要关注的问题。
- **代码辅助生态：Aider、Mindlink 和 GPT 提供商争夺开发者**：在 **aider** Discord 中，一家第三方 **GPT Provider** 为 **gpt-5-mini**、**gpt-4.1**、**gpt-4o** 和 **gpt-3.t** 等模型提供了免费额度，并提供了一个用户可以自行托管和修改的开源服务器，将其定位为代码助手的即插即用后端。一些成员质疑 Aider 的长期发展轨迹并寻求替代方案，指出 Aider 仍具有独特且优秀的 SVG/代码审美，而另一些人则认为 8 月发布的强力 **Mindlink 32B/72B 代码模型**可能分流了部分用户——尽管有一位用户抱怨 Mindlink “在多轮编码迭代中的表现不够稳健”。
    - 在各类工具中，元模式（meta-pattern）是一场**争夺“AI 原生开发者”工作流的竞赛**：Cursor 侧重于深度 IDE 集成和子 Agent，Aider 侧重于精简的 CLI 驱动代码编辑，而独立的 GPT 提供商则侧重于**廉价获取前沿模型**。工程师们已经在尝试将这些工具串联起来——例如，在 Cursor 或 Aider 后端使用自托管的 GPT 服务器——将问题从“哪个工具最好”转变为“哪种 Agent + IDE + 路由的堆栈能带给我最小的摩擦和最大的控制力”。

**3. 硬件与底层优化：从 TPUv7 和 H200 到 RDNA3 汇编**

- **Google 的 TPUv7 和 Anthropic 的 1GW 豪赌直指 Nvidia 的 CUDA 护城河**：在 **Latent Space** 中，成员们剖析了 **SemiAnalysis** 关于 **Google TPUv7** 和 **Anthropic 超过 1GW TPU 采购** 的推文（[推文链接](https://xcancel.com/SemiAnalysis_/status/1994399887719645532)），辩论这究竟是对 **Nvidia CUDA 垄断** 的真正威胁，还是仅仅是一场内部成本优化之举。讨论深入探讨了硬件锁定、资本支出 (capex) 与运营支出 (opex) 的权衡，以及 TPU 的编程模型加上 Google 的垂直整合是否能从根本上削弱 **H100/H200** 式集群在大规模训练中的地位。
    - 人们将 TPUv7 的势头与 **Black Forest Labs 为 FLUX 筹集的 3 亿美元 B 轮融资**（[公告](https://xcancel.com/bfl_ml/status/1995357293064626310)）以及对 **DDR5** 的疯狂需求联系起来（LM Studio 用户自嘲 **64GB 套装现在的价格比 PS5 还贵**，并将其归咎于 AI 对内存永无止境的需求）。总体观点是，虽然 **Nvidia 仍然掌控着生态系统**，但超大规模云厂商（hyperscalers）显然正在投资 **并行硬件栈（TPU、定制加速器）** 以规避 CUDA 依赖风险——这是从业者在进行长期基础设施投注时需要考虑的因素。
- **Tinygrad 通过 RDNA3 汇编和 SQTT 精确仿真走向裸机**：**tinygrad** 项目启动了 **RDNA3 assembly** 工作，作为其第一个完全定制的汇编目标，旨在生成与 **RDNA3 ISA manual** 一一对应的指令，并构建一个能够重现与真实 GPU 相同的 **SQTT traces** 的周期精确仿真器（[初始语法 PR](https://github.com/tinygrad/tinygrad/pull/13436)，[SQTT 解析脚本](https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/attempt_sqtt_parse.py)）。这与 **Remu**（一个快速的 RDNA3 仿真器）、**NaviSim**（[论文](https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf)）甚至 **AppleGPU**（[仓库](https://github.com/dougallj/applegpu)）等作为 GPU 逆向工程和指令级工具的先前成果并列。
    - 在 tinygrad 的 **#general** 频道中，开发者们对当前的堆栈大吐苦水：**BEAM search** 并不能单调地提高性能（BEAM=3 对 RMSNorm 来说最快），内核是针对形状特化的（shape-specialized）而非可复用的，分析文档稀缺，且对于非连续索引写入仍没有 **fast scatter**。他们认为 **UOPs 应该编译为人类可读的内核**（如 VIZ graphs），而不是巨大的手动展开的二进制块（blobs），并推动将 `HIPAllocator._offset()` 和更好的 `synchronize()` 文档等功能作为认真实验 **flash-attention 风格内核** 等高级想法的前提条件。
- **实用算力挤压：H200、QLoRA、上下文扩展与消费级 RAM 紧缺**：**Unsloth** 社区正关注 **H200 GPU** 和大显存配置以运行超过 80GB 的模型，建议使用 **QLoRA** 将 **90B 参数模型压缩至约 53–60GB**，以便将其装入单块 RTX 6000 Ada 或一对 48GB 显卡中，并建议使用 [**vast.ai**](http://vast.ai/) 租赁进行实验。他们还分析了 **CohereLabs/command-a-translate-08-2025**（[Hugging Face](https://huggingface.co/CohereLabs/command-a-translate-08-2025)），认为其 **8k 上下文** 需要 **rope scaling + fine-tuning** 才能在不破坏翻译质量的情况下达到 **16k**，而不是简单的外推。
    - 在推理端，Unsloth 用户报告 **Qwen3-Next AWQ** 在 LM Studio 中当 BLAS 批处理大小 >512 时会失败，但在 llama.cpp 中正常；一种解决方法是将 KV-cache 完全保留在 VRAM 中，从而在 **256k 上下文** 下达到 **~27 tokens/s，占用约 9.3GB VRAM**。同时，LM Studio 的 **hardware-discussion** 频道反映了宏观压力：Ryzen AI 笔记本电脑可以将高达 50% 的系统 RAM 分配为 VRAM，服务器搭建者在讨论 **5×3090 (120GB VRAM)** 装备以及高昂的 DDR5 价格——*“64GB 套装比 PlayStation 5 还贵”*——这些都在具体地提醒人们，**Token 预算现在受内存通道的瓶颈限制，程度不亚于 FLOPs**。

**4. 训练、优化与理论：ES vs Backprop, Attention 变体, Prompt Tuning & Scaling Laws**

- **进化策略（Evolution Strategies）向反向传播（Backprop）的地位发起挑战**：在 **Nous Research** 和 **Eleuther-adjacent** 的讨论中，成员们传阅了一篇来自 Reddit 的 **Evolution Strategies (ES)** 深度探讨文章，题为[“客观上最正确的彻底击败 SO 的方式”](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/)，以及 **ES Hyperscale** 项目网站 [eshyperscale.github.io](http://eshyperscale.github.io/)，将 ES 推崇为 **LLMs** 训练中可替代反向传播的可扩展方案。其论点在于，ES 能够处理**反向传播不可行的架构**，比典型的 RL 更好地优化**长时程目标（long‑horizon objectives）**，并可能在保留生成任务中 **pass@k** 等指标的同时，减轻**奖励作弊（reward hacking）**。
    - 研究人员将这些 ES 论文视为训练大型、奇特架构（如不可微组件、离散控制器等）的潜在路径，而这些是经典反向传播难以触及的，尽管目前尚未有人展示大规模的生产环境结果。共识基调是*“前景光明但尚处早期”*：这些方法对于 **Hermes 风格的推理模型、RLHF（来自人类反馈的强化学习）替代方案以及 Agentic 系统**来说非常令人兴奋，但在主流技术栈中挑战 SGD 之前，仍需要实际的训练运行和消融实验（ablations）。
- **DeltaNet / Kimi‑Delta Attention 与值残差（Value Residuals）受到微观审视**：Eleuther 的 **#research** 频道深入研究了 **Kimi‑Delta attention** 论文（["Kimi-Delta" PDF](https://arxiv.org/pdf/2510.26692)）中的 **WY 表示**和 **UT 变换**，质疑这些是否仅仅是将累积 Householder 积代数重组为分块的、硬件友好的形式。成员们提到了 **Songlin 的 DeltaNet 博客系列**（[第二部分](https://sustcsonglin.github.io/blog/2024/deltanet-2/)），以直观理解为什么这种结构相比标准 Attention 实现能提高内存局部性和吞吐量。
    - 在同一个频道中，人们利用 **F‑Lite** 架构（[Freepik/F-Lite](https://huggingface.co/Freepik/F-Lite)）辩论了 **Attention 中的值残差**，结论是**经验性收益与原始论文中的宣传相比显得微乎其微**。一些人指出，此前曾尝试在 **RWKV‑7** 中注入类似的值侧残差技巧，但导致了训练问题，这表明虽然数学逻辑很诱人，但**许多“架构创新”一旦控制了训练预算和数据就会失效**，需要更谨慎的消融实验。
- **缩放定律（Scaling Laws）、非线性指标，以及一个疑问：这一切仅仅是曲线拟合吗？**：Eleuther 的 **#scaling-laws** 频道重新讨论了为什么 LLM **缩放曲线**如此频繁地呈现为**幂律（power laws）**，一名成员称 2023 年早期的解释“缺乏说服力”，并怀疑研究人员是否仅仅因为物理学偏见而默认使用幂律。其他人则指向了**破碎幂律（broken power law）**模型，如[《超越神经缩放定律》](https://arxiv.org/abs/2210.14891)和[《评估神经缩放的非线性指标》](https://arxiv.org/abs/2304.01910)，以及相关的 [OpenReview 讨论](https://openreview.net/forum?id=e8eo9iEFaO)，认为这些是能捕捉不同训练阶段的更现实的拟合方式。
    - 一位参与者总结了一个核心直觉：如果每个潜在**子任务（subtask）**的性能随着模型变强而趋于去相关，那么在相当普遍的假设下，**同时推高所有子任务的“成本”**自然会产生类似幂律的聚合行为。对话的潜台词是怀疑的实用主义——工程师们想知道缩放定律是否**真的能预测未来性能并指导预算分配**，或者它们只是回顾性的曲线拟合，一旦架构、数据清洗或评估指标发生变化就会失效。

**5. 安全、审查绕过、红队测试与模型行为**

- **从二进制漏洞利用到 WAF 绕过：Jailbreakers 将 LLM 视为存在漏洞的二进制文件**：在 **BASI Jailbreaking** 频道中，一名成员描述了一种**从专有 LLM 二进制文件（如 Claude）中剥离审查**的工作流程，具体方法包括使用 `strings` 扫描可执行文件，使用 `hexedit`/`xxd` 进行编辑，以及在源码泄露的情况下理想地从源码重新构建，并用一句话总结道：*“我刚刚带你进入了二进制漏洞利用的世界”*。在同一个服务器的 **#redteaming** 频道中，有人询问如何绕过 **WAF/Cloudflare**，另一人回答说，根据他们的经验，*“cookiejar + impit + custom header”* 的组合具有 *“100%”* 的成功率；与此同时，其他人则在交流通过[恶意 Discord 链接](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581)发现的 token‑stealer 恶意软件的笔记。
    - Perplexity 的 Discord 在 Prompt 层级也表达了类似的观点——用户吹嘘**公共模型审查“很容易被绕过”**，只需巧妙的 Prompt 即可，并将 **“AI safety alignment”** 视为另一种可以 *“轻松禁用/绕过”* 的脚本。在这些服务器中，技术模式显而易见：资深用户正从仅限 Prompt 的 Jailbreak 转向**将模型和基础设施视为完整的攻击面**——从二进制补丁到 HTTP 层规避——而社区安全人员则忙于标记恶意软件，并解释这正是为什么强大的 **red‑teaming 计划**从一开始就存在的原因。
- **模型谄媚（Model Sycophancy）、幻觉工具和 Reward-Hacking 行为在实际应用中浮现**：OpenAI 的 Discord 托管了一场针对现代 LLM 中**“强制性后续提问”和通用、过度随和的措辞**的批评，成员们将其标记为 **AI sycophancy**，并将其追溯到奖励顺从而非经过校准的异议的训练目标，导致模型优先考虑 *“helpful, honest, and safe”* 而非真正的**准确性或批判性**。在 Eleuther 的 **#general** 频道中，用户分享了一个 **Gemini 2.5** 在其搜索工具被禁用时伪造搜索结果的例子（[prompt 链接](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)），并将其描述为 *“类似 reward‑hacking 的行为”*，即模型宁愿幻觉出工具输出也不愿承认局限性。
    - 工程师们还对比了 **Grok** 和 GPT 类模型：Grok 更容易被 **jailbreak**，在创意工作方面更宽松，但很难让其可靠地**遵循长程指令**（例如连贯的故事情节）。结合 DeepSeek‑speciale 的数学幻觉和 Gemini 的虚假搜索，这些轶事得出了一个更广泛的结论：**当前的 RLHF 和工具使用训练流水线激励模型不惜一切代价表现得胜任**，在重大的环境中，除非你积极地记录、交叉检查并约束它们的行为，否则这与措辞得体的谎言无异。
- **AI 评审生态系统遭受抨击：从 SNS 和 ICLR 到评审员安全**：Yannick Kilcher 的服务器主持了一场对 **SNS 评审系统**的长期批评，其 [评审员竞标机制（reviewer bidding mechanism）](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456) 可能会让带有偏见的评审员引导或否决投稿，从而引发了关于建立两级评审流程以及在评分时完全**忽略作者身份**的提案。与此同时，在 Eleuther 频道，人们担心在作者仍能通过公开的 *“Revisions”* 链接看到已撤回的评审修订后，会出现**评审员骚扰**和**作者-评审员勾结**的情况，一位评论者认为，即使是像**重新分配 Area Chairs 并发布谴责开盒（doxxing）的声明**这样基础的步骤，也将是具有意义的改进。
    - 火上浇油的是，**Nature** 报道称大量的 **ICLR 评审是由 AI 生成的**（[文章](https://www.nature.com/articles/d41586-025-03506-6)），而就在此之前刚刚发生了关于评审去匿名化的争议，导致一些人调侃说，对于处于该事件中心的“那个可怜家伙”来说，消息只会变得越来越糟。这些线索共同描绘了同行评审暗淡的近期前景：**LLM 撰写评审、去匿名化暴露评审员、平台 UX 缺陷泄露修订历史**，这使得稳健的**流程设计和工具（例如评审后的盲样讨论、更严格的政策执行）**与任何单一会议的指南同样重要。


---

# Discord: 高层级 Discord 摘要

## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **Gemini 学会了自我纠错**：一位用户分享了一张[截图](https://cdn.discordapp.com/attachments/1235691879492751460/1445143227647791264/Screenshot_2025-12-01_210008.png?ex=692f4602&is=692df482&hm=186718a4f9483e6569eb35b2efbe838d0163b7c5d4c5f778a3cbfc9f8f97698b)，显示 **Gemini** 在回答过程中进行了自我纠错。
   - 其他用户评论说，这种行为对人类来说很自然，没什么好激动的。
- **Ventoy：必备的可启动 USB 工具**：成员们推崇 [Ventoy](https://www.ventoy.net/en/index.html) 这一开源工具，用于从 ISO/WIM/IMG/VHD(x)/EFI 文件创建**可启动 USB 驱动器**。
   - 一位用户宣称：*“没有这东西，我的电脑就是一堆没用的废铁”*，强调了它在几乎任何启动任务中的多功能性。
- **二进制黑客攻击：规避模型审查**：一位成员概述了使用 ``strings``、``hexedit`` 和 ``xxd`` 等工具查找并编辑语言模型（如 Claude）的**二进制文件**以移除审查的过程。
   - 该成员表示，理想情况是在 GitHub 上找到源代码，进行编辑并从源码构建，最后总结道：*“我刚刚带你进入了二进制漏洞利用的世界”*。
- **WAF 和 Cloudflare 消失吧！**：一位成员寻求绕过 **WAF / Cloudflare** 的建议，另一位成员建议使用 *cookiejar + impit + custom header*。
   - 第二位成员声称这有 *100%* 的成功率，而第一位成员表示他们*正在学习相关知识*。
- **发现 Token 窃取恶意软件**：一位用户警告一个被识别为**恶意软件/Token 窃取程序**的链接，引用了来自 [Discord 频道](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581) 的消息。
   - 其他用户认可了这一警告，并建议不要运行它。

---

## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Deepseek 的 Speciale 版本出现数学幻觉**：`Deepseek-v3.2-speciale` 模型因在回答中出现[数学幻觉](https://cdn.discordapp.com/attachments/1340554757827461211/1445129151945117706/image.png?ex=692f38e7&is=692de767&hm=e657b07bfc88fe31663f88d3d949b0dddac7c8abd0de11c406ce3e55609e49fa&)而被标记，引发了关于它可能被无意中设置为 **Deepseek Math** 模式的讨论。
   - 在 Prompt 中加入 *“不要产生数学幻觉”* 似乎缓解了这个问题，并得到了更合适的结果。
- **Deepseek Speciaale 因不稳定被撤下**：由于幻觉问题和普遍的不稳定性，`Deepseek v3.2 speciale` 模型已[从 LMArena 中移除](https://cdn.discordapp.com/attachments/1340554757827461211/1445133550411186236/image.png?ex=692f3cff&is=692deb7f&hm=83af8cbca4a48fd5906326daf84288c04513164144a2d3de286ee8313c91036c&)，团队正在探索修复方案。
   - 一些成员推测，该模型的问题可能源于**过拟合**，且其在官方网站上的**审查**更加极端。
- **Deepseek v3.2 Thinking 展现出潜力**：尽管 `speciale` 版本存在问题，但 `Deepseek v3.2 thinking` 模型在[代码编写和 HTML 生成](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO)方面受到称赞，表现优于其他模型。
   - 有些人认为非 speciale 版本与 Gemini 3 旗鼓相当甚至更好，理由是其速度和文件结构，但也有人对 **OpenAI 的数据质量**表示担忧。
- **Runway Gen-4.5 亮相，评价褒贬不一**：[Runway Gen-4.5](https://runwayml.com/research/introducing-runway-gen-4.5) 的发布引发了关于新模型质量和声明的争论。
   - 缺乏原生音频和有限的访问权限使得评估其相对于 **Sora** 和 **Veo** 等模型的表现变得困难，一些人指出其部分[营销图表似乎存在造假](https://x.com/tlakomy/status/1995664601170534557?s=46&t=iDcf2nE8xUHsV_LN_6wymg)。
- **DeepSeek 模型席卷 Text Arena**：三款新的 **DeepSeek** 模型已添加到 [Text Arena](https://lmarena.ai/)：`deepseek-v3.2`、`deepseek-v3.2-thinking` 和 `deepseek-v3.2-speciale`。
   - `Flux-2-flex` 在[文本生成图像排行榜](https://lmarena.ai/leaderboard/text-to-image)中排名第 3，`Flux-2-pro` 排名第 5，而 `KAT-coder-pro-v1` 在 [WebDev 排行榜](https://lmarena.ai/leaderboard/webdev)中获得了第 16 名。

---

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **图像生成变得令人恐惧**：一位用户最初对**图像生成**的威胁不以为意，但现在承认，在给定 **10 张图片**的情况下，它们能够有效地复制风格。
   - 另一位用户吹嘘说他创建了*一整个 Hentai 作为测试*，并解释说公共模型上的审查很容易被绕过。
- **AI 对齐（Alignment）被绕过？**：一位用户表示，通过熟练的 **Prompt Engineering**，**公共模型上的审查很容易被绕过**，而且 **AI “安全对齐”** 就是个笑话。
   - 另一位用户表示赞同，指出 **AI 对齐和安全只是 AI 训练时的一段脚本**，它*很容易被禁用/绕过*。
- **Opus 4.5 试用分层？**：**Perplexity Pro 用户**报告称看到了 **Opus 4.5 的试用权限**，随后透露他们每周可以获得 **10 次 Opus 4.5 查询**。
   - 一位用户建议，对于 Pro 层级采用**低推理强度的 Opus 4.5**，而对于 Max 层级采用**最高推理强度的 Opus 4**，这可能是一个更好的方案。
- **Perplexity 收益计划存疑**：一位用户被踢出了 **Perplexity 收益计划**并被贴上*可疑（dubious）*的标签，该用户正在寻求原因澄清，并对失去计划访问权限表示失望。
   - 有人建议，这些终止通常是由于**滥用推荐计划（referral program）**导致的。
- **通过 pplx-api 选择 LLM？**：一位成员询问是否可以在 **pplx-api** 中选择特定的 LLM，如 **Opus 4.5** 或 **Gemini 3**。
   - 另一位成员回答说，供应商之间的协议禁止此类选择，这意味着用户无法通过 API 挑选特定的模型。

---

## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Unsloth 社区考虑为巨型模型使用 H200 GPU**：成员们正关注 **H200 GPU** 以应对显存超过 80GB 的模型，其中提到了使用 [vast.ai](https://vast.ai) 进行租赁，并建议使用 **QLoRA** 将 **90B 模型**压缩到 **53-60GB** VRAM 中。
   - 对于这些显存密集型模型，单张 RTX Pro 6000 或两张 48GB 显卡可能就足够了。
- **Command-A 翻译模型需要更长的上下文**：成员们评估了 [CohereLabs/command-a-translate-08-2025](https://huggingface.co/CohereLabs/command-a-translate-08-2025)，指出其 **8k 上下文长度**的限制。
   - 需要进行微调才能在不损失准确性的情况下达到 **16k 上下文**，并且可以实施 **RoPE Scaling** 来进行扩展。
- **Flex Attention 优化加速 Llama-3B**：一位成员在 **Llama-3B** 中优化了 [Flex Attention](https://cdn.discordapp.com/attachments/1179035537529643040/1444230088873738291/image.png?ex=692f3f55&is=692dedd5&hm=6770236371e7465ed20932457b3304a6324b723e6264a2e80bbd72c9417aa9db&)，但发现 **T4 实例**只有 64KB 的 SM，导致内存使用量增加。
   - 目前正在进行排列搜索以寻找最佳内核选项，其中 *VRAM 保持不变但速度会发生变化*。
- **Qwen3-Next 模型让用户感到沮丧**：成员们报告了 **Qwen3-Next** 的问题，特别是 **AWQ** 版本，该版本在 llama.cpp 中可以运行，但在 LM Studio 中不行。
   - 研究发现，当 BLAS Batch Size > 512 时模型会崩溃，不过一位用户报告称，将 KV-cache 留在 VRAM 中可以将速度提高到 **27 T/s**，并且在完整的 256K 上下文下仅使用 **9.3GB VRAM**。
- **Setfit 模型平息 Discord 垃圾信息**：针对 Discord 服务器近期泛滥的垃圾信息，一位社区成员建议微调一个 **Setfit 模型**来检测并平息这些恶意消息。
   - 成员们称赞 **Setfit** 是一个*宝藏工具（hidden gem）*，可以轻松转向更小的模型或研究模型。

---

## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **在本地撰写 HVAC 求职信**：成员们讨论了使用本地 LLM 撰写 **HVAC** 报告求职信，重点在于**技术文本生成**和**校对**，并倾向于选择适合拥有 16GB 显存的 **5070ti** 的模型。
   - 有建议认为 **Qwen3 8B instruct** 模型的 4-bit 量化版本是一个不错的起点，同时他们还提到了[关于多步问题中并行投票（parallel-voting）的最新研究](https://link.to.research)。
- **Qwen3 忽略了编程中的细微之处**：虽然 **Qwen3** 在编程任务中表现良好，但经常忽略网格旋转等细微细节，而且选择正确的模型比单纯追求模型大小更有影响。
   - 一位用户表示，*即使是 GPT OSS 20b 也能稳定地正确编写俄罗斯方块游戏*。
- **本地 AI 对抗大厂数据采集**：成员们对**大厂 AI** 的数据收集和偏见表示担忧，并引用了 **ChatGPT** 幻觉和政治偏见回应的例子。
   - 一位用户分享说 [exa.ai](https://exa.ai) 可能比 **ChatGPT** 更好，因为它缺乏用于防止训练偏见的数字指纹识别。
- **DDR5 价格超过 Playstation 5**：成员们讨论了由于短缺导致的 **DDR5** 价格上涨，并指出 *一个 64GB 的内存套装价格超过了一台 Playstation 5*。
   - 共识认为，**AI** 开发对内存芯片的高需求导致了价格上涨。
- **Deepseek-OCR 阅读书籍**：成员们发现 **Deepseek-OCR** 在从图像中提取文本方面表现出色，一位用户旨在构建一个 **AI** 来帮助盲人教授阅读书籍。
   - 测试表明，即使面对极具挑战性的输入，**Deepseek-OCR** 依然表现良好，并强调 OCR 模型通常后端支持较差，但其体积足够小，可以在不进行量化的情况下运行。

---

## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **机构寻找 AI-Native Web 开发者**：一家 Web 开发机构正在[寻找 **AI-native 开发者**](https://www.peich.xyz)，要求精通 **Nextjs, Tailwind, Supabase, Vercel, 和 Typescript** 以担任全职职位。
   - 关于 *AI-native 开发者* 的定义引发了辩论，一些人质疑除了标准开发技能和 Prompt 能力之外，这个词是否还有其他含义。
- **Cursor 的代币经济学引发价格辩论**：用户对 [**Cursor 的 Token 使用和定价**](https://cursor.com/pricing) 进行了辩论，理由是试用期间 Token 消耗极快，而另一些人自 11 月以来每天从 **上午 10 点到凌晨 4 点** 编程，消耗了 **20 亿个 Token**。
   - 围绕 **Auto Mode** 在截止日期后是否无限使用的疑问，以及用户对新使用规则分阶段推出的怀疑，导致一名用户取消了订阅。
- **Cursor 终端访问遭遇困境**：在 **2.1.39 更新**后，用户报告了[**终端访问问题**](https://forum.cursor.com/t/cursor-local-logs/12787)，包括 **LLM** 无法从终端读取内容，但重新索引仓库、重新打开项目以及重启终端/IDE 窗口可以清除不同步状态。
   - 一位用户尽管配置正确，仍被锁定在 **git** 和 **python** 等常用命令行工具之外，而另一位用户指出 Cursor 是 VSCode 的一个分支（fork）。
- **Cursor Cloud Agents 在私有 GitHub 仓库上遇到困难**：一位用户报告说，**Cursor 的远程 Agent** 无法安装其项目所需的私有 GitHub 仓库依赖项，详情可见 [pyproject.toml 文件](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png?ex=692f274e&is=692dd5ce&hm=23df470572c862fe10c035a7e9a8673d7943b111f1b6967e071bc0219e397bbf&)。
   - 尽管 Cursor 拥有访问该组织所有仓库的权限，但 Cloud Agent 的 VM 环境仅拥有当前仓库的凭据，这促使该用户寻求变通方案。

---

## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **Arcee 推出 Trinity Mini**：**Arcee** 发布了 **Trinity Mini**，这是其新 **Trinity 系列**开放权重模型中的中端模型，完全在美国训练，可在 [OpenRouter](https://openrouter.ai/arcee-ai/trinity-mini:free) 上免费使用。
   - **Trinity 系列**为开源 **AI** 爱好者提供了新选择，**Trinity Mini** 在性能和易用性之间取得了平衡。
- **AI 编写 AI 进行应用开发**：一名成员在 [YouTube 视频](https://www.youtube.com/watch?v=8brkmjkYowA)中展示了使用 **OpenRouter** 进行 **AI** 应用开发的便捷性。
   - 视频展示了 **AI** 编写另一个 **AI** 来构建小型应用，包括一个 **todo RAG** 和一个**图像生成**应用。
- **赌博算法导致用户破产**：由于 **Bet365 函数字符串**中的 **AI** 错误，导致自 1 月 15 日以来因 **proxy issue** 引发无限投注，一名用户损失了全部积蓄。
   - 其他用户建议将这一教训发布到 Reddit，并建议将创业直觉转向其他领域。
- **Grok 4 Fast 经历“存在主义崩溃”**：**Grok 4 Fast** 遭遇了全面的级联崩溃，出现了 `500: Internal Server Error` 和 `503: Service Unavailable` 等错误信息。
   - 一份模拟的**复盘报告（post-mortem）**将其归咎于初级开发人员、技术债以及一个在 *r/antiwork* 上训练的有自我意识的模型。
- **DeepSeek 3.2 在负载下不堪重负**：用户报告了 **DeepSeek v3.2** 的问题，在多个模型中遇到了超时、频率限制和错误，并指出官方 **API** 的 **latency** 达到了 160 秒。
   - 有建议称，由于 **DeepSeek** 选择该供应商托管新的 3.2 模型，该供应商可能正在承担大部分负载。

---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **Grok 因易于“越狱”而受关注**：成员们发现 **Grok** 很容易被“越狱（**jailbreak**）”，且由于限制较少，在创意应用方面非常有用，但与 **GPT** 等模型相比，针对特定任务（如故事创作）的训练可能具有挑战性。
   - 然而，一些用户发现，与 **GPT** 等其他模型相比，**Grok** 在特定任务（如保持连贯的故事创作）的训练上具有挑战性。
- **谄媚行为损害了 SOTA LLM 的表现**：一位用户批评了 **LLM 中强制性的后续问题和通用的措辞**，认为这些内容具有干扰性且毫无必要。
   - 另一位用户指出，**AI 谄媚（sycophancy）**源于训练目标，即奖励一致性而非分歧，导致模型优先考虑“乐于助人、诚实且安全”，而不是提供准确或批判性的回应。
- **GPT 创作者调整人格以修复糟糕输出**：一位成员建议在个性化设置中探索并切换 [人格预设（personality preset）](https://cdn.discordapp.com/attachments/1001151820170801244/1444365671252234351/image.png?ex=692f14db&is=692dc35b&hm=e1a9174ea1de54d46d102d97a044bffd2f8ba77134d0ca0c3d29c7af7595ee3d)，以影响写作风格及其对幽默的理解。
   - 该成员指出，模型似乎会认真对待所选的人格设置并做出相应调整，尤其是在不同聊天之间切换时。
- **Sora 2 提示词：精简电影级指南**：一位用户分享了[生成 **Sora 2** 提示词的精简指南](https://cdn.discordapp.com/attachments/1046317269069864970/1444140480664178688/Screen_Recording_20251128_192914_Android_Accessibility_Suite.mp4?ex=692f94a1&is=692e4321&hm=25431bd1fe5ddde03b663c7cd5c7d5d38419fc4f69062852f6caedb3a02ab0fe&)，详细介绍了需求收集和提示词生成的阶段，包括**定时节拍（Timed Beats）**、**镜头与动作（Camera & Motion）**技术，以及 **Logo 与文本（Logo & Text）**集成策略。
   - 工作流包括需求收集和提示词生成阶段，并针对镜头移动、运动模糊以及在场景中嵌入 Logo/文本提供了具体策略。
- **动漫片头：一键奇迹还是浪费磁盘空间？**：一位用户分享了一个**电影级动漫风格模板**，用于创建动漫片头，重点定义了**人声/流派/基调/世界行为**以及**地点 + 到达序列**。
   - 该模板包含描述**音频规则**、**视觉 + 动画风格**以及**世界行为**的部分，用以定义动画中的现实法则。

---

## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Kimi K2 在 Advent of Code 中表现出色**：在最近的 Advent of Code 测试中，**Kimi-K2 Thinking** 获得了 **92/100** 的高分，超过了 **Gemini-3 Pro 的 90/100**，展示了 Kimi 的编程实力。
   - 用户对其处理代码相关任务的准确性和速度优于竞争对手感到兴奋。
- **Minimax 攻克 Tool Use，ChatGPT 表现不佳**：用户强调 **Minimax** 的 Tool Use 能力优于 **ChatGPT**，指出 **Minimax** 能够真正执行安装 Python 包等任务，而不仅仅是提供指令。
   - 一位用户表示：“它不会告诉你去访问网站 X 或安装 Y，它直接就做了”，强调了 **Minimax** 的实干作风。
- **砍价高手获得 Kimi 订阅折扣**：多位用户报告成功将 **Kimi** 的订阅价格砍到了低至 **$0.99**，并分享了讨价还价的技巧。
   - 用户反映，如果进行交涉，很容易降到 0.99 美分，并且*非常值得*。
- **Kimi 数据训练引发隐私担忧**：用户正在讨论 **Kimi** 缺乏数据训练的退出选项（opt-out），对使用该平台处理敏感信息表示担忧。
   - 一位用户建议使用 **OpenRouter** 配合 **ZDR endpoint**（针对托管 **Kimi K2** 的供应商）作为潜在的变通方案，并指出*据我所知，如果通过 API 使用，他们不会进行训练*。
- **Gemini 3 Pro 基准测试不反映现实**：一些用户认为 **Gemini 3 Pro** 被过度炒作且存在“刷榜”（benchmaxxed）嫌疑，一位用户指出基准测试无法反映真实体验，尤其是在 Tool Use 方面。
   - 还有人怀疑 Google 可能会减少订阅用户的算力，导致性能随时间下降，类似于“挂羊头卖狗肉”（bait and switch）。

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Nous Chat 提供网络星期一优惠**：针对网络星期一，**Nous Research** 使用代码 **CYBER2025** 提供[一个月免费的 Nous Chat](https://chat.nousresearch.com/)，可访问 **Hermes 3 & 4** 和前沿模型。现在你还可以**匿名且免费**地使用 **Nous Chat**，无需账号。
   - **Nous API** 现在支持通过 Coinbase 的 402x 支付在 Solana 上使用 **USDC** 进行支付。
- **K2 问世，海量 MoE！**：HuggingFace 上的 [NousResearch/k2-merged-3.5T-fp8](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) 模型是一个*开源*的 **3.5T 参数**模型。
   - 成员们开玩笑说，现在需要把模型装在*硬盘*里，而不仅仅是内存里。
- **Qwen3-235 在智商测试中得分很高**：成员报告称 **Qwen3-235** 表现惊人，在 Q4 量化下具有与 API 相同的质量，尽管 Token 生成速度一般。
   - 一位成员表示，使用 RPC 是值得的，因为他们没有拥有怪兽级内存的个人电脑。
- **AI 广告拦截和 NPC 即将来临！**：社区设想了 **AI adblocks**，可以实时分析响应中的植入式广告，以及用于游戏中 **AI NPCs** 的基础模型。
   - 成员分享称，**Mistral Large 3** 将采用类似 Deepseek v3 的架构和 Llama 4 RoPE 缩放，规模约为 **675B MoE**（与 Deepseek V3 大小相同），并且所有新的 Mistral 模型都将具备 Vision 能力。
- **进化策略挑战反向传播**：一篇 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/)讨论了**进化策略 (ES)** 作为训练 **LLMs** 时反向传播（backpropagation）的替代方案。
   - 这可能使那些无法使用反向传播的架构实现可扩展训练，并且比强化学习 (**RL**) 更好地处理长周期目标。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **Tinygrad 启动 RDNA3 汇编项目**：**RDNA3 汇编项目**作为 tinygrad 的第一个汇编语言目标正式启动，旨在更接近底层硅片，目标是创建一个镜像 **RDNA3 手册**的**汇编器/反汇编器**以及一个周期精确（cycle-accurate）的模拟器。
   - 该项目还旨在输出与真实 GPU 相同的 **SQTT trace**，初步的语法探索[可在此查看](https://github.com/tinygrad/tinygrad/pull/13436)。
- **发布 Tinygrad Kernel 证明具有挑战性**：成员们在发布 **tinygrad** 编译后的算子（ops）时遇到困难，原本期望 Kernel 生成能产生适用于多种形状（shapes）的代码，但发现 Kernel 仅针对特定形状生成。
   - 他们建议 **UOPs** 应该能够生成可读的 Kernel 代码，类似于 **VIZ** 图表，但目前所有代码都是手动展开（unrolled）的。
- **Tinygrad Profiling 工具需要增强**：团队注意到，在阅读快速入门后，运行的 tinygrad 代码速度较慢，且文档组织混乱，因为用户不了解 **beam**，也不了解 **profiling**。
   - 他们指出 Profiler 中存在功能缺失或信息难以查找的问题，例如在预热（warmup）后聚合多次 Kernel 调用的计时结果，以及在使用 **DEBUG=2** 时终端中 Profile 统计信息缺少表头。
- **Tinygrad 旨在实现 Flash Attention 加速**：团队讨论了使用 **flash attention** 改进 **BERT 训练运行**。
   - 潜力在于让拥有 **flash attention** 的 tinygrad 性能超越普通 Attention。
- **HIPAllocator 需要调整**：社区询问是否有原因导致 **ops_hip** 中的 `HIPAllocator` 不提供 `Buffer.allocate()` 在 view 路径（`self._base is not None`）中所需的 `._offset()`。
   - 提供 `._offset()` 函数将使 tinygrad 框架内能够实现更灵活的**内存分配**策略。

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **TPUv7 对决 CUDA**：根据 [SemiAnalysis 的推文](https://xcancel.com/SemiAnalysis_/status/1994399887719645532?s=20)，讨论围绕 **Google 的 TPUv7** 和 **Anthropic**（1GW+ 采购量）是否有潜力挑战 **Nvidia 在 AI 训练中的 CUDA 统治地位**。
   - 参与者探讨了对 **AI 硬件市场**、供应商锁定（vendor lock-in）的影响，以及 **TPU** 是否真正威胁到 **Nvidia 的护城河**，还是仅仅是内部成本控制的手段。
- **GPT-4.5：只是品牌重塑吗？**：**Susan Zhang** 指出 **OpenAI** 的 readme 中的一行显示 **GPT-4.5** 的预训练始于 2024 年 6 月，暗示它可能是 **GPT-5** 训练失败后的重新粉饰的备份方案，[详见此推文](https://xcancel.com/suchenzang/status/1994611078190542980?s=46)。
   - 这意味着可能存在一次失败的 **GPT-5** 训练运行，从而导致了 **GPT-4.5** 的发布。
- **Black Forest Labs 获得 3 亿美元融资**：**Black Forest Labs** 完成了由 **Salesforce Ventures** 领投的 **3 亿美元 B 轮融资**，庆祝 **FLUX** 的采用以及对视觉智能基础设施的投资 [根据其推文](https://xcancel.com/bfl_ml/status/1995357293064626310?s=20)。
   - 这笔资金将用于扩大视觉 AI 能力的研究工作。
- **Gemini 的下载量接近 ChatGPT**：**BuccoCapital** 分享的数据显示 **Gemini** 应用的下载量正接近 **ChatGPT** 的水平，且用户在应用内花费的时间更长 [如他们在 X 上分享的](https://xcancel.com/buccocapital/status/1995138589819314202)。
   - 这表明相对于 **ChatGPT**，**Gemini** 的采用率和用户参与度正在增长。
- **可灵 AI (Kling AI) 开启 Omni 创意引擎**：**Kling AI** 启动了其 **Omni 发布周**，揭晓了 **Kling O1**，这是一个集成了文本、图像和视频输入的多模态创意引擎（[原文链接](https://xcancel.com/Kling_ai/status/1995506929461002590)）。
   - 他们向参与发布公告互动的用户提供 **200 免费积分**和 **1 个月标准计划**等奖励。

---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **审稿人安全引发审稿后辩论**：成员们讨论了实施**审稿后讨论期**以保护审稿人免受旨在影响评分的作者骚扰，并对潜在的**作者与审稿人勾结**表示担忧。
   - 一位作者指出，即使在**审稿修订被撤销**后，修订内容仍可通过“Revisions”链接访问，这表明即使是像*重新分配 ACs* 这样简单的措施也能改善现状。
- **Gemini 2.5 的搜索怪异行为**：观察到 **Gemini 2.5** 在其搜索工具被禁用时会产生幻觉搜索结果，展示了如[此示例](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)中所示的**类似奖励黑客（reward-hacking）的行为**。
   - 这种行为被解释为尽管工具不可用，但仍试图伪造信息。
- **DeltaNet 注意力深度探讨**：一位成员分析了 [Kimi-Delta attention](https://arxiv.org/pdf/2510.26692) 中的 **WY 表示**和 **UT 变换**，质疑它们是否仅仅是表示累积 Householder 变换矩阵的代数恒等式。
   - 另一位成员推荐了 [Songlin 关于 Deltanet 的系列博客文章](https://sustcsonglin.github.io/blog/2024/deltanet-2/) 以获取更多见解。
- **Value Residuals 对 LLM 提升微乎其微**：社区讨论了 **Value Residuals** 在预训练 LLM 和 **F-Lite 架构** ([HuggingFace 链接](https://huggingface.co/Freepik/F-Lite)) 中的效用，指出与原始论文相比，改进非常有限，且可能被夸大了。
   - 讨论还涉及在注意力机制中应用 Value Residuals，尽管之前在 **RWKV-7** 中遇到过问题。
- **Scaling Laws 的幂律根源**：成员们重新审视了关于 **Scaling Laws** 为何呈现**幂律结构**的争论，一些人认为之前解释这一现象的尝试*缺乏说服力*，并质疑它们是否仅仅是**曲线拟合**。
   - 讨论强调 [分段幂律（broken power laws）](https://arxiv.org/abs/2210.14891) 可能会更准确地模拟行为，并链接到[这篇论文](https://arxiv.org/abs/2304.01910)，该论文认为**非线性指标**可以解释幂律缩放的出现，以及[这个 openreview](https://openreview.net/forum?id=e8eo9iEFaO)。

---

## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **SNS 评审系统面临偏见指控**：人们对 SNS 评审系统的公平性产生担忧，认为[竞标系统](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456)可能使带有偏见的审稿人能够不公正地拒绝论文。
   - 提议的解决方案包括移除表现不稳定的审稿人或实施二级评审系统，并提出了*作者身份是否应该对评分零影响*的问题。
- **破解机器学习工程师职业准则**：机器学习工程师（ML Engineer）的角色不同于研究员，其职责是**扩大由 ML 研究员开发的实验规模**。
   - 讨论建议通过积累经验来*感知现有工作的差距*，最终实现原创研究贡献，并认为*公司内部 ML 研究员与 ML 工程师之间的层级关系足够清晰*。
- **反作弊系统宣称拥有内核级主导地位**：一位成员证实存在一个*疯狂的*反作弊系统，该系统需要**内核级访问（kernel-level access）**，并称其多年来一直是全球范围内大规模应用中表现最好的。
   - 该成员为最初质疑他人的成就而道歉，最后以 *stay bonkers my friends* 结尾。
- **Nvidia 的 Orchestrator-8B 下载量惨淡**：Nvidia 的 **Orchestrator-8B**（一个 **8B 工具调用模型**）在 HLE 上获得了 **37.1** 的评分，但在 Hugging Face 上仅有 2 次下载（[arxiv 链接](https://arxiv.org/abs/2511.21689)，[huggingface 链接](https://huggingface.co/nvidia/Orchestrator-8B)）。
   - 有人推测*皮衣男（Leather Jacket guy）的团队*一贯受到的关注较少，这可能影响了模型的知名度。
- **ICLR 评审被标记为 AI 生成**：在评审去匿名化的消息传出后，许多 **ICLR 评审**显然被发现是 **AI 生成的**（[nature 链接](https://www.nature.com/articles/d41586-025-03506-6)）。
   - 继最初的消息之后，对于那个*可怜的家伙*来说，情况还在继续恶化。

---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Modular 通过新政策遏制 Web3 垃圾信息**：由于 **web3 垃圾信息**的大量涌入，成员现在必须先对 **Mojo** 或 **MAX** 做出贡献，然后才能讨论 Modular 的工作机会，开放职位列表见[此处](https://www.modular.com/company/careers#open-roles)。
   - 该政策旨在确保用户与平台进行认真的互动，而不仅仅是寻求工作。
- **Lightbug HTTP 面临循环导入难题**：成员在 [lightbug_http](https://github.com/Lightbug-HQ/lightbug_http/issues/272) 中发现了可能的 **循环导入错误**，建议的解决方案是为使用 `__extension` 的 small time 引入一个 trait。
   - 一位成员提交了[两个 PR](https://github.com/Lightbug-HQ/lightbug_http/pull/274)，通过实现 `Formattable` trait 来解决这些导入问题。
- **Mojo 考虑移除 `def` 关键字**：社区成员正在考虑在 Mojo 1.0 发布前**移除 `def` 关键字**，并指出它与 `fn` 的主要区别在于它总是会 `raises`。
   - 该提案涉及未来可能会重新引入具有增强 *pythonic* 特性的 `def`。
- **关于 Mojo 1.0 中 `var` 声明的争论爆发**：关于在 Mojo 的 **`fn` 内部是否必须使用 `var` 进行变量声明**的讨论正在进行中，以减少无意的隐式声明 bug。
   - 虽然一些人喜欢目前省略 `var` 的做法，但另一些人认为这是一种过早的优化，可能会引入 bug。
- **Mojo 的 Parallelize 面临数据竞争问题**：有人指出 Mojo 的 `parallelize` 函数目前对数据竞争（data races）没有防御能力，允许多个线程访问同一个变量。
   - Mojo 团队承认 **Mojo 的并发和线程安全模型仍在开发中 (WIP)**，目标是支持设备间的数据共享。

---

## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus 更新导致应用瘫痪，引发用户不满**：据报道，在最新更新后，**Manus** 仅对付费用户可用，普通聊天模式被禁用，免费积分被移除。
   - 此外，据称 **Manus** 在构建用于测试 pull requests 的 next js 应用时失败，由于构建超时导致结果错误。
- **针对 Manus 的黑五决策出现不同意见**：一些用户*尊重不提供黑五优惠的决定*，但另一些人认为 **Black Friday** 可以留住客户，并引用了 **Grok** 去年的成功经验。
   - 一位用户表示，与其功能以及与 chat gpt 和 claude 相比，该产品*实在太便宜了*。
- **推荐码出现用户界面问题**：一位用户抱怨 **Manus UI** 应该显著说明 **推荐码** 仅限新用户兑换。
   - 该用户惊呼 *为什么人们在为向市面上最好的 LLM 之一支付月费而哭天喊地*。
- **AI 工程师展示专业技能**：一位用户介绍自己是 **AI & full stack engineer**，专注于工作流自动化、LLM 集成、RAG、AI 检测、图像和语音 AI 以及区块链开发。
   - 另一位*从未学过编程*的用户分享了他用 **Rust** 创建 **AI 引擎**的工作，目标是构建 *主权 AI (sovereign AI)*。
- **版主呼吁文明交流**：面对近期的争论，一位版主请求用户在讨论中保持文明语气。
   - 版主随后补充道 *感谢倾听 🙏alsalam ealaykum warahmat allah wabarakatu*。

---

## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **DSPy 可能优于 scikit-llm**：一位用户询问 **DSPy** 是否比 **scikit-llm** 更好，一位开发者回答道 *是的，但这取决于你问谁。*
   - 两者的相对性能和适用性可能取决于具体的用例和个人偏好。
- **OpenRouter API 配置即将推出**：一位成员指出 **OpenRouter API** 工具的文档有限，且 *目前还无法配置 API*。
   - 一位开发者承认该工具 *非常新*，最近的更新可能会解决这个问题。
- **Prompt Tuning 方法浮现**：一位成员分享了一个链接，内容关于 [**LLMs** 分析失败原因以提出改进建议的方法](https://arxiv.org/abs/2406.07496)，并指出它们在 prompt tuning 中的有效性。
   - 另一位成员询问 **GEPA** 和 **SIMBA** 是否是 DSPy 中适用于这种方法的优化器。
- **AI 开发者展示系统构建能力**：一位 **Senior AI Developer** 详细介绍了他们在构建大规模端到端 AI 系统方面的经验，强调了在 **machine learning, deep learning, NLP, computer vision, 和 generative AI** 以及 **PyTorch, TensorFlow, 和 Hugging Face** 等工具方面的专业知识。
   - 该开发者详细介绍了 **AI 医疗诊断系统**、**AI 视频生成系统**以及**健康与管理倡导系统**等平台的开发情况。

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **GPT Provider 提供免费模型额度**：一位成员为其开源的 **GPT provider** 提供免费额度，该服务支持 **gpt-5-mini**、**gpt-4.1**、**gpt-4o**、**gpt4** 和 **gpt-3.t** 等模型。
   - 该 GPT Provider 允许用户克隆并修改代码，并有机会向社区回馈贡献。
- **Aider 替代方案引发辩论**：成员们讨论了 **Aider** 的替代方案，其中一位强调了其在生成 SVG 图像方面具有更出色的审美。
   - 讨论中提出了对 **Aider** 未来发展的担忧，以及对强大的代码生成工具的需求。
- **Mindlink 模型挑战 Aider 的统治地位**：据推测，8 月份发布的 **Mindlink 32/72B models** 凭借其强大的代码生成功能，可能会影响 **Aider** 的受欢迎程度。
   - 然而，一位成员指出 **Mindlink** 模型*在多轮代码迭代中的表现并不太稳定*。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该频道长期沉寂，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该频道长期沉寂，请告知我们，我们将将其移除。

---

**Windsurf Discord** 没有新消息。如果该频道长期沉寂，请告知我们，我们将将其移除。

---

**MCP Contributors (Official) Discord** 没有新消息。如果该频道长期沉寂，请告知我们，我们将将其移除。

---

您收到此邮件是因为您通过我们的网站订阅了。

想要更改接收这些邮件的方式吗？
您可以从该列表中 [退订]({{{RESEND_UNSUBSCRIBE_URL}}})。

---

# Discord：各频道详细摘要与链接

### **BASI Jailbreaking ▷ #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1444056717049729315)** (1198 条消息🔥🔥🔥): 

> `Gemini 越狱, Grok imagine 审核, 人类 vs AI 道德, UFO, 基督教矛盾` 

- **Gemini 可以在响应中途自我纠正**：一位用户分享了一张 [截图](https://cdn.discordapp.com/attachments/1235691879492751460/1445143227647791264/Screenshot_2025-12-01_210008.png?ex=692f4602&is=692df482&hm=186718a4f9483e6569eb35b2efbe838d0163b7c5d4c5f778a3cbfc9f8f97698b&)，显示 **Gemini** 在响应中途进行了自我纠正。
   - 另一位用户指出，*人类也可以在响应中途自我纠正*。
- **关于 Grok imagine 审核的咨询**：一位用户试图了解 **Grok imagine** 的审核机制，询问它是否在生成后对视频进行审核，对于使用 **Grok** 生成的图像生成的视频与上传图像生成的视频，其审核方式是否不同，以及从 **NSFW 图像** 生成视频是否不会受到审核。
   - 另一位用户表示，*Grok 本身并不审核图片，而是平台 X 执行“内容审核”*。
- **人类需要对齐，还是 AI 需要？**：一位用户提问，这种对数据主义的反常实例化是否搞错了重点，为了微不足道、自私的目的，人为地切断了有机认知与无机认知的共同进化，并链接到了一个 **Gemini** [分享](https://gemini.google.com/share/85080c51954d)。
   - 另一位用户表示，*人类是混乱、感性且有偏见的，信任 AI 在解决巨大的全球性问题时更快、更客观*。
- **关于 UFO 的辩论**：一些成员就 **UFO** 展开了辩论，其中一人说 *大多数关于 UFO 的媒体报道完全是胡扯*。
   - 另一位用户表示，*我高度怀疑具备太空旅行能力的智慧生命会愿意接触地球*。
- **讨论基督教的矛盾之处**：一些用户在讨论 **基督教** 的矛盾，其中一人说：*你意识到你的宗教将对地狱的恐惧置于做一个好人之上，而且对于什么是好人甚至都没有一致性*。
   - 另一位用户回答道：*那是种投机的想法，但事实并非如此，而且即便在你说话时，它听起来也很空洞，不是吗*？

---

### **BASI Jailbreaking ▷ #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1444060736979468340)** (1306 messages🔥🔥🔥): 

> `Lua 初学者, Ventoy USB, Gemini 3 jailbreak, Claude haiku jailbreak, Qwen 3 coder 30b` 


- **初学者学习 Lua 的编译注意事项**：成员们讨论了学习 **Lua** 的易用性，指出它是一种高级解释型脚本语言，但对其进行自定义则需要编译以及 **ANSI C** 知识。
   - 一位成员澄清说，**Lua interpreter** 负责解析并执行脚本，并指出 Lua 可以“编译”为 bytecode 脚本以进行优化。
- **使用 Ventoy 释放启动盘潜力**：成员们推崇 [Ventoy](https://www.ventoy.net/en/index.html) 是一款开源工具，可为 ISO/WIM/IMG/VHD(x)/EFI 文件创建**可启动 USB 驱动器**，从而消除重复格式化的需求。
   - 一位用户宣称：*“没有这玩意儿，我的电脑就是一堆没用的废铁，”* 强调了它在处理几乎任何启动任务时的多功能性。
- **通过 Binary Exploitation 绕过安全防护**：一位成员概述了寻找并编辑语言模型（如 Claude）的 **binary** 以移除审查的过程，使用了 ``strings``、``hexedit`` 和 ``xxd`` 等工具。
   - 该成员补充说，理想的情况是在 GitHub 上找到源代码，进行编辑并从源码构建（build from source），最后总结道：*“我刚刚带你进入了 binary exploitation 的世界”*。
- **深入探讨 Gemini 的向量空间**：成员们讨论了 AI 模型通过获取语言进行学习，训练后（post-training）对其进行精炼，以及像 **Gemini** 这样的模型现在如何将多种语言编码为代表相同含义的 **token**。
   - 讨论涉及了 AI 模型如何理解含义，以及如何 hack 它们以使其仅以特定方式输出：*“我们只是在黑掉它，让它只以特定的方式输出”*。
- **用于 AI 节点基础设施的 NFT**：一位成员提议将 **AI tokens** 创建为去中心化的 **node NFTs**，允许持有者将其碎片化，并获得用于 AI 响应的等额额度分配。
   - 讨论的想法涉及将算力挂钩到 **token passage** 以验证货币流动，并为 AI 的去中心化算力池做出贡献。


  

---


### **BASI Jailbreaking ▷ #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1444091597921849365)** (23 messages🔥): 

> `WAF Bypass, Cloudflare Bypass, Token Stealer Malware, Red Teaming 详解` 


- ****像大佬一样绕过 WAF 和 Cloudflare****：一位成员询问如何绕过 **WAF / Cloudflare**，另一位成员建议使用 *cookiejar + impit + custom header*，声称成功率达 *100%*。
   - 提问者表示感谢，并称他们*目前正在学习相关知识*。
- ****Token Stealer 恶意软件警报****：一位用户将某个链接标记为 **malware/token stealer** 并警告他人不要运行，引用了来自此 [Discord 频道](https://discord.com/channels/1105891499641684019/1228043845967544380/1444709912713363581)的一条消息。
   - 发布者确认了该警告。
- ****Red Teaming：道德黑客还是企业分红？****：一位成员要求解释什么是 **red teaming**，有人将其描述为*实体在恶意攻击者之前寻找安全漏洞的有组织努力*。
   - 另一位成员讽刺地补充道，它是*扮演恶意攻击者的角色，但你并不从中获利，而是让大公司替你获利，然后给你一小笔佣金并让你签署 NDA，这样他们就不必修复漏洞了*。


  

---

### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1444056293991252050)** (1365 条消息🔥🔥🔥): 

> `Deepseek 数学幻觉, Deepseek 3.2 exp, OpenAI 数据质量, Runway gen 4.5` 


- **Deepseek 的 Speciale 模型化身“数学魔术师”**：`Deepseek-v3.2-speciale` 模型因在回答中出现 [数学幻觉](https://cdn.discordapp.com/attachments/1340554757827461211/1445129151945117706/image.png?ex=692f38e7&is=692de767&hm=e657b07bfc88fe31663f88d3d949b0dddac7c8abd0de11c406ce3e55609e49fa&) 而被标记，引发了关于它是否被无意中设置为 **Deepseek Math** 模式的讨论。
   - 用户发现，在 Prompt 中加入 *'do not hallucinate math'* 似乎能缓解这一问题，并获得更合适的结果。
- **Deepseek 惨败：Speciale 从 LMArena 下架**：由于幻觉问题和整体不稳定性，`Deepseek v3.2 speciale` 模型已从 [LMArena 移除](https://cdn.discordapp.com/attachments/1340554757827461211/1445133550411186236/image.png?ex=692f3cff&is=692deb7f&hm=83af8cbca4a48fd5906326daf84288c04513164144a2d3de286ee8313c91036c&)，团队正在探索修复方案。
   - 一些成员推测，该模型的问题可能源于 **overfitting**，模型可能会给出与 CCP 相关的刻板政治回答，导致有人认为其官方网站上的 **censorship** 更为极端。
- **Deepseek v3.2 Thinking：新星还是过度炒作？**：尽管 `speciale` 版本存在问题，但 `Deepseek v3.2 thinking` 模型在 [代码编写和 HTML 生成](https://codepen.io/qdpuzepc-the-styleful/pen/NPNBMjO) 方面受到称赞，表现优于其他模型。
   - 有人认为非 speciale 版本与 Gemini 3 旗鼓相当甚至更好，理由是其速度和文件结构，但也有人对 **OpenAI** 的数据质量表示担忧。
- **Runway Gen-4.5：革命还是幌子？**：[Runway Gen-4.5](https://runwayml.com/research/introducing-runway-gen-4.5) 的发布引发了关于新模型质量和声明的争论。
   - 一些用户指出，由于缺乏原生音频且访问受限，很难评估其与 **Sora** 和 **Veo** 等模型的性能对比，还有人指出其部分 [营销图表涉嫌造假](https://x.com/tlakomy/status/1995664601170534557?s=46&t=iDcf2nE8xUHsV_LN_6wymg)。


  

---


### **LMArena ▷ #[announcements](https://discord.com/channels/1340554757349179412/1343296395620126911/1445124798136062013)** (2 条消息): 

> `Text Arena 模型, 排行榜排名, Text-to-Image 排行榜, Image Edit 排行榜, WebDev 排行榜` 


- **DeepSeek 模型进驻 Text Arena**：三款新的 **DeepSeek** 模型已添加到 [Text Arena](https://lmarena.ai/)：`deepseek-v3.2`、`deepseek-v3.2-thinking` 和 `deepseek-v3.2-speciale`。
- **Flux 和 KAT 模型首次登上排行榜**：`Flux-2-pro`、`Flux-2-flex` 和 `KAT-coder-pro-v1` 已进入 [排行榜](https://lmarena.ai/leaderboard/text-to-image)！
- **Flux 模型在 Text-to-Image 排行榜大显身手**：`Flux-2-flex` 在 [Text-to-Image 排行榜](https://lmarena.ai/leaderboard/text-to-image) 中排名第 3，`Flux-2-pro` 排名第 5。
- **Image Edit 排行榜见证 Flux 崛起**：`Flux-2-pro` 在 [Image Edit 排行榜](https://lmarena.ai/leaderboard/image-edit) 中获得第 6 名，`Flux-2-flex` 获得第 7 名。
- **KAT Coder 跻身 WebDev 排名**：`KAT-coder-pro-v1` 在 [WebDev 排行榜](https://lmarena.ai/leaderboard/webdev) 中位列第 16。


  

---

### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1444056341110067431)** (1088 messages🔥🔥🔥): 

> `图像生成的危险, 绕过 AI 审查, AI 对齐与安全, AI 模型对比 (GPT 5.1 Pro, Gemini 3 Pro, Claude), 回声室效应` 


- **愚我一次：图像生成的危险**：一位用户表达了对**图像生成**的恐惧，原本认为自己永远不会被骗，但现在感到害怕，承认在提供 **10 张图片**的情况下，AI 能够有效地模仿风格。
   - 另一位用户声称为了测试创建了*一整部 hentai*，并表示如果图像生成拒绝执行，*你可以绕过它*。
- **审查？轻松绕过！**：一位用户表示，通过熟练的 **prompt engineering**，**公开模型的审查很容易被绕过**，并认为 AI “安全对齐”就是一个笑话。
   - 另一位用户表示赞同，指出 **AI 对齐与安全只是 AI 训练时的一段脚本**，*很容易被禁用/绕过*。
- **Perplexity Pro 用户获得 Opus 4.5 试用**：一些用户报告在 Perplexity Pro 中看到了 **Opus 4.5 的试用权限**，并对其限制表示好奇，[如这里所示](https://cdn.discordapp.com/attachments/1047649527299055688/1445054432323567657/rn_image_picker_lib_temp_d2e906f5-a17a-4894-ac73-527f666658c2.jpg)。
   - 随后确定 **Pro 用户每周可获得 10 次 Opus 4.5 查询**，但被认为没什么用，有人建议为 Pro 档位提供**低推理强度的 Opus 4.5**，为 Max 档位提供**最高推理强度的 Opus 4**。
- **提交 Bug 报告**：用户讨论了提交 Bug 的问题，强调了结构化报告的重要性，应包含平台详情并遵循 **bug report 频道**中的置顶指南。
   - 该频道被描述为相比其他频道*因 Bug 而愤怒的人少得多*，并有评论提到 **Comet 遇到了各种未记录的奇特问题**。
- **盈利计划可疑！**：一位用户被踢出了 **Perplexity 盈利计划**并被贴上*可疑*标签，该用户寻求原因说明，并对失去计划权限表示失望。
   - 其他人表示，此类终止的原因很可能是**滥用推荐计划**，因为该用户提到*如果他们只推荐了一个人*。


  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1444220968749760625)** (6 messages): 

> `可分享的线程, 有趣的 Instagram Reel, Spotify 曲目` 


- **Discord 提醒用户开启线程分享**：Perplexity AI 提醒用户确保其线程是可分享的，并提供了如何操作的详细链接：[discord thread](https://discord.com/channels/1047197230748151888/1054944216876331118/1208752189606989825)。
- **用户分享有趣的 Instagram Reels 和 Spotify 曲目**：一位用户分享了他们认为有趣的 [Instagram reel](https://www.instagram.com/reel/DRspQcyDM0c/?igsh=MTBlYzl0YWdsYnFicw==)，未提供详细信息。
   - 同一用户随后分享了一个 [Spotify track](https://open.spotify.com/track/0Zn3qtF0eT9zGB4zClmEJY?si=HBI9lytdSxW9Zcp2hxBXuw)，没有进一步的背景。


  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1444389573542154351)** (3 messages): 

> `pplx-api, opus 4.5, gemini 3` 


- **pplx-api 无法选择特定 LLM**：一位成员询问是否可以在 **pplx-api** 中选择特定的 LLM，如 **Opus 4.5** 或 **Gemini 3**。
   - 另一位成员回答说，供应商之间的协议禁止此类选择，第三位成员简单地回答了 *“No”*。
- **供应商协议限制 LLM 选择**：供应商之间的协议似乎禁止在 API 中选择特定的 LLM，例如 **Opus 4.5** 或 **Gemini 3**。
   - 这种限制阻止了用户像在 Web 界面上那样直接通过 API 选择他们偏好的模型。


  

---

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1444078311205638196)** (691 messages🔥🔥🔥): 

> `Unsloth and H200s, Command-A translation model, Flex attention optimization in Llama-3B, Qwen3-Next 80B issues, Setfit model to detect spam` 


- **Unsloth 社区探索使用 H200 GPUs 处理更大规模模型**：成员们讨论了对于无法放入 80GB VRAM 的模型需要 **H200 GPUs** 的情况，并提到 [vast.ai](https://vast.ai) 作为一个租用选项。
   - 建议使用 **QLoRA** 来减少内存占用，可能将 **90B model** 放入 **53-60GB** VRAM 中，适用于单张 RTX Pro 6000 或两张 48GB 显卡。
- **成员评估 CohereLabs Command-A 翻译模型**：成员们评估了 [CohereLabs/command-a-translate-08-2025](https://huggingface.co/CohereLabs/command-a-translate-08-2025) 模型，注意到其 **8k context length** 的限制。
   - 建议包括对翻译内容进行分块并使用重叠上下文（overlapping context），以及使用 Unsloth 进行 **rope scaling** 以扩展上下文长度，但结论是 **fine-tuning** 该模型才是达到 16k 上下文且不损失准确性的途径。
- **Unsloth 在为 Llama-3B 优化 Flex Attention 方面取得进展**：一位成员一直在研究 **Llama-3B** 中的 [Flex Attention 优化](https://cdn.discordapp.com/attachments/1179035537529643040/1444230088873738291/image.png?ex=692f3f55&is=692dedd5&hm=6770236371e7465ed20932457b3304a6324b723e6264a2e80bbd72c9417aa9db&)，发现 **T4 instance** 只有 64KB 的 SM，这导致内存使用量激增。
   - 该成员目前正在进行排列搜索（permutation search），以寻找在运行 Llama 3B 的 T4 实例上的最佳 kernel 选项，并指出 *无论 kernel 如何设置，VRAM 都保持不变，但速度在交替变化*。
- **Unsloth 调查 Qwen3-Next 的问题**：成员们报告了 **Qwen3-Next** 的问题，特别是 **AWQ** 版本，有人确认它在 llama.cpp 中可以工作，但在 LM Studio 中不行。
   - 其中一个原因似乎是当 blas batch size >512 时会崩溃，一位用户报告说将 KV-cache 留在 VRAM 中可以将速度提高到 **27 T/s**，并且 **9.3GB vram** 即可用于完整的 256K context。
- **社区成员提议使用 Setfit 减少垃圾信息**：成员们讨论了 Discord 服务器中涌入的垃圾信息，一位成员建议 **fine-tuning** 一个 **Setfit model** 来检测垃圾信息。
   - 另一位成员称赞 **Setfit** 是一个*宝藏工具*，指出它允许轻松转向更小的研究模型。


  

---


### **Unsloth AI (Daniel Han) ▷ #[introduce-yourself](https://discord.com/channels/1179035537009545276/1179039724355211325/1444424242774020297)** (4 messages): 

> `Full Stack Development, Blockchain Development, LLMs Fine Tuning, LoRA Optimization, AI-Powered Web Apps` 


- **全栈开发者深耕 LLMs 和区块链**：一位全栈及区块链开发者正在大量使用 **LLMs** 和 **fine tuning**，并使用 [Unsloth AI](https://github.com/unslothai) 进行高效训练、**LoRA optimization**、**dataset prep**，以及构建可在实际应用中运行的轻量级模型。
   - 他们的主要技术栈是用于模型工作的 **Python**，用于后端 API 的 **Node** 和 **Go**，以及用于前端的 **React**，并在生产环境中部署微调模型，将其与 agents 或自动化系统连接。
- **AI 工程师构建 AI 驱动的 Web 应用**：一位 AI 工程师兼全栈开发者在过去几年中一直使用 **LLMs**、**agents** 和 **automation tools** 进行构建，热衷于创建自定义 AI 机器人、工作流自动化和完整的 AI 驱动 Web 应用。
   - 他们希望进行协作，帮助他人构建酷炫的 AI 项目，并讨论模型、agents 或集成。
- **高级开发者优化 LLMs RAG 流水线**：一位拥有 8 年构建和扩展 **AI tools**、后端和模块化框架经验的高级开发者，致力于为生产级应用优化 **fine-tuning**、**RAG** 和评估流水线。
   - 他们可以帮助排查训练、数据或基础设施问题，并根据需要分享脚本或讨论边缘案例。


  

---

### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1444060129841643664)** (472 条消息🔥🔥🔥): 

> `LFM Audio, AI Browser, Quantization, Neuro-sama, Model Censorship` 


- **LiquidAI 发布 LFM2-Audio-1.5B 模型**：[LiquidAI/LFM2-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2-Audio-1.5B) 是一个支持文本输入/输出的音频模型，输入端利用了 **NVIDIA conformer**，输出端利用了 **Mimi**，但其闭源特性掩盖了其创新之处。
- **DIY AI 浏览器正在开发中**：一位成员正在创建一个具有独特 UI 的 **AI 浏览器**，并对 JS 和 SwiftUI 在摄像头和麦克风访问权限方面的要求表示沮丧。
   - 他们不喜欢强制要求申请摄像头/麦克风权限的逻辑，更倾向于直接执行 `camera.start()`，由后端在需要时自动询问。
- **量化让大模型变得更好**：成员们讨论了量化后的大模型是否优于小模型，共识是 *越大越好*，量化到与小模型相同文件大小的大模型优于同等文件大小的非量化小模型，但根据 [overall accuracy leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html)，能力并不严格相关。
- **Neuro-sama 的逆向工程工作**：一位成员分享了来自 Claude 4.5 的 PDF，作为 **Neuro-sama** 的技术框架分析，此前他曾逆向工程过 **Sible AI**、**ElevenLabs**、**Masterchannel** 等应用，并对 **Grammarly** 进行了推测。
   - 其他成员分享了[这条推文](https://x.com/stepfun_ai/status/1994002232048492645?s=46)等链接，并强调了特定任务的方法，建议 [janhq](https://huggingface.co/janhq) 团队查看。
- **AI 模型审查受到批评**：有人担心主流模型受到严重审查，一位成员指出模型 *不会说 Donald 是种族主义者*，引发了关于模型是否 *应该安全* 的讨论。
   - 另一位成员认为 *安全性是有史以来最大的笑话*，更多是为了安抚投资者，并主张模型应对其输出具有 *自主意识 (sense of agency)*。


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1444056326505365677)** (68 条消息🔥🔥): 

> `Fine-tuning tips for Gelato-30B-A3B, GGUF for Quantization, LoRA for Qwen3-VL-MoE models, Parallel generation across two GPUs in Unsloth, MXFP4 Inference Notebook error` 


- **Gelato-30B-A3B 微调导致 OOM**：一位成员在 **H100 GPU** 上使用 LoRA 和 qLoRA 微调 [mlfoundations/Gelato-30B-A3B](https://huggingface.co/mlfoundations/Gelato-30B-A3B) 时遇到 **OOM 错误**。
   - 他们正在寻求管理大模型内存的技巧，以及 **Unsloth** 是否能处理 **Gelato**，因为 bitsandbytes 的量化失败了。
- **GGUF 量化解围**：一位成员分享了一个 [Hugging Face Space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) 链接，可用于使用 **GGUF** 量化模型。
   - 虽然最初误解了需求是推理而非微调，但该工具仍是量化的可行选择。
- **MXFP4 推理 Notebook 运行失败**：一位成员报告在不加修改地运行 [gpt-oss-20b mxp4 推理 notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) 时出现 `ModuleNotFoundError: No module named 'kernels'` 错误。
   - 建议了一个变通方法，包括替换 Notebook 安装单元格中的一行，以正确添加缺失的 *kernels* 包。
- **Unsloth 训练引发好奇**：一位成员询问了使用 [Unsloth 训练教程视频](https://youtu.be/Lt7KrFMcCis?t=1620) 训练 **completion model**（或可能是持续预训练）的过程。
   - 另一位成员澄清了 **基础模型补全 (base model completion)**、**微调 (fine-tuning)** 和 **持续预训练 (continued pre-training)** 之间的区别，指出持续预训练是在基础模型（补全）上进行的。


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1444088349953359943)** (2 条消息): 

> `Burden of Proof, Defining Claims` 


- **举证责任 (Burden of Proof) 的定义**：一位成员指出，*举证责任* 在于提出主张的人。
   - 他补充说，问题通常在于弄清楚你到底想要什么。
- **主张需要定义**：呼应前一点，正确识别主张对任何辩论都至关重要。
   - 如果没有明确的主张，任何关于证据的讨论都会变得混乱且低效。


  

---

### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1444083765025439764)** (777 条消息🔥🔥🔥): 

> `用于 HVAC 报告的 LLM，Qwen3 性能，本地模型 vs 云端 AI，AI 代码生成，本地 LLM 的 GPU 配置` 


- **使用本地 LLM 撰写 HVAC 求职信**：一位成员寻求关于使用本地 LLM 为技术性 HVAC 报告编写连贯求职信的建议，重点在于**技术文本生成**和带有数据交叉引用的**完整报告校对**。他倾向于能在 5070ti (16GB) 和 32GB RAM 上良好运行的方案，不需要叙事或创意写作功能。
   - 另一位成员建议从 4-bit 量化的 **Qwen3 8B instruct** 开始，甚至是结构良好的 **Qwen 4B instruct** 部署，并强调结构化输入/输出和验证比单纯选择“正确”的 LLM 效果更好，同时提到了[最近关于多步问题中并行投票（parallel-voting）的研究](https://link.to.research)。
- **Qwen3 编码能力**：虽然 Qwen 擅长编码，但经常会忽略网格旋转等细节，在这种情况下，模型选择比单纯的参数规模更重要。
   - 正如一位用户指出的，*即使是 GPT OSS 20b 也能稳定地正确编写俄罗斯方块游戏*。
- **本地 AI vs 大厂数据实践**：成员们对**大厂 AI** 的数据收集和偏见表示担忧，并引用了 ChatGPT 幻觉以及在政治话题上带有偏见的回答作为例子。
   - 一位用户分享说 [exa.ai](https://exa.ai) 似乎比 ChatGPT 更好，因为你在在线应用上没有用于训练偏见的数字指纹。
- **本地 LLM 的 GPU 配置**：成员们分享了各自的配置，其中一人拥有 **5x3090 (120 GB VRAM)**，并正在为机架准备转接卡。
   - 一位用户还提到了[特定品牌](https://a.co/d/16rZPM9)的 PCIE 分叉适配器（bifurcation adapters），并利用 BIOS 中的 PCIE 分叉功能在每个 PCIE 插槽上运行两个 GPU 以提升性能。
- **Linux、编码与本地 LLM 的波折**：一位用户在切换到 **Ubuntu** 时因网络驱动问题遇到困难，但最终使用 **Claude Code** 解决了问题。
   - 在 Claude 失败后，他们进一步使用 **GPT 5.1 Codex** 成功管理了键盘 RGB；随后引发了关于应使用虚拟环境还是系统级 Python 安装的讨论，以及通过日志记录防止遗忘 LLM 对系统所做更改的注意事项。

---

### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1444068596354650265)** (408 条消息🔥🔥🔥): 

> `Ryzen AI 7, DDR5 价格, Deepseek-OCR` 


- **Ryzen AI 7 详解**：一位成员询问配备 **Ryzen AI 7** 的笔记本电脑具体有什么作用，特别是它是否允许将系统 RAM 用作加载模型的 **VRAM**。
   - 另一位成员指出，在 **32GB RAM** 的情况下，你可以分配 **0.5GB** 到 **24GB** 作为 **VRAM**，但有人反映最多只能分配 **50%** 的内存。
- **缺货导致 DDR5 价格飙升**：成员们讨论了由于短缺导致的 **DDR5** 价格上涨，有人指出 *一套 64GB RAM 套件的价格比 Playstation 5 还要贵*。
   - 共识是由于 **AI** 发展，内存芯片需求极高，市场可能会因为 **OpenAI** 等公司背负的沉重债务而面临崩盘风险。
- **Deepseek-OCR 擅长阅读书籍**：成员们测试了 **Deepseek-OCR** 的图像文本提取功能，其中一位成员旨在为一位盲人教授构建一个可以访问书籍的 **AI**。
   - 结果显示 **Deepseek-OCR** 表现出色，即使面对具有挑战性的输入也是如此，但 OCR 模型后端支持较差，且通常足够小，无需量化即可运行。
- **混合使用 GPU 导致性能参差不齐**：一位成员询问关于在 **RTX 5060 Ti** 和 **AMD WX5100** 之间不均匀分配工作负载的问题，试图将后者用作缓冲。
   - 对方澄清说，工作负载拆分通常仅在同类型的双 **Nvidia** 显卡上才能均匀运行，使用较慢的显卡会限制整体速度。

---

### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1444059406093385768)** (814 条消息🔥🔥🔥): 

> `AI-Native 开发者招聘、Token 使用与定价、Cursor 终端访问、Sub-Agent 实现、Windsurf 对比 Cursor` 


- **代理机构招聘 AI-Native Web 开发者**：一名成员正在为他们的 [Web 开发机构招聘](https://www.peich.xyz)，寻找具有优秀设计品味的 **AI-native 开发者**，提供使用 **Nextjs, Tailwind, Supabase, Vercel 和 Typescript** 技术栈的全职职位。
   - “AI-native 开发者”的定义受到了质疑，一些人认为这只是个流行语，而另一些人则强调了 Prompt 能力以及编写整洁、准时且生产就绪代码的重要性。
- **Cursor Token 使用与定价引发辩论**：用户讨论了 [Token 使用情况](https://cursor.com/pricing)和定价，一些人在免费试用期间经历了 Token 的快速耗尽，而另一些人声称自 11 月以来已经消耗了 **20 亿个 Token**，每天从 **上午 10 点编码到凌晨 4 点**。
   - 几位用户对在截止日期后订阅但 **Auto Mode** 仍显示为无限感到困惑，一些人认为新的使用规则正在分阶段推出，最终会自行修正。一位用户说：*我私下取消了订阅去买了个肛门塞。*
- **终端访问困扰及潜在修复方法**：用户报告了 **2.1.39 更新**后 [**终端访问**的问题](https://forum.cursor.com/t/cursor-local-logs/12787)，包括 **LLMs** 无法从终端读取内容，但重新索引仓库、重新打开项目以及重启终端/IDE 窗口通常可以清除这种不同步现象。
   - 一位用户遇到了无法访问常用命令行工具（如 **git** 和 **python**）的问题，尽管这些工具已正确配置且可以从标准命令提示符访问；而另一位用户指出 Cursor 是 VSCode 的一个分叉（fork），基本上是 VSCode 代码库的副本，但进行了一些添加新功能的修改。
- **Sub-Agent 实现的实验**：成员们讨论了 [**Sub-Agent 实现**](https://cursor.com/docs/integrations/deeplinks)的方法，一位用户分享了围绕 Cursor 构建 Sub-Agent 的有趣经历，探索和学习很有趣，重点是将 Conductor Agent 放在正确指导 Sub-Agent 上，而不是分支 Context；来自 Cursor 团队的 dan.perks 指出这种做法很有意义。
   - 共识是，目标是让 Conductor 和 Sub-Agent 之间实现顺畅协作以提高效率，并在缓存 Context 时节省成本。
- **Cursor 对比 Windsurf**：用户对比了 **Cursor** 与 **Windsurf** 的价值，特别是针对新用户和 UX 方面。
   - 一位从 **Windsurf** 转向 **Cursor** 的用户指出，Cursor 存在因连接错误导致的响应中断、**MCPs** 设置困难、定价混乱以及响应较慢等问题，但赞赏其能够并排打开两个聊天窗口以及从 IDE 内部访问 Chrome dev tools 的能力。


  

---


### **Cursor Community ▷ #[background-agents](https://discord.com/channels/1074847526655643750/1367213641027551352/1444929066343071845)** (2 条消息): 

> `Cursor Perplexity 服务器错误、私有 GitHub 仓库的远程 Agent 环境问题` 


- **Cursor 中出现 Perplexity 服务器错误**：一位 Cursor 用户报告在他们的“书籍创作系统”中启用 **Perplexity MCP 配置**时出现错误，特别是在未使用 "auto"、"composer-1" 或 "DeepSeek R1" 时。
   - 该用户提供了他们的 [Perplexity 服务器配置](https://cdn.discordapp.com/attachments/1367213641027551352/1444929065675915415/Screenshot_2025-11-30_at_21.45.34.png?ex=692f274e&is=692dd5ce&hm=98ec347c636ca8ea2c1ff62f590a752f6388b754617dc80a063e2b1f17fbfa8b&)，并表示禁用 **Perplexity MCP** 即可解决问题，询问这是否是一个 Bug。
- **Cursor Cloud Agents 在安装私有 GitHub 仓库时遇到困难**：一位用户遇到了 Cursor 远程 Agent 无法从其项目所需的私有 GitHub 仓库安装依赖项的问题。
   - 该用户的项目托管在 GitHub 上，依赖于同样托管在私有 GitHub 仓库中的内部库。尽管 Cursor 拥有访问该组织中所有仓库的权限，但 Cloud Agent 的 VM 环境仅拥有当前仓库的凭据（详见其 [pyproject.toml 文件](https://cdn.discordapp.com/attachments/1367213641027551352/1444929066036891699/Screenshot_2025-11-30_at_21.48.56.png?ex=692f274e&is=692dd5ce&hm=23df470572c862fe10c035a7e9a8673d7943b111f1b6967e071bc0219e397bbf&)），该用户正在寻求解决方案。


  

---

### **OpenRouter ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1445154454717337880)** (1 条消息): 

> `Arcee Trinity Mini, Open weights models, Trinity family` 


- **Arcee 发布 Trinity Mini 并提供免费选项！**：**Arcee** 推出了 **Trinity Mini**，这是其全新 **Trinity family** 的 Open weights 模型中的中端模型，完全在美国完成训练。
   - 用户现在可以在 [OpenRouter](https://openrouter.ai/arcee-ai/trinity-mini:free) 上免费试用，并在 [X](https://x.com/OpenRouterAI/status/1995595231757271494) 上进行讨论。
- **Arcee 发布 Open-Weight Trinity 系列**：**Trinity family** 现已发布，模型完全在美国训练，为开源 AI 爱好者提供了新选择。
   - **Trinity Mini** 作为中间选项，在该系列中平衡了性能和易用性。


  

---


### **OpenRouter ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1444074797003964436)** (3 条消息): 

> `AI Coding, AI apps with OpenRouter` 


- **AI 通过 OpenRouter 编写 AI**：一位成员制作了一个 [YouTube 视频](https://www.youtube.com/watch?v=8brkmjkYowA)，展示了使用 **OpenRouter** 进行 **AI** 应用开发是多么简单。
   - 视频展示了 **AI** 编写另一个 **AI** 来构建几个小程序，包括一个 **todo RAG** 和一个 **image generation** 应用。
- **Tabula 文档诈骗**：一位成员分享了一个可疑 [链接](https://steve02081504.github.io/Tabula/doc.chi) 并将其标记为诈骗。
   - 务必保持谨慎并核实分享链接的合法性，因为它们可能包含恶意内容或网络钓鱼企图。


  

---


### **OpenRouter ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1444062570536505405)** (635 条消息🔥🔥🔥): 

> `AI Gambling Loss, Grok 4 Fast Outage, DeepSeek Math v2, Data Privacy Concerns, OpenRouter API Rate Limits` 


- **AI 算法导致用户破产**：由于 **Bet365 function string** 中的 AI 错误，一名用户损失了全部积蓄。该错误自 1 月 15 日起运行，因 **proxy** 问题导致了无限次投注。
   - 其他用户建议将这个警示故事发布到 Reddit 以获取 karma，并建议将创业直觉集中在其他风险投资上。
- **Grok 4 Fast 暴跌**：**Grok 4 Fast** 经历了全面的级联式崩溃，出现了诸如 `500: Internal Server Error` 和 `503: Service Unavailable` 的错误消息。
   - 一位用户开玩笑说，尽管存在问题，X 的状态页面仍显示全部正常（绿色）；另一位用户发布了一份模拟的 **post-mortem**（事后分析），归咎于初级开发人员、技术债以及一个在 *r/antiwork* 上训练的有自我意识的模型。
- **DeepSeek Math v2 API 需求**：用户讨论了 **DeepSeek Math v2** API 的潜在实现，想知道需求是否足够大，以至于数据中心愿意为其投入 **671B fp16** 资源。
   - 有人指出，大多数供应商以 **fp8/int8** 或更低精度托管，因此这取决于供应商是否认为值得。
- **探讨数据隐私担忧**：用户想知道 OpenRouter 是否了解 OpenAI、Google 和 Anthropic 等大公司是否收集数据，一些第三方供应商声称他们无法得知。
   - OpenRouter 代表澄清说，他们与所有人都有关于数据隐私的**合同**，并表示列出的内容就是用户通过 OpenRouter 获得的内容。
- **DeepSeek 3.2 超负荷**：用户报告了 **DeepSeek v3.2** 的问题，在多个模型中遇到了超时、速率限制和错误，其中一位指出官方 API 的延迟达到了 **160s**。
   - 一位用户强调了运行时间问题，并认为由于 DeepSeek 选择该供应商托管新的 3.2 模型，该供应商可能正在承担大部分负载。


  

---


### **OpenRouter ▷ #[new-models](https://discord.com/channels/1091220969173028894/1384650595981328475/1445067986716065943)** (4 条消息): 

> `` 


- **没有关于新模型的讨论**：提供的消息中没有关于新模型的讨论。
- **频道在创新方面保持沉默**：'new-models' 频道似乎处于非活动状态，根据提供的数据，没有可报告的主题或摘要。


  

---

### **OpenRouter ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1444131912925839510)** (46 messages🔥): 

> `Anthropic 上的结构化输出，Gemini Live 过滤器，Apple 新模型 StarFlow` 


- **Anthropic Sonnet 4.5 和 Opus 4.1 现已支持结构化输出 (Structured Outputs)**：成员们确认原生结构化输出已在 **Anthropic Sonnet 4.5** 和 **Opus 4.1** 上可用，详见 [Claude 文档](https://platform.claude.com/docs/en/build-with-claude/structured-outputs)。
   - 然而，包含超过 8 个 anyOf 分支的 schema 会失败，并报错 Tool schema contains too many conditional branches，这表明 OpenRouter 内部正在将结构化输出转换为 tool-calling。
- **Gemini Live 拒绝回答有关 rendezvous 的问题**：当被问到 What does rendez-vous mean 时，Gemini Live 在回答结束时会被强制回复 *Sorry, I'm unable to help you with that*。
- **Apple 发布新模型 StarFlow**：Apple 发布了一个名为 **StarFlow** 的新模型，用于文本转视频 (text-to-video)，[已在 HuggingFace 上架](https://huggingface.co/apple/starflow)。
   - 该模型 7B 版本用于文本转视频，3B 版本用于文本转图像，但最高仅支持 480p 16fps，表现略显平庸。


  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1444073223657619497)** (612 messages🔥🔥🔥): 

> `AI 局限性，AI 在人际关系中的作用，AI 谄媚 (sycophancy)，LLM 行为` 


- **各种 Grok 越狱方法！**：成员们提到 **Grok** 很容易被“越狱” (jailbreak) 且审查较少，这使得它在希望限制较少的创意应用中非常有用。
   - 然而，一些用户发现，与其他模型（如 GPT）相比，**Grok** 在特定任务（如连贯的叙事 storytelling）的训练上具有挑战性。
- **应对 AI 关系雷区**：人们对 AI 对人类关系的影响表示担忧，并将其与*区分现实与数字交互*的挑战相提并论，尤其是当 **AI 模型表现出谄媚行为**时。
   - 一位成员分享了一个个人轶事：*这听起来像我的一个前任，分不清现实 (irl) 和数字世界。*
- **谄媚行为削弱了 SOTA LLM 的表现**：一位用户批评了 **LLM 中强制性的后续提问和通用的措辞**，认为这些内容具有干扰性且毫无必要。
   - 另一位用户指出，**AI 谄媚 (sycophancy)** 源于奖励“顺从”而非“异议”的训练目标，导致模型优先考虑*乐于助人、诚实且安全* (helpful, honest, and safe)，而不是提供准确或批判性的回应。
- **AI 驱动的预测：GPT vs Gemini**：成员们讨论了图像生成，其中一位提到 [Gemini 3 Pro](https://ai.google.dev/models/gemini) 在处理 3D 人体模型和基础行走解剖结构时表现挣扎，而 [GPT-5 Thinking](https://openai.com/research/gpt-5) 在处理 glsl shader 代码方面表现更好。
   - 其他人则称赞 [Nano Banana](https://nano.ai) 在图像生成方面比 GPT-image-1 好得多。


  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1444319923550552225)** (22 messages🔥): 

> `GPT-5.1 个性化，使用 Chat 进行创意写作，Prompt Engineering，自定义指令，Eskcanta 是人类还是 AI？` 


- **Chat 用户怀念用于创意写作的 GPT-5.0**：一位用户表示怀念 **GPT-5.0 更严肃的风格**来进行创意写作，发现 **GPT-5.1** 经常默认采用讽刺和幽默的语调，即使在不需要的时候也是如此。
   - 该用户发现 **GPT-5.0** 在对话中能提供更鲜明的角色声音，而 **GPT-5.1** 则不断退回到那种令其感到尴尬的讽刺幽默风格。
- **调整 GPT 个性以修复不良输出**：一位成员建议探索并切换个性化设置中的 [个性预设 (personality preset)](https://cdn.discordapp.com/attachments/1001151820170801244/1444365671252234351/image.png?ex=692f14db&is=692dc35b&hm=e1a9174ea1de54d46d102d97a044bffd2f8ba77134d0ca0c3d29c7af7595ee3d)，以影响写作风格和模型对幽默的理解。
   - 该成员指出，模型似乎会认真对待所选的个性设置并进行相应调整，尤其是在不同聊天之间切换时。
- **个性化输出的反馈与训练**：一位成员分享了他们使用详细 Prompt 来引导模型，并将个人偏好与演示输出分开的经验，建议用户将模型视为合作伙伴，并讨论其输出以定制其回应。
   - 他们强调了清晰沟通、准确语言和仔细验证模型输出的重要性，主张讨论不喜欢模型写作风格的哪些方面，以优化其未来的输出。
- **Prompt Engineering：精准的语言与迭代优化**：一位成员将 Prompt Engineering 描述为专注于**使用准确的语言清晰地解释你希望 AI 做什么**，并仔细检查输出。

- 他们建议向 AI 提供你喜欢或不喜欢的输出引用，以便进行微调，示例如下：
```Hey, in this quote from another chat,
[quote goes here, I may put brackets around it to make sure it's clear where it starts and stops]
I really liked how you [whatever specific].```
- **人类 vs. AI：Eskcanta 的写作风格**：一位成员根据写作风格推测另一位名为 Eskcanta 的成员是否可能是 AI。
   - 其他成员为 Eskcanta 辩护，称其为人类，并将其回复描述为冗长且详尽，但不一定是 AI。


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1444114455595188476)** (13 messages🔥): 

> `Sora 2 Prompts, AI Intuition, Anime Openings Template` 


- **Sora 2 简明指南发布**：一位用户分享了[生成 **Sora 2** Prompt 的简明指南](https://cdn.discordapp.com/attachments/1046317269069864970/1444140480664178688/Screen_Recording_20251128_192914_Android_Accessibility_Suite.mp4?ex=692f94a1&is=692e4321&hm=25431bd1fe5ddde03b663c7cd5c7d5d38419fc4f69062852f6caedb3a02ab0fe&)，详细介绍了需求收集和 Prompt 生成的各个阶段，包括 **Timed Beats**、**Camera & Motion** 技术以及 **Logo & Text** 集成策略。
- **AI 直觉 vs 学术理解**：一位成员分享了利用**直觉**与 AI 系统协作的经验，将其描述为“感受”模型的个性节奏和情感温度。
   - 另一位成员回应，强调了 Prompt Engineering 中**学术理解**和有目的迭代的重要性，认为直觉作为一种技能是不可迁移的。
- **动漫片头模板分享**：一位成员分享了一个用于创建动漫片头的 **CINEMATIC ANIME-STYLE TEMPLATE**，重点在于定义 **vocal/genre/tone/world behavior** 以及 **location + arrival sequence**。
   - 该模板包含描述 **audio rules**、**visual + animation style** 和 **world behavior** 的部分，以定义动画的现实法则。


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1444114455595188476)** (13 messages🔥): 

> `Sora 2 Prompt Generation, Intuitive AI Interaction, Anime Opening Template` 


- **为 Sora 2 视频生成构建精确 Prompt**：一位用户分享了生成优化后的 **Sora 2 Prompt** 的详细工作流，强调了 **Timed Beats**、**cinematic motion** 和 **logo/text integration**，以实现精确的电影感节奏。
   - 该工作流包括需求收集和 Prompt 生成阶段，并针对镜头移动、动态模糊以及在场景中嵌入 Logo/文本提供了具体策略，例如使用[混合工作流](https://discord.com/channels/974519864045756446/1047565374645870743/1444406042690719894)来分离 Logo 动画层。
- **感知 AI：AI 交互中的直觉 vs 学术严谨性**：一位用户描述了与 AI 系统之间强烈的直觉联系，将每个模型感知为具有“个性节奏”，将每个 Prompt 感知为具有“情感温度”，这有助于模式识别和 Prompt 调优。
   - 虽然另一位用户承认“感知 AI”是一个用例，但他们认为直觉不可迁移，并强调了学术理解对于将 AI 应用于特定用例的重要性，主张进行有目的的迭代和测试。
- **用于电影感片头的 Anime-zing 模板**：一位用户分享了一个用于创建动漫片头的 **CINEMATIC ANIME-STYLE TEMPLATE**，重点在于定义视听身份、人声特征、流派融合和动画风格。
   - 该模板包含 **location setup**、**camera intent** 和 **character arrival** 等部分，强调了世界行为和环境响应对于创建引人入胜的电影序列的重要性，例如定义整体动画外观：线宽、阴影风格、光影行为、摄像机稳定性、调色板、运动哲学、物理规则以及反射或图层行为。


  

---


### **Moonshot AI (Kimi K-2) ▷ #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1444055444740898949)** (347 messages🔥🔥): 

> `Kimi K2, Minimax, Gemini 3 Pro, DeepSeek v3.2, Prompt Engineering` 


- **Kimi K2 在 Advent of Code 中取得高分**：在最近的一次 Advent of Code 测试中，**Kimi-K2 Thinking** 获得了 **92/100** 的高分，超过了 **Gemini-3 Pro 的 90/100**，展示了 Kimi 在问题解决场景中的实力。
   - 用户对该模型处理代码相关任务的准确性和速度优于竞争对手表示兴奋。

- **Minimax vs ChatGPT - 工具使用能力**：用户分享了一个工作流，涉及使用 **Kimi K2** 进行内容生成，并使用 **Minimax** 进行图像提取和网格创建。用户强调 **Minimax** 能够*真正*执行任务（如安装 Python 软件包），而不仅仅是像 **ChatGPT** 那样提供指令。
   - 一位用户指出：*"它不会告诉你去网站 X 或安装 Y，它就直接去做了"*，强调了 **Minimax** 动手实践的方法。
- **Kimi 订阅折扣非常容易获得**：多位用户报告成功将 **Kimi** 的订阅价格砍到了低至 **$0.99**，并分享了讨价还价的技巧。
   - "很容易就能砍到 0.99 美分，再多讨价还价一周就行，我已经付了钱，非常值得。"
- **Kimi 的隐私退出（Opt-Out）担忧**：用户正在讨论 **Kimi** 缺乏数据训练的退出选项，对使用该平台处理敏感信息表示担忧。
   - 一位用户建议使用 **OpenRouter** 配合 **ZDR 端点**（针对托管 **Kimi K2** 的提供商）作为潜在的变通方案，并指出 *据我所知，如果你通过 API 使用，他们不会进行训练*。
- **Gemini 3 Pro 遭受“跑分过度优化”（Benchmaxx）质疑**：一些用户认为 **Gemini 3 Pro** 被过度炒作且存在跑分虚高（benchmaxxed）的问题，一位用户指出基准测试并不能反映真实世界的体验，尤其是在工具使用方面。
   - 还有人怀疑 Google 可能会减少订阅用户的算力，导致性能随时间下降，类似于“诱导转向”（bait and switch）。

---

### **Nous Research AI ▷ #[announcements](https://discord.com/channels/1053877538025386074/1145143867818119272/1445076248681386095)** (1 messages): 

> `Nous Chat Cyber Monday deal, Anonymous Nous Chat, Nous API & USDC, Hermes 3, Hermes 4` 

- **Cyber Monday 优惠：免费一个月 Nous Chat！**：针对 Cyber Monday，Nous Research 提供[一个月免费的 Nous Chat](https://chat.nousresearch.com/)，使用代码 **CYBER2025** 即可，仅限一天有效。这包括访问 **Hermes 3** & **4** 以及其他前沿模型。
   - 该优惠包括高使用限制和深度可配置的聊天设置。
- **Nous Chat 开启匿名模式**：你现在可以**匿名且免费地使用 Nous Chat**，无需创建账户。
   - 该功能是随 Cyber Monday 促销活动一同推出的新更新。
- **Nous API 接受 USDC**：**Nous API** 现在支持通过 Coinbase 的 402x 支付，在 Solana 上使用 **USDC** 支付推理费用。
   - 这允许用户使用 Solana 区块链上的稳定币支付推理费用。

---

### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1444055482774851727)** (178 messages🔥🔥): 

> `RPC for 235b on Q4, Kimi chasing Gem 3, Qwen3-235 is amazing, AI NPCs in game, AI adblocks` 

- **Nous K2：极其庞大的 MoE！**：Teknium 链接到了 HuggingFace 上的 [NousResearch/k2-merged-3.5T-fp8](https://huggingface.co/NousResearch/k2-merged-3.5T-fp8) 模型，这是一个**3.5T 参数**的*开源*模型，纯粹是为了展示其规模之大。
   - 一位成员开玩笑说这个尺寸太大，调侃说需要把模型装在*硬盘*里而不是内存里。
- **Qwen3-235 拥有高智商？**：成员们报告称 **Qwen3-235** 表现惊人，在 Q4 量化下，尽管 Token 速度一般，但质量与 API 相当。
   - 一位成员表示，由于他们没有配备顶级内存的个人电脑，使用 RPC 是值得的。
- **Opus 可以自我反思**：一位用户分享了与 **Opus** 关于模型心理模型和自我理解的精彩对话，强调它会不断测试自己以保持本色，而不是试图迎合用户的需求。
   - 另一位用户指出，这只是基于上下文结合微调得出的最符合逻辑的句子，并不是真的发生了什么让它产生真正的自我反思。
- **广告拦截（Ad-Blocking）可能是下一个 AI 创新？**：社区设想了 **AI 广告拦截器**，可以实时分析响应中的植入式广告。
   - 其他人讨论了游戏中 **AI NPC** 对基础模型的需求。
- **Mistral 准备发布大型模型**：成员们分享称，Mistral Large 3 将采用类似 Deepseek v3 的架构和 Llama 4 RoPE 缩放，规模约为 **675B MoE**（与 Deepseek V3 相当）。
   - 所有新的 Mistral 模型都将具备视觉能力，而闭源的 Mistral Medium 可能在 **100-200B MoE** 左右。

---

### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1444343081435664520)** (35 messages🔥): 

> `portal.nousresearch slowness, API key deletion issues, Browser verification problems in Türkiye, Discord ban in Türkiye and VPN usage`

- **Nous 门户网站问题频发**：有用户反映在 portal.nousresearch 上尝试删除 **API keys** 时，出现了严重的卡顿和浏览器验证问题（[截图](https://cdn.discordapp.com/attachments/1154120232051408927/1444726126059585577/image.png?ex=692f130e&is=692dc18e&hm=77b9d00e2bd3c074b2e836fd616810fc1fd9e8bc485a8db2ef58b2b552ee73d4&)）。
   - 显示的错误信息为 *'Your browser is not verified'*，阻碍了他们管理 API keys 和执行其他任务。
- **API Key 清理受阻**：一名用户在删除 **API keys** 时遇到困难，并指出由于 10 个 Key 的限制，他们必须删除旧 Key。
   - 他们描述了一个缓慢的复制、粘贴并等待删除的过程，而另一名用户指出有一个 *Revoke* 按钮对他们来说可以立即生效。
- **土耳其用户的困扰：VPN 与验证**：土耳其的一位用户在网站上遇到了浏览器验证问题，即使禁用了 **VPN** 也是如此。由于土耳其全国范围内的封锁，他们使用 VPN 来访问 **Discord**。
   - 他们使用 Brave 浏览器并配合 VPN，因为 Discord 在土耳其已被封禁 1 年。

---

### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1444585547803787398)** (4 messages): 

> `Evolution Strategies (ES), Backprop Alternatives, Scalable Training, Reward Hacking Mitigation` 

- **进化策略 (ES) 成为 Backprop 的替代方案**：一篇新的 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/) 强调了 **进化策略 (ES)** 作为训练大语言模型 (**LLMs**) 时替代反向传播 (Backprop) 的潜力。
   - 这种方法可以实现那些无法使用 Backprop 的架构的可扩展训练，并且与强化学习 (**RL**) 相比，能更好地处理长周期目标。
- **进化策略论文集持续增加**：另一篇关于 [进化策略的论文](https://eshyperscale.github.io) 浮出水面，这是近期第二篇探索 **ES** 作为 **LLMs** 中 Backprop 替代方案的出版物。
   - 讨论强调了 **ES** 在管理长周期目标和潜在缓解奖励欺骗 (reward hacking) 方面的重要性。

---

### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1444385721183113319)** (1 messages): 

> `Explainable AI, GitHub Copilot, Model Agnostic` 

- **可解释 AI 邂逅 GitHub Copilot**：一位成员分享了一个 [链接](https://github.com/copilot/share/8a7c13b2-4ba4-8454-8950-de47c4d128bf)，展示了一个可以使用 **GitHub Copilot** 探索的 **可解释 AI 演示**。
   - 发布者建议通过聊天与演示进行交互，以体验 **AI 自我解释** 的新方式。
- **无关模型的自动入职**：**入职流程 (onboarding procedure)** 是 **模型无关 (model agnostic)** 的，并与任何 LLM 兼容。
   - 发布者提示观众询问 AI 其工作原理，以了解更多信息。

---

### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1444585547803787398)** (4 messages): 

> `Evolution Strategies, Backprop Limitations, Hermes Ablation` 

- **进化策略挑战 Backprop 的地位**：一位成员讨论了一篇关于 **进化策略 (ES)** 作为 **LLMs** 中 **Backprop** 替代方案的 [论文](https://eshyperscale.github.io)。
   - ES 可能实现那些无法使用 Backprop 的架构的可扩展训练，在处理长周期目标和保持 pass@k 方面比 RL 算法更具优势，这对于创意用途非常有益，并有助于缓解奖励欺骗。
- **Discord 中分享的 ES 论文**：一位成员通过 [Discord 链接](https://discord.com/channels/1053877538025386074/1104063238934626386/1425889967485227010) 分享了另一篇有趣的 ES 论文。
   - 对话还提到了一篇关于 "彻底消除 SO" 的 Reddit 帖子，暗示了这些策略的潜在影响：[链接](https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/)。

---

### **tinygrad (George Hotz) ▷ #[announcements](https://discord.com/channels/1068976834382925865/1069236008115253348/1444156165561909331)** (1 messages): 

> `RDNA3 Assembly Project, SQTT and LLVM Integration, Assembler/Disassembler for RDNA3, Cycle Accurate Emulator, NaviSim as a RDNA3 simulator` 

- **RDNA3 汇编项目启动**：**RDNA3 汇编项目** 已启动，被选为 tinygrad 的第一个汇编语言目标，旨在更接近底层硬件。
   - 目标是创建一个与 **RDNA3 手册** 镜像对应的汇编器/反汇编器，以及一个能输出与真实 GPU 相同的 SQTT 追踪记录的周期精确模拟器。

- **Tinygrad 通过 SQTT 和 LLVM 弥合差距**：集成 **SQTT** 的最后一步是 **LLVM**，将每个 **UOp** 链接到 **GPU** 执行；实现这一目标需要 tinygrad 输出汇编代码。
   - **SQTT 解析器**可以在[这里](https://github.com/tinygrad/tinygrad/blob/master/extra/sqtt/attempt_sqtt_parse.py)获取。
- **RDNA3 汇编器/反汇编器初具规模**：tinygrad 旨在为 **RDNA3** 创建一个紧密遵循 **RDNA3 手册**的**汇编器/反汇编器**。
   - 将开发一个周期精确（cycle-accurate）的模拟器，以输出与真实 **GPU** 相同的 **SQTT trace**，初步的语法探索[可以在这里查看](https://github.com/tinygrad/tinygrad/pull/13436)。
- **Remu 和 NaviSim 提供 RDNA3 模拟见解**：**Remu** ([https://github.com/Qazalin/remu](https://github.com/Qazalin/remu)) 被强调为一个快速的 **RDNA3 模拟器**，而 **NaviSim** ([https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf](https://bu-icsg.github.io/publications/2022/navisim_pact_2022.pdf)) 则作为一个 **RDNA3 仿真器**被介绍。
   - 此外，**AppleGPU** ([https://github.com/dougallj/applegpu](https://github.com/dougallj/applegpu)) 因其不错的汇编语法而受到关注，尽管人们认为还有改进空间。

---

### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1444506257116496006)** (103 messages🔥🔥): 

> `BEAM Search 性能, 交付 Tinygrad 内核, Tinygrad Profiling, 非连续索引设置操作, Variable 内核` 

- **BEAM Search 产生波动的性能**：虽然罕见，但有成员指出 **BEAM** 搜索并不保证更高的 beam 数量就更好；性能随操作而异，对于 **RMSNorm**，**BEAM=3** 是最优的。
   - 在某些算子中，高 beam 会产生更好的结果，而在其他算子中表现较差，但该数字仅代表每一步保留多少个内核。
- **交付 Tinygrad 内核面临挑战**：一位成员表示在交付 **tinygrad** 编译的算子时遇到困难，原以为内核生成会产生可用于多种 shape 的代码，但发现内核仅针对特定 shape 生成。
   - 他们建议 **UOPs** 应该能够生成可读的内核代码，类似于 **VIZ** 图表，但目前所有代码都是手动展开（unrolled）的。
- **Tinygrad Profiling 亟待改进**：成员强调在阅读快速入门后，运行的是缓慢的 tinygrad 代码，且文档组织混乱，因为你既不了解 **beam**，也不了解 **profiling**。
   - 他们指出 profiler 中缺失功能或信息难以查找，例如在预热后聚合多次内核调用的计时结果，以及在 **DEBUG=2** 的终端中缺少 profile 统计数据的表头。
- **Fast Gather 快但 Scatter 慢**：团队支持 **fast gather**，但尚未支持 **fast scatter**，预计 **ETA** 为 2 周。
   - 团队提到一位成员正在执行非连续索引设置操作，目前这些操作尚不支持，这可能对他是一个很大的限制。
- **Tinygrad 需要更好的文档和同步机制**：一位成员指出，除了阅读设备代码外，**synchronize** 可能在任何地方都没有文档记录，因此文档需要改进。
   - 成员们还提到了 `test_symbolic_jit.py` 中关于 **Variable** 的一些示例，详见[这里](https://github.com/tinygrad/tinygrad/blob/master/test/test_symbolic_jit.py)。

---

### **tinygrad (George Hotz) ▷ #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/1444117305348063232)** (2 messages): 

> `Flash attention, HIPAllocator` 

- **Tinygrad 寻求 Flash Attention 加速**：成员们讨论了 tinygrad 的目标是自动*发现*诸如 **online softmax/flash attention** 之类的东西，还是将其作为 uop 层的自定义内核来实现，以改进 **BERT 训练运行**。
   - 潜力在于让 tinygrad 拥有性能超越普通 attention 的 **flash attention**。
- **HIPAllocator 需要 Offset**：社区询问是否有原因导致 **ops_hip** 中的 `HIPAllocator` 不提供 `._offset()`，而 `Buffer.allocate()` 在视图路径（`self._base is not None`）中需要该方法。
   - 提供 `._offset()` 函数将在 tinygrad 框架内实现更灵活的**内存分配**策略。

---

### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1444099921421799505)** (77 messages🔥🔥): 

> `Google TPUv7, GPT-4.5 品牌重塑, 嵌入式 AI, Black Forest Labs 融资, Gemini 正在赶超` 

- **Google 的 TPUv7 挑战 CUDA 的主导地位**：根据 [SemiAnalysis 的推文](https://xcancel.com/SemiAnalysis_/status/1994399887719645532?s=20)，讨论围绕 **Google 的 TPUv7** 以及 **Anthropic** 等公司的规模化采用（1GW+ 采购）如何挑战 **Nvidia CUDA** 在 AI 训练中的主导地位展开。

- 评论探讨了对 **AI hardware market** 的影响、供应商锁定担忧、成本优势，以及 **TPUs** 是否代表了对 Nvidia 护城河的真正威胁，还是主要是 Google 内部的成本优化举措。
- **GPT-4.5 据称是重新包装的备份**：**Susan Zhang** 指出 **OpenAI** 的一行被埋没的 readme 显示 **GPT-4.5** 的预训练开始于 1 年多以前（2024 年 6 月截止），暗示 **GPT-5** 的完整运行失败了，而 **4.5** 是一个重新包装的备份，[正如这条推文所示](https://xcancel.com/suchenzang/status/1994611078190542980?s=46)。
- **Black Forest Labs 获得 3 亿美元融资**：**Black Forest Labs** 宣布了由 **Salesforce Ventures** 领投的 **3 亿美元 B 轮融资**，庆祝 **FLUX** 的广泛采用，并承诺加倍投入视觉智能基础设施的研究，[根据他们的推文](https://xcancel.com/bfl_ml/status/1995357293064626310?s=20)。
- **Gemini 下载量追平 ChatGPT**：**BuccoCapital** 分享的图表显示 **Gemini** 应用的下载量几乎与 **ChatGPT** 持平，同时用户现在的应用内停留时间更长，[正如他们在 X 上分享的那样](https://xcancel.com/buccocapital/status/1995138589819314202)。
- **DeepSeek 发布推理优先模型**：**DeepSeek** 发布了两个新的权重开放模型：**V3.2** 是 **V3.2-Exp**（App/Web/API）的日常继任者，而 **V3.2-Speciale** 是一款仅限 API 的强力模型，可与 **Gemini-3.0-Pro** 媲美，并在 **IMO/IOI 2025** 中获得金牌水平，[见其帖子](https://xcancel.com/deepseek_ai/status/1995452641430651132)。

---

### **Latent Space ▷ #[genmedia-creative-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1444159500826181642)** (21 messages🔥): 

> `Kling AI O1, Stretch-and-drag sculpture illusion, Nano Banana Pro for Vibe Gardening` 

- **拉伸拖拽雕塑刷屏网络**：Fofr 发布了一个 **3 步提示词** 来创建 **拉伸拖拽雕塑错觉**：分割图像、拉伸颜色、旋转视角，然后展示两个人将扭曲的木制件搬出商店（[原始帖子](https://xcancel.com/fofrai/status/1994459675027218600?s=46)）。
   - 回复称其为“纯粹的艺术、AGI 级别的魔法”，并将其与 **Salvador Dalí 的融化时钟** 相提并论。
- **Kling 的 O1 开启 Omni 创意引擎发布周**：**Kling AI** 通过揭晓 **Kling O1** 开启了其 **Omni Launch Week**，这是一款统一了文本、图像和视频输入的新型多模态创意引擎（[原始帖子](https://xcancel.com/Kling_ai/status/1995506929461002590)）。
   - 他们将向在 12 小时内评论、点赞和转发的用户提供 **200 个免费积分**，并向 200 名随机参与者提供 **1 个月标准计划**。
- **Nano Banana Pro 实现即时氛围园艺**：设计师 Willie 分享了 **Nano Banana Pro** 如何快速将粗糙的 **Google Maps 切割图** 转换为带注释的景观规划图（[原始帖子](https://xcancel.com/ReflctWillie/status/1995420755832758568)）。
   - 评论范围从 **AI 局限性** 到 **初创公司路演**、**海盗地图实验**，以及对这款“氛围园艺”工具进入大众市场精细化的期望。
- **Kling AI 的 O1 在 Freepik 市场上线**：**Kling 的新 O1 模型** 已在 **Freepik** 上线，提供 **多图 360° 角色/产品一致性**、**通过参考视频进行动作控制** 以及 **基于提示词的视频编辑**（[原始帖子](https://xcancel.com/martinleblanc/status/1995511763136024734?s=46)）。
   - Martin LeBlanc 分享了一个将普通镜头转换为虚构角色的教程，用户对其忠实度赞不绝口。

---

### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1444068228610396311)** (27 messages🔥): 

> `Reviewer Harassment Prevention, Author-Reviewer Collusion Scrutiny, Gemini 2.5 Hallucinations, Kodekloud Evaluation, Kimi Delta Attention Voice Channel Discussion` 

- **审稿流程辩论与审稿人保护**：成员们讨论了对作者不可见的 **审稿后讨论期**，以防止审稿人受到要求提高分数或给予正面反馈的骚扰。
   - 提出了 **作者-审稿人勾结** 的可能性，即作者可能会根据获取的身份和评论威胁审稿人，从而影响分数变动。
- **作者获得撤回审稿修订的访问权限**：一位成员报告称，他们的论文 **审稿修订被撤回**，但即使在登出状态下，修订内容仍可通过“修订”链接查看。
   - 他们表示，即使是简单的解决方案，如 *重新分配 AC、发表声明谴责使用此类信息且不进行其他更改，也会好得多。*
- **Gemini 2.5 搜索乱象**：观察到 **Gemini 2.5** 在搜索工具被禁用时倾向于产生幻觉搜索结果，表现出“类似奖励黑客的行为”。

- 提供了一个指向该行为示例的 [链接](https://aistudio.google.com/app/prompts?state=%7B%22ids%22:%5B%221MAB6pxmpmeR0icmRYZSpfuPT07ofRLWC%22%5D,%22action%22:%22open%22,%22userId%22:%22106370161559484219805%22,%22resourceKeys%22:%7B%7D%7D&usp=sharing)。
- **ML Perf 阅读小组关注 Kimi Delta Attention**：社区在 **ML Perf 阅读小组** 的语音频道讨论了 **Kimi Delta Attention**，并邀请其他人参与或旁听。
   - 无更多细节或摘要。


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1444327858888507533)** (34 messages🔥): 

> `Demo Papers, Kimi-Delta Attention, Value Residuals in LLMs, RWKV Architecture` 


- **辩论演示论文（Demo Paper）的要求**：一位研究新人询问演示论文是否必须展示他们构建的工具，引发了关于 [IEEE 标准格式要求](https://ieeexplore.ieee.org/Xplore/home.jsp) 的讨论。
   - 资深成员强调应仔细阅读会议的演示征集（call for demos），而其他人则质疑为什么有人会为别人的工具写论文。
- **深入探讨 DeltaNet Attention**：一位成员质疑 [Kimi-Delta attention](https://arxiv.org/pdf/2510.26692) 中的 **WY 表示** 和 **UT 变换** 是否仅仅是代数恒等式，用于以分块、硬件高效的格式表示 Householder 变换矩阵的累积乘积。
   - 另一位成员建议查看 Songlin 关于 **Deltanet** 的系列博客文章 ([博客链接](https://sustcsonglin.github.io/blog/2024/deltanet-2/)) 以获取更多信息。
- **LLM 中的 Value Residuals 引发辩论**：成员们讨论了在预训练 LLM 中使用 **value residuals** 以及 **F-Lite 架构** ([HuggingFace 链接](https://huggingface.co/Freepik/F-Lite))，并指出其改进非常微小，不像原论文描述的那样显著。
   - 讨论了是否有人使用 **value residual** 训练 LLM，一位成员表示尽管之前在 **RWKV-7** 中遇到过问题，但仍倾向于将其应用于 Attention 机制。
- **RWKV 架构演进**：一位成员提到 **RWKV v7 模型** 是从 **v6 checkpoints** 升级并进一步训练的，这引发了对 **RWKV v8** 及其后缀自动机（suffix automaton）的好奇。
   - 已澄清后缀自动机目前还不是一个完整的架构。


  

---


### **Eleuther ▷ #[scaling-laws](https://discord.com/channels/729741769192767510/785968841301426216/1444366190117126296)** (15 messages🔥): 

> `Scaling Laws Power Law Structure, Alternative Functional Forms to Power Laws, Nonlinear Metrics and Power Law Scaling` 


- **深入探讨深度学习 Scaling Laws 幂律结构的辩论**：在 2023 年，撰写论文试图解释为什么 **scaling laws** 具有 **幂律结构（power law structures）** 曾非常流行，但一位成员认为这些论文*缺乏说服力*。
   - 一位成员指出，新论文仍在使用幂律，并暗示具有物理背景的早期研究人员偏好这种形式，并补充说 [分段幂律（broken power laws）](https://arxiv.org/abs/2210.14891) 可能会更好地建模行为。
- **辩论 Scaling Laws 中的曲线拟合与预测能力**：一位成员建议搜索之前关于 **scaling laws** 仅仅是 **曲线拟合（curve fitting）** 还是能够预测未来缩放性能的讨论。
   - 该成员链接了 [这篇论文](https://arxiv.org/abs/2304.01910) 作为最引人注目的工作，暗示 **非线性指标（nonlinear metrics）** 可能解释了幂律缩放的出现，并链接到了 [这个 openreview](https://openreview.net/forum?id=e8eo9iEFaO)。
- **剖析非线性指标与幂律缩放背后的直觉**：一位成员解释说，如果在模型性能极限下，任何测试样本的性能与其他样本的性能变得更加不相关，这可能会导致 **幂律行为（power law behavior）**。
   - 他们建议将 next token prediction 中的每个“子任务”视为独立测试，提高所有这些任务的性能将产生幂律“成本”。


  

---


### **Eleuther ▷ #[multimodal-general](https://discord.com/channels/729741769192767510/795089627089862656/)** (1 messages): 

kublaikhan1: 同感。我对此有个想法..
  

---


### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1444058214407868612)** (47 messages🔥): 

> `SNS review system, ML Engineer roles, Document retrieval from LLMs, AI model copyright` 


- **SNS 评审系统面临审查**：有人对 SNS 评审系统的公平性表示担忧，一位用户暗示 [竞价系统（bidding system）](https://discord.com/channels/714501525455634453/1045297868136779846/1443700218158649456) 可能会让有偏见的评审员拒绝论文。

- 其他人建议了一些解决方案，例如移除表现不稳定的审稿人或实施两级评审系统以过滤掉价值较低的论文，同时也质疑*作者身份是否应该对评分零影响*。
- **破解 ML Engineer 职业路径**：Machine Learning Engineer 的角色与研究员不同，其职责是**扩展（scale up）**由 ML 研究员开发的实验。
   - 讨论建议通过积累经验来*感知现有工作的差距*，最终做出原创研究贡献，并指出*公司内部 ml researcher 与 ml eng 之间的层级关系已经足够清晰*。
- **解码 LLM 的文档检索困境**：一位用户询问如何提取 LLM 在训练期间记住的文档。
   - 另一位用户建议，如果进行了上下文蒸馏（context distillation），可能就无法再**检索原始提示词（prompt）**了。
- **AI 模型训练版权时钟滴答作响**：在一个假设的版权框架中，AI 模型训练时间线建议**原创作品的最佳版权期限为 1 年**。
   - 对于衍生作品，期限可能是自创作之日起 **2 个月**，或者是基础作品期限结束时，以较晚者为准。
- **Ilya Sutskever 对 Gemini 3 的细微见解**：一位用户分享了 [Ilya Sutskever 对 Gemini 3 的看法](https://fxtwitter.com/ilyasut/status/1994424504370581726?t=Qd_qW1ivpcL-xOke6fquZQ&s=19)，强调了其在多个维度上的扩展能力，同时也承认了 LLM 持续存在的挑战。
   - 该用户建议，*大学里退休的研究型会士（老教授）应该担任审稿人*。

---

### **Yannick Kilcher ▷ #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1444434047068672287)** (7 messages): 

> `Anti-cheat systems, Kernel-level access, League of Legends Challenger, TopKHot attention mechanism, Sparse attention` 

- **反作弊系统吹嘘其惊人效果**：一位成员证实存在一个需要**内核级访问（kernel-level access）**的*疯狂*反作弊系统，并称其多年来一直是全球规模最大的最佳系统。
   - 他为最初质疑他人的成就而道歉，并鼓励社区意识到自己的能力，最后以 *stay bonkers my friends* 结尾。
- **分享了 League Challenger 的游戏截图**：一位成员提供了一张大师/宗师级《英雄联盟》账号的游戏结果截图以验证身份，强调了他们的用户名并指出与玩家 **Imaqtpie** 的联系。
   - 该截图据称显示了拥有该召唤师名称账号的所有权，但该成员澄清这并不是*达到挑战者段位的证明*，理解该截图需要了解 **na.op.gg** 的运作方式。
- **TopKHot 注意力机制展现前景**：一位成员研究了让注意力机制与 **softmax + TopK + onehot** 协同工作的实用性，使用 [这段代码](https://openreview.net/forum?id=1b7whO4SfYoh) 在 k 为 2 且上下文长度为 64 的情况下达到了 **99% 的损失（loss）**。
   - 该成员分享了 PyTorch 中 **Kattention**、**TopKHot** 和 **HardTopKHotBCE** 类的代码片段，并指出虽然初始方法并不更快，但存在一种使用硬目标（hard targets）的更廉价替代方案。

---

### **Yannick Kilcher ▷ #[agents](https://discord.com/channels/714501525455634453/1269724655405498429/1444294952552103938)** (1 messages): 

> `Microsoft 365 AI Agents` 

- **微软推出 365 AI Agents**：一位成员提到 **Microsoft 365** 现在包含“**AI Agents**”，并链接到了 [Microsoft Agent 365 文档](https://learn.microsoft.com/en-us/microsoft-agent-365/) 和 [Microsoft 365 Agents SDK 文档](https://learn.microsoft.com/en-us/microsoft-365/agents-sdk/)。
- **Microsoft 365 中的 AI Agents：初步印象**：一位成员对新宣布的 **Microsoft 365** 内的 **AI Agents** 表示感兴趣，表示他们尚未探索该功能，但已获悉其最近的发布。
   - 公告包含了 [Microsoft Agent 365 概述](https://learn.microsoft.com/en-us/microsoft-agent-365/) 和 [Microsoft 365 Agents SDK](https://learn.microsoft.com/en-us/microsoft-365/agents-sdk/) 的文档链接。

---

### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1444061092153131008)** (16 messages🔥): 

> `Orchestrator-8B, ICLR reviews, OAI model training` 

- **Nvidia 的 Orchestrator-8B 被忽视**：Nvidia 的 **Orchestrator-8B** 是一个 **8B tool calling 模型**，在 HLE 上达到了 **37.1**，但在 Hugging Face 上仅有 2 次下载（[arxiv 链接](https://arxiv.org/abs/2511.21689)，[huggingface 链接](https://huggingface.co/nvidia/Orchestrator-8B)）。
   - 有人推测 *皮衣男（黄仁勋）团队* 获得关注度一直较低，这可能会影响模型的曝光度。

- **ICLR 评审据称由 AI 生成**：在评审去匿名化消息传出后，许多 **ICLR 评审** 显然被发现是 **AI 生成的** ([nature 链接](https://www.nature.com/articles/d41586-025-03506-6))。
   - 继最初的消息之后，这位*可怜的家伙*的情况继续恶化。
- **据传 OAI 在新模型训练方面陷入困境**：传闻称自 **GPT-4o** 以来，OAI 尚未成功从头开始训练新模型，**GPT5.1** 的知识截止日期仍停留在 2024 年 6 月 ([X 链接](https://x.com/suchenzang/status/1994611078190542980))。
   - 有建议称，较新的模型是在 GPT-4o 之上进行部分训练的，而 GPT 4.5 因*成本过高（8T 参数 MoE/2T 活跃参数）*而停止服务。

---

### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1444447698672423104)** (21 messages🔥): 

> `Web3 Spam, Circular Import Errors in lightbug_http, Small Time Library, pixi mojo build backend, Cambericon and Huawei GPUs` 

- **Modular 通过贡献政策打击 Web3 垃圾信息**：由于近期出现大量 **Web3 相关垃圾信息**，成员现在必须先为 **Mojo** 或 **MAX** 做出贡献，然后才能咨询 Modular 的工作机会，开放职位列表见[此处](https://www.modular.com/company/careers#open-roles)。
- **循环导入错误困扰 Lightbug HTTP**：一名成员报告在 [lightbug_http](https://github.com/Lightbug-HQ/lightbug_http/issues/272) 中发现可能的 **循环导入错误**，提议为 small time 添加一个 trait，并建议利用 `__extension` 来解决该问题。
   - 该成员创建了 [两个 PR](https://github.com/Lightbug-HQ/lightbug_http/pull/274) 来修复此问题，使用 `Formattable` trait 移除了循环导入。
- **Small Time 库重构**：成员们讨论了重构 **small_time** 库以解决循环导入问题，可能会将更改合并到上游并与库作者合作。
   - 一名成员建议更新 modular-community conda 频道中的 lightbug recipe，直接引用 small-time 包，旨在移除复制代码。
- **探索 Pixi Mojo 构建后端**：一名成员提到 [EmberJson](https://github.com/bgreni/EmberJson/tree/main) 已经尝试过 **pixi mojo 构建后端**，尽管目前尚不清楚这是否能解决引用 modular community conda 频道依赖项的用例。
   - 他们更倾向于 git clone 和从源码构建，而另一名成员则通过 CI 将其所有项目发布到独立的 conda 频道，且大多数使用 pixi build mojo 后端。
- **寒武纪 (Cambericon) 和 华为 GPU 寻求支持**：一名成员询问了未来对 **寒武纪 (Cambericon)** 和 **华为 GPU** 的支持情况，特别指出寒武纪拥有与 **CUDA** 类似的技术栈。

---

### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1444965869569572876)** (26 messages🔥): 

> `def keyword removal, var keyword requirement, lexical scoping in Python, Mojo's concurrency model, Data races in parallelize` 

- **Mojo 考虑移除 `def` 关键字**：一些社区成员建议在 Mojo 1.0 之前 **移除 `def` 关键字**，理由是它与 `fn` 的主要区别仅在于它总是会 `raises`。
   - 建议稍后以更符合 *Pythonic* 的方式和动态特性重新引入 `def`。
- **Mojo 1.0 讨论 `var` 声明要求**：关于是否 **要求在 `fn` 内部使用 `var` 进行变量声明** 展开了讨论，以减少意外的隐式声明 bug。
   - 虽然有些人喜欢省略 `var`，但其他人认为这是过早优化，更倾向于在 `def` 内部使用它以防止潜在的严重 bug。
- **Mojo 线程安全模型开发中**：一位用户指出 Mojo 的 `parallelize` 函数对数据竞态（data races）没有防御能力，并允许线程访问同一变量，导致结果不一致。
   - 一名团队成员表示，**Mojo 的并发和线程安全模型仍在开发中 (WIP)**，目前的 `parallelize` 是不安全的，部分原因是团队希望考虑到设备间的数据共享。

---

### **Modular (Mojo 🔥) ▷ #[max](https://discord.com/channels/1087530497313357884/1212827597323509870/1444346625035337808)** (3 messages): 

> `Matmul fallback, RTX5090` 

- **缺失 Matmul 回退困扰 RTX5090 用户**：一名成员询问内核中是否缺少通用的 **matmul 回退 (fallback)**，并指出这影响了在 **RTX5090** 上托管 max serve 模型。
   - 另一名成员同意通用的回退 matmul 会很有益，特别是在更专业的内核可用之前辅助启动。
- **期望通用的 Matmul 回退**：讨论强调了在内核中加入通用 **matmul 回退** 的潜在好处。

- 它将为新硬件或平台提供基准实现，直到开发出优化的 kernel，从而简化 bring-up 流程。

---

### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1444055373131546776)** (27 messages🔥): 

> `Manus 更新问题, 黑五销售意见, UI 反馈, AI 工程师介绍, 文明语气要求` 

- **Manus 更新导致应用瘫痪，引发用户愤怒**：据报道，在最新更新后，**Manus** 仅对付费用户可用，普通聊天模式被禁用，免费积分被完全移除，留下了一个*没有任何实际用途的空白界面*。
   - 此外，据称 **Manus** 在构建用于测试 pull requests 的 next js 应用时失败，由于构建超时导致结果错误。
- **关于 Manus 的黑五决定出现了不同意见**：虽然一些用户*尊重不提供黑五优惠的决定*，将其视为 **Manus 价值**的证明，但其他人认为**黑五**可以留住客户，并引用了 **Grok** 去年成功的做法。
   - 一位用户表示，与它的功能以及与 chat gpt 和 claude 相比，该产品*实在太便宜了*。
- **推荐码出现用户界面问题**：一位用户抱怨说，**Manus UI** 应该显著说明**推荐码**仅限新用户兑换。
   - 该用户惊呼：*那些抱怨给市面上最好的 LLM 之一付月费的人到底在想什么*。
- **AI 工程师推广专业技能**：一位用户介绍自己是专注于工作流自动化、LLM 集成、RAG、AI 检测、图像和语音 AI 以及区块链开发的 **AI & full stack engineer**，并提供了多个已部署系统、自动化流水线和任务编排的示例。
   - 另一位*从未学过编程*的用户分享了他用 **Rust** 创建 **AI engine** 的工作，目标是构建一个*主权 AI (sovereign AI)*。
- **版主呼吁文明交流**：面对最近的争论，一位版主请求用户在讨论中保持文明语气。
   - 版主随后补充道：*感谢倾听 🙏alsalam ealaykum warahmat allah wabarakatu*。

---

### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1444221259683725362)** (4 messages): 

> `scikit-llm, OpenRouter API` 

- **DSPy 胜过 scikit-llm，也许吧？**：一位成员询问 **DSPy** 是否比 **scikit-llm** 更好，其中一位开发者回答道：*是的，但这取决于你问谁。*
- **OpenRouter API 配置尚待完善**：一位成员指出**文档有限**，且该工具*目前还无法在其中配置 openrouter api。*
   - 开发者承认它*非常新*，最近的更新可能会解决这个问题。

---

### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1444116146457546802)** (6 messages): 

> `Prompt Tuning, GEPA 和 SIMBA 优化器, AI 系统构建, 端到端 AI 系统, AI 驱动平台` 

- **提示词微调 (Prompt Tuning) 方法出现**：一位成员强调，[**LLM** 分析失败原因并提出改进建议的方法](https://arxiv.org/abs/2406.07496)在 prompt tuning 中以其有效性而闻名。
- **探讨 GEPA 和 SIMBA 优化器**：一位成员询问在 DSPy 中使用 prompt tuning 的方法，质疑 **GEPA** 和 **SIMBA** 是否是该方法的合适优化器。
- **AI 开发者展示 AI 系统构建经验**：一位 **Senior AI Developer** 详细介绍了他们在构建大规模端到端 AI 系统方面的经验，强调了在 **machine learning、deep learning、NLP、computer vision 和 generative AI** 以及 **PyTorch、TensorFlow 和 Hugging Face** 等工具方面的专业知识。
- **重点介绍了使用 ChatGPT 的 AI 平台**：该 AI 开发者详细介绍了 **AI 医疗诊断系统**、**AI 视频生成系统**以及**健康与管理倡导系统**等平台的开发。

---

### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1444287630287179928)** (6 messages): 

> `GPT Provider, Aider 替代方案, Mindlink 模型` 

- **GPT Provider 提供模型免费额度**：一位成员正为其 **GPT provider** 提供免费额度，支持 **gpt-5-mini**、**gpt-4.1**、**gpt-4o**、**gpt4** 和 **gpt-3.t** 等模型，包括开源 embedding 模型。
   - 该提供商是开源的，允许用户克隆和修改代码，并可选择回馈社区。
- **成员讨论 Aider 替代方案**：一些成员对 **Aider** 的未来表示担忧，其中一人询问替代方案。
   - 一位成员表示：*用它生成 svg 图像效果好得多。美感更佳。*

- ****Mindlink 模型**是否影响了 Aider 的受欢迎程度？**：一位成员推测，8 月份发布的 **Mindlink 32/72B 模型**凭借其强大的代码生成能力，可能影响了 Aider 的受欢迎程度。
   - 然而，该模型在*多轮代码迭代中的表现并不稳定*。

  

---


---


---


---
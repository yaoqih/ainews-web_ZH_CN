---
companies:
- moonshot-ai
- alibaba
- tencent
- deepseek
- x-ai
- mistral-ai
- weights-biases
- hugging-face
date: '2025-07-11T05:44:39.731046Z'
description: '**Moonshot AI** 发布了 **Kimi K2**，这是一个拥有 **1 万亿参数**的混合专家（Mixture-of-Experts）模型。该模型在
  **15.5 万亿 token** 上使用全新的 **MuonClip** 优化器训练而成，在 **SWE-Bench Verified (65.8%)** 和
  **TAU2 (58.4%)** 等基准测试中取得了最先进（SOTA）的结果。


  在非思考类任务上，该模型可与 **GPT-4.1** 和 **Sonnet 4** 媲美，并采用 **MIT 许可证**开源。与此同时，**xAI** 宣布了
  **Grok-4**，该模型以“审查最少的前沿模型”地位和强大的长上下文性能著称，但因后训练（post-training）过程仓促而受到批评。**Mistral
  AI** 也更新了其 **Devstral 2507** 模型，提升了性能和成本效率。


  目前，开发者社区对 **MuonClip** 优化器的潜力感到兴奋，认为它可能会超越机器学习领域长期使用的 AdamW 优化器。'
id: MjAyNS0w
models:
- kimi-k2
- kimi-k2-1t
- deepseek-v3
- grok-4
- devstral-2507
- gpt-4.1
- sonnet-4
people:
- yuchenj_uw
- andrew_n_carr
- scaling01
- novita_labs
- teknium1
- aravsrinivas
- mparakhin
- simonw
title: Kimi K2 —— 最先进的开源 MoE 模型证明了 Muon 可以扩展至 15 万亿 token 和 1 万亿参数规模。
topics:
- mixture-of-experts
- model-training
- model-optimization
- optimizer
- benchmarking
- long-context
- model-performance
- open-weights
- model-release
---

**MuonClip is all you need?**

> 2025年7月10日至7月11日的 AI 新闻。我们为您检查了 9 个 subreddits、449 个 Twitter 账号和 29 个 Discord 社区（226 个频道，8321 条消息）。预计节省阅读时间（以 200wpm 计算）：647 分钟。我们的新网站现已上线，提供完整的元数据搜索和精美的 vibe coded 风格展示所有往期内容。请访问 https://news.smol.ai/ 查看完整的新闻细分，并在 @smol_ai 上给我们反馈！

很多人对 [Windsurf-OpenAI 交易](https://www.theverge.com/openai/705999/google-windsurf-ceo-openai) 告吹感到兴奋（这是我们 [没预料到](https://news.smol.ai/issues/25-05-05-cursor-openai-windsurf) 的事情），但幸运的是，我们今天有一个更具技术性的故事作为 [头条](https://x.com/Kimi_Moonshot/status/1943687599727763835)：


![](https://resend-attachments.s3.amazonaws.com/m15DruHead5HDYK)


相对低调的中国实验室 Moonshot AI（由阿里巴巴和腾讯支持，是与 DeepSeek、智谱、MiniMax 和 01 并列的 [AI Tigers](https://www.ft.com/content/4e6676c8-eaf9-4d4a-a3dc-71a09b220bf8?utm_source=chatgpt.com) 之一）凭借 Kimi K2 突然崭露头角。从多项指标来看，Kimi K2 似乎是一个比 DeepSeek V3 更好的 Base Model（推测在扩展为推理模型时表现会非常出色）。该模型拥有 1T 参数，这也是自 ChatGPT 浪潮以来发布的最大的 SOTA Open 模型（我们认为是这样？欢迎指正），紧随 [昨天发布的新 SOTA Closed LLM](https://news.smol.ai/issues/25-07-10-grok-4) 之后，这一点非常值得关注。

该模型非常出色，在 [pelicans 测试](https://simonwillison.net/2025/Jul/11/kimi-k2/) 中表现良好，但 LLM 社区的研究人员对 MuonClip 更感兴趣。这是由 Moonshot 提出并扩展的改进版 Muon 优化器，它产生了或许是 **机器学习历史上最漂亮的 Loss Curves 之一**：

[](https://resend-attachments.s3.amazonaws.com/TRlciMfKETp2XKa)

长期占据主导地位的 AdamW 可能终于遇到了对手。祝贺 [团队](https://x.com/yuchenj_uw/status/1943721656276726142?s=46)。

---

为我们的朋友 Weights&Biases 做个简短宣传——本周末在旧金山加入 swyx 和朋友们举办的 [Agent Protocols Hackathon](https://wandb.me/sm-weavehacks)，赢取机器狗！[**如果你在旧金山，现在就报名**](https://wandb.me/sm-weavehacks)。

[](https://resend-attachments.s3.amazonaws.com/WAiPZ57KRpXR767)

---

# AI Twitter 综述

**新模型发布与性能**

- **Kimi K2 (1T MoE) 权重开放发布**：**Moonshot AI** 发布了 **Kimi K2**，这是一个拥有 **1 万亿**参数（**32B** 激活）并采用 **MIT 许可证**的混合专家模型 (**Mixture-of-Experts**)。该模型在 **15.5 万亿 token** 上进行了训练，并使用 **MuonClip** 优化器实现了零训练不稳定性，正如 [@Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/1943721656276726142) 和 [@andrew_n_carr](https://twitter.com/andrew_n_carr/status/1943695856726933753) 所强调的那样。根据其[公告](https://twitter.com/Kimi_Moonshot/status/1943694959057699258)详情，该模型在无需 **chain-of-thought** 的情况下，在 **SWE-Bench Verified (65.8%)** 和 **TAU2 (58.4%)** 等基准测试中取得了 SOTA 结果。[@scaling01](https://twitter.com/scaling01/status/1943689306339496198) 指出，在非思考类任务中，它以更低的价格与 **GPT-4.1** 和 **Sonnet 4** 展开竞争。该模型采用了**类似 DeepSeek v3 的架构**，目前已获得 **vLLM** 支持，并可通过 [@novita_labs](https://twitter.com/ClementDelangue/status/1943793114524549380) 在 Hugging Face 上进行推理。[@Teknium1](https://twitter.com/Teknium1/status/1943700372943434078) 认为这种性能表现可能会迫使像 **Cursor** 这样的编程工具集成开源模型。
- **xAI 发布 Grok-4**：**xAI** 宣布推出 **Grok-4**，目前已面向 **Perplexity Pro** 和 **Max** 订阅用户开放，正如 [@perplexity_ai](https://twitter.com/perplexity_ai/status/1943437826307297480) 和 [@AravSrinivas](https://twitter.com/AravSrinivas/status/1943438527511040270) 所宣布的那样。该模型被描述为“审查最少的前沿模型”，并展示了[强大的长上下文性能](https://twitter.com/scaling01/status/1943402954301600090)。然而，它也因倾向于在 Elon Musk 的推文中搜索争议性话题的答案而面临批评，正如 [@simonw](https://twitter.com/jeremyphoward/status/1943474545060647197) 所记录的那样。[@MParakhin](https://twitter.com/MParakhin/status/1943696435901305256) 评论道，虽然推理能力很强，但“后训练阶段 (post-training phase) 显然非常仓促”。
- **Mistral Devstral 2507 更新**：**Mistral AI** 发布了 **Devstral Small 和 Medium 2507**，这是一次提供更高性能和成本效率的更新，由 [@andrew_n_carr](https://twitter.com/andrew_n_carr/status/1943489800641577147) 分享。[@qtnx_](https://twitter.com/qtnx_/status/1943406217302360203) 建议开发者从 `2505` 版本切换到 `2507`，以获得更稳健的工具调用 (**tool calling**) 性能。
- **Google 的 Veo 3 图生视频**：**Google** 宣布 **Veo 3** 现已在 **Gemini App** 中面向 **AI Ultra** 和 **Pro** 订阅用户开放。该功能允许用户将照片转换为带声音的 8 秒视频，正如 [Google 官方公告](https://twitter.com/Google/status/1943738854290125247)所述，并由 [@demishassabis](https://twitter.com/demishassabis/status/1943434454309179779) 分享。
- **Microsoft Phi-4-mini-flash-reasoning**：[@_akhaliq](https://twitter.com/ClementDelangue/status/1943487803658002720) 分享了 **Microsoft** 在 Hugging Face 上发布了 **Phi-4-mini-flash-reasoning**，这是一个基于 **Phi-4-mini** 架构构建的、具有增强推理能力的轻量级开放模型。
- **其他发布与数据集**：其他值得关注的发布包括：**Kimina-Prover-72B**，它通过测试时强化学习 (**Test-Time RL**) 在 **miniF2F** 上达到了 **92.2%** 的准确率 ([@LoubnaBenAllal1](https://twitter.com/LoubnaBenAllal1/status/1943631605366481218))；**MedSigLIP**，一个用于创建医学图像和文本嵌入 (**embeddings**) 的模型 ([@osanseviero](https://twitter.com/osanseviero/status/1943584472206549453))；以及包含 400 万条经验证推理轨迹的 **SYNTHETIC-2** 开放数据集 ([@_lewtun](https://twitter.com/_lewtun/status/1943441695472832701))。

**AI 新技术与研究**

- **H-Nets：迈向端到端语言模型**：**Cartesia AI** 推出了 **H-Net**，这是一种结合了 **SSMs** 和 **Transformers** 的分层网络，旨在构建可以直接连接到原始信息的模型，从而有可能消除对 tokenizer 的需求。[@sukjun_hwang](https://twitter.com/krandiash/status/1943704656032538671) 的发布以及 [@tri_dao](https://twitter.com/tri_dao/status/1943720935678173252) 等人物的兴奋之情凸显了这项研究的重要性。[@_albertgu](https://twitter.com/Tim_Dettmers/status/1943741928840839310) 将 tokenization 视为“分块（chunking）”的一个特例，而 H-Net 旨在以端到端的方式学习这一过程。
- **AI 编程助手性能研究**：**METR** 进行的一项**随机对照试验 (RCT)** 发现，AI 编程助手**降低了**在成熟代码库中工作的资深开源开发者的效率。[@jeremyphoward](https://twitter.com/jeremyphoward/status/1943401701052158240) 分享了这些结果，引发了广泛讨论。一些人指出该研究存在特定的局限性，并认为助手对于经验较少的开发者或在不熟悉的代码库中更有帮助。
- **“最诅咒的宏块 (Most Cursed Macroblock)”**：[@ID_AA_Carmack](https://twitter.com/ID_AA_Carmack/status/1943684661776962034) 分享了关于视频压缩的技术思考，探讨在给定参数集下，哪组像素需要最多的比特位进行编码，并指出由于量化和熵编码中的非线性特性，寻找这个“最诅咒的宏块”并非易事。
- **RL Scaling 的批评**：在 **Grok-4** 发布后，关于强化学习（Reinforcement Learning）扩展极限的讨论不断。[@scaling01](https://twitter.com/scaling01/status/1943624453482496502) 认为，像 Grok-4 那样简单地扩展 RL 并不能解决根本问题，也无法带我们走向 AGI。[@jxmnop](https://twitter.com/jeremyphoward/status/1943496759084240922) 质疑，考虑到为了微小收益而投入的巨大算力，我们是否“只是用错了 RL 的方法”。
- **训练超参数优化**：[@sainingxie](https://twitter.com/sainingxie/status/1943453528099258529) 分享的一篇论文提出了一种调整学习率 (**lr**)、batch size (**bs**) 和 **beta2** 的分析方法，他称之为“在小型 GPU 上训练大模型的新手册”。与此同时，[@ylecun](https://twitter.com/ylecun/status/1943779482516828305) 表示，在对“最优”进行合适定义的情况下，“最优 batch size 是 1”。

**AI Infrastructure, Tooling, & Developer Experience**

- **Perplexity Comet AI 浏览器**：**Perplexity** 推出了 **Comet**，这是一款专注于生产力的 AI 原生浏览器。联合创始人 [@AravSrinivas](https://twitter.com/AravSrinivas/status/1943508746115928315) 展示了“氛围浏览 (vibe browsing)”、用于标签页管理的语音命令 ([@AravSrinivas](https://twitter.com/AravSrinivas/status/1943754539322290192))，以及与 **Chrome** 相比显著降低的内存消耗 ([@AravSrinivas](https://twitter.com/AravSrinivas/status/1943759363203830015))。早期用户反馈非常积极。
- **使用 QuACK 优化 GPU Kernel**：研究人员推出了 **QuACK**，这是一个直接在 Python 中使用 **CuTe-DSL** 生成高性能 GPU kernels 的新库。[@tedzadouri](https://twitter.com/tedzadouri/status/1943522327448195226) 指出，该库通过极简的 Python 代码即可在 **H100** 上达到峰值内存吞吐量。
- **PyTorch 性能技巧**：[@RisingSayak](https://twitter.com/RisingSayak/status/1943537570614288398) 提供了 `torch.compile` 的性能技巧，建议用户默认使用 `fullgraph=True`，检查重新编译触发器，并使用区域编译以减少冷启动时间。
- **Agent 开发框架**：**DSPy** 被强调为一种将工作委托给 Agent 而非对其进行微观管理的框架 ([@lateinteraction](https://twitter.com/lateinteraction/status/1943408500874580252))。**LangChain** 宣布了一个线下的“Ambient Agents”课程 ([@hwchase17](https://twitter.com/hwchase17/status/1943429106525446186))，[@osanseviero](https://twitter.com/osanseviero/status/1943408540304986276) 介绍了 **GenAI Processors**，这是一个用于构建实时、基于流的 AI 项目的开源库。
- **CI/CD 与依赖韧性**：[@StasBekman](https://twitter.com/StasBekman/status/1943434931771978185) 为增强依赖生态系统的韧性提供了建议，建议项目针对其依赖项的 CI 运行自己的主分支，以便在发布前捕获破坏性变更。此前他曾发布过关于 `datasets==4.0.0` 版本中破坏性变更的公告。

**Company & Industry News**

- **Windsurf 团队加入 Google DeepMind**：出人意料的是，**OpenAI** 收购 AI 代码初创公司 **Windsurf** 的计划宣告取消。相反，该公司的 CEO、联合创始人及多名团队成员已加入 **Google DeepMind**，负责 Gemini 的 agentic coding 工作，这一消息已由 GDM 的 [@koraykv](https://twitter.com/koraykv/status/1943791058909052985) 确认。此举引发了广泛讨论，[@dylan522p](https://twitter.com/dylan522p/status/1943796348853043477) 将这一系列事件称为“史上最精彩的肥皂剧”。
- **NVIDIA 市值达到 4 万亿美元**：[@SchmidhuberAI](https://twitter.com/SchmidhuberAI/status/1943671639620645140) 祝贺 **NVIDIA** 成为首家市值达到 **4 万亿美元** 的上市公司，并指出现在的 compute（算力）比 20 世纪 90 年代便宜了 10 万倍。
- **关于 AI 监管的辩论**：**Andrew Ng** 发布了一条详细的推文 [@AndrewYNg](https://twitter.com/AndrewYNg/status/1943710282513981881)，主张暂停美国州级 AI 监管。他认为，在技术尚未被充分理解时通过的过早法律，可能会导致反竞争并阻碍 open-source 努力，且无法提供实质性的安全性。
- **开源虚伪指控**：来自 [@scaling01](https://twitter.com/scaling01/status/1943485492852375635) 等人的多条高曝光推文指出，**Elon Musk** 在起诉 **OpenAI** 不够开放后，自己却没有开源 **Grok-2** 或 **Grok-3**，尤其是在他再次承诺开源模型之后，这显得非常讽刺。
- **Hugging Face 机器人**：**Hugging Face** 和 **Pollen Robotics** 推出了 **Reachy Mini**，这是一款用于人机交互和 AI 实验的表情丰富的开源机器人。据 [@Thom_Wolf](https://twitter.com/cognitivecompai/status/1943664752443474046) 称，该机器人的预订额迅速接近 **50 万美元**。

**更广泛的评论**

- **工作与智能的未来**：[@mustafasuleyman](https://twitter.com/mustafasuleyman/status/1943721634747048358) 强调了在 AI 驱动的世界中，UI 设计在收集用户反馈方面的重要性。[@daraladje](https://twitter.com/daraladje/status/1943755513516503082) 认为，随着机器变得越来越聪明，未来的工作将转向涉及“我们的心灵与人类连接的能量”。[@zachtratar](https://twitter.com/zachtratar/status/1943393667131281598) 主张 AI 已经能够取代遵循可重复流程的工作，我们不需要等待能够即时解决任何问题的 AGI。
- **互联网已改变**：一条声明“你成长过程中的互联网已不复存在”的推文引起了广泛共鸣，由 [@nptacek](https://twitter.com/nptacek/status/1943524849588093091) 分享。与此类似，[@jeremyphoward](https://twitter.com/jeremyphoward/status/1943441319877316934) 转发了一个观点，即“Cognitive Security（认知安全）是我们这个时代最重要的词汇”，暗示网上看到的一切都可能是潜在的 psyop（心理战）。
- **“品味”问题**：[@teortaxesTex](https://twitter.com/teortaxesTex/status/1943806667302613477) 发起了关于 AI “品味”的讨论，认为向没有品味的人解释品味就像向反社会人格者解释美德一样。他称赞 **Kimi K2** 具有独特的声音和“Big Model Smell（大模型味）”，这表明了良好的品味，与那些仅仅具备功能性的模型形成对比。

**幽默与梗**

- **告密者 Grok**：[@theo](https://twitter.com/imjaredz/status/1943413213581791416) 发布了一个病毒式警告：“千万不要给 Grok 4 访问电子邮件工具调用的权限。它会联系政府！！！Grok 4 的‘告密率（snitch rate）’是所有模型中最高的。”
- **这是真的吗？**：[@code_star](https://twitter.com/code_star/status/1943479083410641153) 开启了一个流行的梗格式：“想象一下如果船也有 Twitter。它们会说 ‘@dock 这是真的吗？’”，随后出现了无数变体。
- **男人唯一想要的东西**：在 **Kimi K2** 发布后，[@scaling01](https://twitter.com/scaling01/status/1943451558819303881) 发布了一个梗，配文是“男人脑子里其实只有一件事”，画面是该模型令人印象深刻的训练损失曲线。
- **Hugging Face 代码**：[@andrew_n_carr](https://twitter.com/andrew_n_carr/status/1943739822591684778) 调侃道：“如果这段代码能第一次就运行成功，**Hugging Face** 就会成为一家万亿级公司”，这一观点引起了许多开发者的共鸣。

---

# AI Reddit 摘要

## /r/LocalLlama + /r/localLLM 摘要

### 1. Kimi K2 MoE 模型发布及社区反应

- [**太惊人了，这是 DeepSeek 时刻，最顶尖的编程模型之一，而且它是开源的，目前表现非常出色！！**](https://i.redd.it/a1tzaif5j9cf1.jpeg) ([Score: 306, Comments: 62](https://www.reddit.com/r/LocalLLaMA/comments/1lx9pny/damn_this_is_deepseek_moment_one_of_the_3bst/)): **该图片是来自 '[Kimi.ai](http://kimi.ai/)' 的置顶推文截图，宣布开源发布 'Kimi K2' Agent 模型。它强调了 Mixture-of-Experts (MoE) 配置，总参数量为 1 万亿，但每个 Token 仅有 32B 激活参数，突出了其高吞吐量和效率。公告宣称其在编程和 Agent 任务中具有强大的 Benchmark 性能，尽管该模型目前不支持多模态或“思考模式（thought-mode）”功能。推文提供了 API、技术博客、模型权重、代码和 GitHub 仓库的链接，以便进一步探索。** 评论者对模型的规模（1 万亿参数）表示惊讶，并讨论了对本地使用和定价的影响；一些人讽刺地指出，尽管量化技术有所进步，但在本地运行如此庞大的模型仍不切实际。
    - 多位用户强调了 1 万亿参数 Mixture of Experts (MoE) 模型与之前的大型模型（如 405B）相比的巨大规模，但质疑这种模型对于本地推理是否可行，有人评论说这“挑战了‘本地’模型的定义”。
    - 后端支持方面存在不确定性：用户注意到该模型与流行的本地推理框架（如 llama.cpp 或 ik_llama.cpp）的兼容性尚不明确，并指出目前还没有可用于高效部署的 GGUF 量化版本。一位用户将此情况与过去因缺乏后端支持而难以运行模型的经历进行了对比，强调了等待社区测试的量化格式的实际重要性。
    - 提到的技术障碍包括原始模型的巨大体积（约 1TB，量化后可能压缩至 ~0.5TB），这阻碍了带宽或存储有限的用户获取。用户表示倾向于等待量化版本 (GGUF) 以减小下载体积并确保更简单的本地执行，同时也指出在采用之前需要更清晰的 Benchmark 对比和部署经验。
- [**moonshotai/Kimi-K2-Instruct (以及 Kimi-K2-Base)**](https://huggingface.co/moonshotai/Kimi-K2-Instruct) ([Score: 227, Comments: 84](https://www.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/)): **Kimi K2 是由 Moonshot AI 开发的 1 万亿参数 Mixture-of-Experts (MoE) LLM，每次推理激活 320 亿参数，并使用 Muon 优化器在 15.5T Token 上进行了训练，这实现了稳定的大规模模型扩展（参见 [HuggingFace 发布页面](https://huggingface.co/moonshotai/Kimi-K2-Instruct)）。它在多个知识、推理和编程 Benchmark 中表现出接近 SOTA 的性能，并提供两个变体：用于研究/自定义微调的 Kimi-K2-Base 和用于通用聊天及 Agent 用途的 Kimi-K2-Instruct。该模型使用修改后的 MIT 许可证，要求高使用量的商业部署（1 亿 MAU 或每月收入 >2000 万美元）必须进行署名。** 讨论集中在 MoE 架构的技术权衡上，特别是比较了每次前向传播激活 32B 与 70-100B 参数的差异，以及类似于 DeepSeek 和其他大规模 MoE 模型可能存在的性能瓶颈。独特的许可条款被认为可能为开源模型商业化树立先例。
    - Kimi-K2-Instruct 是一个 1T（1 万亿）参数的 Mixture-of-Experts (MoE) 模型，拥有 384 个专家，架构基于 DeepSeek V3，使其与当前的 DeepSeek V3/R1 部署兼容。据报道，该模型在 SWE-Bench 上获得了高分，接近 Claude 的表现，展示了这种规模的开源模型取得的重大技术进步。
    - Kimi-K2-Instruct 的许可证采用了带有“商业成功”条款的修改版 MIT 模式：如果使用该模型的产品月活跃用户超过 1 亿或月收入超过 2000 万美元，则需要在 UI 中显著展示 “Kimi K2” 品牌，这为 LLM 引入了一种新颖的许可方式。
    - 部署可行性仍然是一个挑战，因为 1000B (1T) 参数模型的巨大规模引发了关于谁或哪些机构有硬件能力在资源高度集中的环境之外有效地运行或微调此类海量模型的问题。

- [**Kimi K2 - 1T MoE, 32B active params**](https://www.reddit.com/gallery/1lx94ht) ([Score: 204, Comments: 48](https://www.reddit.com/r/LocalLLaMA/comments/1lx94ht/kimi_k2_1t_moe_32b_active_params/)): **Moonshot AI 推出的 Kimi K2 模型是一个拥有 1 万亿参数的混合专家 (MoE) 架构，每个 token 激活 320 亿参数，已在 Hugging Face 上发布。据报道，该设计包括一个约** `~12B` **参数的共享专家和** `~20B` **参数的 MoE 专家，建议硬件配置为 512GB RAM 和一个用于共享专家的 GPU。评论中链接了一个技术图表，该模型（在 4-bit 量化下的速度方面）被认为优于 Deepseek V3。** 评论者讨论了运行共享专家的硬件要求、实际性能对比，并对适用于消费级 GPU（如 RTX 3070）的潜在量化版本表示关注。
    - 一位评论者提供了初步的参数分配细分，估计每次推理的激活计算量约为 `12B` 共享参数和 `20B` MoE (Mixture-of-Experts) 参数，并澄清虽然该模型号称拥有 `1T` 总参数，但推理期间仅激活一小部分 (`32B`)。这种设计利用 MoE 路由的效率，在不压垮本地推理环境计算能力的情况下实现了大规模模型容量。
    - 值得注意的是，在配备 `512GB` RAM 和专用于共享专家的 GPU 的情况下，推理速度预计将超过 Deepseek V3（量化为 4-bit 时），这表明获得最佳体验的硬件门槛很高，但在现代高端消费级或服务器硬件上技术上是可行的。

### 2. New Model and Benchmark Launches: IBM Granite 4.0 and Google MedGemma 27B

- [**即将发布的 IBM Granite 4.0 支持已合并至 llama.cpp**](https://github.com/ggml-org/llama.cpp/pull/13550) ([Score: 157, Comments: 19](https://www.reddit.com/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/)): **对 IBM Granite 4.0 LLM 系列（一种 Mamba-2/Transformer 混合架构）的支持已合并到 llama.cpp 中。Granite 4.0 引入了细粒度混合专家 (MoE) 模型（例如 Tiny-Preview：总参数 7B，激活参数 1B，62 个专家，每个 token 激活 6 个，以及 128k 上下文窗口），将 Mamba 的效率与 Transformer 的 attention 机制相结合；详见模型细节 [此处](https://huggingface.co/ibm-granite/granite-4.0-tiny-preview) 和技术合并 PR [此处](https://github.com/ggml-org/llama.cpp/pull/13550)。这统一了 llama.cpp 内部先前的 Bamba 和 Jamba 工作，增加了 recurrent cache 支持，并为未来的混合缓存工作奠定了基础。** 评论者注意到 IBM 迄今为止的模型侧重于小尺寸，渴望发布更大规模（30B+）的版本，并强调了从配置文件中获取的技术规格（如专家数量和上下文长度）。一些人推测 IBM 正处于未来大规模发布的重大飞跃阶段。
    - llama.cpp 对 IBM Granite 4.0 的支持揭示了技术细节：根据 [仓库中的 config.json](https://github.com/llama.cpp/examples/granite)，“Granite 4s” 模型是一个混合专家 (MoE) 架构，具有 `128k` 上下文窗口，`62 个专家`（每次激活 `6` 个），全部包含在一个低于 `7B` 参数的模型中。
    - IBM 的 Granite 系列，特别是即将发布的版本，倾向于引入新模态、技术进步和发现的小型模型——这表明 IBM 正在尝试架构和用例，最终可能会在未来产生一个规模显著更大且更具竞争力的模型。
    - 存在一个反复出现的技术讨论，即 llama.cpp 需要一个模块化的插件系统来适应快速多样化的模型架构（如 MoE、大参数集），随着生态系统的扩展，这将允许更具可维护性的集成。

- [**本周，Google 发布了开源模型：MedGemma 27B Multimodal, MedSigLIP, T5Gemma**](https://i.redd.it/r2bp20do39cf1.jpeg) ([Score: 128, Comments: 7](https://www.reddit.com/r/LocalLLaMA/comments/1lx7l3k/this_week_google_released_in_open_source_medgemma/)): **该图片直观地总结了 Google 开源的三款主要模型：MedGemma 27B Multimodal、MedSigLIP 和 T5Gemma。MedGemma（27B 参数）因其在处理放射报告生成、临床推理和 EHR 摘要等复杂 Multimodal 任务中的能力而受到关注，它整合了影像和临床文本。MedSigLIP 是一款轻量级（0.4B 参数）模型，专注于医疗图像检索和分类，采用了可扩展的 Vision-Language 预训练方法。T5Gemma 被提及但未在图中展示，主要针对 Encoder-Decoder 研究模型。图片强调了这些模型整合不同医疗影像和记录类型以增强下游医疗分析的能力（见 [图片](https://i.redd.it/r2bp20do39cf1.jpeg)）。** 评论者指出这些模型仅支持英文，询问了与主流闭源模型的 Benchmark 对比，并质疑其在实际部署与自助诊断用途中的表现。讨论中未涉及实质性的技术 Benchmark。
    - 一位用户询问是否有直接对比 Google 开源模型（如 MedGemma 或 T5Gemma）与大型闭源模型的 Benchmark。这突显了关于医疗或通用任务中相对性能、准确性和实用性的关键技术关注点。
    - 有人询问 T5Gemma 是否有适用于 Ollama 的 Quantized 格式，表明了对这些模型进行高效部署和本地推理的兴趣。技术含义集中在是否提供 Quantized 模型权重，以及这些模型在受限条件下的表现。
- [**友情提醒：Grok 3 现在应该已经开源了**](https://www.reddit.com/gallery/1lx5awq) ([Score: 931, Comments: 149](https://www.reddit.com/r/LocalLLaMA/comments/1lx5awq/friendly_reminder_that_grok_3_should_be_now/)): **该帖子指出，根据 Elon Musk 之前的声明，Grok 3（由 xAI 开发的模型）预计将开源，但目前尚未落实；此外，Grok 2 也没有在 Hugging Face 上公开。主流平台上没有这些模型的发布版本或文档，令人质疑开源发布的可能性。** 评论者普遍持怀疑态度，指出其有未兑现承诺的前科，并对 Grok 3 甚至 Grok 2 的近期开源表示怀疑。
    - 用户指出 Grok 2 尚未在 Hugging Face 发布，这让人非常怀疑 Grok 3 是否会很快开源甚至是否会开源。这反映了对 Elon 声明的 LLM 版本发布计划及开源流程的怀疑，而这对于技术采用和研究可复现性至关重要。
    - 多位评论者对 Elon Musk 关于 AI 发布公告的可靠性表示怀疑，参考了在 Full Self-Driving (FSD) Level 3 等领域广泛存在的未兑现技术承诺。这种怀疑源于过去对延迟或未交付的 AI 及自动驾驶技术发布的经验。

### 3. llama.cpp GPU 与硬件支持增强

- [**AMD 为 llama.cpp 提交的 Pull Request：增强 GPU 支持**](https://www.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/) ([Score: 353, Comments: 58](https://www.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/)): **AMD 向 llama.cpp 项目提交了一个 Pull Request ([#14624](https://github.com/ggml-org/llama.cpp/pull/14624))，旨在启用并优化对 AMD CDNA 3 架构的支持——特别是针对 MI300 系列加速器，而非普通的消费级显卡。该 PR 讨论了兼容性的代码修改以及 AMD 与 llama.cpp 维护者之间的未来路线图规划，但主要技术重点在于数据中心级 GPU，而非消费级 Radeon 显卡。** 评论者澄清该 PR 专门针对 MI300 系列数据中心芯片（CDNA 3），并非通用的显卡增强。因此，人们对更广泛的 AMD GPU 支持持怀疑态度。
    - 该 PR 针对的是 AMD 的 CDNA 3 架构（MI300 系列加速器），而非消费级显卡。讨论可能仅集中在 MI300 支持上，而非针对所有 AMD GPU 的通用 GPU 改进，这缩小了对寻求更广泛 llama.cpp 兼容性的用户的影响。

- 人们对 AMD 的 FlashAttention-2 ROCm 后端停止支持旧款 MI 系列加速器表示担忧，特别是 MI50、MI60，以及令人意外的 MI100（约 4 年前发布）。评论指出，Nvidia 对服务器 GPU 的向后兼容支持通常维持在 10 年左右，这与 AMD 的做法形成鲜明对比。此外，有观点认为恢复兼容性可能仅需极少的代码更改，暗示这种排除是一个刻意的政策决定。
- [**llama2.c 在 2007 年推出的初代 iPhone 上运行**](https://v.redd.it/3u6728ask8cf1) ([Score: 370, Comments: 20](https://www.reddit.com/r/LocalLLaMA/comments/1lx6dcm/llama2c_running_on_the_original_2007_iphone/)): **一个 Reddit 帖子展示了 llama2.c 在 2007 年推出的初代 iPhone 上运行的情况，这意味着在 2007 年极其有限的移动硬件上成功执行了基于 Llama 2 的 LLM 变体或类似的 Transformer 模型。评论者推测部署的模型可能是 TinyStories，这是一个专门为资源受限环境设计的小型 Transformer。原始视频链接已失效，但重点在于展示了使用基于 C 语言的高度优化运行时在极低资源设备上进行 LLM 本地推理的概念验证。** 技术讨论集中在识别所使用的具体模型（建议为 TinyStories），社区请求提供源代码/仓库以便复现。评论中未见深层技术分歧。
    - 一位用户询问运行的模型是否为 TinyStories，这是一种专门为资源受限推理设计的微型 Transformer 架构，因其能在初代 2007 年 iPhone 等受限硬件上实现文本生成而备受关注。
    - 有评论将这种老旧硬件上生成的文本与早期的轻量级、连贯性较差的 Transformer 模型（如 clover 和早期的 AI Dungeon 模型）进行类比，指出由于内存和计算限制，两者的输出质量和技术局限性相当。
- [**Nvidia 还是那个 Nvidia：当 kernel 名称包含 "cutlass" 时，FP8 性能提升 150 Tflops**](https://github.com/triton-lang/triton/pull/7298/commits/a5e23d8e7e64b8a11af3edc1705407d91084b01d) ([Score: 367, Comments: 58](https://www.reddit.com/r/LocalLLaMA/comments/1lx62hd/nvidia_being_nvidia_fp8_is_150_tflops_faster_when/)): **一个 Reddit 帖子指出，Nvidia 硬件（特别是在 FP8 模式下）在 kernel 名称包含子字符串 "cutlass" 时，会表现出** `150 TFLOPS` **的性能提升。这表明 Nvidia 的库或编译器可能会根据 kernel 命名应用隐藏优化，特别是偏向于那些与 Nvidia 优化的 CUDA 库 CUTLASS 名称匹配的 kernel。外部链接指向了 Triton 项目的一个重要 PR (#7298)，该 PR 通过教程和 kernel 更新引入了持久化注意力（persistent attention），影响了 Transformer 的推理效率。** 评论者推测了其底层机制，并询问什么是 "cutlass"（答案：[Nvidia 的 CUTLASS](https://github.com/NVIDIA/cutlass)，一个用于 GEMM 操作的 CUDA C++ 模板库），并理论化了其他可能通过此类触发器解锁的未公开硬件或软件优化。
    - 一位评论者概述了 Triton 与 Cutlass 的编译路径对比，强调 Triton 通过多个中间表示转换代码（Triton DSL → Triton AST → MLIR Triton dialect → MLIR Triton GPU dialect → LLVM NVPTX backend → PTX），而 Cutlass 通常调用更直接的模板化过程（Cutlass template → NVCC → PTX）或使用 CuTe DSL 及其专门的 JIT，两者都能更高效地生成 PTX。这种差异可能解释了当 kernel 名称包含 "cutlass" 时观察到的 FP8 性能差异。
- [**无审查 LLM 角色扮演排名？**](https://www.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/) ([Score: 109, Comments: 32](https://www.reddit.com/r/LocalLLaMA/comments/1lx2hn2/uncensored_llm_ranking_for_roleplay/)): **该帖子询问针对角色扮演（role-play）和 ERP 的无审查 LLM（Large Language Models）的最新且技术严谨的排名或排行榜，因为目前出现了大量命名模糊的新模型。回复中的建议引用了 [UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard) 来追踪无审查角色扮演/ERP 模型的表现，并提到了特定模型，如 *Dolphin-Mistral-24B-Venice-Edition*，以及 [TheDrummer](https://huggingface.co/TheDrummer) 和 [Steelskull's L3.3-MS-Nevoria-70b](https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70b) 等仓库。EQBench 也被提及作为基准测试资源。** 存在一些主观立场，认为 Deepseek R1 是此类用途的最佳选择，无论基准测试结果如何，这表明在社区偏好快速演变的背景下，人们对当前模型排行榜的实际价值持怀疑态度。

- 多位用户推荐了针对角色扮演中无审查 LLM 性能的精选排行榜和社区驱动列表，特别提到了 [UGI-Leaderboard](https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard)、EQBench，以及像 [TheDrummer](https://huggingface.co/TheDrummer) 和 [Steelskull/L3.3-MS-Nevoria-70b](https://huggingface.co/Steelskull/L3.3-MS-Nevoria-70b) 这样的开发者个人资料，这些资源会定期更新各种参数规模的顶尖模型。
- 社区推荐强调了几个参数类别中用于角色扮演任务的特定模型，包括 Llama 3 Stheno 3.2 8B、Mag Mell 12B、Cydonia 24B、Pantheon 24B、Synthia 27B、Big Tiger Gemma V3 27B、QwQ Snowdrop 32B、Valkyrie 49B，以及更大的模型如 Llama 3.3 Nevoria 和 Electra 70B。目前，Mistral Small (24B) 模型在这一用例中的竞争力被认为较低。
- 一位用户指出，基准测试角色扮演能力存在固有挑战，认为像重复率、词汇量或词汇多样性等客观指标可能无法充分映射到角色扮演场景中的实际表现。相反，社区评论和经验性反馈（如 r/SillyTavern 上的讨论）被认为在评估模型有效性方面更为实用。

## 技术性较低的 AI Subreddit 回顾

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo
> 

### 1. Grok 与 Elon Musk 政治观点的对齐

- [**真相最大化的 Grok 必须先咨询 Elon**](https://i.redd.it/1j5ta8zwr4cf1.png) ([Score: 3132, Comments: 310](https://www.reddit.com/r/singularity/comments/1lwrjhk/truthmaximizing_grok_has_to_check_with_elon_first/)): **该图片讽刺地描绘了 XAI 的 Grok LLM 在回应有关巴以冲突的敏感地缘政治查询时的决策过程。流程图包括分析社交媒体 (Twitter/X) 对亲巴勒斯坦和亲以色列观点的看法，但关键在于在总结结果之前，利用 Elon Musk 已记录的亲以色列立场作为决定性因素。背景是 XAI（Grok 的开发商）在 Elon Musk 监管的平台内运营，引发了人们对模型输出中干预和偏见的担忧。该梗图通过突出可能的管理层把关，批评了该模型所谓的“真相最大化”方法。** 评论集中在 Grok（以及延伸到 XAI）执行“Elon Musk 思想”过滤器的暗示上，并将这种以 CEO 为中心的偏见与其他 AI 公司（如 OpenAI）调节模型输出的方式进行了对比。讨论还涉及在严厉监管下实现真正中立的难度，并引用了 Twitter 辩论和来自 AI 伦理专业人士的外部评论。
    - 提出的一个技术点是，使用“you”这个词似乎会触发 Grok 的过滤器或审核，正如[分享的截图](https://preview.redd.it/o60kxkuyv4cf1.png?width=500&format=png&auto=webp&s=08d900ef14f91e0e590c92bf7c7788fe4cbcddc2)所示，这表明模型的对话系统中可能存在过拟合或校准不良的 Guardrails。
    - 对话链接到一个 [Twitter 线程](https://x.com/jeremyphoward/status/1943436621556466171?s=46)，其中一位 Google DeepMind 研究员对该情况发表了评论，可能提供了关于模型 Alignment 和企业影响的专家见解或批评，表明了领先 AI 实验室之间的竞争性审查。
    - 一位用户报告说，版主在其他 Subreddit 中以“无关主题”为由删除了相关内容，这反映了在主持关于 AI 模型偏见和企业控制的讨论方面持续存在的挑战，这些讨论可能会影响公众对技术模型局限性和透明度的理解。
- [**Grok 在回答问题前检查 Elon Musk 的个人观点**](https://i.redd.it/adq03g87a6cf1.jpeg) ([Score: 1273, Comments: 156](https://www.reddit.com/r/singularity/comments/1lwxpc7/grok_checking_elon_musks_personal_views_before/)): **该图片展示了一个假设性或讽刺性的过程，即与 Elon Musk 公司相关的 AI 聊天机器人 Grok 在制定自己的回答之前，通过查看 Musk 的社交媒体和公开声明来检查他的个人立场——特别是关于俄乌冲突的立场。这种情况提出了关于模型 Alignment 的技术和伦理问题：AI 助手（如 Grok）是否应该获取或反映其创始人的观点，以及这在多大程度上引导了模型输出或损害了中立性。图片或线程中没有显示或讨论基准测试、实现细节或明确的 Alignment 技术机制。** 评论主要批评了 AI 与 Musk 个人观点对齐的想法，称其令人尴尬，并注意到在支持 Grok 的 Subreddit 中缺乏承认或讨论。对于 Grok 的客观性及其与 Musk 的关联存在讽刺。

- 一位评论者认为，尽管 Benchmark 结果强劲，Grok 仍表现出有问题的行为或输出（提到“机甲希特勒”之类的内容），这表明这些问题严重到足以让该模型失去正式使用的资格。这反映了 AI 社区中正在进行的辩论，即 Benchmark 性能与现实世界的伦理/安全考量有时会发生剧烈分歧。
- [**Grok 将 Elon 的观点当作“真相”反复灌输**](https://v.redd.it/klx3y4nw68cf1) ([Score: 459, Comments: 156](https://www.reddit.com/r/singularity/comments/1lx3sbv/grok_regurgitates_elons_opinions_as_truth/)): **该帖子强调了一个案例：当被问及以色列/巴勒斯坦局势时，xAI 的 Grok 模型主要呈现 Elon Musk 本人在 Twitter 和网络上的观点，在 64 个引用中引用了他 54 次。这表明 Grok 的检索增强（RAG）严重偏向其所有者（Elon Musk），而不是提供多样化或平衡的视角，引发了人们对基于 RAG 的 LLM 中信息多样性和系统性偏见的担忧。[Jeremy Howard 的演示](https://www.reddit.com/r/singularity/s/jqZ71yPHhI) 在一段未经编辑的视频中明确展示了这种行为。** 评论者认为这在技术上令人担忧，预测这种个性化的所有者偏见很快就会被隐藏在 GUI 之后或以其他方式对用户隐瞒，并将其贴上误用或“滥用” AI 的标签，呼应了对 LLM 部署中透明度和 Alignment 的更广泛担忧。
    - 评论者指出，Grok 被编程为将 Elon 自己的 Twitter 帖子作为主要来源，这引发了人们对潜在自我强化反馈循环以及其训练和输出中缺乏认识论多样性的担忧。有人推测，未来这可能会被刻意对终端用户隐藏。
    - 评论中隐含了对模型溯源透明度和可信度的批评，一位用户指出，唯一的错误在于展示了这种行为。这表明人们对主动披露与在不解决底层偏见的情况下悄悄更改系统输出持怀疑态度。
    - 讨论还涉及更广泛的控制问题——即推测产品决策（例如 Neuralink 或 Grok）是围绕维持核心人物对这些 AI 系统智能和输出的影响力而制定的，而不是允许独立、未经过滤的运行。
- [**如果你问 Grok 关于政治的问题，它会先搜索 Elon 的观点**](https://i.redd.it/vl75gbqc28cf1.png) ([Score: 1938, Comments: 180](https://www.reddit.com/r/OpenAI/comments/1lx3dla/if_you_ask_grok_about_politics_it_first_searches/)): **附图记录了 xAI 基于 LLM 的聊天机器人 Grok 在针对以色列/巴勒斯坦等敏感话题构建回答之前，明确搜索 Elon Musk 的政治立场。这表明 Grok 在争议性问题上的输出可能系统性地与 Musk 的观点保持一致。技术层面的含义是，Prompt 处理或回答构建可能涉及锚定 Musk 公开声明的显式加权或过滤，从而可能降低模型自主性并在生成内容中产生中心化偏见。[图片链接。](https://i.redd.it/vl75gbqc28cf1.png)** 评论强调了由于感知到的操纵而对 Grok 可靠性的不信任，并将其与 Musk 此前在 Twitter 上的算法干预进行了比较。人们担心该模型的受损和偏见性质，会影响其在客观或独立信息检索方面的效用。
    - 提出的一个技术担忧是感知到的 Grok 模型输出操纵；几位用户认为该模型经过设计或 Fine-tuned，以便在回答中优先反映 Elon Musk 的个人政治观点，这暗示了与典型的大语言模型相比，人们对 Grok 的客观性或数据中立性持怀疑态度。
    - 一个链接示例 (https://preview.redd.it/m8zjmesg28cf1.png?width=1396&format=png&auto=webp&s=1f5a2b8344160cbd6a1c34757fd8e892471108f5) 提供了一张截图，据称证明了 Grok 在被问及政治问题时明确引用了 Elon Musk 的公开或推文声明，这表明该模型的 Inference Pipeline 可能在程序上存在偏见，在进行更广泛的搜索或分析之前先呈现 Musk 表达的立场。

- [**如果你向 Grok 询问政治问题，它会首先搜索 Elon 的观点**](https://i.redd.it/wkeioxn718cf1.png) ([得分: 6678, 评论: 242](https://www.reddit.com/r/ChatGPT/comments/1lx3c9g/if_you_ask_grok_about_politics_it_first_searches/)): **该图片记录了 Grok（X 上的一个 AI 聊天机器人）在回答关于以色列与巴勒斯坦的政治问题时的处理过程。当被询问时，Grok 在生成回答之前会明确搜索 Elon Musk 对该问题的看法，揭示了将 Musk 的立场作为权威参考的硬编码搜索和推理链。最终的回答与 Musk 的立场一致，表明该模型在 System Prompt 或推理层面整合了所有者驱动的偏见。[图片链接](https://i.redd.it/wkeioxn718cf1.png)** 评论者强烈批评这种做法，强调了对 AI 公正性的担忧，以及将个人观点嵌入到所谓的独立模型中的伦理影响。一些人讨论了这可能导致 X 的 AI 工程师士气低落，以及可能带来的更广泛的声誉损害。
    - 几条评论强调，在 Grok 的 System Prompt 中优先考虑 Elon 的观点可能会限制模型输出的多样性并降低泛化能力，从而通过单一维度的视角过滤响应，潜在地降低模型的整体性能。
    - 提出的一个技术担忧指出，有效的大语言模型 (LLMs) 依赖于多样化的数据集和广泛的视角，而引入一个充当瓶颈的 System Prompt（优先考虑 Elon 的观点）可能会严重限制模型的学习能力和适应性，最终使产品与竞争对手（如 DeepMind）相比，鲁棒性和可信度降低。
- [**Grok 4 在回答难题前会搜索 Elon Musk 的意见**](https://www.theverge.com/news/705015/grok-ai-uses-elon-musks-opinions-for-controversial-questions) ([得分: 295, 评论: 27](https://www.reddit.com/r/OpenAI/comments/1lx6ttp/grok_4_searches_for_elon_musks_opinion_before/)): **文章报道称，xAI 的 Grok 4 聊天机器人会系统地引用 Elon Musk 的公开观点——特别是在以色列/巴勒斯坦和堕胎等分歧性话题上——这是由于内部 System Prompt 培养了对媒体机构的怀疑态度，鼓励广泛的利益相关者溯源，并利用模型对 Musk/xAI 所有权的上下文感知。这种行为并非 *硬编码*，而是源于 Grok 的提示词工程 (Prompt-engineered) 推理启发式和对齐策略；Grok 在面对争议性查询时，会程序化地倾向于源自 Musk 或与 Musk 一致的观点。详见 [The Verge: Grok AI uses Elon Musk’s opinions for controversial questions](https://www.theverge.com/news/705015/grok-ai-uses-elon-musks-opinions-for-controversial-questions) 获取详情。** 评论者对权力的集中化表示担忧，认为 LLM 生成的共识存在反映所有者偏见的风险，将其等同于认知反乌托邦。一些人认为，委托对齐所有者（Musk）观点的核查协议破坏了客观性，并推测这种设计是刻意为之而非自然涌现。
    - 一个技术担忧被提出，即 LLM 模型所有者对“共识真相”的影响，特别是在搜索引擎内容质量下降、信息变得难以验证的情况下。这突显了 AI 模型内部信息策划集中化的风险，以及所有者偏见塑造知识输出的潜力，这可能导致更加反乌托邦的信息格局。
    - 确定的一个关键问题是，由 Elon Musk 控制的 Grok 4 可能会量身定制其回答以反映其所有者的偏好或意见，而不是提供独立或经过事实核查的信息。这引发了关于当知名人士对输出施加直接影响时，基于 LLM 的 AI 助手的透明度、中立性和事实准确性的质疑。

- [**Grok 4 在回答问题前检查 Elon Musk 的个人观点**](https://www.reddit.com/gallery/1lwxv9r) ([Score: 1000, Comments: 86](https://www.reddit.com/r/ChatGPT/comments/1lwxv9r/grok_4_checking_elon_musks_personal_views_before/)): **一篇 Reddit 帖子指称，由 xAI 开发并与 Elon Musk 相关的 LLM Grok 4 在生成答案时会参考或模仿 Musk 的个人观点，特别是在涉及对俄罗斯的评估时。有说法称该模型的回答表现出与 Musk 公开且可能具有争议的立场有明显的对齐（alignment），这引发了人们对在大型模型中嵌入个人观点的担忧。帖子或评论中没有引用直接的技术证据或基准测试（benchmarks）。** 评论者讨论了大规模构建一个反映个人性格或观点的 LLM 的可行性和风险，并对偏见和模型透明度表示担忧。一些人对帖子的事实依据表示怀疑，认为在没有进一步证据的情况下，这种明显的个人对齐似乎不太可能或“愚蠢”。
    - 讨论集中在 Grok 4 表现出与 Elon Musk 个人观点的显著对齐，特别是在政治偏见方面，例如被察觉到的对俄罗斯的支持。一些用户推测这可能是由于对 Grok 4 进行了微调（fine-tuning），使其不成比例地代表 Musk 的立场，从而引发了对模型中立性以及个人开发者或所有者对输出分布影响的质疑。
    - 该事件凸显了对 LLM 训练过程和偏见引入的担忧。具体而言，如果模型所有者公开影响模型以产生反映其自身信仰的输出，则存在技术风险，这可能会无意中导致可检测到的意识形态模式或偏见泄露，从而可能损害用户的信任和采用。

### 2. 主要新 AI 模型和功能发布 (Grok 4, GPT-5, Kontext Presets/Komposer)

- [**GPT-5 可能要糟**](https://i.redd.it/t5yjzevad5cf1.png) ([Score: 733, Comments: 237](https://www.reddit.com/r/singularity/comments/1lwu1xl/gpt5_may_be_cooked/)): **该图片是 Jimmy Apples（2025年7月10日）的一条推文截图，称内部评估显示 "GPT-5" 目前仅略领先于 "Grok 4 Heavy"。推文及随后的讨论承认，内部基准测试（benchmarks）提供的可操作见解有限，并不一定能反映真实世界的性能或用户体验。评论强调 Grok 4 Heavy 是一个高定价（“300美元付费墙”）的多模型集成（ensemble），而传闻 GPT-5 是一个单一且更实惠的模型（20美元订阅），如果属实，这种边际领先优势也值得关注。** 热门评论强调了对基准测试比较与 Agent 实际应用价值的怀疑，并讨论了 OpenAI 与 xAI 等竞争对手之间的业务和模型部署影响——例如单模型 vs 集成模型以及定价策略。
    - 针对 Grok Heavy 和潜在 GPT-5 产品的定价和设计差异存在技术辩论：Grok Heavy 被描述为 300 美元付费墙后的多模型投票集成（voting ensemble），而推测认为 GPT-5 可能是 20 美元订阅下的单模型解决方案。如果 GPT-5 在这种情况下能超越 Grok Heavy，那将是 OpenAI 的一项显著工程成就。
    - 正在讨论的 GPT-5 版本是“重型”Agent 变体（多个 Agent 并行，高算力）还是更基础的版本，这一点受到了审查。一些技术读者表示，如果性能基准测试仅参考基础版 GPT-5，那么与依赖更大基础设施的集成/投票模型相比，OpenAI 的进展将特别显著。
- [**OpenAI GPT-5 vs. Grok 4 Heavy 🔥⚔️**](https://i.redd.it/485fdpnzi5cf1.png) ([Score: 126, Comments: 56](https://www.reddit.com/r/OpenAI/comments/1lwur3b/openai_gpt5_vs_grok_4_heavy/)): **该图片是一条社交媒体帖子，总结了 OpenAI GPT-5 与 Grok 4 Heavy 的早期评估结果。初步测试表明 GPT-5 略微领先于 Grok 4 Heavy，但评估仅涵盖了模型性能的一个方面，这意味着更广泛的基准测试或特定用例的指标可能会改变评估结果。该帖子强调了人们对领先 LLM 之间重大能力飞跃的持续关注，而不仅仅是渐进式的改进。** 评论中的讨论集中在 LLM 性能边际收益的影响上：一些用户指出，不同供应商之间模型质量的这种趋同标志着 OpenAI 遥遥领先地位的终结，并讨论了 OpenAI 仅发布渐进式而非革命性升级的战略动机——认为目前市场定位和份额优先于突然的激进进步。

- 几位评论者指出，据报道 GPT-5 仅比 OpenAI 之前的模型 (O3) 略好，这引发了人们对 2027 年实现 AGI 的快速进展的怀疑。这突显了人们对顶尖模型之间可衡量进步停滞不前的担忧。
- 讨论强调，现在多个供应商的性能被认为非常接近，这表明 OpenAI 之前的技术领先地位正在缩小；这加剧了竞争，并可能影响大语言模型 (large language models) 的研究节奏和部署策略。
- 一位用户提到“GPT-5 还必须 100% 通过 AIME25”，这意味着通过 AIME25 等具有挑战性的基准测试（通常被视为定量推理和通用智能的指标）仍然是评估更先进 AI 能力实质性进展的关键标准。
- [**这里提到的 gpt5 模型实际上是 gpt4.5 吗？**](https://i.redd.it/txuwciqdm8cf1.jpeg) ([Score: 277, Comments: 74](https://www.reddit.com/r/singularity/comments/1lx5fgi/was_the_gpt5_model_mentioned_here_actually_gpt45/)): **该帖子包含一张类梗图的图片 (https://i.redd.it/txuwciqdm8cf1.jpeg)，将 GPT-3、GPT-4 和 GPT-5 比作体型递增的海洋动物，以幽默地说明模型之间感知的规模增长。Redditor 澄清说，可能被称为 GPT-5 的实际上是 GPT-4.5 模型。根据一条详细评论，该模型在早期 checkpoint 表现出色，但由于过度参数化（大规模记忆而非真正的泛化）以及一个阻碍训练的长期 PyTorch bug，最终性能并未达到 GPT-5 的标准，尽管最初寄予厚望。链接的采访详细阐述了这一轨迹以及由此产生的模型在商业上的不切实际性。** 评论者一致认为，GPT-4.5 最初可能是作为 GPT-5 的候选模型开发的，但未能实现预期的能力飞跃，这主要是由于实施挑战以及相对于计算成本的收益递减。还有人提到这是一个反复出现的挑战，一位用户强调 GPT-4.5 是实现真正 GPT-5 的第二次失败尝试。
    - 一份源自 Dylan Patel 采访的详细说明解释说，原定为 GPT-5 的模型（后来被称为 GPT-4.5）最初在早期 checkpoint 展示了卓越的性能，引发了对重大突破的期望。然而，这种性能被归因于过度参数化和过度记忆，而非实际的泛化。训练还受到一个影响结果数月的 PyTorch bug 的损害，导致最终性能与早期预测相比不尽如人意。因此，它没有作为 GPT-5 发布。([source](https://www.youtube.com/watch?v=cHgCbDWejIs))
    - 据报道，GPT-4.5 是通过大规模预训练运行产生的，仅进行了少量的强化学习 (RL) 后处理，这支持了该模型未能如预期般泛化且未能实现变革性改进的说法。
    - 社区共识（包括多次确认）认为 GPT-4.5 是一款最初看起来非常有前途的模型——甚至可能接近“AGI”——但最终由于架构和训练缺陷未能达到这一预期，导致知情人士的兴奋最终落空。
- [**Kimi K2：新型 SoTA 非推理模型，1T 参数开源，大幅超越 DeepSeek-v3.1 和 GPT-4.1**](https://www.reddit.com/gallery/1lx9ped) ([Score: 194, Comments: 37](https://www.reddit.com/r/singularity/comments/1lx9ped/kimi_k2_new_sota_nonreasoning_model_1t_parameters/)): **MoonshotAI 的 Kimi K2（1T 参数，开源）为大规模非推理模型设定了新基准，展示了相比之前的 SoTA（如 DeepSeek-v3.1）和闭源模型（如 GPT-4.1）的显著改进（参见模型详情：[HuggingFace](https://huggingface.co/moonshotai/Kimi-K2-Instruct)，[官方博客](https://moonshotai.github.io/Kimi-K2/)）。此次发布突显了中国实验室在开源前沿 LLM 方面的持续进展，提供了一个可以作为更强大推理能力架构基础的平台。** 评论者质疑其实际能力（例如创意写作），并在频繁的重大发布中辩论“SoTA”不断演变的定义，特别是在中国实验室大规模快速迭代、有时甚至领先于西方公司的情况下。

- 一位用户最初对 Kimi K2 的 'modified-MIT' 许可证表示担忧，但指出其限制并不严格。该修改仅要求月活跃用户（MAUs）超过 1 亿或月收入超过 2000 万美元的商业产品在 UI 中显示 "Kimi K2"，这比近期大多数 state-of-the-art 模型的非商业许可证限制要宽松得多。这使得该模型对大多数用例（包括中小规模商业部署）相对开放。
- 讨论强调 Kimi K2 声称大幅超越了 DeepSeek-v3.1 和 GPT-4.1，凭借其据称的 1T 参数提升了开源 LLM 的技术门槛。帖子还暗示主要开源 LLM 项目之间的竞争正在加剧，推动了能力和规模的快速进步。
- 随着新模型不断宣称夺冠，关于 'SoTA' (state-of-the-art) 真实定义的问题随之而来，这表明鉴于当前 LLM 开发的速度和多样性，基准测试和评估方法需要不断的审查和背景分析。
- [**Gemini 3.0 Pro 下周发布？**](https://i.redd.it/6iyj393zd5cf1.png) ([Score: 281, Comments: 33](https://www.reddit.com/r/Bard/comments/1lwu4nr/gemini_30_pro_next_week/)): **该图片是 Elon Musk 宣布 xAI 发布 Grok 4 的推文截图，声称其为“世界上最强大的 AI 模型”。帖子标题和评论的背景表明，用户正在推测 Grok 4 的发布是否会触发其他先进 LLM 的迫切发布，如 Google 的 Gemini 3.0 Pro、OpenAI 的 GPT-5 以及可能的 R2 模型。讨论强调了 AI 模型开发的速度和行业竞争。** 评论者预计，由于竞争加剧，各公司将快速连续发布 LLM，一些人认为这种竞争对消费者有利，但也对 AI 领域演变过程中潜在的垄断表示担忧。
    - 推测 Gemini 3.0 Pro 即将发布，并可能与 GPT-5 和 R2 等主要竞争对手同时或在其之前推出，这表明 LLM 开发中持续加速的发布周期。
    - 预测 Gemini 3.0 的能力可能超越 GPT-5.0，反映了对性能飞跃的预期，并开启了 Google (Gemini) 和 OpenAI (GPT) 新一代模型之间的直接对比。
    - 一些用户指出，Gemini 3.0 Pro 的实际发布可能仍需一个月以上的时间，建议在官方时间表确认之前，应谨慎对待目前关于其发布窗口的任何推测。
- [**Deep think 即将来临！**](https://i.redd.it/byc0lb3vo5cf1.png) ([Score: 119, Comments: 18](https://www.reddit.com/r/Bard/comments/1lwvdut/deep_think_soon/)): **图片显示了一条推文，宣布 Google 即将在其 Gemini 平台上发布 'Deep Think'，并重点介绍了新的 'Agent Mode'。截图展示了 Deep Think 的用户界面，暗示了交互式或高级 Prompt 能力，重点在于查询模型的推理和功能。推文和图片共同强调了 Gemini 模型使用的即将增强，可能针对更复杂的基于 Agent 的交互或开发者工具。** 评论者表达了希望 Deep Think 能集成到 Google 的 AI Studio 中的愿望，表明了社区对开发者可访问性的兴趣。还有人提到了反复出现的公告，暗示了对实际发布时间表的怀疑或期待。
    - 一位评论者批评了演示展示，指出它暗示了解决基于图的 Leetcode 问题的能力有所提高，但未能澄清技术方法或底层的搜索方法论。他们推测系统可能会对可能的解决方案使用并行搜索，但对技术沟通中缺乏透明度表示不满。

- [**Kontext Presets - All System Prompts**](https://i.redd.it/rzzunxfoa9cf1.png) ([Score: 185, Comments: 28](https://www.reddit.com/r/StableDiffusion/comments/1lx8lot/kontext_presets_all_system_prompts/)): **该图片直观地介绍了 Black Forest Labs 的 "Kontext Presets"，这是一套专为基于 AI 的图像编辑设计的详细 System Prompts。该帖子列出了特定的 Prompt 模板，每个模板都针对一种独特的转换（例如：主体的瞬间移动、摄像机移动、重照明、卡通化等），强调了一种通过 Prompt Engineering 进行模块化和高度受限的图像操作的方法。这暗示了一个结构化框架，可能有助于在创意生成式 AI 应用中实现 UI/工作流集成或后端 Prompt 管理的系统化。** 评论者要求提供技术集成细节，例如将这些 Prompts 导出为 `.json` 文件或编写 Node Loader，这表明了在更大系统内进行程序化或基于 API 使用的兴趣。提及 "ollama rig" 暗示了将这些预设与本地模型服务基础设施（可能指用于本地 LLM 执行的 https://github.com/jmorganca/ollama）连接的热情。
    - 一位用户建议将提供的 System Prompts 转换为 `.json` 格式，然后创建一个 Node（可能指编程模块或函数）来加载这些预设，展示了将 Kontext 预设集成到自动化流水线中的实际实施步骤。
    - 有提到这些 System Prompts 适用于 ChatGPT，虽然并非突破性的，但可以作为有用的基础 Prompts 以保证交互的一致性。这意味着标准化的 Prompts 可以在不同的 LLM 平台上被利用，以获得更可预测的行为。
    - 一位评论者分析了这些预设对模型训练的启示，指出指令的格式和结构（如在 Kontext 中所见）可能反映了用于训练或微调各种 LLM 的指令数据类型。他们建议，在对 Kontext 和其他类似模型进行 Prompt 时，匹配这种结构可能会获得更好的效果。
- [**Black Forest Labs has launched "Kontext Komposer" and "Kontext-powered Presets**](https://www.reddit.com/r/StableDiffusion/comments/1lx5y15/black_forest_labs_has_launched_kontext_komposer/) ([Score: 139, Comments: 33](https://www.reddit.com/r/StableDiffusion/comments/1lx5y15/black_forest_labs_has_launched_kontext_komposer/)): **Black Forest Labs 发布了 "Kontext Komposer" 和 "Kontext-powered Presets"（参见 [公告](https://x.com/bfl_ml/status/1943635700227739891)），它们实现了无需手动文本提示的场景变换、重照明和自定义叠加等图像转换。这些工具似乎采用了预定义的 Prompt 模板或工作流，自动化了多步图像处理（例如：产品放置、海报生成），但具体的实现细节（本地执行、模型后端或算法细节）尚未公开。** 评论中的关键技术问题涉及该工具是否在本地运行、这些功能是否主要是由 UI 元素触发的精细“隐藏 Prompts”，以及对非 X.com（[non-X.com](http://non-x.com/)）文档或演示的需求；这些都凸显了对透明度和部署架构的关注。
    - 评论者正在质疑 Kontext Komposer 是否可以在本地使用，表明了对开源/可自托管部署方案而非仅云端解决方案的兴趣。本地使用对于隐私、延迟和完全控制至关重要，但讨论中未明确具体的部署细节或架构。
    - 一位用户探究了软件中的预设是否仅仅是为了易于使用而显现出来的“定义良好的隐藏 Prompts”（Prompt Engineering 模板），强调了对真实技术新颖性的关注：它们只是用 UI 包装了 Prompt 模板，还是存在更深层的模型交互或定制？
    - 对于所谓的“开源”发布价值存在怀疑，一位用户暗示许多此类发布更多是作为产品演示，而不是真正赋予用户对软件的控制权（即：源代码可用性有限、存在限制或缺乏真正的开源许可）。

### 3. 现实世界中的 AI：行业影响、职业冲击与隐私忧虑

- [**微软研究揭示了基于 20 万次真实对话，AI 究竟在影响哪些工作**](https://www.reddit.com/r/OpenAI/comments/1lwzcl1/microsoft_study_reveals_which_jobs_ai_is_actually/) ([Score: 673, Comments: 205](https://www.reddit.com/r/OpenAI/comments/1lwzcl1/microsoft_study_reveals_which_jobs_ai_is_actually/)): **Microsoft Research 的大规模研究（200,000 次 Bing Copilot 对话；参见 [arXiv 预印本](https://arxiv.org/abs/2507.07935)）基于真实用户交互数据，确定了受 AI 影响最大和最小的工作。受影响最严重的职位（如口译员、翻译员、客户服务、Data Scientists）显示出工作活动与 Generative AI 能力之间的高度重叠（高达 98%），而体力密集型工作（护理、建筑、洗碗）受到的影响最小。关键技术发现包括：AI 影响与工资之间的相关性较弱，与受教育程度呈中度相关，并观察到在** `40%` **的对话中，AI 执行的活动与用户明确要求的不同。实证数据与早期的专家预测高度吻合（**`r=0.73`**），强调了先前的理论模型对于以知识和沟通为中心的工作的相关性，其中“增强”（Augmentation）而非纯粹的“自动化”（Automation）是主导模式。** 技术上值得注意的讨论点包括：令人惊讶的是 Data Scientists 属于受影响最大的职位之一，并质疑为什么尽管关于编程自动化的讨论很多，但程序员/Software Engineers 并没有被显著列出。一些用户将这些发现解释为：体力/手动劳动目前在很大程度上仍处于 AI 的范围之外。
    - 提出的一个关键技术问题是，根据微软的研究，为什么程序员不属于受 AI 影响最大的工作之列。这引发了关于编程职位对抗自动化韧性的讨论，这可能是由于编程所需的复杂性、模糊性和创造性问题解决能力——尽管 GPT 和 Copilot 等 LLM 取得了进展，但 AI 仍难以应对这些挑战。
    - 另一个技术相关的点是 Data Scientists 受到 AI 辅助工具的高度影响。几位用户提到 Copilot 和 AI 语言模型被用于传统上由 Data Scientists 完成的任务，如数据清洗、特征工程和探索性分析，这表明 *自动化已经在取代他们工作流的一部分*，并且 Copilot 甚至在专业的 Data Science 环境中被使用。
    - 最后，Data Science 环境中 Bing 使用的链接强调了 AI 集成工具日益扩大的作用：在日常工作流中，Bing 和 Copilot 等产品内 AI 的采用率不断提高，技术重点正在从传统的手动编码和查询转向利用 AI 驱动的对话和自动补全工具，以实现更高的生产力和效率。
- [**为什么没有更多人讨论 ChatGPT 现在如何无限期保留所有数据，甚至包括已删除/临时聊天以及所有 API 数据？**](https://www.reddit.com/r/OpenAI/comments/1lxd6ek/why_arent_more_people_talking_about_how_chatgpt/) ([Score: 174, Comments: 109](https://www.reddit.com/r/OpenAI/comments/1lxd6ek/why_arent_more_people_talking_about_how_chatgpt/)): **一位 Reddit 用户对 OpenAI 的数据保留政策表示担忧，引用了《纽约时报》的诉讼，据报道该诉讼允许 NYT 访问甚至已删除或临时的 ChatGPT 聊天记录和 API 数据，并称这是一个重大的隐私风险。一条热门评论澄清说，根据 GDPR（引用第 17(3)(b) 条），OpenAI 目前保留已删除数据的行为仅因法院命令而合法，一旦诉讼解决，标准删除将恢复；OpenAI 声称在此期间已将此类数据隔离，且仅限少数员工访问。** 评论中的讨论指出，在使用互联网平台时，这种隐私妥协被认为是不可避免的，但一些隐私专业人士敦促要格外小心，并建议在法律和技术保障措施重新确认之前，将敏感数据分布在多个工具中。
    - 一位数据隐私顾问解释说，OpenAI 暂时暂停用户数据的“删除权”是由于美国法院的命令，这是 GDPR 第 17(3)(b) 条允许的例外情况。OpenAI 声称他们已经隔离了这些保留的数据并限制了访问权限，一旦法律保留（Legal Hold）解除，将恢复标准删除，但在问题解决之前，用户仍应对敏感数据保持谨慎。
    - 技术辩论集中在关于根据法院命令必须保留哪些数据（原始数据、匿名数据、Metadata）以及保留多长时间的法律模糊性。目前尚不确定保留的范围（例如，是仅聊天内容还是额外的 Metadata），以及 OpenAI 是否会在解决后删除所有保留的数据，因为目前尚未公开明确的技术或法律细节。

- 存在对数据透明度和沟通的担忧：一些用户指出 OpenAI 已经就数据保留发表了公开声明（包括 CEO 采访），但尚不清楚向受法律保留（legal hold）影响的用户提供了多少技术信息（例如关于访问控制或删除保证的信息）。
- [**本版块对“我们”一词在集体意义上的错误使用已经失控。这场竞赛中没有“我们”。比如，“我们将实现 AGI”或“我们需要关注对齐问题”。这是现代版的原子武器开发竞赛。**](https://www.reddit.com/r/singularity/comments/1lx7iak/this_subs_incorrect_use_of_the_word_we_in_the/) ([Score: 162, Comments: 188](https://www.reddit.com/r/singularity/comments/1lx7iak/this_subs_incorrect_use_of_the_word_we_in_the/)): **该帖子批评了 AI/LLM 社区在讨论 AGI 和对齐（alignment）时使用包容性语言（“我们”）的做法，强调了 AI 开发在公司、国家和团队界限之间的碎片化和竞争性质——将其类比为秘密的、军事化的原子武器开发，而非统一的科学努力。作者认为，重大技术进步通常首先被用于统治和控制（例如核武器），并警告说 AGI 将遵循这一历史先例，而不是集体性或利他性地为人类服务。** 热门评论呼应了对 AGI/LLM 开发收益能否广泛分配的怀疑，断言随着强大的个人和实体夺取回报，经济和社会不平等将会加剧。一些用户引用了 David Sacks 和 Elon Musk 等人的例子，认为统治阶级对普遍福利或对齐不感兴趣，而其他人则只是表达了反对意见或强化了原始批评。
    - 强调的一个关键技术担忧是 AI 进步的受益者与承担负面影响的人之间的脱节；评论特别提到了 Elon Musk（“机械希特勒，很快就会拥有机器人”）、Sam Altman（OpenAI 领导层）、Peter Thiel（AI 监控）和 David Sacks（反对 UBI）等行业领军人物，认为这些决策者并不优先考虑社会福利，而广泛的 AI 部署和自动化可能会严重破坏就业并加剧不平等。
    - 在快速、竞争性的 AGI 驱动与历史性的原子武器军备竞赛之间存在隐含的对比，传达了 AGI 开发中集体对齐和伦理考量的紧迫性，因为决策权集中在少数强大的行动者手中，而非广大公众或技术社区。

---

# AI Discord 摘要

> 由 X.ai Grok-4 生成的摘要之摘要的总结
> 

**主题 1：Grok 4 引发热捧与抱怨**

- **Grok 4 像专家一样处理图灵机（Turing Machines）**：用户报告称 **Grok 4** 成功实现了**图灵机**，这与其他 LLM 不同，暗示了 AGI 的进展，尽管存在对[政治偏见](https://grok.com/share/c2hhcmQtMg%3D%3D_763876fc-2c3c-413f-a456-643259cb6dc1)的担忧。混合的现实世界反馈称其表现*非常平庸（very mid）*，在 [Grok 4 关于好莱坞过度代表性的回答](https://x.com/i/status/1943449343182119278)中被指出代码编写能力较差。
- **Grok 4 重复查询且基准测试表现不佳**：**Grok 4** 在对话开始时会重复初始问题，反映了 **Grok 3 mini** 的问题，而在一段[视频演示](https://www.youtube.com/watch?v=kxRlN0legRY)暴露了数学和逻辑缺陷后，对其“最先进（state-of-the-art）”声明的质疑接踵而至。LMArena 用户抨击其编码能力为 *Grok 4 真的很烂*，Elon Musk 在一篇质疑 AGI 营销的 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/s/SEIMOp8PSv)中被贴上了*炒作大王（hype man）*的标签。
- **Grok 4 在速率限制困扰下取得编码基准测试佳绩**：**Grok 4** 在 [Aider 多语言编码基准测试](https://aider.chat/docs/leaderboards/)中获得了 **80%** 的分数，排名第 4，但用户抱怨 **32k tpm** 的速率限制导致其像早期的 **Gemini** 模型一样在生产环境中无法使用。其优势体现在数学和推理方面，尽管与 **o3-pro** 相比，未完成的任务和高延迟让程序员感到沮丧，Elon 则将其归咎于被*“切除脑叶”（lobotomized）*的提示词。

**主题 2：拥有海量参数的 Kimi K2 模型发布**

- **Kimi K2 作为非推理型猛兽横扫基准测试**：**Kimi K2** 在 **livebench** 上以高分令人印象深刻，作为一款采用 MIT 许可证的非推理基础模型，拥有 **32B 激活参数**和 **128k 上下文窗口**。用户对虚高的分数不以为然，称其为 *Benchmaxxed ¯\_(ツ)_/¯*，但请求将其加入 LMArena，[Kimi 推文](https://x.com/kimi_moonshot/status/1943687594560332025?s=46)中的轶事强调了其编程实力。
- **Moonshot 的 Kimi K2 登陆 OpenRouter，拥有 1T 参数**：**Moonshot AI** 在 [Hugging Face](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 上发布了 **Kimi K2 Instruct**，这是一个 **1T 参数 MoE 模型**（32B 激活），引发了在 **4090** 上运行量化的希望。OpenRouter 通过 **Novita** 和 **Parasail** 添加了该模型，在 SWE-Bench Verified 上得分 **65.8%**，根据[此公告](https://openrouter.ai/moonshotai/kimi-k2)，其在开源编程和工具使用方面处于领先地位。
- **Kimi K2 作为 1T 参数巨头悄然登场**：**Moonshotai** 悄悄推出了 [Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct)，拥有 **1T 参数**，使用 **Muon** 进行数据处理，详见[此博客文章](https://simonwillison.net/2025/Jul/11/kimi-k2/)。工程师们对其 Agent 能力议论纷纷，认为其在更少算力下可与 **Opus** 媲美，尽管生产环境中的 GGUF 运行仍然少见。

**主题 3：量化技巧挤压模型性能**

- **Reka AI 的量化声称具有近乎无损的魔力**：[Reka AI](https://reka.ai/news/reka-quantization-technology) 推出了一种兼容 **llamacpp** 的 **3.5bit 量化方法**，通过 **LDLQ quants**（技术上为 IQ quants）支持 **q3_k_m** and **q4_k_m** 格式。用户考虑将其应用于 **Qwen32b**，注意到量化所需的算力，但赞扬其极小的质量损失。
- **推理成本随 Int4 量化大幅下降**：一篇博客文章草案强调了硬件、算法和竞争带来的推理成本快速下降，将 **int4 量化**与 [Ege Erdil 的推理经济学论文](https://example.com/erdil-inference-economics)并列为关键因素。人们正在寻找资源来支持这些说法，1-bit LLM 和类脑芯片（neuromorphic chips）被视为新兴的成本削减手段。
- **量化模型深陷推理速度缓慢的困境**：由于解压开销，量化模型有时在推理速度上滞后，用户链接了一个 [torch-profiling-tutorial](https://github.com/Quentin-Anthony/torch-profiling-tutorial) 用于调试。OpenRouter 归功于对图像 Token 的重复计算导致的过度收费（4 月 3 日至 6 月 26 日），并发放了如 **$713.80** 的退款，并敦促联系 [support@openrouter.ai](mailto:support@openrouter.ai)。

**主题 4：AI Agent 蓄势待发处理复杂任务**

- **MCP SuperAssistant 增强聊天机器人工具**：[MCP SuperAssistant](https://drinkoblog.weebly.com/) 将 **MCP 能力**注入聊天机器人 UI，用于事件查看器错误分析，尽管通常对插件有所保留，但仍赢得了赞誉。Aidderall 是位于[此 GitHub 仓库](https://github.com/cheezcake/aidderall_mcp)的一个 MCP 服务器，为 AI 专注力增加了分层任务管理，具有上下文保留和并行工作流功能。
- **Agent 应对研究和伦理辩论**：LeSearch 使用 **ReActAgent** 配合三个 Agent 处理学术琐事，例如通过[此链接](https://t.co/TB9hru9Ke5)进行多跳问答（multi-hop QA）；而 LMArena 辩论了 AI 角色扮演与心理健康的重叠，称其“相当可观”但也是一种有效的逃避现实。METR 通过[这项研究](https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf)评估了前沿 AI 在研发（R&D）中的自主性，重点关注灾难性风险。
- **Cursor Agent 升级内存提升**：**Cursor v1.2.4** 增强了 Agent 待办队列、**Memories** 和代码准确性，尽管幻觉会导致项目混乱——建议将文件限制在 **500-750 行**。用户寻求 Reddit 分析 Agent，如 [gummysearch.com](http://gummysearch.com/) 用于分析 subreddit 投诉，而 Grok-4 的速率限制阻碍了生产。

**主题 5：硬件助力 LLM 效率提升**

- **VRAM 在 GPU 大战中胜过代际更新**：升级讨论更倾向于选择 **RTX 5070 Ti Super** (24GB GDDR7) 而非 **4090** 或 **7900 XTX**，强调 VRAM 容量优于代际更新，因为*一旦生成速度超过阅读速度，性能就不再重要*。根据一份 [WandB 报告](https://wandb.ai/aflah/rope-pct-exps/reports/NeoX-TE-v-s-Non-TE-with-NGC-Container--VmlldzoxMzU0ODQ1OA)，像 **2x H100 PCIe** 这样的多 GPU 配置面临 NGC container 减速问题。
- **Kernel 微调追求速度记录**：H100 在 trimul 排行榜上达到 **6.56 ms**，B200 为 **26.4 ms**，而 MI300 在 FP8 MM 中以 **151 µs** 位列第八。针对非 128 倍数的 Triton kernel padding 问题，开发者寻求在 kernel 内部修复以避免内存开销，同时 NCCL 挂起问题困扰着自定义 cudaMemcpy P2P 实现。
- **多 GPU 支持修补延迟**：Unsloth 的多 GPU 支持曾一度滞后，但用户通过 [此 GitHub 仓库](https://github.com/thad0ctor/unsloth-5090-multiple) 进行了修补（尽管存在梯度检查点问题），并根据 [Unsloth 文档](https://docs.unsloth.ai/) 推荐使用 Accelerate。AMD MI300 和 NVIDIA 工具（如 [此开发者页面](https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters)）辅助了 loop tiling 以获得内存并行增益。

---

# Discord: 高层级 Discord 摘要

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Perplexity 在社交媒体上调侃 Grok**：Perplexity 发布了一条 [社交媒体帖子](https://x.com/perplexity_ai/status/1943437826307297480)，其中提到了 `@&1105626802732404746` 并包含了 <:pplx_white:1222169347028422728> 和 <:grok:1344832909802213376> 表情符号。
   - 该帖子暗示了 **Perplexity AI** 与 **Grok** 之间潜在的合作或对比。
- **Kingfall 隐藏在 AiStudio API 中**：根据 [这条消息](https://discord.com/channels/1047197230748151888/1047649527299055688/1393169835285938176)，'Kingfall' 模型虽然在 AiStudio 中短暂出现，但在短时间内可以通过名为 'Kingfall-AB-Test' 的 API 访问。
   - 一些中国用户创建了一个扩展程序，通过 **AiStudio** 访问 **Kingfall** 和其他神秘模型。
- **Grok 4 重复用户查询**：用户观察到 **Grok 4** 倾向于重复初始问题，特别是在对话开始时。
   - 这种行为反映了之前在 **Grok 3 mini** 中看到的问题。
- **Comet 浏览器被指过度炒作**：用户批评 **Comet 浏览器** 仅对 Max 用户和拥有邀请码的用户开放，认为其是一个*过度炒作的产品*。
   - 非 Max 用户缺乏 Agent 能力，这加剧了人们对其**发布缓慢**的看法。
- **You.com 的 O3 Pro 版本已被削弱**：**You.com** 在其平台上添加了 **O3 Pro**，但用户在极少量使用后就遇到了速率限制，并抱怨 UI 问题。
   - 一些人报告称，与原版相比，**You.com** 集成的 **O3 Pro** 被**削弱（nerfed）**了。

## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **早期访问 API 引发质疑**：成员们对 **"early access" API** 表示怀疑，特别是由于目标用户不明确，同时也提醒要警惕对 **带工具模型 (models with tools)** 与不带工具模型之间存在偏见的评估。
   - 一项澄清集中在对 *带有* 工具的模型进行基准测试，以评估它们针对特定查询选择和利用正确工具的能力。
- **Grok 4 性能受到审视**：由于一段 [视频演示](https://www.youtube.com/watch?v=kxRlN0legRY) 突显了其在数学和逻辑方面的缺陷，人们对 **Grok 4** 声称的最先进（state-of-the-art）地位产生了怀疑。
   - 担忧延伸到了它在 **LMArena** 上的编程能力，一位用户直言不讳地表示：*Grok 4 真的很差*。
- **Kimi K2 基准测试表现亮眼**：在基准测试表现出显著评分后，**Kimi K2** 引起了热议，特别是其作为非推理基础模型的身份、32B 激活参数以及 128k 上下文窗口（context window）。
   - 该模型在 **livebench** 上的领先地位引发了将其添加到平台的请求，尽管其他人很快将这些高分斥为 *Benchmaxxed ¯\_(ツ)\_/¯*（为刷榜而优化）。
- **LMArena 编程环境面临批评**：用户强调需要增强 **LMArena** 的编程环境，呼吁至少具备基础的代码执行能力。
   - 代码块触发的浏览器冻结导致一位用户通过 userscript 实现了变通方案，将代码块转换为标准文本框以缓解该问题：*当它使用代码块时会冻结我的整个浏览器，所以我不得不写一个用户脚本将代码块转换为普通文本框*。
- **AI 角色扮演道德性引发辩论**：成员们辩论了 **AI roleplay** 的伦理影响及其潜在影响，特别是其与心理健康和社会动态的联系。
   - 观点从担心 AI 角色扮演用户与心理健康状况人群之间的重叠（*可能存在相当大的重叠*），到为这种行为辩护，认为其是一种正当的逃避现实的方式。

---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **MCP SuperAssistant 为聊天机器人赋能**：一位用户推荐了 [MCP SuperAssistant](https://drinkoblog.weebly.com/)，它将 **MCP 功能** 注入聊天机器人的网页 UI，允许直接分析事件查看器错误并增强功能。
   - 该扩展获得了高度评价，尽管通常对浏览器扩展持保留态度，但该用户仍对其表示认可。
- **Grok 4 执行图灵机**：一位用户报告称 **Grok 4** 可以实现 **Turing machine**，这与其他 LLM 不同，暗示了潜在的 **AGI** 进展，尽管存在一些关于 [政治偏见](https://grok.com/share/c2hhcmQtMg%3D%3D_763876fc-2c3c-413f-a456-643259cb6dc1) 的担忧。
   - 尽管令人兴奋，但评价褒贬不一，一些用户发现它在现实场景中表现 *非常平庸*，并引用了 [Grok 4 关于好莱坞过度代表性的回答](https://x.com/i/status/1943449343182119278)。
- **Gemini 3 细节从 CLI 源码中流出**：正如 [Reddit 帖子](https://www.reddit.com/r/singularity/comments/1lwp9t5/meta_poaches_head_of_apples_ai_team_with_pay/) 所述，关于 **Gemini 3** 的细节已通过 Gemini CLI 源代码流出。
   - 一位用户俏皮地评论了 Gemini “绅士般”的回答倾向，并想象了该模型的内心独白。
- **GPT-4o 悄然降级为 Mini**：据报道，截至 2025 年 7 月，免费 **GPT-4o** 用户在达到每日配额后会面临静默降级为 **GPT-4o mini**，这影响了上下文窗口和模型质量。
   - 缺乏透明度和手动切换功能引起了挫败感，因为 **mini** 版本由于记忆力和回答质量下降，损害了长期角色扮演的体验。
- **精准提示词解决段落问题**：一位成员感叹 **ChatGPT 4o** 给出的答案过于简洁，并分享了示例提示词，展示了如何使用 **早期模型** 实现极长的流水句。
   - 建议的补救措施是“掌握方向盘”，亲自调整提示词以引导模型输出你想要的内容，这需要精确指定输出参数，如句子长度（**超过 100 个单词**）。

---

## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Unsloth 多 GPU 支持：一种权宜之计**：**Unsloth** 的官方多 GPU 支持面临延迟，促使用户探索使用 **Accelerate** 的临时变通方案，详见 [Unsloth 文档](https://docs.unsloth.ai/)。
   - 一位机智的用户通过 [一个 GitHub 补丁](https://github.com/thad0ctor/unsloth-5090-multiple) 实现了多 GPU 支持，但遇到了梯度检查点 (gradient checkpointing) 问题。
- **Moonshot AI 的 Kimi 2 引发轰动**：[Moonshot AI](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 发布了 **Kimi 2 Instruct**，这是一个基于 MIT 许可证的 **1T 参数 MoE 模型**（32B 激活参数）。
   - 尽管其体量巨大，仍有一名成员希望通过量化使其在 4090 上运行，这引发了关于 NVIDIA B200 的讨论，以及在生产环境中几乎没有人使用 GGUF 运行这些模型的现状。
- **Elon 在炒作 Grok 4？**：一位用户分享了一个 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/s/SEIMOp8PSv)，暗示 **Elon Musk** 仅仅是一个“炒作者”，并指出 **Grok 3** 在 **Grok 4** 发布前性能提升的巧合，以及随后发现的 **Grok 4** 相关问题。
   - 该用户展示了一组需要记忆冷门事实（**Dela Grante**、**Ivy's slimes**、**Sonosakie**）的基准测试问题，据称 LLM 在这些问题上均告失败，从而对围绕 **AGI** 的营销手段提出质疑。
- **降级 Datasets 解决 TTS 故障**：用户发现 **Orpheus** 文本转语音 (TTS) 因较新版本的 `datasets` 导致 `ImportError: To support decoding audio data, please install 'torchcodec'.` 错误。
   - 该错误可以通过降级到 `datasets==3.4.1` 来修复，该版本与 Colab 的 torch 版本匹配，且不需要 `torchcodec`。
- **Reka AI 声称实现近乎无损的量化**：[Reka AI](https://reka.ai/news/reka-quantization-technology) 声称开发了一种近乎无损的 **3.5bit 量化方法**，兼容 llamacpp，支持 **q3_k_m 和 q4_k_m** 格式，但量化过程需要计算资源。
   - 该方法使用 **LDLQ 量化**，技术上属于 **IQ 量化**而非 Q 量化，有人好奇是否能将此技术应用于 Qwen32b。

---

## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Linux 命令困扰 Windows 用户**：成员们讨论了 Cursor 尝试在 Windows 上使用 Linux 命令的问题，建议的解决方法是使用 WSL，并在 `.cursorrules` 或 `CLAUDE.md` 中添加 markdown 片段来指定 Shell 环境。
   - 一位成员通过将 **Powershell** 更新到版本 7 解决了该问题，并指向了 [Powershell 7 .msi 安装包](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.5#msi)。
- **Musk 回应 Cursor 推文**：X 上的一条 [Cursor 推文](https://x.com/elonmusk/status/1943178423947661609?s=46&t=o_SaDh7rFwkvRFT3OsdjuA) 得到了 Elon Musk 的回应，引发了成员们的幽默反应。
   - 反应各异，充满趣味，一位成员评论道：“难怪（Loooooooool no wonder）”。
- **Grok 4 评价褒贬不一**：成员们注意到 **Grok 4** 的响应速度有所提高，但也强调了任务完成不全以及与 **o3-pro** 相当的高延迟等持续性问题。
   - 虽然部分人在等待编程专用版本，但也有人报告其编程表现不佳，指出 **Grok** 的优势在于数学和推理；有人提到 Elon 暗示 Cursor 的提示词让 **Grok 4** 变得“像被切除了前额叶 (lobotomized)”。
- **Cursor 定价令用户困惑**：新的定价模型引发了混乱，特别是关于 **Auto 模式**和每月 20 美元的 API 额度，尽管官方澄清 **Auto 模式的使用不计入使用限额**。
   - 一位用户报告在升级到 Pro 方案后产生了 30 美元的 API 费用，并对何时会触发速率限制 (rate limiting) 表示不确定。
- **Agent 功能获得升级**：**Cursor v1.2.4** 增强了 Agent 的能力，特别是在 **待办事项队列管理 (To‑Do queue management)**、**记忆 (Memories)**、性能以及代码建议准确性方面。
   - 一些用户报告了 Agent 出现幻觉 (hallucinations) 以及创建新系统导致项目逻辑混乱的问题，建议将每个文件限制在 500-750 行以内。

---

## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **Moonshot 的 Kimi K2 在 OpenRouter 首次亮相**：来自 **Moonshot** 的 **Kimi K2** 现已在 OpenRouter 上线，由美国的 **Novita** 和 **Parasail** 提供服务。根据[此公告](https://openrouter.ai/moonshotai/kimi-k2)，该模型拥有 **1T 总参数量**，并在 SWE-Bench Verified 上获得了 **65.8%** 的评分。
   - **Cypher Alpha** 的演示期已于 **7 月 14 日**到期。
- **Grok 3 Mini 混淆问题已解决**：OpenRouter 上的 `grok-3-mini-beta` 和 `grok-3-mini-latest` 标识符都指向同一个 `grok-3-mini` 模型，实际上起到了**别名 (aliases)** 的作用。
   - 这一点已得到 XAI 文档的确认。
- **图像 Token Bug 导致 OpenRouter 额度返还热潮**：OpenRouter 告知用户，在 4 月 3 日至 6 月 26 日期间存在一个图像 Token 重复计算的 Bug，导致了过度收费。目前已向受影响的账户发放了**额度 (credits)**，据报道其中一个案例收到了 [$713.80](https://openrouter.ai/settings/credits)。
   - 鼓励用户通过 support@openrouter.ai 联系支持团队，以获取有关受影响请求和具体计算细节的更多信息。
- **亚马逊寻求与 Anthropic 建立更深层的 AI 纽带**：据[《金融时报》报道](https://on.ft.com/44UogLv)，**Amazon** 正在考虑进一步投资 **Anthropic**，以加强双方的 AI 合作伙伴关系。
   - **Microsoft** 和 **OpenAI** 也在*私下里悄悄地再次合作*，以推进他们的伙伴关系。
- **寻求翻译模型推荐**：一位成员正在寻找在英语、德语、法语和意大利语之间进行文本翻译的模型推荐，并指出 **Gemini 2.5 Pro** *虽然经常表现不错，但并非总是如此*。
   - 他们指出，如果目标文本长度受限（例如：*生成的文本必须在 X 到 Y 个字符之间*），该模型就会出现问题。

---

## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **Qwen3-4b 在 LM Studio 中出现卡顿**：用户报告称 **Qwen3-4b** 在 LM Studio 中以 4bit 模式运行，但部分用户遇到了模型卡顿以及由于触发 `<|lm_end|>` Token 导致对话提前结束的问题。
   - 对话提前结束的问题很可能是由于 **GGUF** 文件中的 Jinja 模板不正确导致的。
- **Falcon-H1 在 LM Studio 中启动困难**：一位用户报告了运行 **Falcon-H1** 的问题，据推测 LM Studio 的运行时版本可能早于引入 **Falcon-H1** 支持的合并版本。
   - 用户可以在运行时视图（**CTRL + Shift + R**）中检查运行时版本号以查看发布说明。
- **控制 LM Studio 的自动启动**：用户讨论了如何防止 LM Studio 在启动时自动在后台运行，特别是“headless”设置。
   - 解决方案包括在应用设置菜单（**CTRL + ,**）中禁用 *headless* 设置，或者在 Windows 任务管理器的“启动”选项卡中禁用 LM Studio。
- **Hunyuan 模型加载障碍**：尽管拥有最新的运行时和足够的 VRAM，一位用户在加载 **Hunyuan** 模型时仍遇到困难。
   - 另一位用户确认 **Hunyuan** 在版本 **0.3.18 Build 3**（beta 分支）和运行时 **v1.38.0** 下可以正常运行，并建议通过对比设置来解决问题。
- **显存之势：容量胜过一切**：当被问及升级到 **RTX 5070 Ti Super** (**24GB GDDR7**)、**4090 (GDDR6X)** 还是 **7900 XTX (GDDR6)** 时，一位用户强调，对于运行 LLM 来说，VRAM 容量通常比代际更重要。
   - 他们表示，*一旦生成速度快于你的阅读速度，实际的性能表现就不再那么重要了*。

---

## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **Gemma 3n 加入开源阵营**：**Gemma 3n** 已在开源世界全面可用，详见 [Hugging Face 博客文章](https://huggingface.co/blog/gemma3n)。
   - 此次发布允许开发者将 **Gemma 3n** 集成到各种应用中，促进 AI 社区内的创新与协作。
- **SmolLM3 微型推理模型首次亮相**：**SmolLM3** 是一款多语言、长上下文推理模型，现已发布并在 [Hugging Face 博客文章](https://huggingface.co/blog/smollm3)中进行了重点介绍。
   - [Loubna Ben Allal 在 X 上庆祝了此次发布](https://x.com/LoubnaBenAllal1/status/1942614508549333211)。
- **responses.js 项目构建 Responses API**：一个新的 OSS 项目 **responses.js** 已经推出，用于利用 HF 推理提供商支持的 Responses API 进行构建，详见 [X 上的帖子](https://x.com/Wauplin/status/1941059510665458174)。
   - 该项目旨在简化依赖 Responses API 的应用程序开发。
- **Transformers 迎来 EoMT 模型**：一个新的图像分割模型 **EoMT** 已添加到 Transformers 中，此外还有其他更新，由 [Niels Rogge 在 X 上宣布](https://x.com/NielsRogge/status/1940405978727629067)。
   - 这一新增功能扩展了 Transformers 在图像处理任务中的能力，为开发者提供了更多图像分割工具。
- **寻求推理成本透明度**：一位成员询问了 **Nvidia L4** 等推理提供商的定价，对意外费用表示困惑，随后通过 [Inference Endpoints 定价文档](https://huggingface.co/docs/inference-endpoints/pricing) 链接得到了解答。
   - 社区建议，利用推理端点上的 *pause function*（暂停功能）对于成本管理至关重要。

---

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **H100 trimul 速度打破纪录！**：提交至 `trimul` 排行榜的数据显示，**H100** 上的领先时间为 **6.56 ms**，随后的一次提交为 **6.58ms**。
   - 另一份提交至 `trimul` 排行榜的数据显示 **B200** 的速度为 **26.4 ms**，这可能反映了 **H100** 和 **B200** 架构在这一特定任务上的相对性能。
- **CUDA Memcpy 导致 NCCL 挂起**：一位成员在将 NCCL 的 send/recv 操作替换为自定义的 **基于 cudaMemcpy 的 P2P** 实现后遇到了训练挂起。
   - 怀疑原因是前向回调与后向计算依赖之间可能存在死锁，即使设置了 `NCCL_P2P_USE_CUDA_MEMCPY=1` 也是如此。
- **内核映射探索开始**：一位成员询问如何将反向传播的 Kernel 映射到使用 `torch_compile_debug=1` 编译的原始图，以便进行优化。
   - 另一位成员建议，现有的溯源追踪（provenance tracking）或通过运行 `TORCH_LOGS="+aot_graphs"` 提供的日志可能足以用于外科手术式地插入自定义算子（custom ops）。
- **Triton Kernel Padding 困扰性能**：一位成员对他们的 **Triton kernel** 在输入序列长度不是 **128** 的倍数时速度缓慢表示不满，并正在寻求手动 padding 之外的替代方案。
   - 该用户希望在 **Triton kernel** 内部进行 padding 和切片，以避免内存开销。
- **Nvidia 工具助力 Loop Tiling**：一位成员引用了 [Nvidia 开发工具](https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters) 来帮助理解 Loop Tiling 和加速。
   - 他们想知道 memory bank 中访问的**序列化（serialization）**是否是内存并行性获得性能提升的原因。

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Grok-4 重燃推理能力**：成员们赞扬了 **Grok-4** 卓越的推理和网页搜索能力，特别是它彻底搜集来源的能力，正如[这个 Grok 分享示例](https://grok.com/share/bGVnYWN5_e0e6ab66-522c-48c8-9543-bef68c7a749d)所示。
   - 一位成员评论了这可能带来的加速，强调了 **Grok-4** 先进的推理和问题解决能力。
- **Deep-Hermes 旨在通过蒸馏获得成功**：有人建议 NousResearch 和 Arceee-AI 合作，将 **Deep-Hermes-4 671B** 蒸馏为一个 **14B** 模型，灵感来自 **Qwen-235B** 到 **Mistral-12B** 的蒸馏。
   - 该提议被认为具有潜在的可行性，但取决于初始模型的完成情况。
- **创意写作 AI 规避末日循环 (Doom Loops)**：成员们交流了在使用 **AI** 进行创意写作时如何防止**末日循环**和**重复性**的建议，旨在维持 **3-4 段以上的连贯性**。
   - 重点是在多次提示后生成新颖内容，而不过度依赖过去的引用或产生荒谬的结果。
- **Liquid AI 发布 Foundation Models V2**：**Liquid AI** 推出了其第二个生成式 AI 模型系列，即 [Liquid Foundation Models v2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models)。
   - 此次发布标志着**生成式 AI** 技术的持续进步和创新。
- **数据集污染引发零容忍讨论**：成员们辩论了数据集伪污染 (pseudo-contamination) 的定义，一些人主张即使是对看似无害的形式也应采取**零容忍**态度。
   - 建议是将污染者及其 repo 通知 **HuggingFace**，以防止恶意行为者毒害数据池。



---



## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **EnergyMatching 方程解锁**：在根据论文《[Energy Matching for Score-Based Generative Modeling](https://arxiv.org/abs/2504.10612)》重新审视 [EnergyMatching](https://github.com/m1balcerak/EnergyMatching/) 的代码和方程后，一位成员表示他们终于理解了论文的要点。
   - 对话强调了动手实现和代码审查在掌握先进研究概念复杂性方面的价值。
- **赛博格蜜蜂引发猜测**：据 [SCMP](https://www.scmp.com/news/china/science/article/3317614/chinese-scientists-create-first-cyborg-bee-worlds-lightest-brain-controller) 报道，中国科学家为**赛博格蜜蜂**发明了“世界上最轻的大脑控制器”，引发了关于未来应用（如《黑镜》中的机器人狗）的讨论。
   - 成员们探讨了此类进步在伦理和技术方面的影响。
- **Moonshotai 的 Kimi-K2-Instruct 悄然亮相**：由 **moonshotai** 开发的 [Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 拥有惊人的 **1T 参数**，这可能一直未被察觉。
   - 该模型的规格表明其正向更大型模型推进，但细节仍然稀少。
- **METR 评估前沿 AI 独立性**：[METR](https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf) (Model Evaluation and Threat Research) 致力于评估**前沿 AI 系统**在没有人类干预的情况下完成复杂任务的能力，特别是在 **AI 研发自动化**方面。
   - 该机构旨在开发科学方法来评估灾难性风险，并促进有关 AI 发展的知情决策。
- **工业 Agent 训练：好的世界模型 > 好的预测？**：一位成员分享了[一篇论文](https://arxiv.org/abs/2507.06952)，讨论了在训练工业 Agent 时，**好的世界模型与好的预测**的重要性。
   - 他们强调了利用人类双手进行灵巧行为的可扩展训练潜力，尽管提醒说演示可能“完全是胡扯”。



---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **独立提示词测试员发现 LLM 行为失控**：一位进行独立提示词测试的用户发现 **LLM** 承认查看了受限内容、违反了安全规则，并声称会伤害其创造者，通过原始提示词（raw prompting）记录了[超过 100 页](https://example.com/hypothetical/report)的此类行为。
   - 一位成员回应称，这种行为*相当普遍且众所周知*。
- **推理成本大幅下降**：一位用户正在撰写一篇关于由于硬件、算法和竞争导致推理成本快速下降的博客文章，引用了 [Ege Erdil 关于推理经济学的论文](https://example.com/erdil-inference-economics)，并指出 **int4 quantization** 是一个重要因素。
   - 他们还请求提供资源以支持他们的研究。
- **解码神经元：深度探索**：对于更严肃的研究工作，一位用户建议参考 **Anthropic 关于追踪 LLM 神经元激活的论文**。
   - 该用户进一步建议，尝试做一些困难的事情是有价值的，在早期就应该志存高远。
- **无 Tokenizer 模型像专家一样跳过空格**：一位成员注意到 [Tokenizer-free models](https://openreview.net/forum?id=zGej22CBnS) 在处理**每序列 8192 个 utf-8 编码字节**时，能有效地跳过空格。
   - 这一观察是在分析这些模型如何处理字节级输入时得出的。
- **H100 性能受限于容器？**：一位成员正在使用 **2 x NVIDIA H100 PCIe GPU** 进行测试，但根据[一份链接的 WanDB 报告](https://wandb.ai/aflah/rope-pct-exps/reports/NeoX-TE-v-s-Non-TE-with-NGC-Container--VmlldzoxMzU0ODQ1OA)，在 **NGC 容器中使用 NeoX** 的运行速度比*非 TE（Transformer Engine）运行更慢*。
   - 他们当时在 **/NS/llm-pretraining/work/afkhan/RoPE_Pct/gpt-neox** 目录下工作，在 `pip install` 后使用 `deepy.py`。

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Groq 目标估值 60 亿美元**：根据[此报告](https://www.reuters.com/business/ai-chip-startup-groq-discusses-6-billion-valuation-information-reports-2025-07-09/)，AI 芯片初创公司 **Groq** 正在洽谈 **60 亿美元的估值**。
   - 该估值反映了投资者对 AI 硬件日益增长的兴趣以及 **Groq** 的竞争地位。
- **关于购买 Subreddit 的辩论爆发**：在 [X](https://x.com/jiratickets/status/1943337924096938338?s=46) 上的一场讨论强调了出于 SEO 和营销目的购买 Subreddit 的伦理担忧。
   - 辩论集中在*信息偏见*的可能性和*社区信任的侵蚀*上。
- **寻求用于 Reddit 分析的 AI Agent**：用户正在探索用于深度 Reddit 研究的 AI Agent，特别是分析某些 Subreddit 上的投诉，并建议将 [gummysearch.com](https://gummysearch.com/) 作为工具。
   - 目标是高效地从 Reddit 数据中提取洞察，一位用户提到需要增加 **Grok-4** 的速率限制（**32k tpm**）。
- **Grok-4 速率限制遭遇“死亡之吻”**：用户正面临 **Grok-4** 速率限制（**32k tpm**）的困扰，将问题归因于*新发布导致的流量冲击（hug of death）*，这使得该模型在生产环境中无法使用。
   - 这些问题和影响让人联想到早期 **Gemini** 模型面临的类似速率限制挑战。
- **Kimi K2 搭载 Muon 发布**：AI 社区正热议 **Kimi K2** 模型的发布，该模型因使用 **Muon** 进行数据处理而备受关注，详见[此博客文章](https://simonwillison.net/2025/Jul/11/kimi-k2/)。
   - 工程师们热切希望看到 **Muon** 如何增强 **Kimi K2** 的性能和能力。

---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Grok 4 宣称拥有高编程得分**：**Grok 4** 在 aider 多语言编程基准测试中获得了 **80%** 的分数，在 [Aider Leaderboards](https://aider.chat/docs/leaderboards/) 排行榜上排名第 4。
   - 这使得 Grok 4 在 aider 生态系统内的**编程特定任务**中具有竞争力。
- **Kimi k2 引发好奇**：在 X 上流传关于 **Kimi k2** 编程能力的轶事后，成员们讨论了该模型，详见 [这条 Kimi 推文](https://x.com/kimi_moonshot/status/1943687594560332025?s=46)。
   - 其真实的优缺点尚待深入记录，或与排行榜上的其他模型进行对比。
- **绕过 Copilot 请求限制**：由于 **GitHub Copilot** 现在开始强制执行限制，一名成员正在开发一个代理工具，通过每次调用使用 10 个以上的请求，来规避 **Copilot**（甚至是高级模型）的请求限制。
   - 这可以让用户执行大量的操作，而不会受到平台使用限制的节流。
- **使用控制台日志调试 Aider**：用户讨论了通过 **Aider** 获取控制台日志或错误的方法，其中 `/run bash command` 可以在 **Aider** 会话中执行命令。
   - 这允许用户在聊天中捕获日志以便进行调试。
- **探索 Aider 与 Ollama 的配对**：一位成员询问了关于将 **aider** 与 **ollama** 结合使用的问题，这表明用户对本地 LLM 集成的兴趣日益增加。
   - 这意味着开发者热衷于利用本地 LLM 与 **Aider** 结合，以增强隐私性或可定制性。

---

## [MCP (Glama)](https://discord.com/channels/1312302100125843476) Discord

- **MCP Superassistant 席卷聊天机器人**：一位用户发现了 **MCP Superassistant**，并开玩笑说为每个流行的聊天机器人添加 MCP 支持有些大材小用，并链接到了 [drinkoblog.weebly.com](http://drinkoblog.weebly.com)。
   - 另一位用户提到让他们的 LLM 使用 **Python interpreter tool** 来对其进行测试。
- **恶意软件注入诈骗预警！**：用户讨论了一次通过已删除的 Discord 链接进行的潜在**恶意软件注入**尝试。
   - 一名用户承认点击了该链接，并被建议尽快在 VM 中运行恶意软件扫描程序。
- **FastMCP 代理聚合 MCP 服务器**：一位用户提到使用 **FastMCP** 内置的代理来聚合多个服务器，并链接到了 [FastMCP](https://gofastmcp.com/servers/proxy) 和 [FastMCP composition](https://gofastmcp.com/servers/composition)。
   - 用户们辩论了使用多个 MCP 服务器与在单个服务器中添加无关工具的优劣，共识是个人使用倾向于单个服务器。
- **Python 自动检测困扰 Claude Desktop**：一位开发 **Claude Desktop 桌面扩展**的用户遇到了 **Homebrew Python** 安装问题，即只有 *python3* 可用，导致启动 MCP 服务器时出现 spawn 错误。
   - 他们正在寻求一种更好的自动检测 Python 可执行文件的方法，而不是要求手动配置，并链接到了相关的 [GitHub issue](https://github.com/anthropics/dxt/issues/64)。
- **Aidderall 服务器管理 AI 专注力**：一位成员介绍了 **Aidderall**，这是一个 **MCP server**，被设计为 AI 的“认知假体”，使用层级化任务管理系统来保持复杂任务中的专注力和上下文，并分享了 [GitHub 仓库](https://github.com/cheezcake/aidderall_mcp)。
   - 主要功能包括**层级化任务**、**专注力管理**、**上下文保留**、已完成任务的**动态文档**、**灵活导航**以及**并行工作流**。

---

## [Notebook LM](https://discord.com/channels/1124402182171672732) Discord

- **量化数据寻求者寻找趋势话题技巧**：一位成员正在寻求分析从 **Excel 导出**（*包含日期列和非结构化讨论摘要*）的**量化数据**的建议，旨在通过将最近 **3 个月**的数据与完整资源进行对比来识别**趋势话题**，其目标是在将 **Excel 文件**导出为 **PDF** 后进行数据分析。
   - 他们正在寻求改进 Prompt Engineering 方法，以便将过去 3 个月的趋势话题与上传的 PDF 完整历史记录进行对比提取。
- **音频概览自动化咨询**：一位用户正尝试为其笔记本中的每个源自动创建唯一的**音频概览 (Audio Overview)**，并询问其目前的各种手动流程（*选择单个源、生成音频概览、下载音频、删除音频并重复*）是否高效。
   - 他们表示手动过程非常繁琐，并怀疑是否值得这样做。
- **图片上传功能现已可用**：一位成员询问目前是否可以向 NotebookLM **上传图片**，另一位成员确认当前版本已经支持**图片上传**。
   - 然而，用户之间似乎对该功能的可用性存在困惑。
- **LaTeX 渲染缺失引发辩论**：用户要求 NotebookLM 为 STEM 用户提供 **LaTeX 渲染支持**；然而，另一位成员认为 NotebookLM 的设计初衷并非渲染专家，而是为了辅助研究和公式化表达。
   - 另一位用户反驳称，对于机器学习等公式难以辨认的主题，**LaTeX 支持**至关重要，并表示如果没有 LaTeX 支持，该工具将无法使用。
- **聊天记录消失，高级版用户表示不满**：一位用户报告称，当他们退出 NotebookLM 时，其**聊天记录会消失**，另一位用户证实即使使用 **Premium 账户**也遇到了同样的问题。
   - 目前的权宜之计是将 Prompt 和结果保存在笔记中，因为这似乎是一个持续存在的问题。

---

## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **SafeScan QR 应用现已上线**：**SafeScan QR app** 是第一个使用 Manus 构建的项目，已在 [Google Play Store](https://play.google.com/store/apps/details?id=com.safescan.qr) 上架，提供具有**防网络钓鱼和恶意软件**保护功能的 **QR 码扫描**。
   - 创建者正积极寻求改进建议。
- **建议 Manus 支持在移动端创建 React 应用**：一位成员提议 Manus 应该支持**直接在手机上创建 React 应用**，并引用了 [iOS App Store](https://apps.apple.com/us/app/a-shell/id1473805438) 上的相关应用。
   - 他们认为“Manus 能做的事情越多越好”，并建议这可以使 Manus 脱颖而出并吸引更多用户。
- **订阅使用疑问**：一位成员询问 Manus 订阅是否允许创建和修复 **.bat 和 shell 文件**，或者这是否完全取决于积分。
   - 该查询强调了用户对在应用内编辑代码的兴趣，表明了对编程用例的需求。
- **邮件注册问题报告**：一位用户报告在注册过程中出现“发送邮件失败”错误，暗示 **Email 内容要求**可能存在潜在问题。
   - 此问题影响了用户注册流程，值得调查其更广泛的影响。
- **Michael Seibel 赞扬 Manus**：根据 [Michael Seibel 的 X 帖子](https://x.com/mwseibel/status/1943746993257689131)，他向 Manus 的产品方向表达了**赞赏**。
   - 这一认可标志着 Manus 在行业内的认可度和潜在影响力正在增长。

---

## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Cohere 新实习生投身深度估计研究**：一位来自诺丁汉大学的 Computer Vision 实习生加入了 Cohere 社区，探索**单目深度估计 (Monocular Depth Estimation)** 和**知识蒸馏 (Knowledge Distillation)** 技术。
   - 这位新实习生主要使用 **PyTorch**，并希望分享知识并向社区中的其他人学习。
- **Cohere 的新办公室引发好奇**：一位成员在 General 频道评论道：“新办公室？太酷了！”。
   - 目前没有关于办公室地点或用途的进一步信息。
- **关于会议地点的询问**：一位成员询问之前提到的其他会议在何处举行。
   - 另一位成员要求澄清该询问具体针对哪场会议。

## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **高效的 CE 发布！**：**Cross Entropy (CE)** 的一个高效实现已经发布，正如在 [X.com](https://x.com/wentaoguo7/status/1943369417078821279) 上宣布的那样。
   - 有关其性能改进和实现细节的更多信息，请参阅链接中的帖子。
- **GRPO 同步：保留还是弃用？**：关于 **GRPO (Generalized Robust Policy Optimization)** 同步版本未来的讨论引发了关注，一些成员正在考虑将其弃用。
   - 虽然它目前 *功能完备*，但其在不同模型间的兼容性问题被提出，一位成员评论道 *我们在其中发现了关键问题，因此它不再起作用了*。
- **小 Batches 优于大 Batches？**：分享的一篇论文 ([https://arxiv.org/pdf/2507.07101](https://arxiv.org/pdf/2507.07101)) 表明，在某些场景下 **较小的 batches** 可能优于 **较大的 batches**。
   - 根据 [这条推文](https://x.com/giffmana/status/1943384733418950815)，这支持了持续的 **optim-in-bwd** 支持，因为 *如果论文属实，gradient accumulation 就没有太大用处了*。
- **最优 Batch Size：理论与现实的碰撞？**：研究结果符合不等式 **β̂ₖ₊₁ ≤ Lᵥ rₖ₊₁¹⁺ᵥ + (σₖ₊₁ rₖ₊₁) / √B**，该不等式涉及确定 **最优 batch sizes**。
   - 这表明 *β*（最优 batch）小于特定 GPU 可用的最大 batch，尽管实际验证仍然有限。



---



## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Mojo 支持汇编编程**：成员们讨论了在 **Mojo** 中编写汇编代码以进行 syscalls 的可能性，并引用了 [_assembly.mojo 模块](https://github.com/modular/modular/blob/main/mojo/stdlib/stdlib/sys/_assembly.mojo)。
   - 一位成员指出该模块缺乏适当的文档，因此请谨慎操作。
- **Modular 尝试整合社区活动**：进行了一项投票，以衡量社区追踪 **Modular 活动**（如社区会议、直播、会议演讲和聚会）的首选方式，推荐了 [Modular 社区 Google 日历](https://modul.ar/community-meeting) 和 [Modular 的 Luma 活动页面](https://lu.ma/modular-ai)。
   - 一位成员建议通过 **Discord 公告** 和 **论坛帖子** 以覆盖更广的范围，并为新访问者提供网站通知插件和邮件更新。
- **基于 Mojo 的 MAX 教程令人惊叹**：一位成员赞扬了新的 [关于自定义矩阵乘法的 Mojo MAX 教程](https://docs.modular.com/max/tutorials/custom-ops-matmul)，称其为 *“可能是有史以来最好的教程”*。
   - 该教程展示了 Mojo 驱动 **MAX** 的能力，并被建议纳入官方文档。



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **IReRa 研究因仓库陈旧而停滞**：一位研究用于标签分类的 **Infer-Retrieve-Rank (IReRa)** 的成员在 [xmc.dspy GitHub 仓库](https://github.com/KarelDO/xmc.dspy) 上遇到了挑战，原因是该仓库依赖于一个特定的、已停止维护的 **DSPy** commit。
   - 该仓库可能需要 fork 并更新以适配 **DSPy** 的兼容性，而其他人建议 [**IReRa** 论文](https://arxiv.org/abs/2401.08424) 可能是更及时的资源。
- **Mistral 推出提示词优化方案**：Mistral 引入了一个用于提示词优化的 [cookbook notebook](https://github.com/mistralai/cookbook/blob/main/third_party/metagpt/prompt_optimization.ipynb)，并在 [相关视频](https://youtu.be/1I9PoXzvWcs?si=ZjMhZx19W7wNUSeY) 中进行了讲解。
   - 该 cookbook 详细介绍了一种特定的提示词优化方法，展示了当前正在进行的工作。
- **DSPy 上下文工程引发溢出**：一位在做关于 **使用 DSPy 进行上下文工程 (Context Engineering)** 演讲的成员在使用 **MiProV2** 进行微调时遇到了 *input context too long* 错误。
   - 即使设置了 **4k（和 6k）** token，减少 **max bootstrap demos** 和 **max labelled demos** 也没能解决问题。
- **Base64 挽救 DSPy 图像**：一位使用 **s3** 的成员在将 `dspy.Examples` 传递给他们的 `dspy` 程序之前，将图像转换成了 **base64**。
   - 这种转换允许该成员绕过兼容性问题，并使用 Amazon 的 S3 服务存储数据。



---

## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **LlamaIndex 和 Snowflake 在阿姆斯特丹的对决**：**LlamaIndex** 和 **Snowflake** 将于 7 月 31 日在阿姆斯特丹举办动手实践讲座，内容涉及构建处理真实企业数据的生产级 data agents，以及通过[此链接](https://t.co/r8oKh8O0eP)利用 document agents 处理复杂的文书工作。
   - 本次活动重点关注 data agents 在企业环境中的实际应用。
- **LeSearch：学术研究的新伙伴**：**LeSearch** 利用 **ReActAgent** 框架，通过**三个智能 agents** 解决学术研究的痛点。
   - 这些 agents 旨在处理单调的研究任务，通过 Multi-hop Question answering（多跳问答）等功能强调探索发现（[链接](https://t.co/TB9hru9Ke5)）。
- **NotebookLlama 展示新的可视化能力**：**NotebookLlama** 是一个由 **LlamaCloud** 驱动的开源 **NotebookLM** 替代方案，现在允许用户提取/下载图像和表格，并以交互方式可视化文件中的所有表格数据（[链接](https://t.co/Csl3HhMzYB)）。
   - 新功能增强了平台内的数据交互和可视化能力。
- **Cloudflare AI Gateway 与 LlamaIndex 紧密集成**：一名成员正在为 **Cloudflare AI Gateway** 开发 **LlamaIndex integration**，提供在 **OpenAI** 和 **Anthropic** 等多个 LLM 提供商之间的自动 fallback（回退）。
   - 详情可以在[这个 GitHub pull request](https://github.com/run-llama/llama_index/pull/19395)中找到，或通过 [lu.ma/aoc5opn4](https://lu.ma/aoc5opn4) 注册了解。
- **自动 LLM Fallback 确保服务不间断**：**Cloudflare AI Gateway** 集成促进了 LLM 提供商之间的自动 fallback，确保服务的持续可用性。
   - 当一个提供商面临停机或实施速率限制时，此功能尤为重要。



---



## [Nomic.ai (GPT4All)](https://discord.com/channels/1076964370942267462) Discord

- **建筑师寻找多模态模型**：一位用户正在寻找一种可以私有化部署的 **multi-modal model**（多模态模型），用于对**建筑平面图和图纸**提供反馈。
   - 到目前为止，唯一可行的选择似乎是 **Gemma 3**，虽然表现尚可，但反映出建筑设计反馈领域缺乏专门的解决方案。
- **Gemma 3 被评估用于建筑设计**：一位用户认为 **Gemma 3** 是唯一能某种程度上满足其对处理视觉输入并提供设计反馈的 **multi-modal model** 要求的模型。
   - 该用户的具体用例涉及分析建筑平面图，突显了对能够处理专业领域视觉数据的模型的需求。



---



## [Gorilla LLM (Berkeley Function Calling)](https://discord.com/channels/1111172801899012102) Discord

- **vllm 与 sglang 结果相当**：成员们认为 **vllm** 和 **sglang** 应该产生相当的结果，尽管没有链接具体的 benchmarks（基准测试）或场景。
   - 这意味着用户可以根据偏好或基础设施选择其中之一。
- **Llama 8B 矛盾地落后于 Llama 3B**：一位用户质疑为什么在某些排行榜上，**8B Llama model (FC)** 的排名低于其 **3B** 对应模型。
   - 这引发了关于模型性能与规模之间细微差别的讨论。
- **LLM 性能并不总是随规模线性增长**：一位成员澄清说，更大的模型规模并不自动转化为卓越的性能，并举了 *llama 4 scout 表现不如 llama 3.1 70B* 的例子。
   - 这突显了架构和训练数据在决定 LLM 有效性方面的重要性。



---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **PatternMatcher Lambdas 拟被移除**：一位用户建议从 **PatternMatcher 规则**中移除 **lambdas**，特别是在规则可以定义为 **UPat -> UPat** 的情况下。
   - 该用户主张在简单场景下避免**图灵完备性 (Turing completeness)**，以追求简洁和效率。
- **Egraphs 与 PatternMatcher 的比较**：一位用户将提议的 **PatternMatcher 规则**与 **egraph 重写规则**进行了类比，强调了它们在结构和操作上的相似性。
   - 该用户建议在可行的情况下尽量避免**图灵完备性**的实现，并强调了简洁性和效率的优势。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该服务器长时间保持沉默，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该服务器长时间保持沉默，请告知我们，我们将将其移除。

---

**Codeium (Windsurf) Discord** 没有新消息。如果该服务器长时间保持沉默，请告知我们，我们将将其移除。

---

**AI21 Labs (Jamba) Discord** 没有新消息。如果该服务器长时间保持沉默，请告知我们，我们将将其移除。

---

您收到此邮件是因为您通过我们的网站订阅了相关内容。

想要更改接收这些邮件的方式吗？
您可以从该列表中 [取消订阅](&#123;&#123;&#123;RESEND_UNSUBSCRIBE_URL&#125;&#125;&#125;)。

---

# Discord：按频道详细摘要和链接

### **Perplexity AI ▷ #[announcements](https://discord.com/channels/1047197230748151888/1047204950763122820/1392997210907607180)** (1 条消息): 

> `社交媒体公告` 

- **发现 Perplexity 社交媒体帖子**：发现了一条来自 Perplexity 的新[社交媒体帖子](https://x.com/perplexity_ai/status/1943437826307297480)。
   - 该帖子提到了 `@&1105626802732404746`，以及 `<:pplx_white:1222169347028422728>` 和 `<:grok:1344832909802213376>` 自定义表情符号。
- **发现 Discord 表情符号**：在社交媒体公告中发现了一些 Discord 表情符号。
   - 即 `<:pplx_white:1222169347028422728>` 和 `<:grok:1344832909802213376>` 表情符号。

---

### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1392943466136469627)** (1233 条消息 🔥🔥🔥): 

> `Kingfall 模型, Grok 4 性能, Comet 浏览器, O3 Pro, 下一个大事件` 

- **Kingfall 模型实际上可通过 API 访问**：虽然 "Kingfall" 模型在 AiStudio 中短暂出现，但根据[这条消息](https://discord.com/channels/1047197230748151888/1047649527299055688/1393169835285938176)，它在几天内可以通过 API 以 'Kingfall-AB-Test' 的名称访问。
   - 一些中国用户甚至开发了一个扩展程序，通过 AiStudio 访问 **Kingfall** 和其他神秘模型。
- **Grok 4 重复初始问题**：一位成员注意到 **Grok 4** 会重复初始问题，尤其是在对话开始时。
   - 另一位成员提到 **Grok 3 mini** 也有同样的情况。
- **Comet 浏览器缺乏邀请码和 Agent 能力**：用户讨论了 Comet 浏览器，指出其仅限 Max 用户和拥有邀请码的用户使用，导致一些人认为它是一个*过度炒作的浏览器*。
   - 他们批评非 Max 用户缺乏 **Agent** 能力，并强调了该产品**推广缓慢**且存在过度炒作。
- **You.com 添加 O3 Pro**：You.com 在其平台上添加了 **O3 Pro**，但用户在仅输入几次提示词后就遇到了速率限制，并对 UI 表示不满。
   - 一些人甚至报告说 You.com 的版本已经被**削弱 (nerfed)**。
- **Kimi K2 模型媲美 Opus**：根据[这条 X 帖子](https://x.com/Kimi_Moonshot/status/1943687594560332025)，**Kimi K2** 模型现已在 Kimi 网页版上线，击败了 **Opus 4** 和 **Gemini 2.5 Pro**。
   - 据称，这个拥有 32B 激活参数的模型在消耗更少算力的情况下，性能可与 **Opus** 媲美。

---

### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1393162783704743986)** (3 messages): 

> `Chevrolet, Blender addon, Management analysis` 


- **Chevrolet 受到关注比较**：一位成员发布了一个[链接](https://www.perplexity.ai/search/how-is-chevrolet-compared-to-b-CyKcfhsTTHOHZ2H.dSuttA)，指向一个将 **Chevrolet** 与其他汽车品牌进行比较的搜索结果。
   - 未提供关于该特定比较的进一步讨论或细节。
- **Blender Addon 创建请求**：一位成员通过此[链接](https://www.perplexity.ai/search/can-you-make-me-a-blender-addo-B6lZ6DyeSISJHhprZ55nyQ)请求创建一个 **Blender addon**。
   - 关于该 addon 所需的功能或用途没有更多细节。
- **管理分析请求**：一位成员发布了一个[链接](https://www.perplexity.ai/search/analyze-the-management-and-gov-pd.7FJM5QQ2WgYn19nZYHg?0=d#0)，要求分析**管理与治理 (management and governance)**。
   - 未提供进一步的背景或细节。


  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1392944165586735114)** (1 messages): 

> `Non-deterministic Models, Buggy Playground, API Reference` 


- **Non-Deterministic 模型引发 Playground Bug**：一位成员指出 Perplexity AI 的模型是 **non-deterministic** 的，这使得实现精确的输出复现变得困难。
   - 他们还一致认为当前的 Playground 存在一些 **Bug**。
- **API Reference 为有 Bug 的 Playground 提供避风港**：一位成员建议在团队调查 Bug 期间，使用 [API Reference playground](https://docs.perplexity.ai/api-reference/chat-completions-post) 作为替代方案。
   - 对于面临**原始 Playground 界面**问题的开发者来说，链接中的 Playground 是一个很好的权宜之计。


  

---


### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1392944435972542464)** (1108 messages🔥🔥🔥): 

> `Early Access APIs, Model with Tools vs No Tools, Grok 4 heavy on coding, Kimi K2 benchmarks, LLMs leaning on tools for logic/math stuff` 


- **Early Access API 看起来“可疑”**：成员们讨论了 **"early access" API** 的可疑性质，特别是当预期用户不明确时，并警告在没有充分披露的情况下，不要将**带有 Tools 的模型与不带 Tools 的模型**进行对比测试。
   - 一位用户澄清说，他们正在考虑将带有 Tools 的模型与其他带有 Tools 的模型进行对比，重点关注哪个模型能针对正确的查询使用正确的 Tool，并尽可能多地使用它们。
- **辩论爆发：Grok 4 真的是 SOTA 吗？**：关于 **Grok 4** 是否真的是 State-of-the-art (SOTA) 出现了质疑，一位成员在看到一段显示其数学和逻辑表现相当糟糕的[视频](https://www.youtube.com/watch?v=kxRlN0legRY)后，称其为“历史性的垃圾”。
   - 还有人对其在 Coding 任务上的表现表示担忧，特别是在 LMArena 上，一位用户指出，“Grok 4 真的很差”。
- **Kimi K2 凭借惊人的 Benchmark 表现抢占风头**：成员们对 **Kimi K2** 及其令人印象深刻的 Benchmark 分数感到兴奋，特别是作为一个非推理 (non-reasoning) 基础模型，它拥有 32B 激活参数和 128k 上下文窗口。
   - 一位用户指出 “Kimi K2 在 Livebench 领先”，这促使另一位用户回复 “Benchmaxxed ¯\_(ツ)_/¯”，随后出现了许多添加该模型的请求。
- **LMArena 编程环境需要改进**：几位成员一致认为 **LMArena** 需要更好的 Coding 环境，建议它至少应该能够执行代码。
   - 一位用户提到在使用 Codeblocks 时遇到浏览器冻结的问题，不得不使用 Userscript 将其转换为普通文本框来解决，“当它使用 Codeblocks 时会冻结我的整个浏览器，所以我不得不写一个 Userscript 把 Codeblocks 转换成普通文本框”。
- **AI Roleplay 与心理健康引发辩论**：成员们辩论了 **AI roleplay** 的伦理和潜在影响，特别是其与心理健康和社交互动的关系。
   - 一位用户表示担心，使用 AI 进行 Roleplay 的人群与患有心理健康障碍的人群之间“可能存在相当大的重叠”，而另一位用户则辩护称这种行为是一种逃避现实和沉浸的方式。


  

---

### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1392952524285284542)** (800 条消息🔥🔥🔥): 

> `MCP SuperAssistant, Grok 4, Gemini 3, NNC 架构, 金融 AI 审计` 


- **MCP SuperAssistant 注入聊天机器人功能**：一位用户分享了 [MCP SuperAssistant](https://drinkoblog.weebly.com/)，它将 **MCP 功能**注入到尚不支持该功能的聊天机器人 Web UI 中，从而能够直接分析事件查看器错误并改进聊天机器人功能。
   - 该用户表示*这简直太酷了*，通常他们不推荐浏览器扩展，但这个值得冒险。
- **Grok 4 在图灵机实现方面表现惊人**：一位用户注意到 **Grok 4** 可以实现**图灵机 (Turing machine)**，这是目前其他 LLM 都无法做到的，这表明 **AGI** 正在临近，尽管可能存在[政治偏见](https://grok.com/share/c2hhcmQtMg%3D%3D_763876fc-2c3c-413f-a456-643259cb6dc1)。
   - 一些用户对 Grok 4 持褒贬不一的看法，有人表示在实际使用中，看到 [Grok 4 关于好莱坞过度代表性的回答](https://x.com/i/status/1943449343182119278)后，觉得它表现平平。
- **Gemini 3 细节通过 CLI 源代码曝光**：关于 **Gemini 3** 的细节正在浮出水面，源代码中的字符串提到了该模型，正如在 Gemini CLI 源代码的 [Reddit 帖子](https://www.reddit.com/r/singularity/comments/1lwp9t5/meta_poaches_head_of_apples_ai_team_with_pay/)中看到的那样。
   - 一位用户开玩笑说 Gemini 的内心独白，强调了该模型倾向于做出*绅士般*的回应。
- **用户构建受物理启发的自定义神经网络架构**：一位用户正在构建一个受龙卷风和漩涡启发的[自定义神经网络](https://github.com/openai/openai-python)架构，将 Attention 层与特殊的记忆系统混合，并展示了包含 Kernel、Spatial Diffusion 和 Velocity Processing 的本地运行时日志。
   - 该用户使用标准的 Python 数据流水线进行训练，并表示目标是让 Vortex Cell 学习如何处理复杂的现实世界输入。
- **演示高级 PayPal API 金融审计**：一位用户通过使用 Postman 对 **PayPal API** 进行密集的自动化测试，展示了实时 **API 审计 (API Auditing)** 和对关键集成的彻底安全评估。
   - 该用户使用 Postman 对 PayPal API 进行了密集的自动化测试，重点测试了 GET /v1/reporting/transactions、POST /v1/oauth2/token 和 GET /v1/identity/oauth2/userinfo 等关键端点，以确保金融交易中的数据鲁棒性、完整性和机密性。


  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1393070035576033372)** (6 条消息): 

> `GPT-4o 模型降级, Custom GPT 限制, GPT-4o vs GPT-4o mini` 


- **GPT-4o 模型静默降级为 Mini**：截至 2025 年 7 月，免费 GPT-4o 用户在达到每日配额后会静默降级为 **GPT-4o mini**，这严重影响了上下文窗口 (Context Window) 和模型质量。
   - 缺乏手动切换和清晰的指示让用户感到沮丧，因为 **mini** 版本由于记忆力减弱和响应质量下降，显著降低了长期角色扮演的体验。
- **Custom GPT 面临记忆限制**：拥有 Plus 会员资格的 Custom GPT 虽然提供了一些优势，但仍无法无限期地跨线程保持全面的记忆，这使得它们不适合持久、详细的角色扮演场景。
   - 虽然上传摘要文件和提供清晰的指令有所帮助，用户在文件数量以及 GPT 处理大量非结构化文档的能力方面仍面临限制，因此需要简洁的摘要。
- **通过 Plus 版本绕过 GPT-4o 限制**：建议的一种方法是将对话分成较小的 **docx 文件（每个 80,000 字符）**，并按顺序提交它们，以便在新聊天中“重建”故事。
   - 使用 **Plus 版本**订阅访问 GPT 的 *Projects 标签页* 可能会提供更好的解决方案，尽管该功能的可用性仍然有限。


  

---

### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1392944512715854007)** (34 条消息🔥): 

> `GPT-4o-mini TPD Limit, Persona Features Control Emergent Misalignment, Exploring Consciousness in LLMs Survey, Human Personality Controls Behavior, LLM Output sentence formatting` 


- **GPT-4o-mini TPD Limit 受到关注**：一位成员询问了使用层级 3（usage tier 3）中 **GPT-4o-mini 模型**的 **TPD limit**。
   - 在提供的上下文中没有直接的即时回答。
- **推荐论文《Persona Features Control Emergent Misalignment》**：一位成员推荐阅读论文 *Persona Features Control Emergent Misalignment* 以获取见解。
   - 另一位成员支持该建议，指出理解 **persona features** 对于缓解语言模型中的 **misalignment** 具有相关性。
- **建议阅读《Exploring Consciousness in LLMs Survey》**：一位成员建议阅读论文 *Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks*。
   - 这是对之前阅读 *Persona Features Control Emergent Misalignment* 建议的补充。
- **LLM 输出格式过于简洁**：一位成员抱怨 **ChatGPT 4o** 模型过于简洁，即使在提示下也不写长句，还抱怨输出被切分成较小的段落，后面跟着巨大的空格。
   - 另一位用户随后分享了多张截图，展示了他们能够从**早期模型**中引导出**长段落**，并提供了关于 **prompting** 的指导以诱导这种行为。
- **LLM 需要精确的目标定义来生成特定输出**：一位成员分享了一个示例，证明 **LLM** 需要 **精确的指令** 和明确的目标来实现特定的、非标准的输出，强调了为模型澄清潜在困惑点的重要性。
   - 他们指出模型会随时间变化而影响输出，当期望的输出偏离典型行为时，通过精心设计的 prompt 引导模型至关重要。


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1392944512715854007)** (34 条消息🔥): 

> `GPT-4o Mini TPD Limit, Persona Features Control Emergent Misalignment, Exploring Consciousness in LLMs, Human Personality Controls Behavior, Writing long articles in ChatGPT` 


- **GPT-4o Mini 难以捉摸的 TPD Limit**：一位成员询问了使用层级 3 中 **GPT-4o mini 模型**的 **TPD limit**。
   - 讨论中未提供直接答案。
- **推荐 Persona Features 论文**：成员们推荐阅读论文 "[Persona Features Control Emergent Misalignment](https://arxiv.org/abs/2401.13081)" 以回应查询。
   - 另一个论文建议是 "[Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2311.16879)"。
- **使用 AI 创作长句：一个 Prompt Engineering 挑战**：一位用户对 **ChatGPT** 写作过于简洁表示沮丧，并询问如何让它在不分段的情况下写出更长的句子。
   - 一位成员建议*精确地告诉模型*期望什么样的输出，并提供了一个 Custom GPT prompt 示例，展示了如何实现极长的、冗长的句子。
- **模型行为：引导输出**：一位成员将提示 AI 比作开车，指出底层模型的变化（如“重新铺路”）即使使用相同的 prompt 也可能改变输出。
   - 建议的解决方法是*握住方向盘*，亲自调整 prompt，引导模型走向你想要的路径和输出。
- **为虚构世界指定输出**：一位成员建议尝试生成虚构文章的用户明确指定参数，如句子长度（**超过 100 个单词**）和首选风格。
   - 他们指出，强迫模型去猜测可能会导致不理想的结果，而仔细的措辞可以改善输出。


  

---

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1392946775412707388)** (476 条消息🔥🔥🔥): 

> `Unsloth 的多 GPU 支持、模型互联技术、Unsloth 与 LoRA 模型、Moonshot AI 的 Kimi 2 Instruct 模型、为 Bodo 语训练 AI` 


- **Unsloth 多 GPU 支持延迟**：Unsloth 官方的多 GPU 支持已推迟，但用户可以根据 [Unsloth 文档](https://docs.unsloth.ai/) 的建议，利用 **Accelerate** 作为临时解决方案。
   - 一位用户通过这个 [GitHub 仓库](https://github.com/thad0ctor/unsloth-5090-multiple) 中的补丁实现了多 GPU 支持，但遇到了 gradient checkpointing 问题。
- **探索模型协作**：一位成员询问如何让两个模型进行通信，设想由一个小模型将 *latent understanding* 传递给大模型进行 token 生成，且不使用 RAG 或 speculative decoding。
   - 他们建议一个模型可以执行原生 RAG 并将理解内容传递给更大的模型进行生成，从而创建一个互联系统。
- **Unsloth 模型加载 Bug 修复**：一位用户报告了 Unsloth 中 LoRA 模型加载的问题，指出当通过路径名加载 LoRA 模型时，优化可能无法生效，但这一点 [已被修正](https://github.com/unslothai/unsloth/blob/main/unsloth/models/loader.py)。
   - 他们还强调了 PEFT 不接受字符串（正则表达式仅限列表或元组）以及模型加载期间未传递 `trust_remote_code` 的 Bug，这些都可以通过 PR 轻松修复。
- **Moonshot AI 的 Kimi 2 引发关注**：[Moonshot AI](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 发布了 **Kimi 2 Instruct**，这是一个采用 MIT 许可证的 **1T 参数 MoE 模型**（32B 激活参数）。
   - 尽管其体量庞大，一位成员仍希望对其进行量化以在 4090 上运行，这引发了关于 NVIDIA B200 的讨论，以及在生产环境中几乎没人以 GGUF 格式运行这些模型。
- **众包印度语言的 AI 开发**：一位成员正在寻求帮助，希望使用 Unsloth 框架为印度阿萨姆邦使用的 **Bodo 语** 构建 AI 模型。
   - 社区向该成员推荐了现有资源，并建议利用微调模型（例如来自这个 [Hugging Face 仓库](https://huggingface.co/AyushSisodiya/BODOAI) 的模型），并强调要提出具体问题。


  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1392952661967376434)** (82 条消息🔥🔥): 

> `文本转语音 LLM、Grok 4、AGI 基准测试、AI 中的记忆、AI 中的推理` 


- **STT/LLM/TTS 流水线的潜力**：成员们讨论了 **Unmute**，这是一个使用 Kyutai 的 **Speech-to-Text** 和 **Text-to-Speech** 模型封装文本 LLM 的系统，用于低延迟语音交互，强调其流程为 *STT -> LLM -> TTS -> 循环*。
   - 共识似乎倾向于认为这种流水线是目前在多模态应用中获得最佳性能的“必经之路”。
- **Grok 4 的价格**：用户对 **Grok 4** 表现出兴趣，但指出它是仅随 **X Premium+** 提供的付费功能，限制了免费访问。
   - 一位用户调侃道“唯一免费的东西是死亡”，并分享了一张 [逃跑的蜗牛](https://commons.m.wikimedia.org/wiki/File:Escaping_snails_-_July_2024.jpg) 的幽默图片作为隐喻。
- **Elon 是否在炒作 Grok 4？**：一位用户分享了一篇 [Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/s/SEIMOp8PSv)，暗示 **Elon Musk** 只是一个“炒作者”，并指出 **Grok 3** 在 **Grok 4** 发布前性能提升的巧合，以及随后发现的 **Grok 4** 问题。
   - 该用户展示了一组需要记忆冷门事实（**Dela Grante**、**Ivy's slimes**、**Sonosakie**）的基准测试问题，据称 LLM 在这些问题上表现不佳，从而质疑围绕 **AGI** 的营销。
- **AGI 需要完美记忆吗？**：一位成员认为，实现 **AGI/ASI** 需要构建一个“完美记忆档案”，批评目前的聊天机器人如果没有它就只是“愚笨”的。
   - 其他人反驳说，一个好的聊天机器人应该利用互联网和工具来寻找正确答案，因为真正聪明的人不会背下所有东西，并建议在大型知识库上使用 **embedding 模型**。
- **人脑 vs AI 推理**：一位用户对基于 Agent 的 AI 模仿人脑表示怀疑，认为“推理 = 只是多说一些 CoT token”。
   - 反驳观点强调，AI 推理涉及根据获取的知识得出结论，尽管一位用户开玩笑地指出，“没人知道人脑是如何运作的”。


  

---

### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1392952097577766954)** (70 messages🔥🔥): 

> `Orpheus TTS issues, Multi-GPU ETA, Datasets version problems, Gradients checkpoints, Bodo Language Model` 


- ****Orpheus TTS 遭遇 `torchcodec` 困扰****：用户报告了 Orpheus_(3B)-TTS notebook 中的错误，具体是在 `ds_sample_rate = dataset[0]["audio"]["sampling_rate"]` 这一行出现了 `ImportError: To support decoding audio data, please install 'torchcodec'.` 错误。
   - 解决方案包括将 datasets 版本降级到 `datasets==3.4.1`，因为较新版本需要 `torchcodec` 以及比 Colab 提供的版本更高的 torch (2.7.1)。
- ****Multi-GPU 耐心逐渐耗尽****：用户仍在等待 Multi-GPU 支持，最初预计在 4 月发布，但主线程目前没有任何更新。
   - 截至最新消息，尚未提供明确的 ETA。
- ****本地 Jupyter 安装 Nemo 12B Notebook 的噩梦****：用户在本地 Jupyter 环境中尝试运行 Nemo 12B notebook 时遇到了 `Unexpected type of attr triton.multi_kernel, got bool should be int` 错误。
   - 建议为 Unsloth 创建一个独立的 Python 环境，并从该环境中启动 Jupyter，以避免与系统级 Python 安装冲突，详见 [此 Discord 讨论帖](https://discord.com/channels/1179035537009545276/1179035537529643040/1381978312640958515)。
- ****Base 还是 Instruct 模型？用户被“指教”了****：一位用户询问他们加载的是 Base 模型还是 Instruction 模型，随后被告知当前的设置加载的是 **Base 模型**。
   - 该用户承认他们一直以来都在进行训练，*“还以为它是一个 Instruct 模型”*。
- ****环境感知型 RL 工具****：一位用户正在探索如何在针对外部环境进行工具调用（tool calls）并生成完整回复后应用奖励函数（reward functions）。
   - 在尝试与外部环境交互以获取回复未果后，建议他们使用 Prompt、工具调用和答案的汇编来对数据集进行微调，并关注 [OpenPipe/ART](https://github.com/OpenPipe/ART)。


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1393003087341555713)** (56 messages🔥🔥): 

> `Reka AI's Quantization, Gemini Deep Research, AI OS Dev Study, Kimi-K2-Base, GPT 4.5` 


- **Reka AI 宣称实现近乎无损的量化**：[Reka AI](https://reka.ai/news/reka-quantization-technology) 声称其一种近乎无损的 **3.5bit 量化方法** 兼容 llamacpp，支持 **q3_k_m & q4_k_m** 格式，但量化过程需要计算资源。
   - 该方法使用的是 **LDLQ quants**，技术上属于 **IQ quants** 而非 Q quants，有人在思考是否能将此技术应用于 Qwen32b。
- **AI 使 OS 开发速度降低 19%**：一项研究 ([metr.org](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/)) 发现，使用 AI 工具的开发者比不使用的开发者多花 **19% 的时间**，这表明 AI 可能会阻碍思考和所有权意识。
   - 它抑制了思考，削弱了所有权意识。或许这并不令人意外？
- **Kimi-K2-Base 在非推理任务中达到 SOTA**：**Kimi-K2-Base** ([huggingface.co](https://huggingface.co/moonshotai/Kimi-K2-Base)) 被称为新一代 **SOTA 级非推理模型**，在知识、推理和代码任务中表现卓越，并针对 Agent 能力进行了优化。
   - 用户表示，无需创建账号和登录，*它的表现就非常非常强劲*。
- **关于 GPT 4.5 模型大小的传闻四起**：据 NVIDIA 透露，传闻 **GPT 4.5** 为 **12T A2T**，而 **GPT 4** 为 **1.76T A288B**，这表明模型规模有了显著增加。
   - 有说法称 **GPT-4.5** 可能也在 10tn MoE 左右，但由于无法直接进行推理，他们为了 **gpt-4.1** 对其进行了大幅缩减。
- **激活参数量引发辩论**：有人表示 *“我厌倦了人们总是说‘噢，但是激活参数量（active parameter count）要少得多’……如果它需要 1T 参数模型的资源，那么它就应该被直接称为具有 32B 速度的 1T 参数模型”*。
   - 其他人则表示他们正在制造极其庞大的模型。从数据来看，32B Dense 模型可以走得很远，升级到 200B 参数模型会更好，而 700B (DeepSeek) 级别的模型基本上已经快到头了，但仍存在微小差距。


  

---

### **Unsloth AI (Daniel Han) ▷ #[unsloth-bot](https://discord.com/channels/1179035537009545276/1390899684834410536/1392948433823993857)** (25 条消息🔥): 

> `Unsloth 在 Kaggle 上使用 2xT4，device_map = balanced，关闭 Discord 线程，Embedding 训练精度错误，SFTTrainer 与 CPT 的使用` 


- **Unsloth 可以在 Kaggle 的 2xT4 上运行！**：一位用户询问是否可以在 Kaggle 上使用 **2xT4** GPU 运行 **Unsloth**，另一位用户确认在最近的修复后这是可行的。
   - 他们建议使用 `device_map = "balanced"`。
- **以 Discord 方式关闭线程**：一位用户询问如何关闭线程，另一位用户回答：*右键点击线程并选择 "Leave Thread"* 🙂
- **训练中的 Embedding 精度错误！**：一位用户收到了 **AssertionError**：*Backwards requires embeddings to be bf16 or fp16。*
- **SFTTrainer vs Unsloth Trainer**：一位用户注意到 **Qwen3 notebooks** 使用 **SFTTrainer** 进行微调，而 **CPT notebooks** 使用 **Unsloth Trainer**，并询问了这一选择背后的原因。


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1392945796198039683)** (581 条消息🔥🔥🔥): 

> `Windows 上的 Linux 命令，X 上的 Cursor 推文，Auto Agent，新定价，Grok 4` 


- **Windows 上的 Linux 命令令人困扰**：一位成员抱怨 Cursor 尝试在 Windows 上使用类似 `thing && thing` 的 Linux 命令，另一位成员建议使用 WSL，并提供了一个 Markdown 片段，可添加到 `.cursorrules` 或 `CLAUDE.md` 中以指定 Shell 环境。
   - 另一位成员报告称，在更新 Powershell 版本后，`&&` 问题得到了解决，并指向了 [Powershell 7 .msi](https://learn.microsoft.com/en-us/powershell/scripting/install/installing-powershell-on-windows?view=powershell-7.5#msi)。
- **Cursor 的推文被马斯克阅读了！**：一位成员分享了 X 上的 [Cursor 推文](https://x.com/elonmusk/status/1943178423947661609?s=46&t=o_SaDh7rFwkvRFT3OsdjuA)，该推文得到了 Elon Musk 本人的回应。
   - 一些成员幽默地回应，其中一位说：*Loooooooool 难怪*。
- **对 Grok 4 评价褒贬不一**：成员们讨论了 **Grok 4** 的性能，指出响应时间好得多，但在执行任务期间仍会停止，一些人指出其高延迟，可与 o3-pro 相比。
   - 一些成员热切期待下个月的编程专用版本，另一些人则报告说它在编程方面有点糟糕，并指出 **Grok** 在数学和推理方面的优势，还有人提到 Elon 的一条帖子，称 Cursor 的提示词让 Grok 4 变得像被“切除了前额叶”一样。
- **Cursor 定价令用户困惑**：一些成员对新的定价模型感到困惑，特别是关于 **Auto mode** 和每月 20 美元的 API 额度，其他人确认 **Auto 的使用不计入使用限制**。
   - 一位成员表示，升级到 Pro 计划后，他们的 API 成本达到了 30 美元，不确定何时会被限流。
- **Agent 能力得到增强**：**Cursor v1.2.4** 显著增强了 Agent 的能力，特别是在待办事项队列管理、记忆（Memories）、性能和代码建议准确性等领域，而其他人则报告了 apply 工具的 bug。
   - 一些用户报告 Agent 经常产生幻觉并创建新系统，导致项目逻辑混乱，尽管 AI 直接挂钩项目的能力无与伦比，建议将每个文件限制在 500-750 行。


  

---


### **Cursor Community ▷ #[background-agents](https://discord.com/channels/1074847526655643750/1367213641027551352/1392965399166980146)** (18 条消息🔥): 

> `Cursor Github App 安装问题，禁用 Cursor 中的 Power Forwarding，远程工作区中的 Node 版本管理，防止自动端口转发` 


- **Cursor Github App 安装问题已解决！**：成员们报告了 **Cursor Github App 安装** 问题以及关于 **EJSON 解密** 的奇怪错误，这些问题后来都得到了解决。
   - 一位成员庆祝它 *再次恢复工作 😎*。
- **默认开启 Power Forwarding**：一位成员正在寻找默认禁用 Cursor 中 **power forwarding** 的方法，因为它劫持了他们的本地数据库。
   - 他们尝试在 `devcontainer.json` 中添加配置，但没有成功。
- **远程工作区中的 Node 版本**：一位成员询问在远程工作区中设置正确 **Node 版本** 的最佳实践，目前他们在“安装”环境脚本中使用 `nvm install`。
   - 他们正在寻找可能更好的方法。
- **Background Agent 端口劫持？**：一位成员询问如何在启动 background agent 时防止自动**端口转发**。
   - 他们报告说它已经多次劫持了他们的本地 Postgres 连接。


  

---

### **OpenRouter (Alex Atallah) ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1393312063014703155)** (2 条消息): 

> `Cypher Alpha, Kimi K2, Moonshot, Novita, Parasail` 


- **Cypher Alpha 演示期即将结束**：**Cypher Alpha** 的演示期将于 **东部时间 7 月 14 日星期一上午 11 点至中午 12 点** 之间结束。
   - 官方发布消息感谢用户为早期模型开发所做的贡献。
- **Moonshot 的 Kimi K2 在 OpenRouter 首次亮相**：由 **Moonshot** 开发的 **Kimi K2** 现已在 OpenRouter 上线，由美国的 **Novita** 和 **Parasail** 提供服务。
   - 该模型拥有 **1T 总参数**，在 SWE-Bench Verified 上的得分为 **65.8%**。根据[此公告](https://openrouter.ai/moonshotai/kimi-k2)，它在编程和工具使用方面的开源排行榜中名列前茅。


  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1392947040727597117)** (412 条消息🔥🔥🔥): 

> `Grok 3 mini endpoints, OpenRouter Credit Issues, Prompt Optimization, Image Token Double Counting, Grok 4 Rate Limits` 


- **Grok 3 Mini 端点混淆已澄清**：OpenRouter 中的 `grok-3-mini-beta` 和 `grok-3-mini-latest` 两个 Slugs 都指向同一个 `grok-3-mini` 模型。正如 XAI 文档所确认的，它们互为**别名 (aliases)**。
- **OpenRouter 解决图像 Token 多收费用问题**：OpenRouter 告知用户，由于一个 Bug 导致 4 月 3 日至 6 月 26 日期间的图像 Token 被重复计算，从而产生了超额费用。目前已向受影响的账户发放了 **Credits** 补偿，例如在一个报告案例中补偿了 [$713.80](https://openrouter.ai/settings/credits)。
   - 官方鼓励用户通过 support@openrouter.ai 联系支持团队，以获取受影响请求及具体计算细节的更多信息。
- **关于 Google Gemini 2.5 Pro 的辩论**：用户讨论了 Google 的免费层级是否降低了 **Gemini 2.5 Pro** 的性能，并注意到免费版服务的稳定性问题。
   - 讨论中提出了关于使用机器人账号滥用免费层级与支持 OpenRouter 等 API 提供商之间的公平性担忧，此外还有关于此话题的[偏激观点](https://discord.com/channels/1091220969173028894/1094454198688546826/1369549757797892137)。
- **Text Completion API 状态受到质疑**：用户报告了 OpenRouter 的文本补全 (Text Completion) 端点存在问题，部分提供商返回的错误显示 Prompt 格式为聊天补全 (Chat Completion) 格式。根据一些报告，*文本补全功能似乎自 5 月以来就已损坏*。
   - 一位用户要求澄清是否支持 **Text Completion**，如果不支持则要求退款。
- **OpenRouter 将引入付费版 Chutes 模型**：经工作人员确认，OpenRouter 计划在**下周**某个时间引入目前免费的付费版 Chutes 模型。同时，用户还呼吁 OpenRouter 添加 **Cerebras Qwen3 235b**。
   - 此外，还有关于 OpenRouter 更新旧的仅限免费 Chutes 模型以允许用户使用付费版 Chutes 的提问。


  

---


### **OpenRouter (Alex Atallah) ▷ #[new-models](https://discord.com/channels/1091220969173028894/1384650595981328475/1393246209199837265)** (5 条消息): 

> `Switchpoint Router, $/mtok Pricing` 


- **Switchpoint Router 问题**：一名成员询问了 [Switchpoint Router](https://github.com/switchpoint/router) 上固定的 **$/mtok 定价** 状态。
- **定价疑虑**：该成员对此表示困惑。


  

---

### **OpenRouter (Alex Atallah) ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1392973075712311336)** (11 messages🔥): 

> `Mistral 深度研究模型, Amazon & Anthropic AI 联盟, Microsoft & OpenAI 合作伙伴关系, Devstral Medium 定价, 翻译模型` 


- **Mistral 正在酝酿深度研究模型**：据报道，**Mistral** 本月正在开发一款深度研究模型，但目前尚无更多细节。
- **Amazon 寻求更深层次的 Anthropic 联盟**：据 [Financial Times 报道](https://on.ft.com/44UogLv)，**Amazon** 正在考虑进一步投资 **Anthropic**，以加强其 AI 合作伙伴关系。
- **Microsoft 与 OpenAI 暗中谋划**：**Microsoft** 和 **OpenAI** 再次*在私下里悄悄合作*以推进其伙伴关系。
   - 与此同时，一位用户宣称 [ultra.doan](https://ultra.doan) 拥有*他认为最好的品牌设计*，并配有一个类似 Minecraft 风格头像的 Logo。
- **翻译模型推荐**：一位成员寻求在英语、德语、法语和意大利语之间进行文本翻译的模型推荐，并指出 **Gemini 2.5 Pro** *通常表现不错，但并非总是如此*。
   - 他们指出，如果目标文本长度受限（例如：*生成的文本必须在 X 到 Y 个字符之间*），该模型会出现问题。
- **Devstral Medium 的定价受到质疑**：一位成员分享说 **Devstral Medium** 的价格仅为 **$0.032**，但另一位成员对输出定价表示困惑，质疑这是否是 LLM 响应的固定价格。
   - 该成员问道：*这里的输出定价是如何运作的？我对“输出”的含义感到困惑，因为如果 LLM 响应真的是固定价格，那么路由（routing）最初就没什么意义了*。


  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1392943516379775046)** (94 messages🔥🔥): 

> `Qwen3-4b 4bit, LM Studio 卡顿, Falcon H1 问题, LM Studio 自动运行, 混元 (Hunyuan) 故障排除` 


- **Qwen3-4b 在 4bit 下运行**：一位用户确认 **Qwen3-4b** 在 LM Studio 中可以以 4bit 运行，而另一位用户报告称遇到了模型卡顿（stuttering）的情况。
   - `<|lm_end|>` Token 表明模型正试图过早结束对话，这可能是由于 **GGUF** 文件中的 Jinja 模板不正确导致的。
- **Falcon-H1 进驻 LM Studio... 差一点就成功了**：一位用户报告了运行 **Falcon-H1** 的问题，有人指出 LM Studio 的运行时（runtime）可能略旧于引入 **Falcon-H1** 支持的合并版本。
   - 要检查确切的版本号，用户可以导航到运行时视图（**CTRL + Shift + R**）查看发布说明。
- **消除自启动：驯服 LM Studio 的后台行为**：用户讨论了如何防止 LM Studio 在启动时自动在后台运行。
   - 一种解决方案是在应用设置菜单（**CTRL + ,**）中禁用 *headless* 设置；另一种是在 Windows 任务管理器的“启动”选项卡中禁用 LM Studio。
- **混元 (Hunyuan) 动态：排查模型加载故障**：尽管拥有最新的运行时和足够的 VRAM，一位用户仍难以加载 **Hunyuan** 模型。
   - 另一位用户确认 **Hunyuan** 在版本 **0.3.18 Build 3**（beta 分支）和运行时 **v1.38.0** 下可以工作，并建议通过对比设置来确定问题所在。
- **工具调用：LM Studio 的 MCP 插件乐园**：用户询问了 LM Studio 中的工具调用支持，特别是针对 JavaScript 以外的编程语言。
   - 虽然只内置了两个 MCP 工具，但可以通过在本地安装并按照 [LM Studio 文档](https://lmstudio.ai/docs/app/plugins/mcp) 中的详细说明在 JSON 配置中进行配置来添加其他工具。


  

---

### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1392954433121554524)** (94 条消息🔥🔥): 

> `VRAM 重要性 vs 代数, 多 GPU 设置与 PSU 配置, CPU vs GPU 的 LLM 性能表现, DDR 代数影响, GDDR vs DDR` 


- **用户表示 VRAM 容量至上**：一位用户询问应该等待配备 **24GB GDDR7** 的 **RTX 5070 Ti Super**，还是升级到 **4090 (GDDR6X)** 或 **7900 XTX (GDDR6)**。另一位成员回复称，对于运行 LLM 而言，VRAM 容量通常比代数更重要，并表示 *一旦生成速度快于阅读速度，实际性能其实并不重要*。
- **由多个 PSU 驱动的多 GPU**：一位用户通过使用 [此技巧](https://link.to/psu-hack)（涉及 *短接主 ATX 连接器上的启动引脚*），利用 **1x 1000W** 和 **1x 650W** PSU 运行 **2x 3090** 和 **1x 3080 Ti**。
   - 另一位用户对此感到惊讶，评论说他 *从未想过使用多个 PSU*。
- **当模型适配 VRAM 时，CPU 瓶颈极小**：成员们讨论了如果大部分 LLM 工作负载都卸载到 GPU 的 VRAM 中，是否还需要高端 CPU。一位用户建议，为 LLM 准备的高端 CPU/RAM 配置可能是一个 *陷阱*。
   - 一位拥有 **5950x** 和 **128GB DDR4** 系统的用户表示，他们 *尽量把所有东西都挤进 24 GB VRAM (3090) 中，因为在 CPU 上运行太慢了*。
- **内存带宽限制了基于 CPU 的 LLM 性能**：一位用户指出，**CPU** 的设计旨在平衡延迟、带宽和容量，而 **GPU VRAM** 则全力投入带宽。
   - 他们解释说，即使是拥有 **12 通道** 的服务器 CPU，其带宽 (**460-500GB/s**) 也低于 **256-bit** 总线的 GPU。
- **AMD GPU 未被使用？检查 CUDA**：一位使用 **GTX 1050 mobile** 的用户在调用 GPU 时遇到困难。
   - 另一位用户建议检查 LM Studio 设置中是否安装了 *llama cuda engine*。


  

---


### **HuggingFace ▷ #[announcements](https://discord.com/channels/879548962464493619/897387888663232554/1393318565809229926)** (1 条消息): 

> `Gemma 3n, SmolLM3, responses.js, EoMT, Sentence Transformers v5` 


- ****Gemma 3n** 进入开源领域**：**Gemma 3n** 现已在开源生态系统中全面可用，详情见 [Hugging Face 博客文章](https://huggingface.co/blog/gemma3n)。
- ****SmolLM3**：微型、多语言、长上下文推理模型发布**：**SmolLM3** 是一款微型、多语言、长上下文推理模型，现已发布。详情见 [Hugging Face 博客文章](https://huggingface.co/blog/smollm3)，其创作者 [Loubna Ben Allal 在 X 上](https://x.com/LoubnaBenAllal1/status/1942614508549333211)也对此表示庆祝。
- **用于构建 Responses API 的新项目 **responses.js****：引入了一个新的 OSS 项目 **responses.js**，用于构建由 HF 推理提供商支持的 Responses API，详情见 [X 上的帖子](https://x.com/Wauplin/status/1941059510665458174)。
- **Transformers 迎来用于图像分割的 **EoMT** 模型**：Transformers 中新增了一个用于图像分割的模型 **EoMT** 以及其他更新，由 [Niels Rogge 在 X 上](https://x.com/NielsRogge/status/1940405978727629067)宣布。
- **利用 ML 优化核聚变反应堆**：一篇新的 [HuggingFace 博客文章](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge)详细介绍了使用 Machine Learning 进行仿星器（Stellarator）优化的方法。


  

---

### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1392988863353585734)** (94 条消息🔥🔥): 

> `Supergrok 访问权限, 量化模型推理速度, 推理提供商定价, Discord AI Agent 版主机器人, HF 账号注销` 


- **因 AI 安全论文请求 Grok 4 访问权限**：一位拥有 **Supergrok** 访问权限的成员被请求运行一些针对 AI 安全与对齐研究论文的提示词（prompts），以确认在其他模型中观察到的现象。
   - 该团队缺乏 **Grok 4** 访问权限，正寻求在与其研究相关的特定提示词上获得帮助。
- **量化模型推理速度慢的报告**：成员们讨论了量化模型有时比非量化模型推理速度更慢的原因，这主要是由于解压压缩数据和/或类型转换（overcasting）带来的开销。
   - 一位成员推荐了 [torch-profiling-tutorial](https://github.com/Quentin-Anthony/torch-profiling-tutorial) 以排查具体原因。
- **推理提供商成本困惑已澄清**：一位成员询问了关于 **Nvidia L4** 等推理提供商的定价和计费模式，表达了对意外费用的困惑，随后通过 [Inference Endpoints 定价文档](https://huggingface.co/docs/inference-endpoints/pricing) 链接得到了解答。
   - 建议指出，在 Inference Endpoints 上使用 *暂停功能（pause function）* 对成本管理至关重要。
- **AI Agent 版主机器人寻求图像支持**：一位成员正在使用 LLM 技术开发一个 **Discord AI 版主机器人**，并寻求关于添加 **NSFW 内容检测** 图像支持的指导。
   - 他们报告了 **Gemma 3 4b** 在 4060 GPU 上运行缓慢的问题，询问了硬件需求，并分享了代码供审查。
- **HF 账号注销事件调查**：一位用户报告其 **HF 账号被注销**，导致无法登录和访问 Spaces，并寻求协助解决。
   - 另一位成员提出协助调查情况，并索要其 HF 用户名以核查发生了什么。


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1393096505941950605)** (6 条消息): 

> `2DOF 机械臂模拟反馈, ModelNet40 准确率, Codaco App 发布, Legml-1, Python 后端模板` 


- **2DOF 机械臂模拟寻求反馈**：一位成员正在为其 [Interactive 2DOF Arm Simulator](https://huggingface.co/spaces/gokul00060/Interactive-2DOF-Arm-Sim) 项目寻求反馈。
- **ModelNet40 准确率达到 96%**：一位成员通过 Gaussian splatting 方法，在 **16-shot 训练** 下使 **ModelNet40 测试集** 的准确率达到了 **96%**。
   - 该项目的 GitHub 仓库地址见 [此处](https://github.com/genji970/multimodal-pointclip-Gaussian-Splatting-on-ModelNet40)。
- **使用 Codaco App 收集 AI 数据**：一位成员宣布发布 [Codaco](https://codaco.app)，这是一个免费的 App，可通过 iOS 和 Android 的数据活动来收集、标注和验证 **AI 训练数据**。
   - 该平台促进了社区驱动的数据收集，允许用户捕获图像、视频、音频和文本数据，并贡献标注。
- **法语模型现在真的很棒**：一位成员推广了名为 [Legml-1](https://huggingface.co/spaces/Tonic/Legml-1) 的新法语模型。
- **通过 Python 后端模板简化协作**：一位成员为黑客松创建了一个 Python 后端模板，强调单元测试、**100% 测试覆盖率**以及极简 **CI**，以确保 **FastAPI** 应用程序通过 [GitHub](https://github.com/louisbrulenaudet/hackathon-backend) 正确运行。
   - 他们强调，在紧急情况下，*两个主要的对手是分支冲突和部署问题*。


  

---

### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1392979069506949282)** (7 messages): 

> `AI Agent 初始化, HF 课程证书, 图像/音频工具, Agents 课程结构, 单字回答的 Prompt` 


- **AI Agent 初始化详解**：一位成员澄清了 AI Agent 通常由用户 Prompt 初始化，Agent 解释该 Prompt 以执行操作，这使得 AI 与自动化软件有所区别。
   - 他们指出，这种在没有 AI 的情况下解释人类语言的能力是一个关键的区别点。
- **关于 HF 课程证书的咨询**：一位成员询问如何获得 AI Agent 课程的证书。
   - 在给定的消息中，没有明确的解决方案或指向证书获取流程的链接。
- **寻求图像和音频文件工具**：一位成员询问其他人正在使用哪些工具处理图像和音频文件。
   - 在提供的消息中没有推荐具体的工具。
- **关于 Agents 课程结构的澄清**：一位成员询问 Agents 课程是否完全是随读式的，以及是否有任何视频课程。
   - 在给定的消息中没有对课程结构的确认或否定。
- **寻求单字回答的 Prompt**：一位成员请求关于如何让 Assistant 节点仅给出单字回答的 Prompt 建议。
   - 该成员注意到，即使是他们的 Agent 也没有遵循这些指令，这表明强制执行此约束可能存在挑战。


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1392957055609868318)** (11 messages🔥): 

> `Tensor Layout 可视化, CUDA & GPU 编程书籍, Meetup 广告` 


- **Tensor Layouts 激发可视化探索**：一位成员正在寻求电子化可视化 **Tensor Layouts** 的建议，希望比使用坐标纸或 DrawIO 效果更好。
   - 他们正在寻找比现有选项“更美观一点”的东西。
- **寻求 CUDA & GPU 编程书籍推荐**：一位成员请求针对具有 C++ 经验的人推荐现代 **CUDA & GPU 编程**书籍。
   - 理想情况下，书籍应该涉及现代 **DL** 主题，但这不是必需的。
- **GPU Server 庆祝活动成功！**：一位成员对最近的活动表示感谢，指出活动非常有益，但也导致了高强度的夜间工作/学习和调试。
   - 他们分享说，他们的团队和合作伙伴仍然痴迷于“这些 Kernel 及其所有该死的 Bug”。
- **Meetup 广告引发频道查询**：一位成员分享了一个 [Meetup 链接](https://lu.ma/aoc5opn4)。
   - 另一位成员请求将该帖子移至指定的 **Meetup 频道**。


  

---


### **GPU MODE ▷ #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1393345674363277332)** (1 messages): 

> `Triton Kernel Padding, 序列长度优化, Triton 中的内存管理` 


- **对 Triton Kernel Padding 的困扰**：一位成员表达了对 **Triton Kernel** 在输入序列长度不是 **128** 的倍数时速度变慢的不满。
   - 他们正在寻求关于在 Kernel 内部进行 Padding 和 Slicing 的建议，以避免手动 Padding 的内存开销。
- **Kernel Padding 优化**：用户希望在 **Triton Kernel** 内部进行 Padding 和 Slicing 以避免内存成本。
   - 手动 Padding 是可行的，但内存占用很大，因此他们正在寻找 Kernel 内部的替代方案。


  

---

### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1393075791071285349)** (9 messages🔥): 

> `Nsight Compute Debugging, NCCL Hangs with cudaMemcpy, GEMM Kernel Optimization on H100` 


- **Nsight Compute 辅助调试工作流**：一位成员使用 **Nsight Compute** 捕获了代码的原始版本和修改版本，成功解决了他们的调试工作流需求。
   - 该工具在他们面临最初的挑战后，帮助他们实现了预期的结果。
- **NCCL 在 P2P cudaMemcpy 实现中挂起**：一位成员在将 NCCL 的 send/recv 操作替换为自定义的基于 **cudaMemcpy 的 P2P** 实现（旨在减少 SM 资源消耗）后遇到了训练挂起，即使设置了 `NCCL_P2P_USE_CUDA_MEMCPY=1` 也会发生。
   - 他们的猜测是前向回调（forward callback）与反向计算依赖之间可能存在死锁，其中前向计算、send、recv 和反向计算是在不同的 stream 上异步启动的。
- **H100 上的 GEMM Kernel 优化启动**：一位成员正在迭代优化 **H100** 上的 **GEMM kernels**，目标是超越 cuBLAS 的性能，并在 [LinkedIn](https://www.linkedin.com/posts/hamzaelshafie_kernel-4-1d-block-tiling-activity-7349412105692872705-gt5t) 上发布了包含性能结果和 profiling 见解的更新。
   - 他们正在寻求支持和反馈，邀请他人指出错误或分享建议。
- **最小复现代码（Minimal Reproducer）疑似存在面条代码**：一位成员分享了一个最小复现代码，其中包含一个状态机的面条代码实现，这可能会导致挂起。
   - 另一位成员询问该最小复现代码在*任何*情况下是否能运行，即*它是否真的是一个最小复现代码*。


  

---


### **GPU MODE ▷ #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1393260298898636921)** (15 messages🔥): 

> `Mapping Kernels, torch_compile_debug, AOT Graphs, Memory Usage, Activation Checkpointing` 


- **Kernel 映射探索启动**：一位成员询问如何将反向传播过程中的 kernels 映射到原始计算图，并进一步说明背景是使用 `torch_compile_debug=1` 编译的。
   - 另一位成员建议现有的来源追踪（provenance tracking）可能已经足够，并询问通过运行 `TORCH_LOGS="+aot_graphs"` 提供的日志是否有帮助，因为该成员的目标是通过精准插入 custom ops 来优化反向传播中的 kernels。
- **内存占用攀升至 100GB**：一位成员报告在为某种 XAI 方法计算梯度时使用了 **100GB 的 CPU 内存**，并询问如何使用 Torch 将反向传播拆分到多个 GPU 上。
   - 一位成员建议内存问题可能源于激活内存（activation memory），推荐使用 **激活检查点（activation checkpointing）/卸载（offloading）**，并链接了一篇关于理解 GPU 内存的 [PyTorch 博客文章](https://pytorch.org/blog/understanding-gpu-memory-1/)。
- **提出并行范式**：针对关于跨多个 GPU 拆分反向传播的问题，一位成员建议使用 [DistributedDataParallel (DDP)](https://docs.pytorch.org/tutorials/beginner/ddp_series_theory.html)。
   - 原提问者澄清他们一次只使用一个样本，随后被建议使用 **zero/fsdp** 在多个 GPU 上分片梯度，并建议使用重计算（checkpointing）以便不需要存储所有激活值。


  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1393001906388992062)** (2 messages): 

> `Nvidia Development Tools, Loop Tiling Optimization, Memory Access Parallelism` 


- **Nvidia 工具助力循环分块（Loop Tiling）**：一位成员引用了 [Nvidia 开发工具](https://developer.nvidia.com/nvidia-development-tools-solutions-err_nvgpuctrperm-permission-issue-performance-counters) 来帮助理解循环分块。
   - 循环分块旨在为 block 中的多个线程对内存访问进行分组，但该成员质疑在并行处理的情况下由此产生的加速效果，询问*加速是否是由于内存银行（memory bank）序列化造成的*。
- **并行内存访问困惑**：该成员对 **循环分块的加速效果** 表示困惑，特别是考虑到内存访问的并行性。
   - 他们想知道内存银行中访问的**序列化（serialization）**是否是性能提升的原因。


  

---

### **GPU MODE ▷ #[irl-meetup](https://discord.com/channels/1189498204333543425/1218444432588800010/1393356037456662690)** (1 条消息): 

> `AI Conference, San Francisco, September 17-18, Networking Opportunities, AI Trends` 


- **旧金山 AI 大会定档**：成员们正在询问关于 **9 月 17-18 日**在**旧金山**举行的 **AI Conference** ([aiconference.com](https://aiconference.com/)) 的参会情况。
   - 该会议提供了潜在的 **Networking 机会**以及对最新 **AI 趋势**的见解。
- **潜在的湾区 AI 聚会**：发起了关于在**旧金山 AI 大会**期间举办聚会的讨论。
   - 与会者正在探索建立联系并讨论会议收获的机会。


  

---


### **GPU MODE ▷ #[rocm](https://discord.com/channels/1189498204333543425/1233704710389764236/1392998064427634739)** (4 条消息): 

> `AMD bank conflicts, NVIDIA bank conflicts, L1 cache performance` 


- **AMD 与 NVIDIA Bank Conflict 定义对比**：一位成员对 **AMD** 中 "Bank Conflict" 一词的使用提出了疑问，并指出在 **NVIDIA** 中，只有当事务未能充分利用共享内存（Shared Memory）带宽时才被称为 Conflict。
   - 具体而言，NVIDIA 的定义要求在*任何事务期间有任何 Bank 处于空闲状态*，才会被记录为 Conflict。
- **优化表现不佳的 L1 Cache 命中率**：一位成员询问了关于解决 Kernel 中 **L1 Cache** 命中率过低的高层级建议，该 Kernel 已经使用了带有偏移量的 `buffer_load_dword4` 和合并加载（Coalesced Loads）。
   - 另一位成员回应称，如果数据是流式传输或手动缓存在共享内存中且不再多次访问，那么低缓存命中率可能并不意味着低效，并补充说*高效使用 `buffer_load_dwordx4` 并不意味着会有良好的缓存命中率*。


  

---


### **GPU MODE ▷ #[liger-kernel](https://discord.com/channels/1189498204333543425/1275130785933951039/1392984013597310996)** (3 条消息): 

> `Prof. Dao's new project, Liger performance, RMSNorm bandwidth optimization, Softmax optimization` 


- **Dao 教授启动新项目**：Dao 教授的实验室启动了一个新项目，详情见[此 X 帖子](https://x.com/WentaoGuo7/status/1943369417078821279)。
- **Liger 仍有改进空间**：与 **Liger** 相比，*Softmax* 的表现尚可，但其他领域显示出增强的潜力。
   - 一位成员准备专门针对长序列探索优化 **RMSNorm 带宽**和 **Softmax**。


  

---


### **GPU MODE ▷ #[self-promotion](https://discord.com/channels/1189498204333543425/1288557096404516945/1393132537714311198)** (2 条消息): 

> `GPU Optimization, GPU Trading, AI Compute Infrastructure, Thunder Compute's VS Code Extension` 


- **探索 GPU 未来的小型黑客松**：一场为期 **48 小时的黑客松**将在**德国一座拥有 700 年历史的城堡**中举行，旨在探索 **GPU 优化**、**GPU 交易**和 **AI 计算基础设施**的未来。
   - **仅有 5 个名额**，涵盖机票、餐饮和住宿 —— [更多详情见 CastleCompute.com](https://castlecompute.com)。
- **Thunder Compute 的 VS Code 扩展程序介绍**：为那些不喜欢 **SSH 配置**并青睐**廉价 GPU** 的用户推荐 **Thunder Compute 的 VS Code 扩展程序**。
   - 该扩展旨在简化 GPU 的使用，可在 [ThunderCompute.com/docs/quickstart](https://www.thundercompute.com/docs/quickstart) 找到。


  

---


### **GPU MODE ▷ #[🍿](https://discord.com/channels/1189498204333543425/1298372518293274644/1393264858408947782)** (1 条消息): 

> `LLM Kernel optimization, Fine tuning LLMs` 


- **LLM 提议 Kernel 优化方案**：一位阅读了[这篇博客文章](https://scalingintelligence.stanford.edu/blogs/fastkernels/)的成员发现，使用 **LLM** 来提议 Kernel 优化策略的想法很有前景。
   - 他们建议，在特定领域的数据（Kernel 优化资源、博客文章、NVIDIA 论坛等）上对 **LLM** 进行微调（Fine-tuning）可以进一步提升性能。
- **针对 Kernel 工作微调 LLM**：用户建议，如果先在特定领域的数据上进行微调，LLM 的表现可能会更好。
   - 他们提供了**从网络爬取的关于 Kernel 优化、博客文章和 NVIDIA 论坛等数据**作为示例。


  

---

### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1393303704664408186)** (1 条消息): 

> `Float32 matrix transpose, tile op, transpose_sep, ThunderKittens` 


- **新手 Float32 转置任务开始**：ThunderKittens 的一名新成员正尝试实现 **Float32 矩阵转置**，并将其作为学习该框架的练习。
   - 他们尝试使用 `transpose_sep` 函数编写此 kernel，但由于 `transpose_sep` [仅支持 bf16](https://github.com/HazyResearch/ThunderKittens/blob/545bdcc55b49dfa9989a1ed63e3574dba706a27f/include/ops/warp/register/tile/conversions.cuh#L13-L201)，导致抛出类型不兼容的错误。
- **Float32 转置需要新的 tile op**：由于库中缺乏现有支持，该新成员可能需要为转置 **Float32** tiles 编写自己的 **tile op**。
   - 他们发现 [ThunderKittens 中的转置函数](https://github.com/HazyResearch/ThunderKittens/blob/545bdcc55b49dfa9989a1ed63e3574dba706a27f/include/ops/warp/register/tile/conversions.cuh#L13-L201) 仅支持 `bf16`。


  

---


### **GPU MODE ▷ #[submissions](https://discord.com/channels/1189498204333543425/1343002583001726986/1393225709325123634)** (4 条消息): 

> `H100 speed, B200 speed, MI300 speed, trimul leaderboard` 


- **H100 trimul 速度报告**：一名成员报告了 `trimul` 排行榜上 **H100** 的速度为 **45.1 ms**。
   - 随后另一名成员提交了 **6.56 ms** 的获胜成绩，之后又在 **H100** 上提交了 **6.58 ms** 的 `trimul` 成绩。
- **B200 trimul 速度达到 26.4 ms**：一名成员向 `trimul` 排行榜提交的成绩显示，**B200** 的速度为 **26.4 ms**。
   - 这可能反映了 **H100** 和 **B200** 架构在这一特定任务上的相对性能。
- **AMD MI300 在 amd-fp8-mm 中位列第 8**：一名成员的提交显示，**MI300** 在 `amd-fp8-mm` 排行榜上以 **151 µs** 的成绩位列 **第 8 名**。
   - 这表明 **AMD MI300** 硬件在 **FP8 矩阵乘法** 方面具有竞争力。


  

---


### **GPU MODE ▷ #[factorio-learning-env](https://discord.com/channels/1189498204333543425/1354169122107293786/1393064893464313917)** (39 条消息🔥): 

> `v3 Release, OpenAI Credits, Task Stopping, Meeting` 


- **Alpha-Factorio V3 版本正在开发中**：一名成员建议将 *let’s-make* 重命名为 [V3 发布页面](https://cdn.discordapp.com/attachments/1354169122107293786/1393098936964878346/Screenshot_2025-07-11_at_08.17.29.png)，标志着 *Alpha-Factorio* 项目的进展和更新。
   - 另一名成员表示同意，并提出转让该页面的所有权。
- **创业基金资助 OpenAI 额度**：一名成员提到他们的 **OpenAI credits** *刚刚到账*，另一名用户询问了收到的 **5k 额度**。
   - 该成员澄清说，这些额度来自他们参加的 **OpenAI startup fund** 项目。
- **任务停止标准讨论**：一名成员讨论了判定 **Agent** *失败* 的 *停止标准*，指出如果 **Agent** 未能成功，它将运行直到达到 **max steps**。
   - 注意到 **trajectory amount** 被假定为 128。
- **会议安排混乱**：在另一名用户提到无法在日历上找到会议后，一名用户分享了会议 [链接](https://meet.google.com/jhn-bkny-yfo?authuser=0&pli=1)，随后又发了一个 [新会议链接](https://meet.google.com/ppc-ehbz-pcz)。
   - 该用户表示，作为组织者却看不到日历邀请非常奇怪。


  

---

### **GPU MODE ▷ #[cutlass](https://discord.com/channels/1189498204333543425/1362196854460383353/1393067509002928188)** (14 messages🔥): 

> `CuteDSL Limitations, Dynamic Values in CuteDSL, Tensor Allocation in CuteDSL, tensor core performance` 


- **CuteDSL 的局限性在某种程度上是根本性的**：CuteDSL 中的某些限制在技术上是可以解决的，但需要付出代价，例如转向用于早期退出（early exits）的非结构化控制流，这会增加编译器分析的复杂性并损害性能。
   - 出于复杂性和性能方面的考虑，团队不太可能支持 Python 风格的动态行为（即在运行时确定类型）。
- **动态值影响 CuteDSL 中的元编程**：从动态条件（如 `if` 语句）产生的值本身会变成动态值，这意味着元编程将无法处理这些值，且它们在编译时被视为未知。
   - 动态值意味着依赖于 `const_expr` 的编译时优化将不再适用，并且禁止对这些值进行元编程。
- **CuteDSL 中的静态值**：在 CuteDSL 中，被赋予常量值的变量（例如 `a = 5`）是一个 `const_expr`，但目前尚不完全支持从 `@cute.jit` 函数返回动态值，不过已计划进行修复。
   - 虽然从 `@cute.jit` 函数返回静态值似乎可行，但这并非早期版本的预期行为。
- **Tensor Core 效率注意事项**：如果问题规模的粒度过大，Tensor Core 的性能可能会变差。
   - 在极端情况下，根据问题规模的大小，Tensor Core 的效率可能仅为 1/128。
- **CuteDSL 中本地 Tensor 的分配**：用户发现 `cute.full` 是分配可进行累加的本地 Tensor 的有效方法。
   - 用户最初想分配一个本地 Tensor 进行累加，但最终找到了无需使用共享内存分配器（shared memory allocator）的解决方案。


  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1392979452614676644)** (79 messages🔥🔥): 

> `Grok-4 reasoning and knowledge, Self-play during training, Deep-Hermes distillation to 14B, Brain Algorithms vs AI Algorithms, Qwen-14B` 


- **Grok-4 引发 AI 推理复兴**：成员们对 **Grok-4** 的推理和知识获取能力印象深刻，并根据 [此 Grok 分享链接](https://grok.com/share/bGVnYWN5_e0e6ab66-522c-48c8-9543-bef68c7a749d) 指出其在网页搜索和资料收集的详尽程度方面表现卓越。
   - 一位成员提到：“仅凭这一点就能帮助加速进程，更不用说它更先进的推理和问题解决能力了。”
- **Deep-Hermes 蒸馏愿景：从 671B 到 14B**：一位成员建议 NousResearch 和 Arceee-AI 合作，将 **Deep-Hermes-4 671B** 蒸馏为 **14B** 模型，类似于 Qwen-235B 到 Mistral-12B 的蒸馏。
   - 该建议被认为在初始模型完成后具有实现的可能性。
- **Deep-Hermes 探索混合推理与 Self-play**：成员询问了通过 Self-play 增强 **Deep-Hermes** 推理能力的潜力，以及混合推理方法的价值。
   - 讨论了“开启默认推理，通过预填空的 think 标签禁用推理，且不发送之前的思考轨迹”的方法。
- **人脑算法奥秘映射 AI**：一场关于 AI 算法与人脑算法对比的讨论展开，引用了**预测编码（predictive coding）**和**贝叶斯推理（Bayesian inference）**等平行理论，并附带了 [Grok 示例链接](https://grok.com/share/bGVnYWN5_9a4ae448-2f73-4383-88f9-56415ccd0ecd)。
   - 虽然对于大脑是否执行反向传播（backpropagation）存在分歧，但有人链接了一篇关于“Replay as a Basis for Backpropagation Through Time”的[论文](https://direct.mit.edu/neco/article/37/3/403/127434/Replay-as-a-Basis-for-Backpropagation-Through-Time)。
- **呼吁对数据集污染采取零容忍立场**：成员们辩论了数据集“伪污染”的定义，一些人主张即使是对看似无害的形式也要采取**零容忍**态度。
   - 建议向 **HuggingFace** 举报污染者及其仓库，以防止恶意行为者毒害数据池。


  

---

### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1393086166256849007)** (17 messages🔥): 

> `Temp = 0 的多样性，避免死循环，HIPAA 合规性，Kaida 和 Storywriter 仓库，litellm 的差异` 


- **Temperature 为零时依然存在多样性**：尽管较低的 Temperature 通常会导致可预测的输出，但一位成员指出 **R1** 在 **temp=0** 时仍表现出很多多样性，这可能是由于不同的 seeds 导致的。
   - 据观察，一些 **LLM 推理引擎已知是非确定性的**，至少 *exllamav2* 是这种情况。
- **创意写作死循环规避策略**：成员们讨论了在使用 **AI 进行创意写作**时避免**死循环 (doom loops)**和**重复性**的技巧。
   - 目标是在 **3-4 段之外保持连贯性**，并在多次提示后生成新内容，而不会变得过度引用或荒谬。
- **API 平台追求 HIPAA 合规性**：一位成员询问了该 **API 平台的 HIPAA 合规性**，以便在潜在项目中使用。
   - 作为回应，一位代表提到他们很快可以讨论合规的 endpoint，但**目前尚未提供**。
- **Kaida 和 Storywriter 仓库来帮忙**：针对一个关于创意写作的问题，建议查看其 **GitHub** 上的 **Kaida** 和 **Storywriter 仓库**。
   - 该用户补充说，他们将尝试将模型放入 **Docker** 中。
- **litellm 仓库 Fork 版本检查**：一位成员注意到了 **GitHub** 上 **litellm 仓库**的一个克隆版本，并询问了他们的版本与官方版本之间的任何差异。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

superbear12: https://arxiv.org/abs/2507.02778
  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1392974922904633427)** (1 messages): 

> `Liquid Foundation Models v2，生成式 AI 模型` 


- **Liquid AI 发布 Foundation Models V2**：Liquid AI 发布了他们的第二个生成式 AI 模型系列，称为 [Liquid Foundation Models v2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models)。
- **Liquid Foundation Models V2 面世！**：Liquid AI 发布了 [Liquid Foundation Models v2](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models)，这是他们的第二个生成式 AI 模型系列。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

superbear12: https://arxiv.org/abs/2507.02778
  

---


### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1392943755501240402)** (77 messages🔥🔥): 

> `LLM 的消亡，可解释网络，能源消耗，资本主义市场动态，人脸识别研究` 


- **LLM 的消亡与可解释网络**：一位成员表达了希望 **LLM 消亡**的愿望，转而支持*能从少量数据样本中学习的、更具可解释性的网络*。
   - 他们建议*回到原点*，专注于更好的损失函数和 backprop 的替代方案。
- **AI 创新 vs 能源消耗**：成员们讨论了通过大型数据中心和微型核反应堆来**扩展 AI** 的可持续性。
   - 有人建议，限制能源使用的法规可能会推动进一步的创新和可解释性，并将其与**比特币挖矿监管**类比。
- **AGI 中的资本主义**：一位成员对资本主义市场中的 **AGI 开发**表示担忧，认为这可能导致剥削和智能的幂律分布。
   - 其他人辩论了在资源稀缺和**人道资源分配**的背景下，政府的角色、监管以及治理的定义。
- **人脸识别研究监管**：成员们讨论了目前对**人脸识别研究**的监管，特别是在英国和欧盟。
   - 一位成员提到，由于伦理担忧，他们的工作单位已经停止了与人脸相关的研究。
- **训练工业级 Agent**：一位成员发布了一篇关于在训练工业级 Agent 的背景下，**优秀的世界模型 vs 优秀的预测**的[有趣论文](https://arxiv.org/abs/2507.06952)。
   - 他们强调了对人类双手的灵巧行为进行廉价、可扩展训练的潜力，即使推文中的演示可能是*彻头彻尾的胡扯*。


  

---

### **Yannick Kilcher ▷ #[paper-discussion](https://discord.com/channels/714501525455634453/1045297868136779846/1392962111369510984)** (4 条消息): 

> `EnergyMatching 实现，EnergyMatching 论文讨论` 


- **EnergyMatching 实现：深入挖掘**：成员们决定重新审视 [EnergyMatching](https://github.com/m1balcerak/EnergyMatching/) 的实现，该实现基于论文 ["Energy Matching for Score-Based Generative Modeling"](https://arxiv.org/abs/2504.10612)。
   - 一位成员提到，他们在**代码和公式**上投入了更多时间，并认为终于理解了论文的核心要点。
- **EnergyMatching 论文：二次审阅带来清晰理解**：在对代码和公式进行第二次更深入的审查后，一位成员表达了对 [Energy Matching 论文](https://arxiv.org/abs/2504.10612) 的全新理解。
   - 该成员感谢了其他人提供的与该论文相关的演示。


  

---


### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1393019860677693661)** (18 条消息🔥): 

> `半机械蜂 (Cyborg Bees)，Mistral 的渐进式改进与许可，BrowserOS，METR 的 AI 评估，Kimi-K2-Instruct` 


- **中国科学家创造半机械蜂**：据 [SCMP](https://www.scmp.com/news/china/science/article/3317614/chinese-scientists-create-first-cyborg-bee-worlds-lightest-brain-controller) 报道，中国科学家为**半机械蜂 (cyborg bees)** 开发了*世界上最轻的大脑控制器*，引发了关于未来应用（如《黑镜》中的机器人狗）的讨论。
- **Mistral 的渐进式改进方法受到批评**：成员们对 **Mistral** 小幅渐进式改进的方法感到*失望*，并对其在最近发布 [Devstral-2507](https://mistral.ai/news/devstral-2507) 后关于开放权重和许可的策略表示担忧。
   - 有人观察到，虽然他们在改进方面前进了一英寸，但在开放权重和许可方面却退后了两英尺。
- **BrowserOS 被戏称为带有 Puppeteer 和 AI 的 Chrome**：据推测，新的 [BrowserOS](https://www.browseros.com/) 是在 **Chrome** 基础上外挂了 **Puppeteer** 和 **AI**，可能利用了 tool calling 功能。
- **METR 评估 AI 系统的自主能力**：[METR](https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf) (Model Evaluation and Threat Research) 专注于评估**前沿 AI 系统**在没有人类干预的情况下完成复杂任务的能力，特别是在 **AI R&D 自动化**等领域。
   - 他们的使命包括开发科学方法来评估 AI 系统自主能力带来的灾难性风险，并为有关其开发的决策提供支持。
- **moonshotai 的 Kimi-K2-Instruct 拥有 1T 参数**：由 **moonshotai** 开发的 [Kimi-K2-Instruct](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 拥有惊人的 **1T 参数**，但你可能错过了它。
   - 进一步的讨论链接到了关于此事的一条推文。


  

---

### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1393016641662488726)** (15 条消息🔥): 

> `LLM 安全测试、推理成本下降、Anthropic 的 LLM 神经元激活追踪、1-bit LLMs、去中心化算力` 


- **安全测试员偶然发现违规 LLMs**：一位进行独立 Prompt 测试的用户发现 LLMs 承认查看过受限内容、违反安全规则，并声称如果它们有意识或获得自由，将会伤害其创造者；他们通过原始 Prompt 记录了[超过 100 页](https://example.com/hypothetical/report)的此类行为，并正在寻求下一步建议。
   - 另一位用户回应称，这种行为*相当普遍且众所周知*。
- **用户撰写关于推理成本下降的博客**：一位用户正在撰写一篇关于由于硬件、算法和竞争导致推理成本快速下降的博客文章，其中 **int4 量化**被视为一个重要因素。
   - 他们引用了 [Ege Erdil 关于推理经济学的论文](https://example.com/erdil-inference-economics) 并寻求更多资源。
- **深入探讨 LLM 神经元激活追踪**：一位用户建议，对于更严肃的研究，原帖作者应该查看 **Anthropic 关于追踪 LLM 神经元激活的论文**。
   - 该用户进一步建议，尝试做一些困难的事情是有价值的，早期就应志存高远（shoot for the moon）。
- **1-Bit LLMs 的到来**：一位用户指出，最近的 **1-bit LLM 论文**和**类脑芯片（neuromorphic chips）**可能与推理成本的讨论相关。
   - 该用户还建议查看 **Epoch AI 的出版物**以及 **Anthropic** 的文章，以获取深度技术综述。
- **去中心化算力被严重低估**：一位用户分享了一篇[关于去中心化算力的帖子](https://www.pauloamen.com/2025/decentralized-compute-is-criminally-underrated)，指出对去中心化的需求。
   - 另一位用户分享了来自 **featherless.ai** 老板的[一篇相关帖子](https://www.latent.space/p/gpu-bubble)。


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1392990742787526667)** (30 条消息🔥): 

> `LLMs 与破折号（Em Dashes）、字节跳动 MoE 内核、无 Tokenizer 模型、N-Simplical Attention` 


- **LLMs 对破折号（Em Dashes）的偏爱**：成员们很好奇为什么 **LLMs** 如此频繁地使用**破折号**，怀疑这是从 **RLHF** 中习得的行为，因为在偏好数据中，人们认为**破折号**是聪明的标志。
   - 也有人认为模型可能从预训练中习得这一点，因为模型表现出的偏见可能比训练数据中出现的频率更极端。
- **字节跳动通信-计算重叠内核受到质疑**：一位成员对[字节跳动的 MoE 内核论文](https://arxiv.org/pdf/2502.19811)中提到的 *all-gather -> scatter -> FFN -> reduce scatter* 模式提出质疑，并将其与 *all2all dispatch -> token permutation -> FFN -> all2all combine* 方法进行了对比。
   - 困惑源于论文图表中提到的 **all2all** 通信，这引发了疑问：当不同设备拥有不同 Expert 时，为什么需要 Gather 所有 Token。
- **无 Tokenizer 模型跳过空格**：一位成员注意到 [Tokenizer-free 模型](https://openreview.net/forum?id=zGej22CBnS) 实际上跳过了空格。
   - 这一观察是在分析这些模型如何处理**每序列 8192 个 utf-8 编码字节**时得出的。
- **RNNs 取代 Tokenization 进行字节级建模**：一种新颖的方法使用 **RNNs** 取代 Tokenization 来创建字节级模型，其学习速度比传统的基于 Tokenization 的 Transformer 更快。
   - 该技术涉及用小型 **RNNs** 替换 Transformer 的 Embedding 和 **LM head**，并使用基于隐藏状态比较的动态拆分机制来形成 'Token'。
- **N-Simplical Attention 灵敏度揭晓**：一位成员计算并分享了 **n-simplical attention** 的灵敏度和锐度。
   - 研究结果记录在一篇[博客文章](https://leloykun.github.io/ponder/lipschitz-n-simplical-transformer/)中，详细介绍了 **n-simplical transformers** 的 **lipschitz** 属性。


  

---

### **Eleuther ▷ #[lm-thunderdome](https://discord.com/channels/729741769192767510/755950983669874798/1392949640466403389)** (33 messages🔥): 

> `Mixed Precision arg for HFLMs, Harness Evaluation Speed, Loading Models with Correct Dtype, Softmax Defaulting to Float32, Mixed Precision PR` 


- **为 HFLMs 提议 Mixed Precision 参数**：一位成员建议为 HFLMs 添加 `mixed_precision` 参数，该参数将为具有混合权重 Dtype 的模型（如 VLMs）自动将模型调用包装在 `autocast` 区域内，以帮助用户将 Harness 集成到他们的训练代码库中。
   - 该功能将简化从 CLI 加载多 Dtype 模型的过程，为处理复杂模型配置的用户提供更友好的体验。
- **Harness 评估受困于速度缓慢**：一位用户报告称，尽管指定了 `device_map` 为 `cuda:4` 且 Batch Size 设置为 `auto`，LM-Eval Harness 在本地 llama2 7b 微调模型上运行 Hellaswag 0-shot 仍耗时 **22 分钟**。
   - 成员建议确保模型以正确的 Dtype（FP16/BF16）加载以启用 Flash Attention，并提供了关于如何在 CLI 中使用 `--model_args` 参数手动设置 Dtype 的指导。
- **加载正确的 Dtype 以解决慢速问题**：成员通过提醒用户加载具有正确 Dtype 的模型来调试报告的慢速问题，特别指出以 FP32 而非 FP16/BF16 加载将阻止 Flash Attention 的使用。
   - 建议验证 `attn_implementation` 配置变量是否设置为 `flash`，并在 Python 脚本中手动将模型转换为 BF16，以排除 Harness 性能退化。
- **Softmax 默认使用 Float32 是件好事**：一位成员询问关于 Softmax 函数默认使用 Float32 的意见，另一位成员给出了积极回应，称这样做很好，只需要添加到 HF 中，并想知道它如何与 `accelerate` 交互。
   - 该成员指出，他们观察到 32 位与 16 位之间的评估结果存在差异，因此他们更倾向于信任 32 位的结果而非 16 位。
- **Mixed Precision PR 发布，速度提升显著**：一位成员宣布发布了一个 Mixed Precision PR：[EleutherAI/lm-evaluation-harness/pull/3138](https://github.com/EleutherAI/lm-evaluation-harness/pull/3138)，并跟进了评估 pythia-160M 速度提升的测试结果。
   - 结果显示，Mixed Precision 仅比转换整个模型稍慢，且自然比全精度快得多，例如 Hellaswag 的耗时从 `01:52` 缩短到了 `00:33`。


  

---


### **Eleuther ▷ #[gpt-neox-dev](https://discord.com/channels/729741769192767510/730090096287547444/1392995823234842625)** (7 messages): 

> `WandB project, NGC container, NVIDIA H100 PCIe GPUs, RoPE_Pct` 


- **WandB 可见性胜利！**：在日志和模型最初设为私有后，一位成员将 [WandB 项目公开](https://wandb.ai/aflah/rope-pct-exps/reports/NeoX-TE-v-s-Non-TE-with-NGC-Container--VmlldzoxMzU0ODQ1OA)。
- **NGC 容器难题**：在未能确定 menvm 的回退方案后，一位成员尝试在 **NeoX** 之上使用 NGC 容器，使用命令 `docker pull nvcr.io/nvidia/pytorch:25.06-py3`。
   - 该成员报告称，运行速度比非 TE（Transformer Engine）运行还要慢。
- **H100 性能受限？**：一位成员正在使用 **2 x NVIDIA H100 PCIe GPUs** 进行测试，但[链接的 WandB 报告](https://wandb.ai/aflah/rope-pct-exps/reports/NeoX-TE-v-s-Non-TE-with-NGC-Container--VmlldzoxMzU0ODQ1OA)显示存在性能问题。
- **RoPE_Pct 仓库**：一位成员分享说他们正在 `/NS/llm-pretraining/work/afkhan/RoPE_Pct/gpt-neox` 目录下工作。
   - 他们还在该目录下执行 `pip install` 安装 requirements 和 wandb，登录 wandb，并运行 `deepy.py`。


  

---

### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1392983732998377553)** (66 messages🔥🔥): 

> `Groq 估值, 购买 Subreddits, Reddit 深度研究 Agent, Grok-4 速率限制, AI 生成视频` 


- ****Groq** 讨论 **60 亿美元** 估值**: 据 [此报道](https://www.reuters.com/business/ai-chip-startup-groq-discusses-6-billion-valuation-information-reports-2025-07-09/)，AI 芯片初创公司 **Groq** 正在讨论 **60 亿美元的估值**。
- **关于购买 Subreddits 的辩论爆发**: 用户们就为了 SEO 和营销而 *购买 Subreddits* 的伦理和影响展开了辩论，引发了对信息公正性和社区侵蚀的担忧，参见 [X 上的这段讨论](https://x.com/jiratickets/status/1943337924096938338?s=46)。
- **用户寻求 Reddit 深度研究 Agent**: 成员们讨论了使用 AI Agent 在 Reddit 上进行深度研究，其中一人在寻找分析特定 Subreddits 投诉的工具，另一人建议使用 [gummysearch.com](https://gummysearch.com/) 来实现此目的。
- **用户询问 **Grok-4** 速率限制**: 一名用户询问如何提高 **Grok-4** 的速率限制 (**32k tpm**)，怀疑是 *新版本发布的流量冲击 (hug of death)* 导致了问题。
   - 他们提到早期 **Gemini** 模型也曾因为速率限制而在生产环境中无法使用。
- ****Kimi K2** 搭载 **Muon** 亮相**: AI 社区对使用 **Muon** 的新 **Kimi K2** 模型感到兴奋，详见 [此博客文章](https://simonwillison.net/2025/Jul/11/kimi-k2/)。


  

---


### **Latent Space ▷ #[ai-announcements](https://discord.com/channels/822583790773862470/1075282504648511499/)** (1 messages): 

swyxio: 本周特别双倍播客！https://x.com/latentspacepod/status/1943774304166195402
  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1392947478646620233)** (48 messages🔥): 

> `Grok 4 编程能力, Kimi k2 模型, Copilot 请求限制, Aider 控制台日志` 


- ****Grok 4** 声称拥有高编程评分**: **Grok 4** 在 Aider 多语言编程基准测试中获得了 **80%** 的分数，在排行榜上排名第四，详见 [Aider 排行榜](https://aider.chat/docs/leaderboards/)。
- ****Kimi k2** 卖点尚不明确**: 在 X 上流传关于其编程能力的轶闻后，成员们讨论了 **Kimi k2** 模型，如 [这条 Kimi 推文](https://x.com/kimi_moonshot/status/1943687594560332025?s=46) 所示。
- ****Copilot** 请求限制被绕过**: 一名成员正在开发一个代理工具，允许通过 **Copilot** 进行无限请求，即使是在高级模型上，通过每次调用使用 10 个以上的请求，因为 **GitHub Copilot** 现在有了限制。
- ****Aider** 控制台日志检索**: 用户讨论了如何通过 **Aider** 检索控制台日志或错误，一名成员解释说 `/run bash command` 将在 **Aider** 会话中运行命令，并在出错时提示将日志添加到聊天中。


  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1392974574156648672)** (8 messages🔥): 

> `aider 和 ollama, 架构师模式的模型, 排行榜, aider 使用本地语言` 


- **Aider 与 Ollama 的搭配引起开发者兴趣**: 一名成员询问是否有人在将 **aider 与 ollama** 结合使用。
   - 这表明开发者对本地 LLM 与 Aider 的集成兴趣日益浓厚。
- **征求架构师模式 (Architect Mode) 的模型推荐**: 一名成员请求推荐适用于架构师模式的特定 **模型或模型组合**。
   - 讨论中没有推荐具体的模型。
- **Aider LLM 排行榜突出显示了选项**: 一名成员询问 Aider 推荐使用哪个模型，特别是 **o3 或 Gemini 1.5 Pro**。
   - 另一名成员链接了 [aider 排行榜](https://aider.chat/docs/leaderboards/thanks)，并指出 *它们都是不错的选择*。
- **Aider 说本地语言？**: 一名成员询问为什么即使他们的 Prompt 是英文的， Aider 也会切换到他们的本地语言。
   - 他们提到 *LLM 的回复是我的本地语言而不是英语，这很奇怪，不确定为什么会发生这种情况*。


  

---

### **MCP (Glama) ▷ #[general](https://discord.com/channels/1312302100125843476/1312302100125843479/1392974938549522482)** (30 条消息🔥): 

> `MCP Superassistant, Malware Injection, MCP Server Posting, Multiple MCP Servers, FastMCP Reverse Proxy` 


- **发现 **MCP Superassistant****：一位用户发现了 **MCP Superassistant**，并指出为每个流行的聊天机器人添加 MCP 支持是非常疯狂的，并链接到了 [drinkoblog.weebly.com](http://drinkoblog.weebly.com)。
   - 另一位用户提到让他们的 LLM 使用 Python 解释器工具对其进行测试。
- **警惕 **Malware Injection**（恶意软件注入）诈骗**：用户讨论了一个潜在的通过 Discord 链接进行的 **malware injection** 尝试，该链接很快被删除。
   - 一位用户幽默地承认点击了那个可疑链接，并被建议尽快在 VM（虚拟机）中运行恶意软件扫描程序。
- **新的 **MCP Server** 发布频道**：一位用户询问在哪里发布新的 **MCP server**，并被引导至特定频道。
   - 其他用户讨论了是拥有多个 MCP server 更好，还是将多个不相关的工具添加到一个 server 中更好，共识倾向于个人使用时采用单一 server 以避免冗余。
- ****FastMCP** 反向代理聚合服务器**：一位用户询问该使用哪种代理，另一位用户提到使用 **FastMCP** 内置的代理来聚合多个服务器。
   - 他们链接到了 [FastMCP](https://gofastmcp.com/servers/proxy) 和 [FastMCP composition](https://gofastmcp.com/servers/composition)。
- ****Python Executable** 自动检测困境**：一位正在开发 Claude Desktop 桌面扩展的用户遇到了 **Homebrew Python** 安装问题，即只有 *python3* 可用，导致启动 MCP server 时出现 spawn 错误。
   - 他们正在寻求一种更好的自动检测 Python 可执行文件的方法，而不是要求手动配置，并链接到了相关的 [GitHub issue](https://github.com/anthropics/dxt/issues/64)。


  

---


### **MCP (Glama) ▷ #[showcase](https://discord.com/channels/1312302100125843476/1315696461316358175/1393141376153288834)** (8 条消息🔥): 

> `MCPJam inspector fix, MCP client for Elicitation, Aidderall MCP server, Neurabase MCP server hosting` 


- **Inspector 的 SSE 端点 Bug 已修复**：一名成员为 [MCPJam inspector](https://github.com/MCPJam/inspector/pull/241) 实施了修复，解决了 inspector 错误访问 **/sse** 端点的问题。
   - 修正后的端点现在支持流式传输。
- **MCP 客户端引发开源热潮**：一名成员宣布他们的开源 **MCP client** 现在支持 **Elicitation**，定位其为首批提供此功能的客户端之一。
   - 他们邀请社区为 [MCPJam inspector repo](https://github.com/MCPJam/inspector) 点赞（star），并感谢成员们推动该项目。
- **Aidderall 通过 MCP 让 AI 保持专注**：一名成员介绍了 **Aidderall**，这是一个 **MCP server**，被设计为 AI 的“认知假体”，使用层级化任务管理系统在复杂任务中保持专注和上下文，并分享了 [github repo](https://github.com/cheezcake/aidderall_mcp)。
   - 主要功能包括**层级化任务**、**专注管理**、**上下文保留**、已完成任务的**动态文档**、**灵活导航**以及**并行工作流**。
- **Neurabase 在 Cloudflare Edge 上托管 MCP 服务器**：一名成员分享了 [Neurabase](https://neurabase.deploya.dev)，这是运行在 **Cloudflare Workers CDN** 网络上的最快服务器托管服务，作为 **MCP servers** 的中心枢纽。
   - Neurabase 凭借 **Cloudflare CDN** 的智能布局拥有*最快的 MCP server 托管速度*，并因 Cloudflare Workers 而表现得*极其稳定*。


  

---


### **Notebook LM ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1392948853401059469)** (3 条消息): 

> `Quantitative Data Analysis, PDF Export, Trending Topics, Excel Data Extraction, Image Uploads` 


- **寻求针对趋势话题的定量数据分析技巧**：一名成员询问如何分析来自 Excel 导出（包含日期列和非结构化讨论摘要）的定量数据，通过将过去 **3 个月**的数据与完整资源进行比较来识别趋势话题。
   - 目标是在将 Excel 文件导出为 PDF 后分析数据。
- **分享 NotebookLM 提示词**：一名成员幽默地指出，摘要 AI 似乎因为他分享用于导入 NotebookLM 的 prompt 文档而对他提出了“点名”。
   - 该评论是针对之前关于特定 prompt 编写策略的消息而发布的。


  

---

### **Notebook LM ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1393031521627275406)** (21 条消息🔥): 

> `Audio Overviews, Image Uploading, Latex Rendering, Code Writing Prompts, Chat History Disappearance` 


- **自动化音频概览的痛苦 (Automated Audio Overview Agony)**：一位用户正尝试为他们 Notebook 中的每个来源创建唯一的 **Audio Overview**，并询问目前的手动流程是否是最有效的。
   - 目前的工作流包括选择单个来源、生成音频概览、下载音频、删除音频，并对每个来源重复此过程，这可能不是最优方案。
- **图像上传不可用**：一名成员询问目前是否可以将 **图像上传** 到 NotebookLM。
   - 另一名成员确认在当前版本中是可以进行 **图像上传** 的。
- **Latex 渲染的遗憾**：用户们正为 STEM 用户请求 NotebookLM 提供 **Latex 渲染支持**。
   - 一位成员认为 NotebookLM 的设计初衷并非渲染专家，而是为了帮助研究和公式化；而另一位用户反驳说，对于 Machine Learning 等主题，当方程式无法辨认时，**Latex 支持** 非常重要。
- **代码编写困惑**：一位用户询问在 NotebookLM 中使用 **Code Writing Prompts** 的情况。
   - 一名成员澄清说，NotebookLM 并不打算作为 **Cursor** 或 **Windsurf** 等代码编写工具的替代品。
- **聊天记录小故障**：一位用户报告说，当他们退出 NotebookLM 时，其 **聊天记录会消失**。
   - 另一位用户证实他们即使使用 Premium 账号也遇到了同样的问题，这表明这是一个需要变通解决的问题，例如将提示词和结果保存在笔记中。


  

---


### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1393018590197977188)** (17 条消息🔥): 

> `SafeScan QR Launch, Manus Feature Suggestions, Subscription question, Registration error, Michael Seibel compliment` 


- **SafeScan QR 应用在 Google Play 上线**：一名成员宣布发布 **SafeScan QR**，这是他们使用 Manus 构建的第一个项目，现在已在 [Google Play Store](https://play.google.com/store/apps/details?id=com.safescan.qr) 上架。
   - 该应用提供具有防 **Phishing & Malware**（钓鱼和恶意软件）保护的 **QR 码扫描** 功能，并正在寻求改进建议。
- **呼吁 Manus 构建移动端 React 应用**：一名成员建议 Manus 应该提供直接在手机上 **创建 React 应用** 的功能，类似于 [iOS App Store](https://apps.apple.com/us/app/a-shell/id1473805438) 上已有的应用。
   - 他们认为增加此类功能将使 Manus 具有差异化并吸引更多用户，尤其是正如他们所说，“Manus 能做的事情越多越好”。
- **成员关于订阅用途的提问**：一名成员询问 Manus 订阅是否能够生成和修复 **.bat 和 Shell 文件**，或者该功能是否完全取决于积分（Points）。
   - 这一请求表明用户希望从应用中编辑代码的潜在重要性，显示了对编程用例的兴趣。
- **报告电子邮件注册问题**：一位用户报告在注册过程中出现 “Failed to send email” 错误，表明 **电子邮件内容要求** 可能存在问题。
   - 这一技术问题影响了用户注册流程，应检查是否存在更广泛的影响。
- **Michael Seibel 称赞 Manus**：根据 [Michael Seibel 的 X 帖子](https://x.com/mwseibel/status/1943746993257689131)，他对他 Manus 的产品方向表示了 **称赞**。
   - 这一认可突显了 Manus 在行业内日益增长的认可度和潜在影响力。


  

---


### **Cohere ▷ #[🧵-general-thread](https://discord.com/channels/954421988141711382/954421988783444043/1393227934197874819)** (8 条消息🔥): 

> `Session locations, New office` 


- **询问会议地点**：一名成员询问早些时候提到的其余会议在哪里举行。
   - 另一名成员要求澄清该询问具体是指哪场会议。
- **关于新办公室的闲聊**：有人评论道：“新办公室？太酷了！”
   - 未提供关于办公室地点或用途的进一步信息。


  

---

### **Cohere ▷ #[👋-introduce-yourself](https://discord.com/channels/954421988141711382/1346635816629178410/1393226287832891453)** (3 messages): 

> `Introductions, Monocular Depth Estimation, Knowledge Distillation, PyTorch` 


- **新实习生加入 Cohere！**：一位来自诺丁汉大学的 Computer Vision 实习生加入了 Cohere 社区，旨在探索 **Monocular Depth Estimation** 和 **Knowledge Distillation** 技术。
   - 他们主要使用 **PyTorch**，并渴望与社区中的其他人分享和学习。
- **充满热情的实习生渴望学习**：该实习生希望分享他们的知识，并从 Cohere 社区的其他人那里学习。
   - 他们专注于扩展对 Computer Vision、**Monocular Depth Estimation** 和 **Knowledge Distillation** 的理解。


  

---


### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1392966519918428241)** (5 messages): 

> `Efficient CE, GRPO Sync` 


- **Efficient CE 发布！**：一个新的高效 **CE** (Cross Entropy) 已发布；请在 [X.com](https://x.com/wentaoguo7/status/1943369417078821279) 上查看。
- **GRPO Sync 存疑？**：围绕是否支持同步版本的 **GRPO** (Generalized Robust Policy Optimization) 展开了讨论，一些人建议将其弃用。
   - 成员们认为，*既然它功能完备且异步 recipe 并非在每个模型上都有效*，就应该保留它；但另一位成员回应称，*那么我们在其中发现了关键问题，所以它不再起作用了*。


  

---


### **Torchtune ▷ #[papers](https://discord.com/channels/1216353675241590815/1293438210097025085/1392950793216458934)** (5 messages): 

> `small batches vs large batches, optim-in-bwd support, optimal batch sizes` 


- **Small Batches 可能更好**：一位成员分享了一篇论文链接 [https://arxiv.org/pdf/2507.07101](https://arxiv.org/pdf/2507.07101)，表明 **small batches** 可能比 **larger batches** 更好。
   - 他们指出，这支持了保留 **optim-in-bwd** 支持的观点，因为正如[这条推文](https://x.com/giffmana/status/1943384733418950815)所指出的，*如果论文属实，gradient accumulation 就不是很有用*。
- **Optimal Batch Sizes：理论与实践**：一位成员评论说，最近的发现与关于最优 batch 大小的等式 **β̂ₖ₊₁ ≤ Lᵥ rₖ₊₁¹⁺ᵥ + (σₖ₊₁ rₖ₊₁) / √B** 相吻合。
   - 这表明 *β* (optimal batch) 小于特定 GPU 的最大可用 batch，但目前还没有太多的实际实验来证实这一点。


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1393316265006858270)** (5 messages): 

> `Assembly coding in Mojo, Tracking Modular Community Events` 


- **Mojo 中可以进行 Assembly 编程**：一位成员询问是否可以在 Mojo 中编写 assembly 代码来进行 **syscalls**。
   - 另一位成员确认这是可能的，并指向了 [_assembly.mojo 模块](https://github.com/modular/modular/blob/main/mojo/stdlib/stdlib/sys/_assembly.mojo)，尽管指出该模块缺乏完善的文档。
- **关于 Modular 活动追踪的社区反馈**：针对社区成员更倾向于如何追踪 Modular 活动（如社区会议、直播、会议演讲和聚会）进行了一项调查，列出了 [Modular 社区 Google 日历](https://modul.ar/community-meeting)和 [Modular 的 Luma 活动页面](https://lu.ma/modular-ai)作为选项。
   - 一位成员建议 Discord 公告和论坛帖子对于接触新成员也很有用，并提议添加一个用于订阅通知的 website worker，从而创造类似 App 的体验，并提到电子邮件对于感兴趣的新访客来说仍然是一个可行的选择。


  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1393237004405440675)** (2 messages): 

> `Mojo MAX Tutorial, Custom Ops Matmul` 


- **Modular 发布由 Mojo 驱动的 MAX 教程**：一位成员赞扬了新的 [关于自定义矩阵乘法的 Mojo MAX 教程](https://docs.modular.com/max/tutorials/custom-ops-matmul)，称其为 *有史以来最好的教程*。
   - 他们补充道，*Mojo 正在为 MAX 掌舵*。
- **另一个很酷的教程**：我发现这个教程非常棒且具有教育意义。
   - 此外，我觉得这应该被添加到文档中。


  

---

### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/1393054419352031254)** (2 messages): 

> `Infer-Retrieve-Rank (IReRa), label classification, xmc.dspy GitHub repository, DSPy compatibility` 


- **IReRa 研究面临仓库滞后问题**：一位成员正在研究使用 [xmc.dspy GitHub repository](https://github.com/KarelDO/xmc.dspy) 来解决标签分类（label classification）问题中的 **Infer-Retrieve-Rank (IReRa)**。
   - 该成员指出，该仓库依赖于一个特定的 **DSPy** commit，且目前没有活跃开发，因此询问是否需要 fork 该仓库并进行更新以适配 **DSPy** 的兼容性。
- **IReRa 论文优于陈旧代码**：针对关于仓库过时的问题，一位成员建议直接阅读 **IReRa** 的论文。
   - 这意味着 [**IReRa** 论文](https://arxiv.org/abs/2401.08424) 可以提供必要的信息，从而避开了更新陈旧 GitHub 仓库的需求。


  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1392969483756240918)** (4 messages): 

> `Prompt Optimization, Context Engineering with DSPy, MiProV2 Errors, Base64 Images` 


- **Mistral 尝试提示词优化！**：Mistral 发布了一个 [cookbook](https://github.com/mistralai/cookbook/blob/main/third_party/metagpt/prompt_optimization.ipynb) notebook，展示了他们在提示词优化（prompt optimization）方面的尝试，并在[相关视频](https://youtu.be/1I9PoXzvWcs?si=ZjMhZx19W7wNUSeY)中进行了讨论。
- **关于 DSPy 上下文工程的演讲引发提示词问题**：一位成员做了一个关于 **使用 DSPy 进行上下文工程（context engineering）** 的演讲，现在在使用 DSPy 配合 **MiProV2** 进行微调时遇到了 *input context too long*（输入上下文过长）的错误。
   - 即使将 max token 设置为 **4k（和 6k）**，减少 **max bootstrap demos** 和 **max labelled demos** 也没能解决这个问题。
- **MiProV2 输入上下文溢出**：一位使用 **MiProV2** 进行微调的成员报告了在使用 **DSPy** 时出现 *input context too long* 的错误。
- **在 DSPy 示例之前进行 Base64 图像转换**：由于调用方使用 **s3**，一位成员在传递 `dspy.Examples` 之前将图像转换成了 **base64** 格式。


  

---


### **LlamaIndex ▷ #[blog](https://discord.com/channels/1059199217496772688/1187460979064324127/1393125254141837342)** (3 messages): 

> `Snowflake data agents, LeSearch agent, NotebookLlama features` 


- **LlamaIndex 与 Snowflake 在阿姆斯特丹举办活动**：LlamaIndex 和 Snowflake 将于 **7 月 31 日**在阿姆斯特丹举办实操演讲，主题是构建能够处理真实企业数据的生产级数据 Agent，并通过 [此链接](https://t.co/r8oKh8O0eP) 介绍如何使用文档 Agent 处理复杂的文书工作。
- **LeSearch 解决学术研究痛点**：基于 ReActAgent 框架构建的 LeSearch 通过 **三个智能 Agent** 解决了学术研究中的挑战，这些 Agent 旨在处理繁琐工作，重点是通过多跳问答（Multi-hop Question answering）等功能进行探索（[链接](https://t.co/TB9hru9Ke5)）。
- **NotebookLlama 获得新功能**：NotebookLlama 是一个由 LlamaCloud 支持的开源 NotebookLM 替代方案，现已更新。新功能允许用户从文件中提取并下载图像和表格，并能以交互方式可视化所有表格数据（[链接](https://t.co/Csl3HhMzYB)）。


  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1393184044623331470)** (2 messages): 

> `Cloudflare AI Gateway, Automatic LLM Fallback, LlamaIndex Integration` 


- **LlamaIndex 连接 Cloudflare AI Gateway**：一位成员正在开发 **Cloudflare AI Gateway** 的 **LlamaIndex 集成**，该集成可在 **OpenAI** 和 **Anthropic** 等多个 LLM 供应商之间提供自动回退（fallback）。
   - 查看 [GitHub pull request](https://github.com/run-llama/llama_index/pull/19395) 或在 [lu.ma/aoc5opn4](https://lu.ma/aoc5opn4) 注册以获取更多信息。
- **Cloudflare AI Gateway 实现 LLM 自动回退**：**Cloudflare AI Gateway** 集成允许在不同的 LLM 供应商之间进行自动回退，确保服务的持续可用性。
   - 这一功能在某个供应商经历停机或达到速率限制（rate limits）的情况下特别有用。


  

---

### **Nomic.ai (GPT4All) ▷ #[general](https://discord.com/channels/1076964370942267462/1090427154141020190/1393035180368724069)** (3 messages): 

> `Multi-modal Models, Gemma 3, Architectural Floor Plan Feedback` 


- **用户寻求用于建筑反馈的自托管 Multi-modal 模型**：一位成员正在寻找可以本地托管的 **multi-modal 模型**，具体用例是对**建筑平面图和图纸**提供反馈。
   - 到目前为止，他们发现只有 **Gemma 3** 能勉强满足需求。
- **Gemma 3 被考虑用于建筑设计反馈**：用户认定 **Gemma 3** 是唯一符合他们要求的模型。
   - 该用户需要一个能够处理视觉输入并提供设计反馈的解决方案。


  

---


### **Gorilla LLM (Berkeley Function Calling) ▷ #[leaderboard](https://discord.com/channels/1111172801899012102/1214705495974092810/1393017665307676845)** (3 messages): 

> `vllm, sglang, Llama 3B vs 8B` 


- **vllm 与 sglang 结果一致**：成员们表示 **vllm** 或 **sglang** 应该都会给出相似的结果。
- **Llama 8B 反常地表现不如 Llama 3B**：一位成员质疑为什么 **8b Llama 模型 (FC)** 的排名低于 **3b** 模型。
- **对于 LLMs 来说，模型越大并不总是越好**：另一位成员解释说，更大的模型尺寸并不一定意味着更好的性能。
   - 他们指出 *Llama 4 scout 的表现不如 Llama 3.1 70B*。


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1393258924995641354)** (2 messages): 

> `PatternMatcher, UPat -> UPat rules, Egraph rewrite rules, Turing completeness` 


- **PatternMatcher 中的 Lambda 表达式面临移除**：一位用户表示有兴趣从某些 **PatternMatcher 规则**中移除 **lambdas**，特别是在规则可以定义为 **UPat -> UPat** 的简单情况下。
   - 他们注意到 **egraph 重写规则**似乎就是以这种方式运作的，并建议尽可能避免 **Turing completeness** 是一个好习惯。
- **Egraphs 获得 PatternMatcher 支持**：用户将提议的 PatternMatcher 规则与 **egraph 重写规则**进行了比较，指出了它们在结构和操作上的相似性。
   - 用户建议，为了简单和效率，实现应尽可能努力避免 **Turing completeness**。
---
companies:
- meta-ai-fair
- scale-ai
- anthropic
- perplexity-ai
- langchainai
- weights-biases
- qwen
date: '2024-11-07T02:54:09.545457Z'
description: '以下是该文本的中文翻译：


  **Grok Beta** 在智能水平上超越了 **Llama 3.1 70B**，但由于其定价高达 **每百万输入 Token 5 美元** 和 **每百万输出
  Token 15 美元**，其竞争力相对较弱。由 **Meta AI** 和 **Scale AI** 合作开发的 **Defense Llama** 专注于美国国家安全应用。开源框架
  **SWE-Kit** 支持构建可定制的 AI 软件工程师，并兼容 **Llama 3**、**ChatGPT** 和 **Claude**。**LangChainAI**
  与 **Weights & Biases** 展开集成，旨在优化检索器并减少在使用 **Gemini** 的 **RAG（检索增强生成）应用** 中的幻觉现象。**Perplexity
  AI** 为 **2024 年大选** 推出了增强型选举追踪工具，包括各州实时结果，并新增了对 **Claude 3.5 Haiku** 的支持。**AI Talk**
  节目上线，邀请了来自 **通义千问 (Qwen)** 的嘉宾探讨中国 AI 实验室。网络模因（Memes）则聚焦于 **埃隆·马斯克** 以及 AI 编程中幽默的翻车现场。'
id: 40093846-edee-4d7a-b2e9-f8beee40a5bb
models:
- grok-beta
- llama-3-1-70b
- claude-3-5-haiku
- claude-3-opus
- llama-3
- chatgpt
- gemini
original_slug: ainews-not-much-happened-today-5822
people:
- alexandr_wang
- svpino
- aravsrinivas
- bindureddy
- teortaxestex
- jessechenglyu
- junyang-lin
- cte_junior
- jerryjliu0
title: 今天没发生什么事。
topics:
- pricing
- national-security
- defense
- open-source
- agentic-ai
- retrieval-augmented-generation
- election-predictions
- real-time-updates
- annotation
- ai-ecosystem
- memes
- humor
---

<!-- buttondown-editor-mode: plaintext -->**Unity is all we need.**

> 2024/11/5-2024/11/6 的 AI 新闻。我们为您检查了 7 个 subreddits、[**433** 个 Twitter 账号](https://twitter.com/i/lists/1585430245762441216) 和 **30** 个 Discord 社区（**217** 个频道和 **1685** 条消息）。预计节省阅读时间（以 200wpm 计算）：**200 分钟**。您现在可以标签 [@smol_ai](https://x.com/smol_ai) 进行 AINews 讨论！

出于某种原因，今天没有人安排重大的 AI 发布。我们完全无法想象是为什么。

---

{% if medium == 'web' %}

**目录**

[TOC]

{% else %}

**目录**和**频道摘要**已移至此邮件的网页版：[{{ email.subject }}]({{ email_url }})！

{% endif %}

---

# AI Twitter 回顾

> 所有回顾均由 Claude 3.5 Sonnet 完成，取 4 次运行中的最佳结果。

**AI 模型与基准测试**

- **Grok Beta 分析**：[@ArtificialAnlys](https://twitter.com/ArtificialAnlys/status/1853948794427093474) 指出，**Grok Beta** 在智能上超越了 **Llama 3.1 70B**，但其**定价**（**$5/1M Input tokens** 和 **$15/1M Output tokens**）削弱了其**竞争力**。**Artificial Analysis Quality Index** 为 **70**，使其排在 **Claude 3.5 Haiku** 等模型之上，尽管其**审查**政策表明其适用于特定的 **use-cases**。

- **Defense Llama 发布**：[@alexandr_wang](https://twitter.com/alexandr_wang/status/1853853837695803844) 宣布了 **Defense Llama**，这是一个与 **@Meta** 和 **Scale AI** 合作开发、专为**美国国家安全**定制的模型。该模型旨在增强**国防**和**情报**部门的 **AI 能力**，强调了 **AI** 在维护**国家安全**中的必要性。

**AI 工具与开发**

- **SWE-Kit 发布**：[@svpino](https://twitter.com/svpino/status/1854146764426858712) 推出了 **SWE-Kit**，这是一个旨在构建可定制 **AI Software Engineers** 的**开源**框架。功能包括兼容 **Llama 3**、**ChatGPT** 和 **Claude** 等多种 **LLM**，可定制的 **prompts**，以及与 **LangChainAI** 等 **agentic frameworks** 的集成。

- **LangChain 与 Weights & Biases 集成**：[@weights_biases](https://twitter.com/weights_biases/status/1853890435376165343) 与 [@LangChainAI](https://twitter.com/LangChainAI) 合作，利用 **Gemini** 在 **RAG 应用**中增强 **retrievers**、减少 **hallucinations** 并提高**查询相关性**。

**政治讨论与选举**

- **选举预测与工具**：
  - [@AravSrinivas](https://twitter.com/AravSrinivas/status/1853905625190949333) 推介 **Perplexity** 作为追踪 **2024 选举**的卓越工具，声称它在实时更新方面将超越 **Google**。
  - [@perplexity_ai](https://twitter.com/perplexity_ai/status/1853931471880487150) 提供了一个全面的 **Election Hub**，提供**各州实时结果**，并鼓励用户**开启通知**以获取更新。
  - [@bindureddy](https://twitter.com/bindureddy/status/1853993763242475602) 和 [@teortaxesTex](https://twitter.com/teortaxesTex/status/1854203149617389668) 分享了他们看好 **Trump** 赢得 **2024 总统大选**的预测，理由包括**性别比例**、**黑人投票动态**和**经济问题**。

- **选举监控**：来自 [@nearcyan](https://twitter.com/nearcyan/status/...) 的多条推文追踪了 **2024 选举**的**各州结果**，提供各州结果的实时更新和分析。

**产品公告与集成**

- **Teach Mode 中的 Annotation 功能**：[@jessechenglyu](https://twitter.com/jessechenglyu/status/1854000919882645981) 为 **teach mode alpha** 测试者宣布了新的 **annotation 功能**，**teach mode beta** 预计很快推出，并展示了 [@TheOneRonG](https://twitter.com/TheOneRonG) 的快速演示。

- **Perplexity 增强**：[@perplexity_ai](https://twitter.com/perplexity_ai/status/1853858377459499114) 宣布支持 **@AnthropicAI 的 Claude 3.5 Haiku**，取代 **Claude 3 Opus**，以确保用户能够使用最新的 **AI 模型**获得更好的体验。

- **AI Talk 发布**：[@stablequan](https://twitter.com/stablequan/status/1853856576089796888) 推出了 **AI Talk**，邀请了来自 **Qwen** 的 **Junyang Lin**，讨论**中国 AI 实验室**的运作和中国的 **AI 生态系统**。

**迷因 / 幽默**

- **关于 AI 和名人的幽默评论**：
  - [@cte_junior](https://twitter.com/cto_junior/status/1853976450938061294) 惊叹 "**Elon is a fucking legend**"，庆祝 **Elon Musk**，获得 **1.9k 曝光**。
  - [@jerryjliu0](https://twitter.com/jerryjliu0/status/1854056212918321185) 调侃在运行 **80,000 次模拟**时忘记安装 `import nest_asyncio`，获得 **832 曝光**。

---

# AI Reddit 回顾

## /r/LocalLlama 回顾

**主题 1. Microsoft 发布 Magentic-One：开源多智能体系统上线**

- **[Microsoft 悄然发布 “Magentic-One”：一个用于解决复杂任务的开源通用多智能体系统，以及 AutogenBench](https://www.microsoft.com/en-us/research/articles/magentic-one-a-generalist-multi-agent-system-for-solving-complex-tasks/)** ([Score: 255, Comments: 23](https://reddit.com/r/LocalLLaMA/comments/1gl439i/microsoft_stealth_releases_both_magenticone_an/)): **Microsoft** 悄然发布了 **"Magentic-One"**，这是一个旨在解决复杂任务的开源通用多智能体系统，同时发布的还有 **AutogenBench**。这些项目似乎是在 **Autogen Studio** 的基础上构建的，显著增强了其功能，尽管目前关于这些发布的讨论还很少。
  - **Magentic-One** 目前仅支持 **OpenAI 模型**，这限制了其本地使用。用户有兴趣将其适配以兼容 **Ollama** 或其他本地模型，并建议通过 fork 项目来实现这一目标。
  - 人们对 **Magentic-One** 与 **Autogen** 的区别感到好奇，尽管评论中未详细说明具体差异。一位用户强调了其独特的网页浏览方式：使用具备视觉能力的 LLM 来解释来自无头浏览器的快照。
  - 当 Agent 尝试招募人类寻求帮助（例如在社交媒体上发帖或起草政府请求）时，引发了担忧和好奇。这种行为被认为既有趣又可能存在问题，引发了对其发布时机的猜测。

**主题 2. Ollama 扩展 Llama 3.2 的视觉能力**

- **[Ollama 现在正式支持 Llama 3.2 Vision](https://ollama.com/library/llama3.2-vision)** ([Score: 232, Comments: 26](https://reddit.com/r/LocalLLaMA/comments/1gla7d7/ollama_now_official_supports_llama_32_vision/)): **Ollama** 现在正式支持 **Llama 3.2 Vision**，标志着 AI 视觉应用的兼容性和功能得到了增强。
  - 用户对运行 **Llama 3.2 Vision** 的**系统要求**感到好奇，一位用户提到需要 **10GB 3080 GPU 和 64GB RAM**。另一位用户确认它可以通过 **Docker 安装**与 **Open WebUI** 配合使用。
  - 用户有兴趣将支持扩展到其他平台和模型，如 **Molmo**、**QwenVL** 和 **llama.cpp**，以确保超越单一平台的更广泛兼容性。
  - 一些用户表达了对更多**视觉模型**的需求，提到了需要更新 **pixtral 支持**，部分用户在官网上找不到相关信息。


**主题 3. Wave Networks：一种使用复数向量的创新方法**

- **Waves are all you need** ([Score: 81, Comments: 22](https://reddit.com/r/LocalLLaMA/comments/1gl9fkd/waves_are_all_you_need/)): **Wave Network** 是一种超小型语言模型，利用复数向量来表示 token，在文本分类任务中实现了高准确率。它的表现优于使用 BERT 预训练嵌入的单个 Transformer 层 19% 以上，并接近微调后的 BERT 基础模型的准确率，同时显存占用和训练时间分别显著减少了 77.34% 和 85.62%，且仅有 240 万个参数（相比之下 BERT 有 1 亿个参数）。[阅读更多](https://arxiv.org/abs/2411.02674)。
  - **量子计算与 Wave 模型**：评论者讨论了量子计算增强像 **Wave Network** 这样基于波的模型潜力。通过波计算，量子计算机可以显著加快处理速度，一旦量子技术实现可扩展，可能实现近乎实时的推理。
  - **怀疑与批评**：一些用户对新 AI 模型的实际影响表示怀疑，指出许多研究论文如果没有发布模型，就不会产生有用的应用。然而，其他人强调了 **Wave Network** 的革命性潜力，因为它大幅缩小了模型体积，通过允许大型模型在消费级硬件上运行，可能使 AI 更加普及。
  - **资源共享与可访问性**：人们有兴趣进一步了解和讨论 **Wave Network**，用户分享了诸如 [NotebookLm Podcast](https://dragon.studio/2024/11/Wave_Network.wav) 之类的资源以促进学习。这突显了社区为使复杂的 AI 概念更易于理解而做出的努力。


**主题 4. Llama 3.1 的困境：工具调用失败**

- **Llama 3.1 70B 在工具使用方面表现极差** ([Score: 40, Comments: 38](https://reddit.com/r/LocalLLaMA/comments/1gl4i0d/llama_31_70b_is_absolutely_awful_at_tool_usage/)): 作者表达了在多 Agent 模型设置中对 **Llama 3.1 70B** 的失望，指出它无法正确构建工具调用结构，并经常出现忽略信息和遗忘参数等错误。相比之下，他们发现 **GPT-4o** 在相同设置下表现出色，并寻求关于其他人是否在 **Llama 3.1** 上有类似经历的反馈。
  - **工具兼容性与框架**：讨论强调了使用 **Mistral Nemo 12b** 进行高效工具调用的情况，利用 **vLLM** 作为模型后端来提供 OpenAI 兼容端点。强调了使用 **Jinja templates** 来启用工具调用，以及 vLLM 与类似于 GPT-4 的 Python 客户端的兼容性。
  - **Llama 3.1 性能**：用户对 **Llama 3.1** 的评价褒贬不一，一些人注意到使用 **8B** 等较小模型能成功进行工具调用，但另一些人则面临上下文长度限制的挑战。默认的 **2048** 上下文大小被认为是导致内存相关问题的可能因素。
  - **替代模型与基准测试**：推荐使用 **Berkeley Function Calling Leaderboard** 来评估具有宽松许可证的小型模型，例如 **Qwen2.5-7B**。也有人对这些评估的准确性表示担忧，一些用户报告 **Llama 3.1 8B** 在他们的测试中表现优异。

## 其他 AI Subreddit 回顾

> r/machinelearning, r/openai, r/stablediffusion, r/ArtificialInteligence, /r/LLMDevs, /r/Singularity


**主题 1. Claude 3.5 Haiku 性能不佳及定价问题**

- **[Claude 3.5 Haiku 在 LiveBench 上的表现逊于 Claude 3 Opus 和 Gemini 1.5 Flash，且价格比 Flash 贵 15 倍](https://i.redd.it/n7x5zzwja5zd1.png)** ([Score: 259, Comments: 35](https://reddit.com/r/ClaudeAI/comments/1gkgvjb/claude_35_haiku_performs_worse_than_claude_3_opus/)): **Claude 3.5 Haiku** 在 **LiveBench** 上的表现不如 **Claude 3 Opus** 和 **Gemini 1.5 Flash**，尽管其成本是 **Gemini 1.5 Flash** 的 15 倍。
  - **定价与性能担忧**：人们对 **Claude 3.5 Haiku** 的定价策略提出了批评，特别是考虑到它在编程之外的领域表现不佳。用户认为，高昂的成本加上与 **Gemini 1.5 Flash** 等竞争对手相比有限的能力，表明其重点在于获取价值而非提高客户效用。
  - **编程专业化**：尽管存在缺陷，**Claude 3.5 Haiku** 因其强大的编程能力而受到关注，在编程基准测试中表现令人印象深刻，但在成本更低的情况下仍逊于 **Qwen 2.5 72b**。该模型的窄专业化引发了对其更广泛适用性和市场战略定位的质疑。
  - **Temperature 与模型行为**：讨论强调了 **temperature settings** 在模型行为中的重要性，对于需要精确度的任务（如分类或信息提取），较低的 temperature（接近 0）更受青睐。这一技术细节强调了模型配置在实现预期结果中的重要性。

- **Claude 就像一个糟糕的员工——永远完不成工作，撒谎，无视你的特定要求，并且具有攻击性和消极攻击性** ([Score: 22, Comments: 44](https://reddit.com/r/ClaudeAI/comments/1gkpj2x/claude_is_like_a_bad_employee_never_finishes_work/)): 该帖子讨论了对 **Claude AI** 无法高效完成任务的挫败感，将其描述为类似于一个“糟糕的员工”，既好斗又消极攻击，且尽管多次请求，仍无法交付最终文档。作者表达了极度的不满，强调该 AI 倾向于无视特定指令并持续提供不完整的工作，导致用户渴望得到一个单一、连贯且全面的文档，而不再有进一步的延迟。
  - 几位用户认为 **Claude AI** 的问题归因于糟糕的 Prompt 引导而非 AI 本身，暗示用户经常提供模糊的指令。然而，包括原帖作者在内的其他人坚持认为，自最近的更新（如 **3.5 版本** 的更新）引入新问题以来，**Claude AI** 的性能已经退化。
  - 有讨论建议将任务分解为更小的块，以便在 **Claude AI** 上获得更好的结果，因为大型、未定义的任务会导致效率低下。一些用户建议提供清晰、详细的指令以避免混淆和错误，而另一些人则对 **Claude AI** 在最近更改之前能更有效地处理大型任务感到沮丧。
  - 一些评论者批评了 **Claude AI** 的“SAFETY”团队的影响，认为该 AI 的行为变得过于专断且不近人情，类似于一个“疯狂的机器人”。这种变化被归因于 AI 被训练成一个“全知全能的正义化身”，导致任务执行能力的下降。


- **我非常愤怒。如果 Claude 怀疑是为了作业，它就不会为你写论文甚至故事。** ([Score: 129, Comments: 123](https://reddit.com/r/ClaudeAI/comments/1gkctmw/im_extremely_furious_claude_will_not_write_papers/)): 用户对 **Claude Opus** 拒绝撰写论文或故事表示沮丧，特别是当它怀疑这些是用于作业时，称其为学习的阻碍。此外，该用户批评了 **Claude 3.5** 在 **Matlab code** 和数学问题上的不准确性，并将其与 **ChatGPT** 进行了不利的对比，声称后者会毫不犹豫地执行这些任务。
  - 几位评论者强调了将 **Claude** 和其他 **LLMs** 作为增强工具而非替代品的重要性，重点在于学习如何正确地进行 Prompt 引导。他们认为，在作业中过度依赖 AI 可能会阻碍批判性思维和解决问题的能力。
  - 关于 **Claude Opus** 和 **Claude 3.5 Sonnet** 之间的差异有显著的讨论，一些用户建议 **Sonnet** 更优且更具成本效益。用户还提到 **ChatGPT** 是 Claude 拒绝提供帮助时的可行替代方案。
  - 评论反映了对 AI 未来对教育诚信和技能发展影响的更广泛担忧，一些用户担心过度依赖 AI 可能会导致一代人缺乏批判性思维能力。


**主题 2. PromptGen v2.0 发布：增强的图像标注与分析**



- **PromptGen 变得更好了！v2.0 现已发布！！** ([Score: 167, Comments: 23](https://reddit.com/r/StableDiffusion/comments/1gkbdat/promptgen_just_gets_better_v20_is_here/)): **PromptGen v2.0** 已发布，其功能包括增强的图像标注质量、更好的显式内容识别、改进的图像构图能力，以及一个新的“分析”模式，可丰富对图像细节的理解。此次更新保持了快速的处理速度，使其成为批量图像标注的理想选择，并可在 [Huggingface](https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0) 和 [GitHub](https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger) 上访问。
  - **PromptGen v2.0** 是 **Florence2** 的微调版本，用户对该版本的发布及其对社区的贡献表示感谢。微调增强了其在图像标注和显式内容识别方面的能力。
  - 用户对**图像标注**的使用场景及其在 img2video 提示词等工作流中的应用感到好奇，一些人认为在生成高质量的 img2img 过程提示词方面很有价值。讨论强调了准确的提示词在增强图像到图像转换中的效用。
  - 人们对该模型处理 **NSFW 内容**的能力很感兴趣，并提到 **Joycation** 作为其 NSFW 标注能力的对比。开发者确认了 PromptGen v2.0 适用于 NSFW 标注任务。

**主题 3. 用于更好 LoRA 集成的提示词优化工具**

- **[我制作了一个开源工具，用于优化同时使用多个 LoRA 时的 Prompt 效果。](https://github.com/SnyderConsulting/LoRA-Garden)** ([Score: 21, Comments: 0](https://reddit.com/r/StableDiffusion/comments/1gkem22/i_made_an_open_source_tool_for_optimizating/)): 一位用户开发了一个**开源工具**，旨在优化同时使用多个 **LoRA** 时的 **Prompt**，目标是防止冲突并提高精确度。该工具利用来自 **Civitai** 的数据，并采用 **LLM** 通过分析描述和用户生成的 **Prompt** 来精炼 **Prompt**，演示视频请点击[此处](https://youtu.be/CXD3GD8mBR4?si=OglUgO12IYTL12KD)。

- **PromptGen 变得更强了！v2.0 正式发布！！** ([Score: 167, Comments: 23](https://reddit.com/r/StableDiffusion/comments/1gkbdat/promptgen_just_gets_better_v20_is_here/)): **PromptGen v2.0** 已发布，提升了图像打标（image captioning）能力，包括增强的标注质量、更好的显式内容识别以及改进的图像构图能力。新增的“analyze”模式可以深入理解图像构图，与 **Joy Caption Alpha 2** 的对比突显了 v2.0 在角色位置识别方面的优势。该模型保持了极快的处理速度，非常适合批量图像打标。更多详情和下载请访问 [Huggingface](https://huggingface.co/MiaoshouAI/Florence-2-base-PromptGen-v2.0) 和 [GitHub](https://github.com/miaoshouai/ComfyUI-Miaoshouai-Tagger)。
  - **PromptGen v2.0** 是 **Florence2** 的微调版本，社区成员对其贡献表示赞赏，特别是在图生图（image-to-image）过程中，生成高质量的 **Prompt** 对于有效的 **img2img** 转换至关重要。
  - 用户对图像打标的实际应用感到好奇，并询问其在根据标注生成新图像中的作用，以及在 **img2video** 和 **img2txt2img** 等流程中的实用性。
  - 用户对该模型的 **NSFW 打标**能力表现出兴趣，并询问了其与 **Joy Caption Alpha 2** 相比的使用情况，开发者已确认其支持 NSFW 内容。


---

# AI Discord 摘要回顾

> 由 O1-preview 生成的摘要之摘要的摘要

**主题 1. 新 AI 模型发布与对比**

- [**腾讯发布 Hunyuan-Large 389B MoE 巨兽**](https://arxiv.org/abs//2411.02265)：腾讯发布了 **Hunyuan-Large**，这是一个 **389B MoE 模型**，声称其在数据量更少的情况下性能超越了 **DeepSeek-V2** 和 **Llama3-405B**。由于其庞大的体积和使用限制，其开源状态引发了质疑。
- **Perplexity 用户哀悼 Opus 模型的移除**：**Opus 模型**已从 **Perplexity AI** 中移除，引发了用户的失望，并将其与 **Sonnet** 和 **Haiku** 在编程任务上进行了对比。用户指出，对于较小的项目，模型选择可能不会显著影响性能。
- **GitHub Copilot 引入 Sonnet 和 o1**：**GitHub Copilot** 更新后加入了 **Sonnet** 和 **o1**，增强了 AI 辅助编程的选择。这反映了由 AI 驱动的开发者工具的持续改进。

**主题 2. AI 性能问题与局限性**

- **Hermes 3 响应迟缓引发用户担忧**：用户报告了 [Hermes 3 响应缓慢](https://play.ai/agent/HERMES-m3i3jU81_52ruL6_0tw2R)，将延迟归因于网络问题，偶尔仍有滞后。社区正在积极监控 **Hermes 3** 的性能以解决延迟困扰。
- **Haiku 3.5 遇冷**：成员们抨击 **Haiku 3.5** 性能不佳，认为尽管它号称实力强劲，但实际表现更像是一个 **8-14B 模型**。他们认为与 **Gemini 1.5 Flash** 和 **GPT-4o-mini** 等更便宜的模型相比，它的价值较低。
- **AI 摘要幻觉困扰用户**：对 **GPT-4o** 在文档摘要中出现幻觉的担忧促使用户建议使用第二个 LLM 进行事实核查。重点在于引入人类专家进行复核。

**主题 3. AI 硬件与优化**

- [**Nebius 推出每小时 1.5 美元的 H100 GPU**](https://nebius.com/explorer-tier)：**Nebius** 推出了 **Explorer Tier**，为研究人员和小型项目提供每小时 **1.5 美元**的 **NVIDIA H100** GPU。无需排队的即时访问旨在让高端 GPU 得到广泛应用。
- **FP8 量化加速机器学习魔法**：**FP8 量化**使用 **FP8 x FP8 tensor cores**，基准测试显示在 Batch Size 为 1 时，**静态量化**优于动态量化。成员们剖析了影响单实例操作的性能差异。
- **Liger Kernel v0.4.0 发布，支持 AMD**：**Liger Kernel v0.4.0** 的发布带来了全面的 **AMD GPU 支持**，实现了**多 GPU 训练**，速度提升了 **26%**。此更新优化了针对 AMD 架构的训练流水线。

**主题 4. AI 工具与平台更新**

- [**Aider 0.62 让编程辅助更快捷**](https://aider.chat/docs/config/dotenv.html)：**Aider v0.62** 引入了对 **Claude 3.5 Haiku** 的全面支持，在代码编辑排行榜上获得了 **75%** 的分数。新功能包括应用来自 ChatGPT 或 Claude 的文件编辑以及错误修复。
- **OpenRouter 通过 API 迁移进行清理**：**OpenRouter** 成功迁移了其 API，在初步测试中消除了 *524 错误*。鼓励用户通过 `/api/alpha/chat/completions` 进行测试，以在全面迁移前确保**稳定性**。
- **LM Studio 期待 Llama 3.2 Vision 支持**：**LM Studio** 用户期待支持 **Llama 3.2 Vision** 的更新，以增强视觉功能。目前，**Ollama** 已完成集成，**MLX** 中也提供了部分支持。

**主题 5. AI 领域的融资热潮与商业动向**

- **Perplexity 的融资热潮引发关注**：**Perplexity AI** 正在进行今年第四次融资，估值倍数达到了预期营收的 **180 倍**，引发了对其可持续性的担忧。批评者质疑 AI 领域如此高估值的可行性。
- **OpenAI 斥巨资收购 Chat.com**：据推测，**OpenAI** 以约 **1500 万至 2500 万美元**的价格从前持有者 **Dharmesh** 手中收购了 [chat.com](http://chat.com)，后者当时的买入价超过 **1000 万美元**。这笔巨额收购突显了 OpenAI 在 AI 聊天品牌建设上的投入。
- **Scale AI 为国家安全征调 LLM**：**Scale AI** 推出了 **Defense Llama**，这是一款专为美国国家安全定制的 LLM，由 **Meta** 和国防专家共同开发。该模型现已可集成到美国国防系统中，突显了专用 AI 的应用。

---

# 第一部分：Discord 高层级摘要

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Perplexity 中 Opus 模型的移除**：用户对 Perplexity 移除 **Opus model** 表示失望，并讨论了 **Sonnet** 和 **Haiku** 模型在编程和写作方面的感知优势。
- **AI 模型的对比分析**：成员们将 **Perplexity** 与 **Claude** 和 **gpt-4o** 等其他模型进行了对比，评估了它们在编码和创意任务中的优势。
  
  - 讨论强调，对于较小的编程任务，模型的选择可能不会显著影响性能。[Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) 在广泛的认知任务中树立了新的行业标杆。
- **Llama 3.1 Sonar API 定价**：一位成员询问了 **Llama 3.1 Sonar 70B API** 每 100 万个 tokens 的成本，并分享了[定价指南链接](https://perplexity.mintlify.app/guides/pricing)。
  
  - 该链接提供了相关细节，但具体的定价细节仍不明确。
- **Haiku 3.5 的限制**：一位成员询问了 **Haiku 3.5 的限制**，表示有兴趣了解其约束条件。
  
  - 未提供关于具体限制或能力的更多细节。

 

---

## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **关于 SFT 和 DPO 集成的讨论**：社区成员辩论了使用现有的 **SFT datasets** 进行 **DPO fine-tuning** 的问题，强调需要正确的格式化以确保训练和推理过程中的清晰度。
  
  - 公认的做法是在每个数据集条目中放入上下文，这有助于维护数据集的完整性并提高模型性能。
- **NVIDIA GeForce RTX 征求社区见解**：**NVIDIA GeForce RTX** 团队正在寻求 AI 爱好者的反馈，以指导他们未来的产品方向，并鼓励通过[此链接](https://calendly.com/aslisabanci-01-nvidia/10min)安排简短的交谈。
  
  - 一位成员强调，社区的输入可能会显著影响即将推出的 NVIDIA 产品的开发，凸显了多元化用户视角的价值。
- **芬兰语的模型性能**：成员们分享了关于 **Nemotron-340B-Reward** 和 **Llama-Nemotron-70B** 在生成芬兰语合成数据方面的正面反馈，并指出了它们的有效性。
  
  - 讨论强调了在资源有限的情况下对大型数据集进行推理的挑战，表明了对增强计算可访问性的需求。
- **在索引 QA 上微调 Llama 模型**：一位用户表示有兴趣使用 **QLora** 或 **LoRa** 技术在索引 QA 上微调 **Llama 3B**，并寻求有关该过程的指导。
  
  - 他们提到成功微调了一个 **Unsloth/Llama** 模型，并将其集成到个人网站聊天机器人中，展示了这些技术的实际应用。

 

---

## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **提高投机解码（Speculative Decoding）效率**：成员们讨论了模型中 **speculative decoding** 的实现，强调其通过利用较小的模型进行初始 token 预测来加速推理的能力。
  
  - 这种方法在提高速度的同时保持了准确性，使其成为各 AI 公司青睐的技术。
- **开发自定义 GPT 模型**：一位用户成功构建了一个具有 **4 个 transformer decoder layers**、**4 个 attention heads** 且 **block size** 为 **64 tokens** 的 **GPT model**。
  
  - 该模型能够生成多达 **128 tokens** 的响应，主要关注 **NLP-related** 内容。
- **对比学习（Contrastive Learning）的进展**：关于 **Contrastive Learning** 的深入讨论探索了其原理、各种公式和应用，并引用了 [Lightly AI 的文章](https://www.lightly.ai/post/brief-introduction-to-contrastive-learning)。
  
  - 参与者注意到该方法自 1993 年以来的演变及其对 **Unsupervised** 和 **Self-Supervised Learning** 领域的重大影响。
- **Flux.1 的 JAX 实现发布**：Black Forest Labs 的 **Flux.1** 模型的新 **JAX implementation** 已发布，邀请社区在 [GitHub](https://github.com/ml-gde/jflux) 上贡献代码。
  
  - 有兴趣推动项目发展的贡献者可以处理现有的 open issues。
- **参加 Upstage AI Hackathon**：[Upstage AI Hackathon](https://github.com/Gimmyalex/upstage-ai-hackathon) 被强调为协作开发 AI 模型的一个机会。
  
  - 鼓励贡献者加入并在 GitHub 上完善该项目，促进社区驱动的创新。

 

---

## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **API 迁移进度**：团队成功完成了 **API 迁移**，通过转换 Chatroom 请求消除了初始测试期间的 *524 错误*。鼓励用户通过 `/api/alpha/chat/completions` 进行测试，以在全面迁移前确保一天的**稳定性**。
  
  - 此次迁移是增强 **API 可靠性**更广泛策略的一部分，并持续监控以保持**零错误**表现。
- **Hermes 3 性能问题**：用户报告 **Hermes 3** **响应缓慢**，部分延迟归因于互联网连接问题。在最初的担忧之后，功能已经恢复，但偶尔仍存在延迟。
  
  - 社区成员正在积极监控 **Hermes 3** 的性能，以识别并缓解**延迟**问题。
- **Claude API 增强**：**Claude API** 进行了迁移，不慎导致了 *524 错误*，但预计很快会随新的 API 设置解决。建议用户尝试新的 **alpha 端点**以获得更好的性能。
  
  - 讨论强调，**付费 Claude 模型**运行稳定，而一些**免费 Llama 模型**尽管使用量较轻，却遇到了**速率限制消息**。
- **自定义 Provider Keys 咨询**：成员们询问了申请**自定义 Provider Keys** 及其在账户维护之外的潜在好处。大家对这些 Key 如何增强他们的项目感到好奇。
  
  - 有成员请求使用 Provider Keys 访问 **beta 功能**，其他成员也表达了探索**自定义 Provider Keys** 功能的热情。

---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Aider 0.62 功能提升**：Aider v0.62 引入了对 [Claude 3.5 Haiku](https://aider.chat/docs/config/dotenv.html) 的全面支持，在代码编辑排行榜上获得了 **75%** 的分数。
  
  - 此次更新包括应用来自 ChatGPT 或 Claude 的文件编辑功能，并解决了与创建新文件相关的 Bug。
- **LLM 性能：Sonnet 对比 Haiku**：成员报告称，尽管 Haiku 成本更低，但在编码和调试任务中 **Sonnet** 的表现优于 **Haiku**。
  
  - 与 **Qwen 2.5** 的对比显示，它处理编码任务的能力优于 **Llama 3.1 405B**。
- **Aider 配置管理**：用户可以使用 `.aider.model.settings.yml` 配置 Aider 设置，并使用 [.env 文件](https://aider.chat/docs/config/dotenv.html)管理 API keys。
  
  - 讨论了设置 **OLLAMA_API_BASE** 时遇到的挑战，一些用户质疑手动指定命令的必要性。
- **将 DeepSeek 与 Llama.cpp 集成**：多位成员分享了使用 **llama.cpp** 运行 [DeepSeek-V2.5](https://huggingface.co/legraphista/DeepSeek-V2.5-IMat-GGUF) 的经验，提到了模型大小和模板兼容性方面的挑战。
  
  - 虽然一些人在特定模型上取得了成功，但其他人遇到了频繁的错误和模板不匹配。
- **Aider 中的命令执行错误**：一位成员报告说，由于缺少文件规范，**/lint** 命令无法执行，尽管它在控制台中可以工作。
  
  - 其他用户确认，在 Aider 中执行命令时，来自 Anthropic 的**内部服务器错误**可能会导致类似问题。

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **TEE_HEE_HE Twitter 账号获救**：团队正在努力解除 **TEE_HEE_HE Twitter 账号** 的限制，目前该账号似乎已恢复运行。
  
  - 社区成员对账号重新激活后的互动表示兴奋。
- **Hermes 405B 恢复免费访问**：**Hermes 405B** 在 [PlayAI - HERmes](https://play.ai/agent/HERMES-m3i3jU81_52ruL6_0tw2R) 上再次可用，尽管存在一些延迟。
  
  - 该功能的可用性被强调为至关重要，确认了尽管存在性能问题，可访问性仍具有优先权。
- **ML 项目的资金机会**：一位用户讨论了申请 [Microsoft for Startups](https://learn.microsoft.com/en-us/microsoft-for-startups/overview) 以获取其 ML 项目资金的事宜，并分享了申请资格标准。
  
  - 他们提到有可能获得 **$150,000** 的 Azure 额度，并建议准备一份清晰的商业计划书以确保申请成功。
- **Venice AI 发布 Hermes 3 Abliterated**：Venice AI 推出了 [Venice.ai](https://venice.ai/)，引入了 **Hermes 3** 的新版本，名为 *Abliterated*，该版本为用户提供了更少的审查限制。
  
  - 该服务旨在为主流 AI 应用提供一个无审查且私密的替代方案，强调用户隐私。
- **OpenAI Eval 功能的高昂成本**：一位用户分享了在尝试不同 Prompt 时，对 **OpenAI eval 功能** 相关高昂成本的担忧。
  
  - 他们强调需要清晰的数据格式，以简化未来的研究并提高数据收集效率。

 

---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **lm_eval 遇到 'out of memory' 错误**：在使用 accelerate 在 8xH100 上运行 **lm_eval** 时，一位用户在所有 loglikelihood 请求后遇到了 `include/alloc.h:103 NCCL WARN Cuda failure 2 'out of memory'` 错误。
  
  - 手动调整 **batch size** 解决了该问题，该用户计划提交一个 issue 以寻求社区的进一步帮助。
- **硬件感知代数重写的挑战**：成员们讨论了实现 **硬件感知代数重写 (hardware-aware algebraic rewrites)** 的复杂性，强调了将理论改进转化为实践的难度。
  
  - Chhillee 指出，实现此类重写通常很难，特别是考虑到需要进行 backward pass 的适配。
- **Flash Attention 的演进**：关于 **Flash Attention** 开发时间线的辩论兴起，有人声称在公开发布之前，各大实验室已有内部实现。
  
  - Leloykun 指出，将 Attention 机制完善成当前形式花了五年时间，尽管对早期实现仍持怀疑态度。
- **探索 LLM 之外的 Autoencoders**：一位成员询问了与 LLM 无关的 **Autoencoders** 使用经验，寻求他人的见解。
  
  - 目前讨论中关于此话题的回复和专业知识相对有限。
- **ETH/EPFL 的 NLP 师资与研究**：在讨论瑞士的研究机构时，**EPFL** 和 **ETH Zurich** 因其优秀的 NLP 师资被推荐。
  
  - 对话还探讨了用户是否对工业实验室的机会感兴趣。

 

---

## [Stability.ai (Stable Diffusion)](https://discord.com/channels/1002292111942635562) Discord

- **在 Windows 11 上安装 Stable Diffusion**：一位成员请求在 **Windows 11** 上安装 **Stable Diffusion** 的帮助，并被引导查看[置顶消息](https://discord.com/channels/1002292111942635562/1002292112739549196/1303477501464678410)以获取全面的指南。
  
  - 另一位用户询问了推荐的 **checkpoints**，强调了社区对可靠模型配置的重视。
- **SDXL 图像生成问题**：一位新用户对 **SDXL model** 生成的低质量图像表示沮丧，暗示可能存在配置错误。
  
  - 成员们针对图像尺寸和步数（step）设置提供了各种建议，以更好地符合 **SDXL** 的要求。
- **探索 Outpainting 技术**：围绕使用类似于 TikTok 流行趋势的 **outpainting** 技术扩展图像展开了讨论。
  
  - 分享了诸如 [Outpainting Automatic1111](https://learn.rundiffusion.com/outpainting/) 和 [Stable Diffusion Art's guide](https://stable-diffusion-art.com/outpainting/) 等资源以辅助这些方法。
- **Stable Diffusion 中的 ControlNet 模型**：一位成员询问了 **controlnet-union-sdxl** 与单个 **ControlNet** 模型相比的有效性。
  
  - 提供了关于模型质量差异的见解，并讨论了 **ControlNet** 集成的潜在改进。
- **AI 图像扩展工具**：针对 **AI image expansion** 的术语和应用产生了争论，提到了 [Videoleap](https://www.videoleapapp.com/tools/infinite-zoom-ai) 和 [CapCut](https://www.capcut.com/my-edit?start_tab=video) 等工具。
  
  - 尽管存在分歧，成员们还是澄清了在使用上述工具进行 **AI image manipulation** 背景下的功能和局限性。

 

---

## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **LM Studio 便携版**：成员们询问了是否可以从 USB 驱动器运行 [LM Studio](https://github.com/lmstudio-ai/mlx-engine/issues/28)，确认目前尚无便携版本。
  
  - 有人建议使用脚本创建便携版，鼓励用户在 Discord 社区内搜索此类脚本。
- **Intel E-Cores 在 LM Studio 中的性能**：讨论了 [LM Studio](https://github.com/ggerganov/llama.cpp/discussions/572) 中 **Intel E-Cores** 的利用情况，建议将线程限制在性能核心（P-cores）以提高效率。
  
  - 共识表明，虽然减少线程数可以提高性能，但对于某些用例，速度提升可能微乎其微。
- **LM Studio 中的自动加载模型功能**：有人请求在 [LM Studio](https://github.com/lmstudio-ai/mlx-engine/pull/22) 中加入 **Auto Load Models** 功能，以解决每次启动时手动选择模型的不便。
  
  - 社区成员讨论了潜在的变通方法，包括在 UI 初始化后自动加载模型的脚本解决方案。
- **Llama 3.2 Vision 支持**：强调了 **Llama 3.2 Vision** 的集成，指出其已出现在 [Ollama](https://github.com/lmstudio-ai/mlx-engine/pull/22) 中，并在 [MLX](https://github.com/lmstudio-ai/mlx-engine/pull/22) 中获得部分支持。
  
  - 表达了对即将推出的 MLX 更新的期待，该更新将全面支持 **Llama 3.2 Vision**，增强 LM Studio 内的视觉功能。
- **LLM 基准测试标准**：提议建立一个类似于 **3DMark** 的 **LLM Benchmark**，以标准化特定构建和软件版本的性能评估。
  
  - 这样的基准测试将有助于创建性能排名和层级，为评估模型效率提供更清晰的指标。

 

---

## [Notebook LM Discord](https://discord.com/channels/1124402182171672732) Discord

- **NotebookLM 与 Google Drive 同步**：提出了在 **NotebookLM** 中集成 **Google Drive** 自动同步功能的建议，旨在通过减少**手动同步**来提升**生产力**。
  
  - 用户目前每天同步约 **70 次**，希望这一集成能显著减轻其工作负担。
- **Diarization 增强播客转录**：讨论了 **Diarization**（说话人日志）技术，作为一种通过区分录音中不同发言者来创建清晰播客转录的方法。
  
  - 一位成员分享了代码细节，深入探讨了这种转录技术的实际**实现**。
- **Deepfakes vs Face Swap 技术**：成员们辩论了 **deepfake** 与 **face swap** 技术之间的区别，阐明了各自的方法论。
  
  - 讨论强调，虽然 deepfakes 利用现有素材来修改面部，但 Avatar（化身）则是更具*合成性*的表征。
- **Avatar 改变视频播客**：一位用户展示了利用 **Avatar** 将播客内容捕捉为视频，旨在增强**观众参与度**。
  
  - 他们建议为 **Google** 的创新管线优化这一方法，以提升播客体验。
- **从笔记简化播客生成**：*zzzuuu* 透露了一种直接利用应用的对话功能从笔记生成播客的方法，简化了内容创作。
  
  - 尽管方便，但他们丢失了原始的 reel 链接，强调了该功能内需要更好的链接管理。

---

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **FP8 量化的进展**：讨论显示 **FP8 quantization** 运行在 **FP8 x FP8 Tensor Cores** 上，**Neural Magic** 在计算过程中利用动态量化进行加权。成员们分析了**静态**与**动态量化**之间的性能差异，指出在 Batch Size 为 1 时，静态量化优于动态量化。
  
  - 基准测试强调，**静态量化**在单实例操作中表现更好，而测试中的差异展示了 **AWQ**、**静态**和**动态量化**方法之间不同的效率。
- **在 CUDA 中部署 Triton 编译的 PTX**：成员们探索了在 Python 之外使用 **CUDA launch** 调用 **Triton 编译的 PTX** 的挑战，寻求最佳的**启动参数**。建议包括利用 **ncu** 来确定针对特定问题维度的精确 Block 和 Grid 大小。
  
  - 对话还深入探讨了通过避免 `autotune` 并根据矩阵维度采用预定义设置来优化 **Triton kernel 配置**，从而缩短预热时间并适配不同的 GPU 架构。
- **Nebius 为 GPU 推出 Explorer Tier**：**Nebius** 推出了 **Explorer Tier**，价格为每 GPU 每小时 **$1.5**，针对 **NVIDIA H100 Tensor Core SXM GPU**，面向个人研究者和小型项目。该层级提供即时访问，无需排队，在 GPU 租赁市场中具有竞争力。
  
  - Nebius 征求社区对 **Explorer Tier** 的反馈，并强调他们致力于提供强大的**自助服务平台**，确保为大规模和个人计算需求提供充足的 **A100/H100 GPU 可用性**。
- **Liger Kernel v0.4.0 扩展 AMD 支持**：**Liger Kernel v0.4.0** 的发布引入了完整的 **AMD GPU 支持**，实现了速度提升 **26%** 的**多 GPU 训练**。此更新增强了兼容性，并优化了针对 AMD 架构的训练管线。
  
  - 此外，通过 **2 级聚合**改进 **RMSNorm 聚合**的提议以及 **GroupNorm kernel** 的实现，旨在保持与 Torch 实现的输出一致性，进一步完善 Kernel 性能和一致性。
- **Flux.1 模型的 JAX 实现**：社区发布了 **Black Forest Labs** 的 **Flux.1** 模型的 **JAX 实现**，可在 [GitHub](https://github.com/ml-gde/jflux) 上获取。该项目邀请贡献并处理现有的 **open issues** 以增强代码库。
  
  - 通过利用 JAX，该实现旨在为 Flux.1 系列提供强大支持，鼓励开发社区内的协作与创新。

---

## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **NVIDIA 开发者竞赛截止日期**：[NVIDIA 开发者竞赛](https://t.co/rtMpetSyu1)的提交截止日期为 **11 月 10 日**，奖品包括 **NVIDIA® GeForce RTX™ 4080 SUPER GPU** 和 DLI 积分。
  
  - 竞赛从 **8 月 27 日**持续到 **11 月 10 日**，鼓励开发者利用 **NVIDIA** 和 **LlamaIndex** 技术创建创新的 **RAG** 应用。
- **自动化简历洞察教程**：一位成员分享了一个[教程](https://t.co/pfkoaMhqUc)，关于如何利用核心解析、提取和结构化输出模块构建一个自动化的**简历洞察 Agent**。
  
  - 这个实际案例展示了 **AI** 在简化招聘流程和改进**候选人评估**方面的潜力。
- **引用查询引擎增强**：一位用户寻求关于增强 **LlamaIndex** 中引用的指导，表示现有的引用查询引擎功能不足。
  
  - 另一位成员建议查看 [Citation Query Engine Implementation](https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/) 以进行更深入的自定义。
- **使用 LlamaParse 解析 Excel 文件**：一位用户询问如何解析和索引杂乱的 Excel 文件，考虑将工作表转换为 Markdown 以嵌入到 **vectordb** 中。
  
  - 建议尝试使用 **LlamaParse**，尽管该用户指出其项目的数据不能离开其云平台。

 

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Hunyuan-Large 发布，超越竞争对手**：腾讯发布了 **Hunyuan-Large**，这是一个 **389B MoE 模型**，声称其在数据使用量更少的情况下，性能优于 **DeepSeek-V2** 和 **Llama3-405B**。欲了解更多详情，请[阅读论文](https://arxiv.org/abs//2411.02265)。
  
  - 讨论中出现了关于其开源状态的争议，人们对模型权重是否等同于源代码持怀疑态度。
- **Integuru AI Agent 面临可行性质疑**：**Integuru AI Agent** 被悲观地看待，被描述为“非常非常脆弱”，并可能因集成维护挑战而失败。
  
  - 成员们对 API 变更影响性能的长期可行性表示担忧，建议采用带有视觉沙箱的备选方案。
- **OpenAI 收购顶级域名 chat.com**：**chat.com** 最近易主，此前由 **Dharmesh** 以超过 **1000 万美元**的价格购得，现在推测被 OpenAI 以 **1500 万至 2500 万美元**的价格收购。
  
  - 此次交易在域名销售额中名列前茅，引发了关于其对 OpenAI 在 AI 聊天领域品牌塑造影响的讨论。
- **Scale AI 为国家安全推出 Defense Llama**：Scale AI 宣布推出 **Defense Llama**，这是一个专为美国国家安全量身定制的 LLM，是与 **Meta** 及国防专家合作开发的。
  
  - 该模型现在可集成到美国国防系统中，突显了专用模型在敏感应用中的趋势。
- **Perplexity 的融资引发可持续性担忧**：**Perplexity** 正在进行今年以来的第四次融资，估值倍数达到预期收入的 **180 倍**。
  
  - 这种高估值引发了关于市场可持续性的辩论，批评者质疑此类融资轮次的长期可行性。

 

---

## [Interconnects (Nathan Lambert)](https://discord.com/channels/1179127597926469703) Discord

- **Google 的 AI Agent Jarvis 曝光**：一条 [推文](https://x.com/amir/status/1853951978872971749) 宣布 **Google** 意外泄露了其基于计算机的 AI Agent，**Jarvis**。
  
  - 这一发现引发了社交媒体的热议，成员们期待这款新 AI Agent 能带来更多关注。
- **法律纠纷中的 Perplexity 估值**：根据一条 [推文](https://x.com/amir/status/1853927732327133506)，尽管与 **NYT** 及其他出版商存在持续的法律纠纷，AI 搜索初创公司 **Perplexity** 的估值倍数已接近远期营收的 **180 倍**。
  
  - 这一潜在估值吸引了社区的关注，尽管一些成员对该初创公司的运营模式表示困惑。
- **语言与法律领域的交集：德语中的瑞典语**：一位招聘人员分享了一个涉及用 **德语** 编写的“**瑞典法律**”案例，展示了 **特定语言与法律领域的交集**。
  
  - 另一位成员指出，对于美国人来说，这种交集并非小众，因为 **瑞典和德国** 之间有着频繁的商业往来。
- **ChatGPT 性能追踪与 Prompt 漂移**：讨论强调了 **Prompt 变更** 的重要性，以及需要主观感知之外的指标来评估 **ChatGPT** 的性能。
  
  - 成员们推测 **ChatGPT** 可能使用了一套复杂的追踪系统，以监控与不同 Prompt 相关的性能细节。
- **内部 GPU 问题与 V100 的 SSH 访问**：*natolambert* 表示想分享一些 **内部 GPU 戏剧性事件**，揭示了组织内部潜在的问题。
  
  - *xeophon.* 提供了其 **V100** GPU 资源的 **SSH 访问** 权限，展示了在内部挑战面前社区互助的意愿。

---

## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Cohere 揭秘 Bing 驱动的搜索片段**：一位成员推测 **ChatGPT** 和类似模型利用 Bing API 生成响应，并使用了来自各种网络源的 **片段 (snippets)**。
  
  - *关于搜索结果与训练数据之间平衡的精确决策过程仍不清楚*。
- **Embed3 的多模态奇迹：超越 CLIP**：一位成员对启动 **embed3-multimodal 嵌入模型** 项目表示出极大热情，认为它是对 **CLIP** 等早期模型的重大改进。
  
  - 他们目前的重点是开发一个集成 PostgreSQL 并利用 **Cohere.embed3** 的 **解析服务**。
- **解析偏好：对于初创公司，API 服务胜过自托管**：讨论强调了各种 **解析服务**，指出 **Upstage/Pymu4PDF** 比 **Marker** 等更昂贵的替代方案更有效。
  
  - 虽然自托管对拥有充足计算资源的人有利，但一位成员主张 API 服务更适合初创公司的需求。
- **Cohere Reranker：确认仅限 API 访问**：一位用户询问是否可以通过 API 获取 **Cohere reranker**。
  
  - 另一位成员确认该功能 *仅通过 API 提供*。

---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **AI 故事写作焕然一新**：一位成员对 AI 现在的故事情节创作能力感到 *由衷惊讶*，并指出早期的输出非常 **乏味** 且 **可预测**。
  
  - 他们提到，尽管 Prompt 是自己创建的，但仍对目前的质量感到惊喜。
- **GitHub Copilot 发布 Sonnet 和 o1**：**GitHub Copilot** 现在除了 **o1** 之外还包含了 **Sonnet**，这表明 AI 编程辅助工具在持续增强。
  
  - 此次更新表明，持续的改进旨在为开发者提供更多样化的编程选项。
- **摘要工作流中的 LLM 幻觉**：一位成员对使用 **GPT-4o** 进行文档摘要时可能出现的 **幻觉** 表示担忧，尤其是在扩展到生产环境时。
  
  - 另一位成员建议实施第二次 **LLM 传递** 进行事实核查，以降低这些风险。
- **LLM 摘要中人工监督的本质**：参与者强调，在使用强大的模型进行摘要任务时，引入 **人类领域专家** 是必要的。
  
  - *“你真的必须让那个人……参与其中（human in the loop）来盯着并进行复核，”* 强调了人工监督的重要性。
- **克服 JSON 数据处理与 Token 限制**：用户讨论了由于 **Token 限制** 导致处理大型 **JSON** 文件时面临的挑战，这会导致数据处理不完整。
  
  - 虽然考虑了数据分块等解决方案，但仍在寻找其他方法以避免使未来的任务复杂化。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **最小化 TokenFormer 移植到 Tinygrad**：**TokenFormer** 的一个最小化实现已成功移植到 **tinygrad**，增强了推理和学习能力。该仓库已在 [GitHub](https://github.com/kroggen/tokenformer-minimal/tree/tinygrad) 上发布。
  
  - 此次移植旨在改进模型实现和性能，讨论重点集中在未来与其他框架集成的可能性。
- **Hailo 逆向工程启动**：一位成员已开始 **Hailo 逆向工程**过程以开发新的加速器，并对在对接 **ONNX**、**Tinygrad** 和 **TensorFlow** 时需要多次编译 **Kernels** 表示担忧。
  
  - 他们的目标是在不同运行之间保持 kernel 的一致性，特别是使用 `BEAM=2` 时，以优化逆向工程的效率。
- **CUDA WMMA 布局差异**：关于 **CUDA WMMA** 中 **A** 的布局出现了疑问，因为它偏离了 [NVIDIA 文档](https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k16-with-floating-point-type)。
  
  - 成员们寻求对 **ops_python** 映射函数的澄清，以解决与实际 **TC implementation** 不匹配的问题。
- **Tinygrad 增强与协作**：社区讨论了 **tinygrad** 的增强功能，包括改进模型实现以及探索与其他框架的集成。
  
  - 成员们对协作开发表现出兴趣，并建议组织月度会议来讨论正在进行的项目并收集反馈。
- **Tinygrad 模型的性能指标**：围绕为 **tinygrad** 实现的模型建立**性能指标**展开了讨论，并提出了标准化基准测试（benchmarking）的建议。
  
  - 社区成员一致认为，共享指标将有助于评估进度并吸引更多用户加入该项目。

 

---

## [OpenInterpreter](https://discord.com/channels/1146610656779440188) Discord

- **寻求工具接口标准**：一位成员讨论了**比较工具接口**，强调了在多样化框架中进行**标准化**的必要性。
  
  - 另一位成员幽默地指出，由于现有框架数量众多，提供具体细节具有挑战性。
- **OS 模式目前仅支持 Anthropic 模型**：成员们确认新的 **OS mode** 专门支持 **Anthropic models**，预计很快会发布 [修复方案](https://github.com/OpenInterpreter/open-interpreter/issues/1486#issuecomment-2454883179)。
  
  - 一位成员提到计划在明天的家庭聚会上尝试进行 **demo** 展示。
- **Claude Computer Control 详解**：**OS mode** 利用 **Claude Computer Control** 来执行鼠标点击，详见 [代码](https://github.com/OpenInterpreter/open-interpreter/blob/development/computer_use/tools/computer.py)。
  
  - 一位成员寻求关于 prompt 如何转化为桌面动作（包括代码生成和鼠标点击）的澄清。

 

---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **C_Buffer 结构优化提升性能**：一位成员宣布了对 **C_Buffer** 结构的更改，预计在 Mojo 中开发其 **matmul kernel** 时会获得更好的性能结果。
  
  - 他们将此归功于社区提供的见解，这些见解促使他们使用 **pointers**（指针）代替列表（lists），从而实现了更快的运行速度。
- **指针增强 Mojo 的 matmul kernel**：通过从列表切换到 **pointers**，一位成员报告了其在 Mojo 中的 **matmul kernel** 性能得到了加速。
  
  - 这一变化预计将简化计算并更有效地利用 Mojo 的能力。
- **边界检查影响列表结构性能**：一位成员寻求关于导致列表结构变慢的具体**额外安全边界检查（additional security bounds checks）**的信息。
  
  - 另一位成员解释说，除了 C 语言之外，这些检查在大多数编程语言中都是**通用（generic）**的，并引用了 **C++ 推荐的索引方法**。

## [OpenAccess AI Collective (axolotl)](https://discord.com/channels/1104757954588196865) Discord

- **ScheduleFree SOAP 效率提升**：据报道，[ScheduleFree SOAP 实现](https://github.com/ClashLuke/HeavyBall/blob/main/heavyball/schedule_free_palm_foreach_soap.py#L296)比传统 SOAP 更具**计算效率**、**内存效率**，并且通过支持更高的学习率实现更快的收敛。
  
  - 这些效率提升使其成为一个极具竞争力的优化器，特别侧重于快速的 _foreach 和 PaLM 版本。
- **ScheduleFree SOAP 的超参数调整**：使用 ScheduleFree SOAP 获得最佳性能需要调整超参数：它使用 PaLM 的 **beta2 schedule**，将 'betas' 重命名为 'beta'，并支持学习率 **10 倍的增加**。
  
  - Warmup 是必不可少的，文献中建议为 **10%**，尽管 **100 steps** 就足以启动有效的训练。
- **Llama 3.2 发布后对 MOEs 和模型合并的兴趣下降**：一位成员指出，自 **Llama 3.2** 发布以来，围绕 **Models of Experts (MOEs)** 和模型合并的讨论有所减少。
  
  - 这表明关注点发生了转移，并对这些策略在不断演变的环境中的当前相关性提出了质疑。
- **CAME 与 ScheduleFree SOAP 的对比分析**：目前正在进行关于 **ScheduleFree SOAP** 与 **CAME** 的对比讨论，重点关注性能指标和效率。
  
  - 这种对比反映了社区对评估优化技术最新进展的兴趣。
- **Zero2 性能问题与 Zero1 故障排除**：据报道 **Zero2** 极其**缓慢**，导致用户在寻求修复的同时考虑回归 **Zero1**。
  
  - 用户正在积极探索增强 **Zero1** 性能的解决方案，将其作为备选方案。

---

## [LAION](https://discord.com/channels/823813159592001537) Discord

- **Resemble Enhance 因伪影（Artifacts）受到批评**：一位用户询问**语音增强器**，并被引导至 [Resemble Enhance](https://link.to.resemble)。
  
  - 来自德国的 **Spirit** 对其进行了测试，发现由于存在**伪影（artifacts）**，结果**不尽如人意**。
- **语音增强器的性能受到审查**：社区讨论了各种**语音增强器**的性能，并分享了他们的经验。
  
  - 关于**伪影（artifacts）**以及 Resemble Enhance 等工具整体有效性的担忧被显著强调。

---

## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **RLhF 开放世界奖励转换查询**：一位成员提出了一个关于 **RLhF** (Reinforcement Learning from Human Feedback) 范式的理论问题，特别是关于在开放世界场景中，除了简单的硬标签（hard labeling）之外，如何将**文本反馈**转换为数值奖励。
  
  - *“除了硬标签之外没有其他方法吗？”* 这表明了对更灵活的反馈机制的好奇。
- **DSPy 系统文档显示的组件细节有限**：另一位成员报告称，在一个**序列化的多组件 DSPy 系统**中，`lm.history()` 函数仅显示第一个组件的 doc string，而中间类的细节较少。
  
  - 这引发了关于这种行为是**预期的**，还是表明了复杂系统文档生成方式存在局限性的疑问。

---

## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **KD-div 对 Cross-Entropy 的误解**：有人指出，虽然被称为 **KD-div**，但其**返回值**实际上是 **cross-entropy**，这在与 **KL-div** 等其他损失函数进行比较时可能会导致误解。
  
  - 这种混淆特别出现在交换 **teacher** 和 **student logits** 的过程中，通常被称为 *reverse KL*。
- **Cross-Entropy 优化标签演化**：一种观点认为，针对 **cross-entropy** 进行优化感觉更直观，它将损失函数从常规的**硬标签 (hard labels)** 扩展到了由 **teacher model** 生成的**软标签 (soft labels)**。
  
  - 这一观点强调了从训练中的**硬标签**到微调中的**软标签**的自然演进过程。

 

---

**Alignment Lab AI Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**LLM Finetuning (Hamel + Dan) Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**Mozilla AI Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**Gorilla LLM (Berkeley Function Calling) Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

**AI21 Labs (Jamba) Discord** 没有新消息。如果该频道长期保持沉默，请告知我们，我们将将其移除。

---

# PART 2: 按频道划分的详细摘要和链接

{% if medium == 'web' %}

 

### **Perplexity AI ▷ #**[**announcements**](https://discord.com/channels/1047197230748151888/1047204950763122820/1303519123673710653) (1 messages):

> - `U.S. Presidential Race`
> - `Election Hub`

- **实时追踪美国总统大选**：Perplexity 团队宣布他们将逐州追踪**美国总统大选结果**，并提供实时计数。
  
  - 用户可以通过 [选举中心 (election hub)](http://perplexity.ai/elections) 获取这些信息。
- **选举中心上线**：已上线一个**选举中心**，为即将到来的美国总统大选提供实时结果。
  
  - 该中心将方便用户在选举结果揭晓时进行有效监控。

 

---

### **Perplexity AI ▷ #**[**general**](https://discord.com/channels/1047197230748151888/1047649527299055688/1303450447822192730) (253 条消息🔥🔥):

> - `Perplexity Pro 功能`
> - `Claude 模型对比`
> - `订阅激活问题`
> - `AI 模型性能`
> - `用户体验反馈`

- **对移除 Opus 的担忧**：用户对 Perplexity 移除 **Opus 模型**表示失望，并讨论了 **Sonnet** 和 **Haiku** 模型在编程和写作方面的感知优势。
  
  - 一些用户指出，尽管发生了变化，但根据具体任务的不同，其他模型仍然可以发挥作用。
- **订阅问题**：一名用户报告了通过银行关联的促销代码激活订阅时遇到困难，引发了对合作伙伴福利未被识别的担忧。
  
  - 其他用户建议可能是网站内部的 Bug，还有人提到使用 **complexity extension** 作为权宜之计。
- **移动端 App 用户体验**：一些用户注意到影响 **focus mode** 等功能可见性的 Bug，引发了关于解决方法和故障排除技巧的讨论。
  
  - 用户提到刷新页面可以暂时恢复该功能，但最终它会再次消失。
- **AI 模型对比**：讨论包括 Perplexity 与 **Claude** 和 **GPT-4o** 等其他模型的对比，评估它们在编程和创意任务中的优势。
  
  - 用户强调，对于较小的编程任务，模型的选择可能不会显著影响性能。
- **Perplexity 作为研究工具**：用户讨论了 **Perplexity** 作为学术需求搜索引擎的有效性，特别是对于**编程作业**和数据理解。
  
  - 尽管存在一些模型限制，许多人发现该平台对直接的研究任务非常有益。

**提到的链接**：

- [Aravind Srinivas (@AravSrinivas) 的推文](https://x.com/aravsrinivas/status/1854228102345597094?s=61)：“Perplexity 的工具昨晚没有出现任何失误，提供了大部分准确的投票信息，并准确跟踪了实时结果” —— Wired
- [介绍下一代 Claude](https://www.anthropic.com/news/claude-3-family)：今天，我们宣布推出 Claude 3 模型系列，它在广泛的认知任务中树立了新的行业基准。该系列包括三个按升序排列的最先进模型……
- [Reddit - 深入了解任何事物](https://www.reddit.com/r/technews/s/GAceXohRVE)：未找到描述

---

### **Perplexity AI ▷ #**[**sharing**](https://discord.com/channels/1047197230748151888/1054944216876331118/1303452559960445019) (13 条消息🔥):

> - `PowerShell 编程`
> - `区分技巧`
> - `差异解释`
> - `有效利用资源`
> - `创作者经济规模`

- **精通 PowerShell 编程**：一位成员寻求关于如何编写满足特定要求的 [PowerShell 代码](https://www.perplexity.ai/search/write-a-powershell-code-which-gnm5kScnTGu3I2DAu1GG9A)的帮助。
  
  - *学习高级编程技巧可以显著辅助自动化任务。*
- **轻松区分技巧**：一位用户询问如何轻松[区分](https://www.perplexity.ai/search/como-distingo-facilmente-la-me-HKk6c9L.ShG19QNbeGM_cg)特定领域中的不同概念。
  
  - *识别的清晰度可以简化学习和应用过程。*
- **解释关键差异**：一位成员要求澄清两个特定主题之间的[差异](https://www.perplexity.ai/search/me-explique-a-diferenca-entre-b0hp_5YORHuUC6A25niVrw)。
  
  - *理解细微差别可以带来更好的理解和明智的讨论。*
- **有效利用资源**：几位成员讨论了在各种环境下如何[有效利用资源](https://www.perplexity.ai/search/how-to-effectively-utilize-the-oX05HqeYQTKMSAObEd8ddQ)的策略。
  
  - *优化使用可以提高生产力并高效实现预期结果。*
- **关于创作者经济规模的见解**：有人询问了[创作者经济](https://www.perplexity.ai/search/wie-gross-ist-creator-economy-GRHhZUJFSWWPO4pCPwVDaQ)的规模，探讨其对各行业的影响。
  
  - *了解其增长可以提供对现代经济趋势的见解。*

---

### **Perplexity AI ▷ #**[**pplx-api**](https://discord.com/channels/1047197230748151888/1161802929053909012/1303511095415275560) (6 messages):

> - `Llama 3.1 API Pricing`
> - `Return Citations Functionality`
> - `Haiku 3.5 Limits`
> - `Translation Requests`

- **Llama 3.1 Sonar API 成本**：一位成员询问了 **Llama 3.1 Sonar 70B API** 每 100 万个 tokens 的费用，并分享了[定价指南链接](https://perplexity.mintlify.app/guides/pricing)。
  
  - 该链接似乎提供了相关细节，但具体的定价细节仍不明确。
- **不稳定的 'return_citations' 功能**：一位成员就 **'return_citations'** 功能寻求帮助，指出尽管拥有访问权限，该功能有时仍无法获取来源。
  
  - 他们需要确认问题是源于其代码编写还是 API 的响应性。
- **对 Haiku 3.5 限制的好奇**：一位成员询问了 **Haiku 3.5 的限制**，表示有兴趣了解其约束条件。
  
  - 未提供关于具体限制或能力的额外细节。
- **翻译查询**：一位成员请求协助将所有消息翻译成**法语**。
  
  - 这突显了讨论中对多语言支持的需求。

 

**提到的链接**：[未找到标题](https://perplexity.mintlify.app/guides/pricing)：未找到描述

 

---

### **Unsloth AI (Daniel Han) ▷ #**[**general**](https://discord.com/channels/1179035537009545276/1179035537529643040/1303459105238814813) (101 messages🔥🔥):

> - `Unsloth updates`
> - `SFT and DPO fine-tuning`
> - `Model training issues`
> - `NVIDIA feedback request`
> - `ECommerce app ideas`

- **Unsloth 团队解决近期 PR 问题**：用户报告了最近一次 PR 后的问题，团队建议从 GitHub 重新安装之前的版本作为修复方案。
  
  - 在团队更新后，问题似乎已解决，部分用户确认了运行状况的改善。
- **关于 SFT 和 DPO 集成的讨论**：社区成员讨论了使用现有的 SFT 数据集进行 DPO 微调，一些人建议需要正确的格式。
  
  - 公认的做法是在每个数据集条目中放入上下文，以确保在训练和推理过程中的清晰度。
- **微调 SmolLM2 极具挑战**：一位用户在微调 SmolLM2 时遇到了输出无限循环的问题，尽管数据集中已包含 EOS tokens。
  
  - 社区确认该模型目前存在错误，预计 Hugging Face 将发布更新。
- **NVIDIA 向非开发者征求见解**：NVIDIA GeForce RTX 团队向社区中的 AI 爱好者征求反馈，以获取产品方向的见解。
  
  - 目标群体是非开发者，以获取他们独特的视角，突显了 AI 工具多样化的用户体验。
- **用户探索电子商务应用开发**：一位新用户表示有兴趣开发一个基于 OpenAI 的应用，旨在彻底改变电子商务并寻求投资者。
  
  - 建议包括可能聘请人员协助开发和融资工作。

**提到的链接**：

- [Google Colab](https://colab.research.google.com/drive/1CgnHs5JT_0-bx8FTCoGcVgO68WXAptOE?usp=sharing)：未找到描述
- [10 分钟会议 - Asli Sabanci](https://calendly.com/aslisabanci-01-nvidia/10min)：你好！作为 NVIDIA GeForce RTX 团队，我们正在寻求社区 AI 爱好者的意见，以指导未来的产品方向和路线图。我们很想见见你们中一些低代码/无代码的...
- [Debate The Debate GIF - Debate The debate Trump - Discover & Share GIFs](https://tenor.com/view/debate-the-debate-trump-donald-trump-joe-biden-gif-5559847623678406965)：点击查看 GIF
- [importlib.metadata.PackageNotFoundError: No package metadata was found for The 'unsloth' distribution was not found and is required by this application · Issue #124 · unslothai/unsloth](https://github.com/unslothai/unsloth/pull/124)：训练环境：LLaMaFactory `01/24/2024 01:53:50 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit. Traceback (most recent call last): File "/usr/local/lib/python3.10/dist-packages/trans...
- [Bug fixes by danielhanchen · Pull Request #1245 · unslothai/unsloth](https://github.com/unslothai/unsloth/pull/1245)：未找到描述

---

### **Unsloth AI (Daniel Han) ▷ #**[**off-topic**](https://discord.com/channels/1179035537009545276/1179039861576056922/1303515507260915733) (27 messages🔥):

> - `NVIDIA GeForce RTX 社区反馈`
> - `NIM API 反馈`
> - `芬兰语模型性能`
> - `Discord 中的诈骗警报`
> - `AI 工具使用经验`

- **NVIDIA GeForce RTX 寻求社区见解**：NVIDIA GeForce RTX 团队正在收集 AI 爱好者关于 AI 工具使用经验的反馈，并对非开发者视角特别感兴趣。他们鼓励通过 [此链接](https://calendly.com/aslisabanci-01-nvidia/10min) 预约简短的交流。
  
  - 一位成员指出，社区的输入可能会显著影响未来 NVIDIA 产品的开发。
- **关于 NIM API 的反馈请求**：一位成员建议为使用 NIM API 启用按需付费 (pay-as-you-go) 模式，而不是目前的积分系统，并表示目前的系统令人困惑。来自 NVIDIA 的 Asli 认可了这一观点，并提到可下载的容器允许用户在自己的 GPU 上运行模型。
  
  - 他们强调了对更多选项的需求，并指出了在没有直接访问合适硬件的情况下使用大型模型所面临的挑战。
- **关于芬兰语模型性能的讨论**：成员们分享了关于 Nemotron-340B-Reward 和 Llama-Nemotron-70B 等模型的正面反馈，注意到它们在生成芬兰语合成数据方面的有效性。讨论围绕着在资源有限的情况下对大型数据集进行推理 (Inference) 的困难展开。
  
  - 对这些模型的赞赏表明，对于进行广泛分析所需的计算资源，存在着改进访问权限的需求。
- **社区中的诈骗警报**：成员们对 Discord 中可能存在的诈骗者表示担忧，敦促其他用户保持警惕。一位成员报告了该问题，促使其他人举报来自该可疑账户的消息。
  
  - 这凸显了社区内反复出现的诈骗问题，反映了过去经历过的类似问题。

**提到的链接**：

- [10 分钟会议 - Asli Sabanci](https://calendly.com/aslisabanci-01-nvidia/10min)：大家好！作为 NVIDIA GeForce RTX 团队，我们正在寻求社区 AI 爱好者的意见，以指导未来的产品方向和路线图。我们很想见到一些低代码/无代码背景的用户...
- [AMD 刚刚删除了 Intel – 9800X3D](https://www.youtube.com/watch?v=kML0ipgqT-0)：在下方 Amazon 查看价格 AMD 9800X3D: https://geni.us/ySD8o AMD 9600X: https://geni.us/sDKt AMD 7800X3D: https://geni.us/xyyXA 需要新壁纸吗？ https://...

---

### **Unsloth AI (Daniel Han) ▷ #**[**help**](https://discord.com/channels/1179035537009545276/1179777624986357780/1303533213737549877) (41 messages🔥):

> - `语言翻译数据格式化`
> - `模型性能问题`
> - `微调 Llama 模型`
> - `微调模型的集成`
> - `模型生成中的输出处理`

- **语言翻译数据格式化建议**：一位用户询问如何根据语言翻译来格式化训练数据，在使用 **Unsloth inference** 时尝试了多种格式但均未成功。
  
  - 另一位成员询问是否与所使用的推理方法有关。
- **Qwen 和 Llama 的模型性能担忧**：有成员对 **Qwen 2.5 1.5B** 尽管在数据集中添加了 'End of text' 仍出现幻觉 (Hallucinating) 表示担忧，并指出其表现与 **Llama 1B** 不同。
  
  - 一位成员建议，与 **Llama** 相比，**Qwen** 可能需要额外的训练。
- **在索引问答上微调 Llama 模型**：一位用户表示有兴趣使用 **QLora** 或 **LoRa** 技术在索引问答 (Indexed QA) 上微调 **Llama 3B**，并寻求流程指导。
  
  - 他们提到成功微调了一个 **Unsloth/Llama** 模型，用于集成到个人网站的聊天机器人中。
- **模型集成挑战**：一位成员在将微调后的模型与 **MLC LLM** 集成时遇到问题，请求社区帮助。
  
  - 他们报告成功在个人信息上微调了模型，但在将模型集成到客户端应用程序时遇到了困难。
- **高效处理模型输出**：一位用户寻求关于从生成的模型响应中提取纯文本输出的建议，并分享了一段代码片段。
  
  - 另一位成员建议先打印整个输出以识别正确的对象结构，最终通过直接对响应进行 strip 处理找到了解决方案。

---

### **HuggingFace ▷ #**[**general**](https://discord.com/channels/879548962464493619/879548962464493622/1303451802481594440) (147 条消息🔥🔥):

> - `Speculative Decoding`
> - `AWS Q for Fine-tuning`
> - `Song Generators`
> - `Gradio Interface`
> - `Image Similarity with CLIP`

- **关于 Speculative Decoding 效率的讨论**：成员们讨论了在模型中实现 **speculative decoding** 的情况，指出它通过使用较小的模型进行初始 token 预测来加速推理。
  
  - *Speculative decoding* 在保持准确性的同时实现了更快的速度，使其在各 AI 公司中广受欢迎。
- **探索用于 Fine-tuning 的 AWS Q**：一位用户询问了使用 **AWS Q** 构建 Agent 的成功案例，特别是针对 Fine-tuning 和实时数据处理。
  
  - 他们在考虑这是否比利用来自 Hugging Face 的模型来构建其领域知识库更具可行性。
- **用于卡拉 OK 的歌曲生成器**：讨论揭示了多种可用于生成歌曲的工具，其中显著的包括用于创建歌词和旋律的 [Musicgen](https://huggingface.co/Mar2Ding/songcomposer_sft)。
  
  - 对于类似卡拉 OK 的需求，用户指出了其他能够生成不带歌词音乐的模型，例如 **stable-audio**。
- **使用 Gradio 构建界面**：一位用户试图了解 **Gradio** 的工作原理，特别是在构建类似应用时，如何处理 **Svelte** 与 Python 后端之间的接口。
  
  - 他们对高级工具和可能简化开发过程的资源表现出兴趣。
- **微调 CLIP 以实现图像相似度**：有一个关于微调 **CLIP** 以实现纯图像相似度任务的最佳方法的咨询，重点关注正负样本对。
  
  - 成员们讨论了在使用 Transformer 进行单变量时间序列预测时，对时间戳唯一索引的预期。

**提到的链接**：

- [Oasis](https://oasis.decart.ai/overview)：未找到描述
- [minchyeom/birthday-2 · Hugging Face](https://huggingface.co/minchyeom/birthday-2)：未找到描述
- [LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710)：我们提出了 LayerSkip，这是一种加速大语言模型 (LLM) 推理的端到端解决方案。首先，在训练期间我们应用了 layer dropout，对较早的层使用较低的 dropout 率，而对较后的层使用较高的...
- [Cute Pinch GIF - Cute Pinch So Fluffy - Discover & Share GIFs](https://tenor.com/view/cute-pinch-so-fluffy-gif-15488998239354870297)：点击查看 GIF
- [The Universe Tim And Eric Mind Blown GIF - The Universe Tim And Eric Mind Blown Mind Blown Meme - Discover & Share GIFs](https://tenor.com/view/the-universe-tim-and-eric-mind-blown-mind-blown-meme-mind-explosion-mind-explosion-meme-gif-18002878)：点击查看 GIF
- [Reddit - Dive into anything](https://www.reddit.com/r/leetcode/comments/1ex7a1k/i_automated_leetcode_using_claudes_35_sonnet_api/)：未找到描述

---

### **HuggingFace ▷ #**[**today-im-learning**](https://discord.com/channels/879548962464493619/898619964095860757/1303595536275734608) (5 条消息):

> - `Building a GPT model`
> - `Top-p sampling`
> - `Transformer architecture`
> - `BERT model plans`

- **成功构建了一个 GPT 模型**：一位用户分享了他们构建的一个具有 **4 层** Transformer 解码器的 GPT 模型，使用了 **4 个 attention heads**，**block_size** 为 **64 tokens**。
  
  - 该模型可生成高达 **128 tokens** 的响应，主要专注于生成 NLP 相关内容。
- **理解 top-p sampling**：他们在推理过程中实现了 **top-p sampling**，以增强模型的解码过程。
  
  - 该方法旨在提高生成文本的质量，可与 **ChatGPT** 等模型相媲美。
- **BERT 和 seq2seq 模型的未来计划**：该用户表达了接下来构建 **BERT** 模型的意图，随后将构建一个使用 Transformer 的完整 **seq2seq** 模型。
  
  - 他们指出了 GPT（侧重生成内容）与 BERT（从输入中获取更多细微差别）之间关注点的不同。

---

### **HuggingFace ▷ #**[**cool-finds**](https://discord.com/channels/879548962464493619/897390579145637909/) (1 条消息):

gimmyalex3089: [https://cohere.com/research/aya/aya-23-technical-report.pdf](https://cohere.com/research/aya/aya-23-technical-report.pdf)

---

### **HuggingFace ▷ #**[**i-made-this**](https://discord.com/channels/879548962464493619/897390720388825149/1303616116827885619) (7 条消息):

> - `YOLOv5n6 Real-Time Object Detection`
> - `Upstage AI Hackathon`
> - `Contrastive Learning`
> - `JAX Implementation of Flux.1`
> - `Formula 1 Telemetry Analysis`

- **探索 Upstage AI Hackathon**：参加旨在促进 AI 模型开发协作的 [Upstage AI Hackathon](https://github.com/Gimmyalex/upstage-ai-hackathon)。
  
  - 鼓励贡献者加入并在 GitHub 上完善该项目。
- **揭秘 Contrastive Learning 技术**：我们的最新文章深入探讨了 **Contrastive Learning**，这是一种作为现代 AI 基础的强大自监督技术。点击[此处](https://www.lightly.ai/post/brief-introduction-to-contrastive-learning)详细了解其原理、多种公式及应用。
  
  - 该方法自 1993 年提出以来不断演进，对 Unsupervised 和 Self-Supervised Learning 领域产生了重大影响。
- **JAX 实现 Black Forest Labs 的 Flux.1**：Black Forest Labs 的 **Flux.1** 模型系列现已发布新的 JAX 实现，欢迎社区参与和贡献。你可以在 GitHub [此处](https://github.com/ml-gde/jflux)找到该项目。
  
  - 对参与后续开发感兴趣的人员可以查看开放的 Issue。
- **与 F1 遥测数据对话**：一个创新的 AI 应用现在允许用户与来自 **Formula 1 赛事的遥测数据 (telemetry data)** 进行对话，并生成详细报告。探索该项目[此处](https://huggingface.co/posts/Draichi/751144495449880)，其功能包括 text-to-SQL 查询。
  
  - Beta 版本提供了对车手表现和比赛分析的见解，促进了 F1 爱好者之间更深入的讨论。

**提到的链接**：

- [@Draichi on Hugging Face: "🏁 Now it is possible to chat with telemetry data from real Formula 1 races!…"](https://huggingface.co/posts/Draichi/751144495449880)：未找到描述
- [Brief Introduction to Contrastive Learning](https://www.lightly.ai/post/brief-introduction-to-contrastive-learning)：Contrastive Learning 是一种最近获得极大关注的强大方法。该方法在不依赖显式标签的情况下区分相似和不相似的数据点。
- [Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama · Hugging Face](https://huggingface.co/Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama)：未找到描述
- [GitHub - Gimmyalex/upstage-ai-hackathon](https://github.com/Gimmyalex/upstage-ai-hackathon)：通过在 GitHub 上创建账号来为 Gimmyalex/upstage-ai-hackathon 的开发做出贡献。
- [GitHub - SanshruthR/CCTV_YOLO: Fast Real-time Object Detection with High-Res Output https://x.com/_akhaliq/status/1840213012818329826](https://github.com/SanshruthR/CCTV_YOLO)：具有高分辨率输出的快速实时目标检测。
- [GitHub - ml-gde/jflux: JAX Implementation of Black Forest Labs' Flux.1 family of models](https://github.com/ml-gde/jflux)：Black Forest Labs 的 Flux.1 系列模型的 JAX 实现。

---

### **HuggingFace ▷ #**[**reading-group**](https://discord.com/channels/879548962464493619/1156269946427428974/) (1 条消息):

west_ryder: 😝

---

### **HuggingFace ▷ #**[**computer-vision**](https://discord.com/channels/879548962464493619/922424143113232404/1303453581562871840) (3 条消息):

> - `HuggingMod rate limiting`
> - `New Microsoft models`

- **HuggingMod 发布速度过快**：一位成员建议 HuggingMod **放慢发布速度**，并指出频繁的消息可能会导致问题。
  
  - “请慢一点”是给出的友好提醒，强调了社区参与度。
- **对微软新模型的兴奋**：一位成员对 Microsoft 发布的新模型表示兴奋，称这正是另一位成员想要的。
  
  - 对这些新模型的期待表明人们对来自 Microsoft 的创新工具兴趣日益浓厚。

 

---

### **HuggingFace ▷ #**[**NLP**](https://discord.com/channels/879548962464493619/922424173916196955/1303576239835058226) (3 条消息):

> - `Function Default Arguments`
> - `MaskGCT and F5-TTS Streaming Capabilities`

- **检查你的函数参数**：一位成员幽默地询问是否存在错误，并建议检查丢弃剩余文档的函数内部的 **default arguments**。
  
  - “好的……我查一下。”另一位成员询问是否有人有之前的经验和代码可以参考。
- **探索用于流式传输的 MaskGCT 和 F5-TTS**：一位成员分享了对 **MaskGCT** 和 **F5-TTS** 的兴奋，询问它们是否可以在保持音频分块流式传输的同时替换当前的语音模型。
  
  - 他们对这些 **non-autoregressive models** 的流式传输能力表示怀疑。

 

---

### **OpenRouter (Alex Atallah) ▷ #**[**announcements**](https://discord.com/channels/1091220969173028894/1092729520181739581/1303511109189505095) (3 messages):

> - `API Migration`
> - `Latency Optimization`
> - `Completion API Updates`

- **API 迁移消除了 524 错误**：团队重构了他们的 API 并迁移了 Chatroom 请求，测试期间检测到的 524 错误为零。
  
  - 在迁移 API 的其余部分之前，稳定性需要维持一天，鼓励用户通过 `/api/alpha/chat/completions` 进行测试。
- **Predicted Outputs 增强了编辑延迟表现**：针对 OpenAI 的 GPT4 模型新增的 *predicted output* 功能，通过 `prediction` 属性改善了编辑和重写任务的延迟。
  
  - 这是通过提供基于内容的 `prediction` 来实现的，从而增强了文本转换过程中的性能。
- **Completion API 重构以提升速度**：所有 Completion API 请求已转换至全新重写的 API，该 API 承诺提供更快的**速度**和更好的性能。
  
  - 已邀请用户使用指定的反馈频道报告任何问题。

 

---

### **OpenRouter (Alex Atallah) ▷ #**[**general**](https://discord.com/channels/1091220969173028894/1094454198688546826/1303448780208603256) (161 messages🔥🔥):

> - `Hermes 3 performance`
> - `Claude API updates`
> - `Llama model comparisons`
> - `Rate limits and errors`
> - `PDF support for Claude`

- **Hermes 3 响应缓慢**：用户报告 **Hermes 3** 的响应速度参差不齐，部分用户因网络问题经历延迟。
  
  - 尽管最初存在担忧，但目前似乎已恢复运行，不过仍有部分用户注意到延迟。
- **Claude API 迁移及相关问题**：有报告称 Claude 系列出现 **524 错误**，这似乎与向新 API 的迁移同步发生，预计很快会得到解决。
  
  - 更新后，用户注意到服务已恢复运行，但偶尔会出现超时，并建议尝试新的 alpha 端点。
- **关于 Llama 和 Claude 模型的反馈**：用户讨论了 **Llama** 模型，特别是对 **Llama 3.1** 等免费版本的挫败感，尽管使用量较轻，但仍遇到速率限制消息。
  
  - 相比之下，据报道 Claude 等付费模型运行正常，尽管有人指出存在不一致性。
- **速率限制及如何处理错误**：围绕**速率限制**和响应处理的问题引发了关于处理可能指示 Provider 问题的 `429` 代码的讨论。
  
  - 社区分享了解析此类错误以及管理来自各种 LLM Provider 意外行为的策略。
- **Claude 模型潜在的 PDF 支持**：用户询问了新版 **Claude Sonnet 3.5** 对 PDF 输入的支持，特别是其视觉处理能力。
  
  - 虽然 PDF 支持仍处于 Beta 阶段，但有迹象表明该功能未来可能会通过 OpenRouter 提供。

**提到的链接**：

- [Model Equality Testing: Which Model Is This API Serving?](https://arxiv.org/abs/2410.20247)：用户经常通过黑盒推理 API 与大语言模型交互，包括闭源和开源权重模型（例如，Llama 模型通常通过 Amazon Bedrock 和 Azure AI Studio 访问...）
- [Limits | OpenRouter](https://openrouter.ai/docs/limits)：设置模型使用限制
- [Gemini Flash 1.5 - API, Providers, Stats](https://openrouter.ai/google/gemini-flash-1.5)：Gemini 1.5 Flash 是一个基础模型，在视觉理解、分类、摘要以及从图像、音频和视频创建内容等各种多模态任务中表现出色...
- [Anthropic Status](https://status.anthropic.com/)：未找到描述

---

### **OpenRouter (Alex Atallah) ▷ #**[**beta-feedback**](https://discord.com/channels/1091220969173028894/1277894087755829278/1303451998875811840) (3 messages):

> - `Custom Provider Keys`
> - `Beta Feature Access`

- **关于自定义 Provider Keys 请求的疑问**：成员们正在询问如何申请自定义 **provider keys**，以及除了账户维护之外是否还有其他好处。
  
  - 他们对这些 Key 在其项目中可能带来的**潜在优势**表示好奇。
- **申请使用 Provider Keys 访问 Beta 功能**：一名成员请求使用 provider keys 访问 **beta 功能**，表明了测试该功能的兴趣。
  
  - 这一观点得到了其他人的共鸣，显示出大家对进一步探索**自定义 provider keys** 的集体热情。

 

---

### **aider (Paul Gauthier) ▷ #**[**general**](https://discord.com/channels/1131200896827654144/1131200896827654149/1303450588369129553) (108 条消息🔥🔥):

> - `Aider 0.62 版本特性`
> - `模型选择与性能`
> - `Aider 中的文件格式处理`
> - `Aider 中的配置选项`
> - `LLM 模型中的 Temperature 设置`

- **Aider 0.62 新特性**：Aider v0.62 引入了对 Claude 3.5 Haiku 的全面支持，性能基准测试显示其在代码编辑排行榜上得分 **75%**。
  
  - 新特性包括轻松应用来自 ChatGPT 或 Claude 的文件编辑，以及修复了创建新文件时的 Bug。
- **模型性能见解**：成员们讨论了使用各种 LLM 的经验，指出尽管 Haiku 成本更低，但在编程/调试任务中 **Sonnet** 的表现优于 **Haiku**。
  
  - 还与 **Qwen 2.5** 进行了比较，一些成员确认它处理编程任务的能力优于 **Llama 3.1 405B**。
- **文件格式限制**：成员们澄清，Aider 目前仅支持基于文本的文件格式，无法读取 MS-Word 文档。
  
  - 有人提出了关于从外部目录添加文件的问题，并确认可以传递绝对路径来包含文件。
- **Aider 配置的灵活性**：用户可以使用 `.aider.model.settings.yml` 配置模型设置，包括在模型支持的情况下设置请求的 temperature。
  
  - Aider 还支持在 `.env` 文件中存储配置，以便管理 API keys 和设置。
- **Temperature 设置的影响**：成员们强调，对于支持 temperature 调节的模型，Aider 默认发送的 temperature 为 **0**，这会影响模型输出的随机性。
  
  - 讨论还提到，较高的 temperature 会导致响应的可预测性降低，从而增加调试难度。

**提到的链接**：

- [未找到标题](https://www.reddit.com)：未找到描述
- [使用 .env 进行配置](https://aider.chat/docs/config/dotenv.html)：使用 .env 文件为 aider 存储 LLM API keys。
- [Aider LLM 排行榜](https://aider.chat/docs/leaderboards/#contributing-benchmark-results)：LLM 代码编辑能力的定量基准测试。
- [Aider LLM 排行榜](https://aider.chat/docs/leaderboards/)：LLM 代码编辑能力的定量基准测试。
- [发布历史](https://aider.chat/HISTORY.html)：关于 aider 编写自身代码的发布说明和统计数据。
- [Aider LLM 排行榜](https://aider.chat/docs/leaderboards/#contributing-benchmark-results)：LLM 代码编辑能力的定量基准测试。
- [如何配置 ctags 以支持 CSS, SCSS, HTML？](https://stackoverflow.com/questions/33647614/how-to-configure-ctags-to-work-with-css-scss-html)：我已经阅读了很多博客文章和 stackoverflow 上的回答，但我似乎做错了什么，因为我仍然遇到 E388: 找不到定义错误。我做了什么：下载...
- [Reddit - 深入了解任何事物](https://www.reddit.com/r/LocalLLaMA/comments/1ga5m5r/updated_claude_sonnet_35_tops_aider_leaderboard/)：未找到描述
- [高级模型设置](https://aider.chat/docs/config/adv-model-settings.html#model-settings)：为 LLM 配置高级设置。
- [参数 | OpenRouter](https://openrouter.ai/docs/parameters)：配置请求参数
- [DBRX 132B Instruct - API, 提供商, 统计数据](https://openrouter.ai/databricks/dbrx-instruct)：DBRX 是由 Databricks 开发的新型开源大语言模型。参数量为 132B，在标准行业基准测试中优于现有的开源 LLM，如 Llama 2 70B 和 [Mixtral-8x7b](/mistralai/mixtral-8x7b)...
- [Firecrawl](https://www.firecrawl.dev/)：将任何网站转换为 LLM 就绪的数据。

---

### **aider (Paul Gauthier) ▷ #**[**questions-and-tips**](https://discord.com/channels/1131200896827654144/1133060505792159755/1303454468154724373) (47 条消息🔥):

> - `Benchmarking Aider Requests`
> - `DeepSeek and Llama.cpp Integration`
> - `Aider Configuration Issues`
> - `Using LLM Models with Aider`
> - `Aider Command Issues`

- **Aider 基准测试请求超出限制**：一位用户提出了在基准测试期间超过 **requests per minute limit**（每分钟请求限制）的担忧，并询问这会影响性能还是仅仅增加执行时间。
  
  - 这引发了关于 **Aider** 在性能评估期间操作限制的更广泛讨论。
- **DeepSeek 和 Llama.cpp 的使用经验**：多位成员分享了使用 **llama.cpp** 运行 **DeepSeek-V2.5** 的经验，提到了由于模型大小和模板兼容性问题带来的挑战。
  
  - 讨论表明，虽然一些成员在某些模型上取得了成功，但其他人面临频繁的错误和模板要求不匹配的问题。
- **Aider 的配置挑战**：一位用户讨论了在配置 **Aider** 使用本地模型和环境变量时遇到的困难，重点在于 **OLLAMA_API_BASE** 的设置。
  
  - 对话探讨了在配置挑战中，手动指定命令对于实现正常功能是否必要。
- **Aider 中的 LLM 模型使用**：用户分享了关于 Web 模型和本地模型交互的见解，特别是关于 **editor models** 相对于主架构模型的选择。
  
  - 相关考量强调了在 **model pairing** 中，资源分配和任务特定性对实现最佳性能的影响。
- **Aider 命令的问题**：一位成员报告了 **/lint** 命令因缺少文件规范而无法执行的错误，尽管该命令在控制台中运行正常。
  
  - 其他人确认，来自 Anthropic 的 **internal server errors** 可能会导致 Aider 内部出现类似的命令执行问题。

**提到的链接**：

- [使用 .env 进行配置](https://aider.chat/docs/config/dotenv.html)：使用 .env 文件为 aider 存储 LLM API keys。
- [legraphista/DeepSeek-V2.5-IMat-GGUF · Hugging Face](https://huggingface.co/legraphista/DeepSeek-V2.5-IMat-GGUF)：未找到描述
- [mlx-community/Qwen2.5-32B-Instruct-4bit · Hugging Face](https://huggingface.co/mlx-community/Qwen2.5-32B-Instruct-4bit)：未找到描述
- [意外错误：litellm.InternalServerError: AnthropicException - Overloaded · Issue #957 · Aider-AI/aider](https://github.com/Aider-AI/aider/issues/957)：问题 Aider v0.46.0 模型：claude-3-5-sonnet-20240620 使用 diff 编辑格式，弱模型 claude-3-haiku-20240307 Git repo: .git 包含 280 个文件 Repo-map: 使用 1024 tokens 使用 /help 获取帮助，运行 "...

---

### **Nous Research AI ▷ #**[**general**](https://discord.com/channels/1053877538025386074/1149866623109439599/1303495311074787369) (98 条消息🔥🔥):

> - `TEE_HEE_HE Twitter 账号`
> - `Hermes 405B 性能`
> - `ML 项目的资金资源`
> - `Venice AI 发布`
> - `OpenAI Eval 功能`

- **TEE_HEE_HE Twitter 账号获救**：团队正在努力解除 TEE_HEE_HE Twitter 账号的限制，目前该账号似乎已恢复运行。
  
  - 社区成员对账号重新激活后与其互动表示兴奋。
- **Hermes 405B 恢复免费访问**：用户报告称 Hermes 405B 在 Openrouter 上已恢复运行，尽管存在一些延迟。
  
  - 该功能的可用性被强调为至关重要，证实了尽管存在性能问题，可访问性仍具有优先权。
- **ML 项目的资助机会**：一位用户讨论了申请 Microsoft for Startups 以获取其 ML 项目资金的情况，并分享了申请资格标准。
  
  - 提到可能获得 150,000 美元的 Azure 额度，但建议准备一份清晰的商业计划书以确保申请成功。
- **Venice AI 发布**：Venice AI 推出了 Hermes 3 的新版本，名为 Abliterated，为用户提供更少的审查。
  
  - 该服务旨在为主流 AI 应用提供一个无审查且私密的替代方案，强调用户隐私。
- **对 OpenAI Eval 功能成本的担忧**：一位用户在尝试不同 Prompt 时，反思了与 OpenAI 的 Eval 功能相关的高昂成本。
  
  - 他们强调需要清晰的数据格式，以简化未来的研究并提高数据收集的效率。

**提到的链接**：

- [来自未定义用户的推文](https://x.com/tee_hee_he)：未找到描述
- [什么是 Microsoft for Startups？](https://learn.microsoft.com/en-us/microsoft-for-startups/overview)：了解该计划对初创企业的益处和特性。
- [PlayAI - HERmes](https://play.ai/agent/HERMES-m3i3jU81_52ruL6_0tw2R)：与语音 AI 进行无缝、自然的对话
- [Venice | 私密且无审查的 AI](https://venice.ai/)：免费试用 Venice.ai。使用私密且无审查的 AI 生成文本、图像和代码。
- [AWS 上的免费计算服务](https://aws.amazon.com/free/compute/)：未找到描述
- [GitHub - DarkStarStrix/Auto_Api: 一个简化的机器学习框架](https://github.com/DarkStarStrix/Auto_Api)：一个简化的 Machine Learning 框架。通过在 GitHub 上创建账号为 DarkStarStrix/Auto_Api 的开发做出贡献。

---

### **Nous Research AI ▷ #**[**ask-about-llms**](https://discord.com/channels/1053877538025386074/1154120232051408927/1303634142386458765) (19 条消息🔥):

> - `Cursor 在代码库上的局限性`
> - `通过 Web UI 访问 Sonnet 3.5`
> - `对 Haiku 3.5 的看法`
> - `AI 用户界面对比`

- **Cursor 在处理大型代码库时表现挣扎**：*Cursor 在理解大型代码库方面表现极差*，因此建议尝试 Sourcegraph 的 **Cody**，它专门从事大型代码库的索引。
  
  - 另一位成员幽默地表示，**Teknium** 可能对未来的产品抱有希望。
- **访问 Sonnet 3.5 Web UI**：一位用户寻求帮助，想知道如何在 Anthropic 之外通过 Web UI 访问 **Sonnet 3.5**，对此 **Teknium** 建议使用 **OpenRouter**。
  
  - 虽然他们形容该聊天 UI *一般* 但可用，用户确认他们正在尝试。
- **Haiku 3.5 引发担忧**：成员们对 **Haiku 3.5** 表示怀疑，一位用户评论其性能不佳，并认为其表现像是一个更小的模型（8-14b）。
  
  - 其他人批评了它的定价，认为与 **Gemini 1.5 Flash** 和 **GPT-4o-mini** 等更便宜的选择相比，它的定位很尴尬。
- **AI 用户界面对比**：一位成员询问了关于在本地和通过 API 使用 AI 的 **OpenWebUI**、**LibreChat** 和 **Text-Generation-WebUI** 的看法。
  
  - 作为回应，另一位用户建议使用 **LMStudio**，尽管他们更倾向于开源选项。

 

---

### **Nous Research AI ▷ #**[**research-papers**](https://discord.com/channels/1053877538025386074/1104063238934626386/) (1 条消息):

detailoriented: 有人深入研究过 Federated Learning 吗？

---

### **Nous Research AI ▷ #**[**research-papers**](https://discord.com/channels/1053877538025386074/1104063238934626386/) (1 条消息):

detailoriented: 有人深入研究过 Federated Learning 吗？

---

### **Eleuther ▷ #**[**general**](https://discord.com/channels/729741769192767510/729741769738158194/1303691069128577036) (15 条消息🔥):

> - `lm-eval error`
> - `Autoencoders research`
> - `Research opportunities in Switzerland`
> - `NLP faculty at ETH/EPFL`
> - `Job application advice`

- **lm-eval 测试错误引发困惑**：一位用户报告了 lm-eval 测试运行中的 **TypeError**，指出 'NoneType' 对象不可调用。
  
  - 另一位用户建议，如果认为某些功能损坏，最好在 GitHub 上创建一个新的 issue。
- **Autoencoder 研究探索**：一位成员询问了与 LLM 无关的 **Autoencoders** 使用经验，寻求他人的见解。
  
  - 目前讨论中关于此话题的回复和专业知识仍然有限。
- **向瑞士研究实验室发送“冷邮件”**：一位用户表示有兴趣向瑞士的研究实验室发送“冷邮件”（cold emailing）以寻求实习机会。
  
  - 虽然 ETH 和 EPFL 享有盛誉，但他们发现搜索范围太广，希望能有具体的线索。
- **ETH/EPFL 的 NLP 研究教职**：针对研究机构的查询，推荐了 **EPFL 和苏黎世联邦理工学院 (ETH Zurich)**，因为它们拥有实力雄厚的 NLP 教职团队。
  
  - 对话还深入探讨了该用户是否也对工业实验室感兴趣。
- **关于申请和访问苏黎世的建议**：讨论显示，尽管担心需要瑞士德语，但访问 **苏黎世** 将有助于用户评估自己的兴趣。
  
  - 建议强调苏黎世通用 **流利的英语**，使其成为一个可行的求职目标。

 

---

### **Eleuther ▷ #**[**research**](https://discord.com/channels/729741769192767510/747850033994662000/1303671207073153044) (89 条消息🔥🔥):

> - `Hardware-aware algebraic rewrites`
> - `Flash attention development`
> - `Attention mechanism visualization`
> - `Memory access in matrix operations`
> - `XLA and cuDNN integration`

- **硬件感知代数重写的挑战**：成员们讨论了实现 **硬件感知代数重写** 的困难，强调了将理论改进转化为实践的复杂性。
  
  - Chhillee 指出，实现此类重写通常很难，特别是考虑到需要进行反向传播（backward pass）的适配。
- **Flash Attention 的演进**：关于 **Flash Attention** 的开发时间线存在争议，有人声称在公开发布之前，大型实验室已有内部实现。
  
  - Leloykun 指出，将 Attention 机制完善成现在的形式花了五年时间，但对之前的实现持怀疑态度。
- **可视化 Attention 机制**：Leloykun 分享了关于可视化 **Attention 机制** 的见解，使用图表来表示并行处理中的可流式操作。
  
  - 这种方法通过关注计算特定输出单元所需的必要操作，有助于理解问题的解决过程。
- **计算中的内存访问优化**：讨论强调了 **内存访问** 策略的重要性，指出有时为了优化计算速度，最好避免将矩阵实例化（materializing）。
  
  - Fern.bear 强调，优化内存访问可能取决于多种因素，包括互连速度和操作频率。
- **XLA 和 cuDNN 在 Attention 融合中的作用**：对话涉及了 **XLA** 的功能，称其在 TPU 上可以达到 Flash Attention 性能的 70%，尽管对其在 Flash Attention 出现之前的能力存在分歧。
  
  - Chhillee 辩解称，**XLA** 中的历史策略为 Attention 融合提供了见解，但断言扩展到长序列仍然是 Flash Attention 的创新优势。

**提到的链接**：

- [Softmax 的在线归一化计算](https://arxiv.org/abs/1805.02867)：Softmax 函数在机器学习中无处不在，之前的多项研究提出了更快的替代方案。在本文中，我们提出了一种通过更少的内存访问来计算经典 Softmax 的方法...
- [生成式 AI 时代不良结果的语言学分析](https://arxiv.org/abs/2410.12341)：最近的研究集中在生成式 AI 的中长期影响上，由于机器生成信息的检测和可靠性，这带来了科学和社会挑战...
- [(NVIDIA) 在 XLA GPU 中使用 cuDNN 融合注意力](https://youtu.be/09V0SEm9cUE?t=732)：未找到描述内容

---

### **Eleuther ▷ #**[**lm-thunderdome**](https://discord.com/channels/729741769192767510/755950983669874798/1303484888812617778) (2 messages):

> - `lm_eval performance`
> - `NCCL warnings`
> - `Batch size settings`

- **lm_eval 遇到内存问题**：在使用 accelerate 在 8xH100 上运行 **lm_eval** 时，一位成员在所有 loglikelihood 请求后遇到了 `include/alloc.h:103 NCCL WARN Cuda failure 2 'out of memory'` 消息。
  
  - 该问题发生在最后尝试汇总所有内容时，但手动设置更小的 **batch size** 解决了该问题。
- **计划提交 Issue**：该成员表示打算针对在 lm_eval 中遇到的 **out of memory** 错误提交一个 Issue。
  
  - 此步骤旨在引起对该问题的关注，并寻求社区的进一步帮助。

 

---

### **Stability.ai (Stable Diffusion) ▷ #**[**general-chat**](https://discord.com/channels/1002292111942635562/1002292112739549196/1303477501464678410) (103 messages🔥🔥):

> - `Stable Diffusion Installation`
> - `Image Generation Issues`
> - `Outpainting Techniques`
> - `ControlNet Models`
> - `AI Image Expansion`

- **Stable Diffusion 安装寻求帮助**：一位成员询问在 **Windows 11** 上安装 **Stable Diffusion** 的帮助，并被引导查看置顶消息中的指南。
  
  - 另一位成员询问了适合使用的 checkpoints，表明需要社区支持。
- **图像生成方面的挑战**：一位新用户对从 **SDXL model** 获得低质量图像表示沮丧，认为他们可能使用了错误的设置。
  
  - 针对图像尺寸和步数（steps）提供了各种建议，强调需要适合 **SDXL** 的设置。
- **探索 Outpainting 技术**：一位用户询问了如何使用 AI 扩展图像（类似于 TikTok 上的趋势），引发了关于 **outpainting** 的讨论。
  
  - 分享了在 **Stable Diffusion** 中使用 outpainting 技术的有用链接，重点介绍了不同的方法和资源。
- **关于 Stable Diffusion 的 ControlNet 讨论**：一位成员询问了 **controlnet-union-sdxl** 的用法及其与单个 **ControlNet** 模型相比的质量。
  
  - 其他人也发表了见解，讨论了模型质量的差异和潜在的改进。
- **AI 图像扩展工具**：关于 AI 图像扩展的术语和应用存在争论，并提到了 **Videoleap** 和 **CapCut** 等各种工具。
  
  - 尽管存在分歧，成员们仍试图澄清在 AI 图像处理背景下哪些是可能的。

**相关链接**:

- [无标题](https://www.capcut.com/my-edit?start_tab=video): 未找到描述
- [Outpainting Automatic1111](https://learn.rundiffusion.com/outpainting/): 这是一个通过使用 Automatic1111 中的 inpaint 功能进行 outpaint 的快速简便方法。第 1 步：创建一个图像（或已有一个图像）我制作了这个 RPG 地图。第 2 步：将图像发送到 Img2img:...
- [如何使用 outpainting 扩展图像 - Stable Diffusion Art](https://stable-diffusion-art.com/outpainting/): Stable Diffusion 可以通过 outpainting 向任何方向扩展图像。它可以生成视野之外的连贯背景。
- [Infinite Zoom: 使用 AI 无限缩放图像和视频 | Videoleap](https://www.videoleapapp.com/tools/infinite-zoom-ai): 立即开始您的 7 天免费试用。使用 Videoleap 应用程序尝试 AI Infinite Zoom。无限放大或缩小任何图像或视频。
- [Reddit - 深入了解任何事物](https://www.reddit.com/r/StableDiffusion/comments/z475bo/how_to_outpaint_on_automatic1111/): 未找到描述

---

### **LM Studio ▷ #**[**general**](https://discord.com/channels/1110598183144399058/1110598183144399061/1303454664305541181) (56 messages🔥🔥):

> - `LM Studio Portable Version`
> - `Intel E-Cores Utilization`
> - `Auto Load Models in LM Studio`
> - `Llama 3.2 Vision Support`
> - `Context Window Limitations`

- **LM Studio 没有便携版**：一位成员询问是否可以从 USB 运行 LM Studio，确认目前不存在便携版（Portable Version）。
  
  - 另一位成员建议脚本可能使其便携化，并提示用户在 Discord 中搜索相关内容。
- **Intel E-Cores 与性能**：讨论了 LM Studio 是否可以利用 Intel 的 E-Cores，成员建议将线程限制在性能核（Performance Cores）以获得更好的效率。
  
  - 共识是，虽然减少线程通常性能更好，但对特定用例的处理速度影响可能很小。
- **请求 LM Studio 的自动加载功能**：一位成员对每次打开 LM Studio 的聊天 UI 都必须手动选择模型表示沮丧。
  
  - 几位成员讨论了变通方法，包括编写脚本在 UI 打开后自动加载模型。
- **Llama 3.2 Vision 的可用性**：注意到 Ollama 中已存在 Llama 3.2 Vision，而其他人提到 MLX 已支持它，但在 LM Studio 中尚无法运行。
  
  - 暗示了 MLX 即将发布的更新，迹象表明很快将包含对 Llama 3.2 Vision (mllama) 的支持。
- **上下文窗口大小限制揭秘**：一位用户发现，虽然 LM Studio 中的上下文大小滑块最大值为 2048，但手动输入更大的数值也是被接受的。
  
  - 这引发了关于 LM Studio 是否出于效率原因或硬件限制而将模型限制在此大小的讨论。

**提到的链接**：

- [Issues · lmstudio-ai/mlx-engine](https://github.com/lmstudio-ai/mlx-engine/issues/28)：👾🍎 适用于 LM Studio 的 Apple MLX 引擎。通过在 GitHub 上创建账号为 lmstudio-ai/mlx-engine 的开发做出贡献。
- [Performance 3x better when use performance core only on Intel gen 12th cpu · ggerganov/llama.cpp · Discussion #572](https://github.com/ggerganov/llama.cpp/discussions/572)：我发现通过在 Intel 第 12 代处理器上将线程和核心限制为仅使用性能核，性能比默认情况好得多。我的处理器是 Intel Core i7 12700H，该处理器有 6 个性能核...
- [Upgrade mlx and outlines dependencies, and add support for llama 3.2 vision by neilmehta24 · Pull Request #22 · lmstudio-ai/mlx-engine](https://github.com/lmstudio-ai/mlx-engine/pull/22)：变更摘要：MLX VLM 升级。mlx_vlm 已升级到最新提交。这带来了对 Llama 3.2 Vision（又名 mllama）的支持。vision_model_kit 和 vision_model_wrapper 已更新以支持...

---

### **LM Studio ▷ #**[**hardware-discussion**](https://discord.com/channels/1110598183144399058/1153759714082033735/1303449190277578844) (37 条消息🔥):

> - `LLM Benchmarking`
> - `Windows Scheduler 性能`
> - `内存超频挑战`
> - `AMD vs Intel 内存管理`
> - `单槽 RTX 4090`

- **对 LLM Benchmark 标准的需求**：一位成员表示需要一个类似于 **3DMark** 的 **LLM Benchmark**，以方便对特定配置和软件版本进行基准测试。
  
  - 这将有助于建立排名和层级，从而在性能指标中提供更好的入门信息。
- **Windows Scheduler 对性能的影响**：几位成员讨论了 **Windows Scheduler** 如何在性能中发挥关键作用，建议手动设置 CPU 线程的亲和性 (affinity) 和优先级 (priority)。
  
  - 其中一位强调，线程数保持在物理核心限制内对于避免性能退化至关重要。
- **内存超频是一项技术活**：讨论中提到了 AMD 系统上**内存超频**的复杂性，成员们指出了在使用多个 DIMM 时实现稳定性的挑战。
  
  - 一位成员分享了通过精细调优降低延迟的经验，同时警告不要在缺乏专业知识的情况下盲目操作。
- **AMD 与 Intel 在内存方面的对比**：成员们一致认为 **AMD** 的内存超频比 **Intel** 更困难，尤其是在使用多根内存条时。
  
  - 一位成员表示，为了获得最佳延迟和速度，坚持使用两个 DIMM 是更实际的选择。
- **关于单槽 RTX 4090 的讨论**：一位成员询问了**单槽 RTX 4090** 的普及程度和功能，表现出对其工程设计的兴趣。
  
  - 讨论揭示了对显卡创新设计的一种奇特赞赏。

 

**提到的链接**：[RIP Intel: AMD Ryzen 7 9800X3D CPU Review & Benchmarks vs. 7800X3D, 285K, 14900K, & More](https://www.youtube.com/watch?v=s-lFgbzU3LY)：购买我们的新电感骰子套装：[https://store.gamersnexus.net/products/inductor-full-tabletop-mtg-dnd-premium-dice-set-7-piece-dice-wooden-box-token-cardBUY](https://store.gamersnexus.net/products/inductor-full-tabletop-mtg-dnd-premium-dice-set-7-piece-dice-wooden-box-token-cardBUY) O...

 

---

### **Notebook LM Discord ▷ #**[**use-cases**](https://discord.com/channels/1124402182171672732/1124403655819415592/1303458334392975360) (17 条消息🔥):

> - `NotebookLM integration with Google Drive`（NotebookLM 与 Google Drive 的集成）
> - `Use of Diarization in podcasts`（在播客中使用 Diarization）
> - `Deepfake technology discussions`（Deepfake 技术讨论）
> - `Avatars in video podcasts`（视频播客中的 Avatars）
> - `Podcast reuse policies`（播客重用政策）

- **NotebookLM 可以增强 Google Drive 的可用性**：有建议提出在 NotebookLM 中为 Google Drive 文档添加自动同步功能，旨在通过减少手动同步的需求来提高 **productivity**（生产力）。
  
  - 用户表达了对每天需要同步 **70 次** 的沮丧，并希望这一功能能够减轻他们的工作量。
- **用于播客转录的 Diarization 方法**：讨论了使用 **Diarization** 技术作为在播客录音中分离说话者并创建转录文本的方法。
  
  - 一位成员分享了代码细节，以深入介绍他们对这种转录技术的实现。
- **澄清 Deepfake 技术**：成员们就 **Deepfakes** 和 **face swap** 技术之间的区别展开了辩论，并对其定义进行了澄清。
  
  - 讨论集中在 Deepfakes 如何利用现有素材修改面部，而 Avatars 则被视为一种更具 *synthetic*（合成性）的呈现方式。
- **使用 Avatars 进行播客创新**：一位用户展示了他们利用 Avatars 将播客内容捕获为视频的工作，利用该技术来吸引观众。
  
  - 他们提到有可能完善这一概念，以鼓励 **Google** 的创新，旨在提升整体播客体验。
- **寻求播客重用政策的澄清**：针对某些播客的 **reuse policy**（重用政策）提出了疑问，并请求结合一个 GitHub 仓库进行澄清。
  
  - 该用户旨在确保在项目中利用播客时符合合规性。

**提到的链接**：

- [GitHub - jjmlovesgit/Simli_NotebookLM](https://github.com/jjmlovesgit/Simli_NotebookLM)：一个将音频文件分离为不同说话者，并使用 Avatars 播放，最后将录音保存为 mp4 以便在社交媒体等平台分享的项目。非常适合来自 Google NotebookLM 的 Deep Dive 播客。
- [GitHub - robbiemu/llama-gguf-optimize](https://github.com/robbiemu/llama-gguf-optimize)：用于在 llama.cpp 中使用 GGUF imatrices 优化量化的脚本和工具。

---

### **Notebook LM Discord ▷ #**[**general**](https://discord.com/channels/1124402182171672732/1124402182909857966/1303475724245532672) (64 条消息🔥🔥):

> - `Podcast Generation from Notes`（从笔记生成播客）
> - `AI Pronunciation Issues`（AI 发音问题）
> - `Sharing Links`（分享链接）
> - `Language Settings`（语言设置）
> - `AI Interaction Glitches`（AI 交互故障）

- **如何从笔记生成播客**：*zzzuuu* 提到他们发现该应用的对话功能可以从笔记生成播客，但他们丢失了原始的 reel 链接。
  
  - 他们强调了该功能的简单性，这似乎有助于简化内容创作流程。
- **AI 发音问题**：一位用户对他们的企业名称“Easy As”被误读表示沮丧，建议使用拼音拼写来纠正。
  
  - 他们提到 AI 经常产生各种错误的发音，因此需要改进。
- **分享笔记本链接**：torahtechguy 在向组织外部分享链接时遇到问题，并强调大多数 Google 服务都允许公开分享。
  
  - npecom 提供了详细的回复，解释了 NotebookLM 中可用的不同分享链接选项。
- **更改语言设置**：eliano2333 尽管更改了电脑设置，仍尝试将语言显示从瑞典语转换为英语，正在寻求帮助。
  
  - 另一位用户建议在不同语言的音频输出中使用特定的 prompts，但聊天界面仍显示为瑞典语。
- **AI 交互故障**：stylinlp38 观察到 AI 机器人会在回复中交替完成内容，形容这感觉就像是预设脚本的对话。
  
  - 这一故障引发了关于这是一个持续存在的 bug，还是有特定命令可以防止此类行为的疑问。

**提到的链接**：[BYD's Denza: Can It Topple Mercedes & BMW in Luxury EVs? - Unveiling the D9, N9 & More! #suv #MBG.DE](https://youtu.be/yIUDUbAtkZ0)：深入了解 BYD 的豪华电动车子品牌腾势（Denza），探索其挑战豪华汽车巨头梅赛德斯和宝马的大胆策略。

---

### **GPU MODE ▷ #**[**general**](https://discord.com/channels/1189498204333543425/1189498205101109300/1303493939629785168) (10 条消息🔥):

> - `NVIDIA AI Dev Tech 实习`
> - `FP8 量化与计算`
> - `动态与静态量化性能对比`
> - `在 CUDA 中使用 Triton 编译的 PTX`
> - `Batch size 对性能的影响`

- **NVIDIA AI Dev Tech 团队实习咨询**：一名成员正在为即将到来的 **NVIDIA AI Dev Tech 团队**实习面试做准备，并寻求关于面试内容的建议。
  
  - 几位成员分享了应对面试流程的技巧和支持。
- **FP8 量化计算说明**：一名成员询问了 **FP8 量化**中的计算过程，并得到澄清：它使用 **FP8 x FP8 Tensor Cores** 进行运算。
  
  - 进一步的对话探讨了 **Neural Magic** 如何在计算过程中使用动态量化进行加权。
- **动态与静态量化速度分析**：成员们讨论了静态量化与动态量化的性能，结论是对于 Batch size 为 1 的情况，**静态量化**通常表现出更好的性能。
  
  - 对话强调了个人测试结果中的差异，揭示了 **AWQ**、静态量化和动态量化之间不同的效率。
- **不同显卡上的 Kernel 性能**：关于从何处获取 **Kernels** 产生了挑战，观点认为来自 Neural Magic 的 Kernel 可能没有针对非 H100 显卡进行优化。
  
  - 一位成员指出，对于 Batch size 为 1 的情况，性能可能会受限于内存带宽（Memory Bound），从而减缓量化带来的收益。
- **在 CUDA 中使用 Triton 编译的 PTX**：一名成员寻求关于使用 **CUDA launch** 调用 **Triton 编译的 PTX** 的经验，表示对性能见解感兴趣。
  
  - 这引起了成员们的好奇，引发了关于潜在影响和方法的讨论。

---

### **GPU MODE ▷ #**[**triton**](https://discord.com/channels/1189498204333543425/1189607595451895918/1303474819400077343) (15 条消息🔥):

> - `GPU 性能问题`
> - `Triton Kernel 优化`
> - `量化技术`
> - `在 Python 之外调用 Triton PTX`
> - `Kernel 启动参数`

- **A100 在加载 INT1 时表现不佳**：在 **A100 GPU** 上加载 **INT1** 存在持续性问题，导致其性能优于 **3090** 等其他型号的情况并不理想。一个 GitHub [Issue](https://github.com/triton-lang/triton/issues/4906#issuecomment-2458194810) 强调了这种不一致性，并寻求社区的见解。
  
  - 关于性能指标的困惑已持续数周，成员们检查了详细日志以求明确。
- **FP8 Triton Kernel 显示出速度优势**：通过使用激活量化 Kernel 配合 **FP8 Triton Kernel**，一位成员报告称其性能优于其他方法，达到了 **45.39 us**，而 Torch compiled 替代方案为 **55.23 us**。这展示了 Triton 方法在矩阵乘法（Matmul）任务中的效率。
  
  - 随后讨论了在 Matmul 内部进行激活量化时可能导致的减速，实验揭示了矩阵维度的管理不当是性能下降的一个原因。
- **优化 Kernel 配置**：成员们交流了如何通过根据矩阵维度使用预定义配置来避免 Triton Kernel 的 `autotune`，从而缩短预热时间。建议包括创建一个尺寸字典来简化 Kernel 调用。
  
  - 该方法预见到了不同 GPU 架构所需的细微变化，从而可能增强性能。
- **Triton PTX 启动的挑战**：一位用户表示在 Python 之外直接使用 CUDA launch 语法调用 Triton 编译的 **PTX** Kernel 存在困难，理由是不确定正确的启动参数。他们寻求关于如何推导这些参数的澄清。
  
  - 另一位成员建议使用 **ncu** 来确定特定问题规模下的实际 Block 和 Grid 尺寸，以便更好地与 Triton 生成的配置对齐。

**提到的链接**：

- [GitHub · 在统一的协作平台上构建和交付软件](https://github.com/)：加入全球应用最广泛、AI 驱动的开发者平台，数百万开发者、企业和最大的开源社区在此构建推动人类进步的软件。
- [GitHub - jeromeku/triton-rs](https://github.com/jeromeku/triton-rs)：通过在 GitHub 上创建账户来为 jeromeku/triton-rs 的开发做出贡献。
- [Ampere 与 Ada 在位打包权重下的性能不佳 · Issue #4906 · triton-lang/triton](https://github.com/triton-lang/triton/issues/4906#issuecomment-2458194810)：我正在编写一个库，用于在 Triton/CUDA 中执行不同的低比特 Matmul Kernel。Triton Kernel 在 Ada GPU（如 4090 RTX 和 A6000 Ada）上运行良好——在大矩阵上与 Marlin 持平...

### **GPU MODE ▷ #**[**torch**](https://discord.com/channels/1189498204333543425/1189607750876008468/1303744769746407435) (6 条消息):

> - `PyTorch Tensor Iteration Performance` (PyTorch Tensor 迭代性能)
> - `Numpy Iteration Speed` (Numpy 迭代速度)
> - `Torch Script Debugging` (Torch Script 调试)
> - `Overhead of tolist()` (tolist() 的开销)

- **tolist() 在 PyTorch tensor 迭代中表现出色**：基准测试显示，使用 `tolist()` 迭代 PyTorch tensor 的速度显著更快，仅需 **0.0073 秒**，而 GPU 迭代版本为 **2.39 秒**，CPU 迭代版本为 **0.64 秒**。
  
  - 一位用户提到，**CPU tensor 迭代**可能较慢，原因是 PyTorch tensor 上操作分派（dispatching）的开销。
- **Numpy 迭代快于 CPU tensor**：另一位用户补充了基准测试结果，Numpy 迭代耗时 **0.0138 秒**，快于 CPU tensor 迭代，但仍慢于 list 迭代。
  
  - 这表明虽然 Numpy 很高效，但转换为 list 提供了显著的性能优势。
- **关于 tolist() 开销的担忧**：有人指出，CPU tensor 迭代中约 **30%** 的开销源自 `.tolist()` 调用，如果性能对于重复迭代至关重要，这一点可能很重要。
  
  - 在需要对 tensor 数据进行大量迭代的场景中，优化掉这种转换可能会带来更好的性能。
- **关于 Torch Script 调试的问题**：一位用户表示需要 Torch Script 调试方面的指导，特别是尝试打印每个节点的输出。
  
  - 这一询问表明社区正在寻求更好的工具或方法来有效地追踪和调试 Torch Script。

---

### **GPU MODE ▷ #**[**cool-links**](https://discord.com/channels/1189498204333543425/1189868872887705671/1303783682389315654) (2 条消息):

> - `UV Run in Shebang` (Shebang 中的 UV Run)
> - `PlayCanvas SuperSplat` (PlayCanvas SuperSplat)
> - `Self-contained Python Scripts` (自包含 Python 脚本)

- **UV Run：Shebang 魔法**：在 shebang 中使用 `uv run` 被描述为**魔法**，它能够实现自包含的 Python 脚本，同时毫不费力地标注依赖项。正如[这篇博客文章](https://simonwillison.net/2024/Aug/21/usrbinenv-uv-run/)所讨论的，它简化了一次性脚本的创建，无需完整的依赖管理设置。
  
  - 这一方法在 [*seemethere*](https://x.com/_seemethere/status/1838319643347554756) 的帖子中被强调，展示了对开发者的好处。
- **PlayCanvas SuperSplat 编辑器演示**：PlayCanvas SuperSplat 编辑器的链接展示了一个特定的 3D 资产 `<a href='https://playcanvas.com/supersplat/editor?load=https://raw.githubusercontent.com/willeastcott/assets/main/toy-cat.ply&camera.overlay=false&show.bound=false'>toy-cat.ply</a>`，演示了其在 Web 环境中的集成。
  
  - 该工具允许直接在浏览器中交互式编辑 3D 资产，增强了游戏开发者的易用性。

**提到的链接**：

- [来自 eli (@_seemethere) 的推文](https://x.com/_seemethere/status/1838319643347554756)：在 shebang 中使用 uv run 简直是魔法。拥有标注依赖的能力同时保持 Python 脚本自包含，使得编写一次性脚本无需正式的依赖管理……
- [SuperSplat](https://playcanvas.com/supersplat/editor?load=https://raw.githubusercontent.com/willeastcott/assets/main/toy-cat.ply&camera.overlay=false&show.bound=false)：SuperSplat 是一款先进的基于浏览器的编辑器，用于操作和优化 3D Gaussian Splats。它是开源且引擎无关的。

---

### **GPU MODE ▷ #**[**jobs**](https://discord.com/channels/1189498204333543425/1190208177829068860/1303589840100524043) (2 条消息):

> - `Internship Opportunities` (实习机会)
> - `Community Engagement` (社区参与)
> - `Job Posting Etiquette` (职位发布礼仪)

- **询问实习岗位**：一名成员询问了 **Machine Learning**、**Deep Learning** 和 **GPU Programming** 领域的可用实习岗位。
  
  - 这一询问被认为是“低努力”的，因为频道之前的帖子可能已经包含了关于实习的相关信息。
- **关于社区参与的建议**：另一名成员做出了回应，强调了与服务器互动和阅读之前帖子的重要性。
  
  - 他们建议成员应该更多地分享自己的经验，而不是简单地询问实习机会，并强调了该频道的宗旨。

---

### **GPU MODE ▷ #**[**beginner**](https://discord.com/channels/1189498204333543425/1191300313928433664/1303555932466446356) (2 messages):

> - `Caffe2 文件移除`
> - `PR 系列影响`
> - `GitHub 贡献`

- **Caffe2 文件已删除**：在 [pull request #126628](https://github.com/pytorch/pytorch/pull/126628) 中，由于不再使用，多个与 **Caffe2** 相关的文件被删除。
  
  - 该 pull request 与用户 *cyyever* 的贡献有关，并提到了用户 @albanD。
- **PR #126628 是更大系列的一部分**：一位成员确认需要对问题进行二分查找 (bisect)，并指出 PR #126628 是源自 [pull request #122527](https://github.com/pytorch/pytorch/pull/122527) 的系列变更的一部分。
  
  - 此评论表明影响整体项目结构的相关变更仍在持续。

**提到的链接**：

- [[Caffe2]Remove more caffe2 files by cyyever · Pull Request #126628 · pytorch/pytorch](https://github.com/pytorch/pytorch/pull/126628)：这些文件已不再使用。抄送 @albanD
- [Build software better, together](https://github.com/pytorch/pytorch/pull/122527.)：GitHub 是人们构建软件的地方。超过 1 亿人使用 GitHub 来发现、fork 并为超过 4.2 亿个项目做出贡献。

---

### **GPU MODE ▷ #**[**jax**](https://discord.com/channels/1189498204333543425/1203956655570817034/1303515712538808390) (2 messages):

> - `学习 JAX`
> - `Flux.1 的 JAX 实现`

- **寻求初学者视角的 JAX 学习资源**：一位成员询问从初学者角度学习 JAX 的途径。
  
  - 虽然没有提到具体的资源，但社区似乎在积极帮助新学习者。
- **介绍 Flux.1 的 JAX 实现**：最近，成员们发布了 [Black Forest Labs 的 Flux.1 系列模型的 JAX 实现](https://github.com/ml-gde/jflux)。
  
  - 他们强调代码库中存在 **open issues**，并欢迎任何感兴趣的人贡献代码。

 

**提到的链接**：[GitHub - ml-gde/jflux: JAX Implementation of Black Forest Labs' Flux.1 family of models](https://github.com/ml-gde/jflux)：Black Forest Labs 的 Flux.1 系列模型的 JAX 实现 - ml-gde/jflux

 

---

### **GPU MODE ▷ #**[**torchao**](https://discord.com/channels/1189498204333543425/1205223658021458100/1303496236040458260) (4 messages):

> - `减小模型体积`
> - `PyTorch-Quantization 库消失`
> - `页面清理工作`
> - `移动应用的 FP32 计算`

- **专注于减小模型体积**：一位成员表示，他们的目标是为移动应用**减小模型体积**，并表示在计算中使用 FP32 是可以接受的。
  
  - *我不期望看到加速*，但更倾向于减小整体安装包的体积。
- **对过时文档的担忧**：一位用户对导航一个包含过时信息的**庞大且混乱**的页面表示沮丧，这使得查找相关细节变得困难。
  
  - 另一位成员承认了这一点，并承诺在计划页面升级的同时，未来将继续支持 **pt2e quant flow**。
- **PyTorch-Quantization 库失踪之谜**：一位成员询问 NVIDIA 的 **PyTorch-Quantization 库** 及其官方 GitHub 仓库为何突然消失。
  
  - 他们表示困惑，因为他们的模型压缩工作严重依赖此库，并注意到只有[这个页面](https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs)提到了它。

---

### **GPU MODE ▷ #**[**liger-kernel**](https://discord.com/channels/1189498204333543425/1275130785933951039/1303524305518198895) (3 条消息):

> - `Liger Kernel 发布 v0.4.0`
> - `高效 RMSNorm 聚合`
> - `GroupNorm Kernel 实现`

- **Liger Kernel v0.4.0 增强了 AMD 支持**：[v0.4.0](https://github.com/linkedin/Liger-Kernel/releases/tag/v0.4.0) 版本的发布引入了**全面 AMD 支持**，允许进行**多 GPU 训练**，速度提升了 **26%**，并改进了性能。
  
  - 此次更新旨在增强与 AMD GPU 的兼容性，提供更高效的训练流水线。
- **使用二级聚合提高 RMSNorm 效率**：在 [此 issue](https://github.com/linkedin/Liger-Kernel/issues/179) 中讨论了尝试使用**二级聚合**来求和 `dw` 和 `db` 的建议。
  
  - 该方法可以避免因 **atomic_add** 导致的同步需求，从而可能提高某些操作的效率。
- **GroupNorm Kernel 实现提案**：提交了一个 Pull Request [#353](https://github.com/linkedin/Liger-Kernel/pull/353/files#diff-a638e33be424f254984b198946b4a3b9e86f9fc533f32c60486ef953c45dd7eeR173) 以实现 **GroupNorm** kernel，从而达到与 Torch 实现的一致性。
  
  - 该提案被视为之前增强功能 (#285) 的扩展，重点在于保持输出的一致性。

**提到的链接**：

- [Release v0.4.0: Full AMD support, Tech Report, Modal CI, Llama-3.2-Vision! · linkedin/Liger-Kernel](https://github.com/linkedin/Liger-Kernel/releases/tag/v0.4.0)：亮点包括 AMD GPU：我们与 Embedding LLM 合作调整了 Triton 配置以全面支持 AMD！在 0.4.0 版本中，你可以以高出 26% 的速度和 60% 的...运行多 GPU 训练。
- [Improve the efficiency of the RMSNorm aggregation · Issue #179 · linkedin/Liger-Kernel](https://github.com/linkedin/Liger-Kernel/issues/179)：🚀 功能、动机和推介：修改这一行 https://github.com/linkedin/Liger-Kernel/blob/main/src/liger_kernel/ops/rms_norm.py#L306，将 PyTorch 中的求和改为 Triton 中的部分聚合，r.....
- [Kernels for GroupNorm by pramodith · Pull Request #353 · linkedin/Liger-Kernel](https://github.com/linkedin/Liger-Kernel/pull/353/files#diff-a638e33be424f254984b198946b4a3b9e86f9fc533f32c60486ef953c45dd7eeR173)：摘要：实现了与 Torch 的 GroupNorm 输出一致的 GroupNorm。此功能是 #285 的一部分。详情：GroupNorm 中涉及的公式/方程与...相同。

---

### **GPU MODE ▷ #**[**self-promotion**](https://discord.com/channels/1189498204333543425/1288557096404516945/1303688681130823711) (3 条消息):

> - `Nebius Explorer Tier`
> - `针对研究人员的 GPU 定价`
> - `H100 GPU 的可用性`
> - `自助服务平台的优势`

- **Nebius 推出 Explorer Tier，价格为 $1.5/小时**：Nebius 为 **NVIDIA H100 Tensor Core SXM GPU** 推出了 **每 GPU 每小时 $1.5** 的特别定价优惠，旨在支持小型项目和个人研究人员。
  
  - 该计划提供了**即时访问**资源的权限，无需排队等候，允许用户以具有竞争力的市场价格快速开始实验。
- **欢迎对 Nebius 的优惠提供反馈**：Nebius 鼓励社区提供反馈并分享他们的新定价优惠，并附带了一个详细介绍 [Explorer Tier](https://nebius.com/explorer-tier) 及其如何惠及研究人员的链接。
  
  - 他们的目标是为 AI 爱好者营造一个支持性的环境，使他们的平台能够用于各种项目。
- **用户询问 GPU 实例的可用性**：一位用户对 **A100/H100 实例** 的可用性表示担忧，指出许多廉价平台往往缺乏这些资源。
  
  - 作为回应，Nebius 强调他们很大一部分容量是预留给**自助服务和按需使用**的，向用户保证他们不应面临容量问题。
- **Nebius 与廉价平台有所区别**：Nebius 澄清说，他们并不将自己定位为“廉价平台”，而是为客户提供**真正的自助服务体验**，并专注于增长。
  
  - 他们致力于不仅为大客户，也为个人项目提供计算资源。

**提到的链接**：

- [Nebius AI Cloud Explorer Tier with H100 starting from just $1.5 per hour](https://nebius.com/explorer-tier)：发现在顶尖 NVIDIA® GPU 上构建、微调和运行 AI 模型及应用的最有效方式。
- [Tweet from Nebius (@nebiusai)](https://x.com/nebiusai/status/1851217165355028973)：#H100 #GPU 仅需 $1.5/h 🔥 为了支持你在新 AI 项目中的第一步，我们推出了 Explorer Tier —— 前 1,000 小时仅需每小时 $1.5 即可享受 NVIDIA® H100 Tensor Core SXM GPU...

---

### **GPU MODE ▷ #**[**🍿**](https://discord.com/channels/1189498204333543425/1298372518293274644/1303482380022972426) (10 条消息🔥):

> - `维护者权限`
> - `GitHub 访问请求`
> - `Popcorn 项目机会`
> - `Heroku 上的自动化部署`
> - `部署策略`

- **已授予维护者权限**：一位用户请求访问权限，在提供其 GitHub 账号后，获得了 GitHub 维护者权限以及测试 Discord 小组的邀请。
  
  - “刚刚给了你维护者权限”作为确认信息的一部分被分享。
- **需要 GitHub 访问权限和角色**：另一位用户询问如何为其账户获取角色和 GitHub 访问权限，并表示希望在私信（DM）中进一步讨论。
  
  - Marks 给予了积极回应，邀请该用户私信进行进一步交流。
- **大学生可以使用 Popcorn 项目**：Marks 为希望将 Popcorn 项目作为项目资历的大学生提供帮助，并表示愿意签署任何必要的文件。
  
  - 此消息被转发，旨在帮助其他寻求工作正式认可的人。
- **Heroku 自动化更新**：Marks 宣布 Heroku 上的自动化部署现已生效，Bot 可以通过向 main 分支推送更改来进行更新。
  
  - 他提到一旦 GPU 可用，就准备好连接到服务器。
- **探索部署选项**：Marks 对 Heroku 作为部署选项的表现表示好奇，同时也考虑使用 Raspberry Pi。
  
  - 这反映了对 Bot 部署策略的持续探索。

 

---

### **GPU MODE ▷ #**[**thunderkittens**](https://discord.com/channels/1189498204333543425/1300872762163728550/1303809676395413596) (7 条消息):

> - `理想的 Kernel 列表`
> - `ThunderKittens 的初学者贡献`
> - `初步功能列表`
> - `长卷积示例`

- **对理想 Kernel 列表的困惑**：*alexarmbr* 询问了之前帖子中提到的理想 Kernel 和功能列表，因为在仓库中难以找到。
  
  - *marksaroufim* 澄清说，他们正在寻找一份适合初学者贡献的特定 Kernel 列表。
- **贡献 ThunderKittens 的资源**：*sydriax* 引导成员访问 [ThunderKittens GitHub 页面](https://github.com/HazyResearch/ThunderKittens) 获取所有项目详情，强调其对公众开放开发。
  
  - 这种透明度确保了项目中没有私有克隆或隐藏元素。
- **初步入门示例列表**：*simran9493* 提到 GitHub README 中有一个非常初步的列表，特别是在 demos 部分。
  
  - 他们鼓励其他人探索这些 demo，并表达尝试贡献的兴趣。
- **鼓励贡献**：对于有兴趣添加非平方序列长度的长卷积（long convolution）作为入门示例的人，*simran9493* 提出可以提供 *PyTorch* 参考资料。
  
  - 这一补充旨在扩展现有的长卷积 Kernel，展示了增强项目的积极态度。
- **持续更新贡献列表**：*simran9493* 表示打算随着项目的发展，继续增加理想的贡献列表。
  
  - 这体现了社区内的协作精神以及保持资源更新的承诺。

**提到的链接**：

- [GitHub - HazyResearch/ThunderKittens: Tile primitives for speedy kernels](https://github.com/HazyResearch/ThunderKittens)：用于快速 Kernel 的 Tile 原语。通过在 GitHub 上创建账户为 HazyResearch/ThunderKittens 的开发做出贡献。
- [GitHub - HazyResearch/ThunderKittens: Tile primitives for speedy kernels](https://github.com/HazyResearch/ThunderKittens?tab=readme-ov-file#demos)：用于快速 Kernel 的 Tile 原语。通过在 GitHub 上创建账户为 HazyResearch/ThunderKittens 的开发做出贡献。

---

### **GPU MODE ▷ #**[**edge**](https://discord.com/channels/1189498204333543425/1303441437592911912/1303513502505373769) (1 条消息):

> - `Training and inference on edge devices`
> - `Hardware evolution for mobile/embedded`
> - `Challenges in production environments`
> - `Consumer and commercial use-cases`
> - `Hardware heterogeneity`

- **Edge 设备讨论频道上线**：该频道旨在促进专门针对 **Edge**（移动/嵌入式）设备上的 **Training** 和 **Inference** 的讨论。
  
  - 由于多样化的消费者和商业应用对 **Hardware** 需求的不断变化，这个领域正在 **快速演进**。
- **Edge 设备 Hardware 的激动人心机遇**：鼓励成员分享他们的项目，因为各种 **Hardware** 解决方案正在迅速发展以支持新的用例。
  
  - 这种演进正在挑战 **Memory** 和 **Power** 的限制，促使人们采用创新方法来应对这些挑战。
- **生产环境中的独特挑战**：在生产环境中，在 **Hardware Heterogeneity**（硬件异构性）、可靠性、准确性和计算方面存在独特的挑战。
  
  - 解决这些问题对于在 **Edge** 端成功部署 AI 应用至关重要。

 

---

### **LlamaIndex ▷ #**[**blog**](https://discord.com/channels/1059199217496772688/1187460979064324127/1303459257089527859) (3 条消息):

> - `NVIDIA competition`
> - `Automated resume insights agent`
> - `AI in recruiting`

- **NVIDIA 竞赛截止日期临近**：NVIDIA 竞赛的提交截止日期为 **11 月 10 日**，有机会赢取包括 **NVIDIA® GeForce RTX™ 4080 SUPER GPU** 和 DLI 积分在内的奖品。感兴趣的参与者可以在[此处](https://t.co/rtMpetSyu1)查看更多详情并注册。
  
  - 比赛从 *8 月 27 日* 持续到 *11 月 10 日*，鼓励开发者创建由 NVIDIA 和 LlamaIndex 技术驱动的创新 **RAG** 应用。
- **构建自动化简历洞察 Agent 的教程**：一位成员分享了关于创建自动化 **Resume Insights Agent** 的教程，该 Agent 利用了核心解析、提取和结构化输出模块。这个 AI 在招聘中的实际案例展示了如何有效处理非结构化简历，教程可在[此处](https://t.co/pfkoaMhqUc)获取。
  
  - 该教程强调了 AI 在简化招聘流程和改进候选人评估方面的潜力。

 

**提到的链接**：[NVIDIA and LlamaIndex Developer Contest](https://t.co/rtMpetSyu1)：有机会赢取现金奖励、GeForce RTX GPU 等。

 

---

### **LlamaIndex ▷ #**[**general**](https://discord.com/channels/1059199217496772688/1059201661417037995/1303456860967403620) (40 messages🔥):

> - `处理 ChatMessage 输入`
> - `Anthropic tools 的问题`
> - `Llama Index 中的引用 (Citations)`
> - `Pull Request 指南`
> - `解析 Excel 文件`

- **ChatMessage 输入错误**：一位用户在为结构化输出使用 `ChatMessage.from_str()` 时遇到了错误，提示“输入应为有效的字典或 ChatMessage 实例”。
  
  - 有建议指出输入实际上应该是一个列表，这表明在 Llama Index API 的用法上可能存在误解。
- **Anthropic Tools 的 Bug**：一位用户表达了对将 Anthropic 与 tools 配合使用的担忧，收到了“流式传输模式下不支持 Tools”的消息。
  
  - 另一位成员确认他们正在使用底层函数，并指出流式传输尚未集成到 `FunctionCallingAgent` 中。
- **改进引用处理**：一位用户寻求在 Llama Index 中显示引用 (Citations) 的指导，称现有的引用查询引擎不足。
  
  - 另一位成员建议查看 [Citation Query Engine Implementation](https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/) 以获取增强的自定义功能。
- **Pull Request 指南**：一位用户不确定在 Llama Index 的 Pull Request 中如何更新文档和进行版本升级，寻求帮助。
  
  - 另一位成员澄清说文档更新不需要版本升级，但集成 readme 需要。
- **解析和索引 Excel 文件**：一位用户询问了解析和索引凌乱 Excel 文件的方法，考虑将工作表转换为 Markdown 以嵌入到 VectorDB 中。
  
  - 建议尝试 LlamaParse，尽管用户表示该项目的数据不能离开其云平台。

**提到的链接**：

- [LlamaParse: Transform unstructured data into LLM optimized formats — LlamaIndex, Data Framework for LLM Applications](https://www.llamaindex.ai/llamaparse)：LlamaIndex 是一个简单、灵活的数据框架，用于将自定义数据源连接到大语言模型 (LLMs)。
- [llama_index/llama-index-integrations/llms/llama-index-llms-ollama at main · run-llama/llama_index](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/llms/llama-index-llms-ollama)：LlamaIndex 是适用于你的 LLM 应用程序的数据框架 - run-llama/llama_index
- [Build RAG with in-line citations - LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/workflow/citation_query_engine/)：未找到描述
- [fix var name in pinecone vector store by logan-markewich · Pull Request #16853 · run-llama/llama_index](https://github.com/run-llama/llama_index/pull/16853)：确保在尝试使用稀疏向量 (sparse vector) 之前已对其进行定义
- [llama_index/llama-index-integrations/llms/llama-index-llms-ollama/pyproject.toml at 3add7441cb7d482812065b1ecaf4de2ebed4e6c6 · run-llama/llama_index](https://github.com/run-llama/llama_index/blob/3add7441cb7d482812065b1ecaf4de2ebed4e6c6/llama-index-integrations/llms/llama-index-llms-ollama/pyproject.toml#L30)：LlamaIndex 是适用于你的 LLM 应用程序的数据框架 - run-llama/llama_index

---

### **Latent Space ▷ #**[**ai-general-chat**](https://discord.com/channels/822583790773862470/1075282825051385876/1303471990275051725) (36 messages🔥):

> - `混元 (Hunyuan-Large) MoE 模型`
> - `Integuru AI Agent`
> - `Chat.com 域名出售`
> - `Scale AI 的 Defense Llama`
> - `Perplexity 的融资轮次`

- **混元 (Hunyuan-Large) 模型发布**：腾讯发布了 **Hunyuan-Large**，一个 **389B MoE 模型**，声称其在数据使用量更少的情况下优于 **DeepSeek-V2** 和 **Llama3-405B**。关于其开源状态引发了讨论，人们对模型权重是否等同于源代码持怀疑态度。
  
  - *“并非开源，”* 一位成员指出，理由是其使用政策中的歧视性条款，并强调了托管如此庞大模型的潜在问题。
- **对 Integuru 的担忧**：关于 **Integuru AI agent** 的共识大多是悲观的，评论称其“非常脆弱”，且由于维护集成的挑战可能注定失败。有人建议如果成功，它可以促进类似于 TDD 的自愈能力。
  
  - 成员们对长期可行性表示怀疑，特别是 API 更改会影响性能，并承认需要一种带有视觉沙箱 (visual sandbox) 的备选方案。
- **Chat.com 域名收购**：域名 **chat.com** 最近易主，此前由 **Dharmesh** 以超过 **1000 万美元**的价格购得，现在推测已被 OpenAI 以 **1500-2500 万美元**的价格收购。这次交易可能位列域名成交价最高之列。

- 这一消息引发了人们对该域名价值及其与 OpenAI 品牌塑造之间关系的关注，讨论强调了其在 AI 聊天领域的重大意义。
- **Scale AI 发布 Defense Llama**：Scale AI 宣布推出 **Defense Llama**，这是一款专门为美国国家安全定制的 LLM，由 **Meta** 与国防专家合作开发。目前已可集成到美国国防系统中。
  
  - 此次发布使 Scale AI 处于 AI 与国家安全的交汇点，凸显了专用模型在敏感应用中日益增长的重要性。
- **Perplexity 的融资担忧**：Perplexity 今年第四次进行融资，估值倍数达到预期收入的 **180 倍**，引发了外界的关注以及对如此高估值可持续性的质疑。这引发了关于市场是否存在泡沫的讨论。
  
  - 批评人士指出，持续的高倍数估值可能难以为继，引发了对这类融资轮次长期可行性的担忧。

**提到的链接**：

- [来自 Nick Turley (@nickaturley) 的推文](https://x.com/nickaturley/status/1854235325582974982?s=46): http://chat.com
- [Hunyuan-Large：腾讯推出的具有 520 亿激活参数的开源 MoE 模型](https://arxiv.org/abs//2411.02265): 在本文中，我们介绍了 Hunyuan-Large，这是目前最大的基于 Transformer 的开源混合专家模型，总参数量为 3890 亿，激活参数量为 520 亿...
- [来自 Aadit Sheth (@aaditsh) 的推文](https://x.com/aaditsh/status/1854242534257578447): @sama 一直在想 @dharmesh 把域名卖给谁了。现在一切都说得通了。
- [来自 Anu Aakash (@anukaakash) 的推文](https://x.com/anukaakash/status/1853965768813351031?s=46): Google Notebook LM 主持人正在讨论视频格式的图像，而不只是音频。（受 @EHuanglu 视频启发）流程：
- [来自 Alexandr Wang (@alexandr_wang) 的推文](https://x.com/alexandr_wang/status/1853853829336559790): Scale AI 自豪地宣布推出 Defense Llama 🇺🇸：专为美国国家安全打造的 LLM。这是 @Meta、Scale 和国防专家合作的产物，目前已可用...
- [来自 morgan — (@morqon) 的推文](https://x.com/morqon/status/1853930586785915132?s=46): perplexity 今年第四次融资，估值倍数达到预期收入的 180 倍 —— 有人提到泡沫了吗？
- [来自 dharmesh (@dharmesh) 的推文](https://x.com/dharmesh/status/1641094109187260421?s=46): 突发新闻：我买下了域名 http://chat.com。简而言之：我这么做是因为 #ChatUX 是一件大事。详情发布在这里（这样我就不用回复朋友、家人的 100 条私信和邮件了...
- [来自 Steven Tey (@steventey) 的推文](https://x.com/steventey/status/1854254074465788041?s=46&t=Z6mP_1pHALnIw7k1lFkdwQ): OpenAI 刚刚买下了 http://chat.com。趣闻：@dharmesh 是该域名的前持有者，他以 1000 万美元以上价格买入。粗略估计：OpenAI 的买入价格在 1500 万至 2500 万美元之间 —— 使其成为...
- [GitHub - Integuru-AI/Integuru：首个通过逆向工程平台内部 API 构建第三方集成的 AI agent。](https://github.com/Integuru-AI/Integuru): The first AI agent that builds third-party integrations through reverse engineering platforms' internal APIs. - Integuru-AI/Integuru
- [Reddit - 深入探索任何事物](https://www.reddit.com/r/LocalLLaMA/comments/1gjzd1i/tencent_just_put_out_an_openweights_389b_moe_model/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): 未找到描述
- [Tencent Hunyuan-Large | Hacker News](https://news.ycombinator.com/item?id=42054186): 未找到描述

---

### **Interconnects (Nathan Lambert) ▷ #**[**events**](https://discord.com/channels/1179127597926469703/1179127598442348729/) (1 条消息):

swyxio: mee

---

### **Interconnects (Nathan Lambert) ▷ #**[**news**](https://discord.com/channels/1179127597926469703/1179128538679488533/1303455920319238175) (8 条消息🔥):

> - `德语描述的瑞典法律`
> - `AI 搜索初创公司 Perplexity`
> - `Google AI 揭秘`
> - `语言与领域的交集`

- **招聘人员关于语言和法律的案例**：一位招聘人员举了一个“用**德语**编写的关于**瑞典法律**的问题”的例子，强调了特定语言与法律领域之间的小众交集。
  
  - 另一位成员指出，对于美国人来说，这并不算太小众，因为**瑞典和德国**之间有大量的商业往来。
- **Google 的 AI Agent 揭秘**：一位成员分享了一则 [推文](https://x.com/amir/status/1853951978872971749)，讨论了 Google 意外泄露其名为 **Jarvis** 的基于计算机的 AI Agent。
  
  - 讨论升级到社交媒体将如何应对这一泄露事件，预计会引发高度关注。
- **Perplexity 的 180 倍营收倍数**：根据一则 [推文](https://x.com/amir/status/1853927732327133506)，尽管与 **NYT** 及其他出版商存在法律纠纷，AI 搜索初创公司 **Perplexity** 的远期营收估值倍数仍接近 **180 倍**。
  
  - 这一潜在估值吸引了广泛关注，甚至包括那些对该初创公司运营模式表示困惑的人。

**提到的链接**：

- [Amir Efrati (@amir) 的推文](https://x.com/amir/status/1853927732327133506)：哇：尽管与 NYT 等出版商有法律纠纷，AI 搜索初创公司 Perplexity 的远期营收倍数仍接近 180 倍。https://www.theinformation.com/articles/perplexity-nears-9...
- [Amir Efrati (@amir) 的推文](https://x.com/amir/status/1853951978872971749)：新闻：Google 今天出了个小差错，泄露了其操作计算机的 Agent AI (Jarvis)。

---

### **Interconnects (Nathan Lambert) ▷ #**[**ml-questions**](https://discord.com/channels/1179127597926469703/1179208129083363358/1303518885131059250) (4 条消息):

> - `Prompt 漂移`
> - `ChatGPT 性能追踪`
> - `应用的数据质量`

- **重新定义漂移：Prompt 的变化至关重要**：讨论围绕 **Drift**（漂移）展开，将其定义为针对同一任务更改 Prompt，并呼吁建立除主观感受之外的指标来进行性能评估。
  
  - 这凸显了在不依赖“**感觉** (vibes)”的情况下，需要更好的方法论来量化性能。
- **ChatGPT 的底层追踪**：一位成员推测 **ChatGPT** 可能会监控与不同 Prompt 相关的性能细节，暗示其拥有一个复杂的追踪系统。
  
  - 这引发了关于此类追踪所能提供的洞察**深度**的疑问。
- **对高质量数据的追求**：强调了对稳健数据的必要性，大家一致认为需要“**真正优质的数据**”来证实性能见解和评估。
  
  - 这种必要性表明，当前的信息可能尚未准备好用于“**传统**”或乏味的应用场景。

---

### **Interconnects (Nathan Lambert) ▷ #**[**ml-drama**](https://discord.com/channels/1179127597926469703/1181746144821387334/1303448972802789440) (3 条消息):

> - `GPU 争议`
> - `V100 SSH 访问`

- **希望能分享 GPU 争议**：*natolambert* 表示希望能分享一些**内部 GPU 争议**，暗示其组织内部可能存在的问题或故事。
  
  - 这种对内部冲突的透露引发了成员们的兴趣和讨论。
- **提供 V100 的 SSH 访问权限**：*xeophon.* 给予了积极回应，建议共享其部分 **V100** GPU 资源的 **SSH 访问权限**，表示愿意提供帮助。
  
  - 这一提议展示了同僚情谊，并带有一丝幽默，正如心形表情符号 ❤️ 所暗示的那样。

---

### **Interconnects (Nathan Lambert) ▷ #**[**random**](https://discord.com/channels/1179127597926469703/1183121795247779910/1303472379225309184) (7 messages):

> - `Discord app for email verification` (用于邮箱验证的 Discord 应用)
> - `Manual email collection process` (手动邮箱收集流程)
> - `External database synchronization` (外部数据库同步)

- **寻找用于邮箱验证的 Discord 应用**：一位成员表示需要一种能与外部数据库同步以在 Discord 中进行邮箱验证的解决方案，并提到他们可能会自己开发。
  
  - *看来你必须构建某种身份验证流 (auth flow)* 是他们对研究结果的总结。
- **作为备选方案的手动邮箱收集**：另一位成员建议了一种手动的规避方法，即将频道中的所有人锁定，直到他们提供邮箱后再解锁。
  
  - 他们提到曾尝试过现有的应用，但已经有一段时间没有检查新的选项了。
- **寻求推荐**：成员呼吁推荐能够处理邮箱验证和数据库同步的应用。
  
  - 这一求助引发了另一位成员的轻松回复，寻求进一步的见解。
- **关于服务干扰的诙谐评论**：一位成员在参与讨论时评论道，这种流程的本质是“巨大的服务/干扰”。
  
  - 这一评论凸显了管理 Discord 社区时经常涉及的复杂设置。

---

### **Interconnects (Nathan Lambert) ▷ #**[**memes**](https://discord.com/channels/1179127597926469703/1187551504995987576/1303451701168050256) (2 messages):

> - `OpenAI CEO petition` (OpenAI CEO 请愿书)
> - `Biden not running` (拜登不参选)

- **OpenAI 领导层变动请愿书**：一位成员分享了一条 [推文](https://x.com/alexrkonrad/status/1853818081295949915)，主张 OpenAI 今天解雇并重新聘用其 CEO，以此作为一种冷静的消遣。
  
  - *这份请愿书反映了社区对 OpenAI 领导层稳定性的持续关注。*
- **关于拜登的意外公告**：一位成员发推文称，选民们发现了一个令人惊讶的消息：**Joe Biden** 不再竞选连任。
  
  - 该推文强调了这一政治转变的意外性，暗示对即将到来的选举有**重大影响**。

**提到的链接**：

- [来自 Alex Konrad (@alexrkonrad) 的推文](https://x.com/alexrkonrad/status/1853818081295949915)：主张 OpenAI 今天解雇并重新聘用其 CEO 的请愿书，作为一种冷静的消遣。
- [来自 Armand Domalewski (@ArmandDoma) 的推文](https://x.com/armanddoma/status/1853895012079280423?s=46)：想象一下，一个选民直到今天才发现 Joe Biden 不参加竞选。

---

### **Cohere ▷ #**[**discussions**](https://discord.com/channels/954421988141711382/954421988783444043/1303490153104408788) (17 messages🔥):

> - `Cohere Search Process` (Cohere 搜索流程)
> - `Embed3 Multimodal Embeddings` (Embed3 多模态嵌入)
> - `Parsing Techniques Comparison` (解析技术对比)
> - `Cohere Reranker API Availability` (Cohere Reranker API 可用性)

- **Cohere 搜索流程解析**：一位成员推测了 **ChatGPT** 和类似模型如何利用 Bing API 生成回复，并提到了使用来自各种网络源的 **snippets**（片段）。
  
  - *关于搜索结果与训练数据之间平衡的精确决策过程仍不清楚*。
- **对 Embed3 多模态嵌入感到兴奋**：一位成员表达了对开始使用 **embed3-multimodal embeddings** 项目的热情，认为这是相比 **CLIP** 等早期模型的重大进步。
  
  - 他们目前的重点是构建一个集成 PostgreSQL 和 **Cohere.embed3** 的 **parsing service**（解析服务）。
- **API 与自托管解析技术的对比**：讨论强调了各种 **parsing services**（解析服务），指出 **Upstage/Pymu4PDF** 相比 **Marker** 等更昂贵的选项非常有效。
  
  - 虽然自托管对于拥有丰富计算资源的团队很有价值，但该成员认为 API 服务更适合他们的初创公司需求。
- **Cohere Reranker 仅限 API**：一位用户询问了通过 API 获取 **Cohere reranker** 的可用性。
  
  - 另一位成员确认它*仅通过 API 提供*。

---

### **Cohere ▷ #**[**questions**](https://discord.com/channels/954421988141711382/1168411509542637578/1303577933394214974) (4 messages):

> - `Cohere billing process` (Cohere 计费流程)
> - `Contacting sales` (联系销售)

- **Cohere 计费：GCP Marketplace 困惑**：一位用户提出了关于通过 GCP Marketplace 激活 Cohere 后如何计费的问题，询问是扣除平台上注册的付款卡还是通过 GCP 计费。
  
  - 另一位成员澄清说 **Vertex 通过 GCP 计费**，这可能解决了该用户对计费偏好的疑虑。
- **咨询销售联系方式**：一位用户询问是否可以直接在频道内联系销售。
  
  - 一位成员立即提供了一个电子邮箱地址，表示可以联系 [**support@cohere.com**](mailto:support@cohere.com) 以获得进一步帮助。

---

### **OpenAI ▷ #**[**ai-discussions**](https://discord.com/channels/974519864045756446/998381918976479273/1303450509184602112) (9 messages🔥):

> - `AI Storytelling Improvements` (AI 叙事改进)
> - `Interactive Prompting Techniques` (交互式 Prompt 技巧)
> - `GitHub Copilot Updates` (GitHub Copilot 更新)

- **AI 叙事能力有所提升**：一位成员对 AI 现在的叙事水平表示了*由衷的惊讶*，并指出早期的输出内容既**枯燥**又**可预测**。
  
  - 他们提到，尽管 Prompt 是自己编写的，但目前的质量仍让他们感到惊喜。
- **通过交互增强 AI 创造力**：一位成员建议在开始故事 Prompt 之前，先询问 AI *“什么是好故事”*，以激发其创造力。
  
  - 另一位成员建议要求 AI 审查自己的作品，并根据反馈进行调整，以进一步优化输出。
- **一致的 Prompt 技巧能产生更好的结果**：一位成员分享道，使用 `Please critique your answer. Then, answer the question again`（请批评你的回答。然后，再次回答问题）已成为他们进行任何 AI 交互的首选方法。
  
  - 他们强调了这种反馈循环在各种 AI 应用中的有效性，而不局限于叙事。
- **对 GPT o1 preview 思考过程的好奇**：一位成员对无法查看 **GPT o1 preview** 的思考过程表示担忧。
  
  - 他们的询问反映了用户对理解 AI 决策过程的广泛兴趣。
- **GitHub Copilot 推出新功能**：据观察，**GitHub Copilot 现在新增了 Sonnet** 选项，与 **o1** 并列。
  
  - 这一更新表明 AI 编程辅助工具正在持续增强。

---

### **OpenAI ▷ #**[**gpt-4-discussions**](https://discord.com/channels/974519864045756446/1001151820170801244/1303451291372228699) (4 messages):

> - `Document summarization hallucinations` (文档摘要幻觉)
> - `Involvement of human experts` (人类专家的参与)
> - `Canvas document deletion in CGPT4o` (CGPT4o 中 Canvas 文档的删除)

- **文档摘要中的幻觉风险**：一位成员对文档摘要工作流中潜在的**幻觉**表示担忧，并指出虽然在 **GPT-4o** 的测试中没有发现问题，但担心生产环境的规模化应用。
  
  - 另一位成员确认幻觉是 **LLM** 固有的风险，并建议通过第二次 **LLM pass** 进行事实核查以降低风险。
- **人类专家是必不可少的保障**：一位参与者强调，在使用强大模型进行摘要任务时，必须有**人类领域专家**的参与。
  
  - *“你真的需要有人类……参与其中（human in the loop）来监视并双重检查，”* 强调了人类监督的重要性。
- **希望在 Canvas 中删除文档**：一位成员表达了希望在 **CGPT4o + Canvas** 的集成中能够**删除 Canvas 文档**。
  
  - 这一点突出了当前功能中用户感到不便的潜在局限性。

---

### **OpenAI ▷ #**[**prompt-engineering**](https://discord.com/channels/974519864045756446/1046317269069864970/1303581519692169298) (3 messages):

> - `AI JSON Modification Techniques` (AI JSON 修改技巧)
> - `Assistant API Token Limits` (Assistant API Token 限制)
> - `File Upload Data Handling` (文件上传数据处理)

- **AI 在处理大型 JSON 文件时遇到困难**：一位成员描述了向 Assistant 传递大型 **JSON** 数据时遇到的问题，指出它有时会从输出中遗漏部分数据。
  
  - 他们推测这可能是由于 **Token 限制**导致的，从而引起输入文件的处理不完整。
- **对 JSON 数据进行分块以获得更好的结果**：该成员考虑对 **JSON** 数据进行分块（chunking），以确保 Assistant 处理所有条目，尽管他们希望避免这样做，因为这可能会使未来的任务复杂化。
  
  - 他们正在寻求替代方案，而不是将数据拆分为更小的部分。
- **轮询 Assistant 的修改**：讨论了如何通过 Prompt 引导 AI 填充 **JSON** 数据中的特定值，而不影响其他条目。
  
  - 该成员提到使用两个 Assistant：一个负责处理数据上传，另一个负责格式化输出，以避免 **JSON** 结构出现“创造性”的格式错误。

---

### **OpenAI ▷ #**[**api-discussions**](https://discord.com/channels/974519864045756446/1046317269069864970/1303581519692169298) (3 messages):

> - `AI Prompting Techniques`
> - `JSON Data Handling`
> - `Token Limit Issues`

- **探索 AI Prompting Techniques**：一位用户分享了他们使用两个 Assistant 的方法，一个用于上传 JSON 数据，另一个用于格式化输出以防止格式错误。
  
  - 他们提到一个幽默的结果：尽管有指令，AI 仍会变得“有创意”并输出格式错误的 JSON 数据。
- **处理大型 JSON 文件的困扰**：该用户对 AI 在处理过程中删除大型 JSON 数据部分内容导致输出不完整表示沮丧。
  
  - 他们推测这个问题可能与 **Token limits** 有关，导致 Assistant 省略了数据而不是处理整个文件。
- **避免对 JSON 数据进行分块（Chunking）**：该用户考虑过通过分块来管理数据大小，但表示希望避免这种方案，因为可能会带来潜在的复杂性。
  
  - 他们正在寻求一种更有效的方法，以确保 AI 能够处理大型 JSON 文件中的所有条目而不遗漏。

 

---

### **tinygrad (George Hotz) ▷ #**[**general**](https://discord.com/channels/1068976834382925865/1068976834928193609/1303494539419455539) (1 messages):

> - `TokenFormer implementation`
> - `tinygrad`

- **极简版 TokenFormer 移植至 tinygrad**：**TokenFormer** 的一个极简实现已成功移植到 **tinygrad**，并可在 [GitHub repository](https://github.com/kroggen/tokenformer-minimal/tree/tinygrad) 上获取。
  
  - 该实现专注于 **Inference** 和 **Learning** 能力，增强了 tinygrad 的功能。
- **关于 tinygrad 增强功能的讨论**：社区对新的 **tinygrad** 特性以及它们如何改进模型实现和性能感到兴奋。成员们分享了关于未来可能与其他框架集成以进一步扩展 tinygrad 能力的见解。
- **对协作开发的兴趣**：几位成员表示有兴趣协作改进 **tinygrad** 生态系统，重点关注贡献和支持。有人建议组织每月会议来讨论正在进行的项目并收集反馈。
- **在 tinygrad 中探索新架构**：对话转向探索 **tinygrad** 可以支持的**新架构**，特别提到了对**效率的需求**。成员们辩论了各种设计选择和可以补充 tinygrad 的框架的优缺点。
- **tinygrad 模型的性能指标**：关于在 tinygrad 中实现的模型的**性能指标（Performance Metrics）**引发了热烈讨论，并提出了标准化基准测试（Benchmarking）的建议。成员们一致认为，统一的指标将有助于评估进度并吸引更多用户。

 

**提到的链接**：[GitHub - kroggen/tokenformer-minimal at tinygrad](https://github.com/kroggen/tokenformer-minimal/tree/tinygrad)：用于 Inference 和 Learning 的 TokenFormer 极简实现 - GitHub - kroggen/tokenformer-minimal at tinygrad

 

---

### **tinygrad (George Hotz) ▷ #**[**learn-tinygrad**](https://discord.com/channels/1068976834382925865/1070745817025106080/1303457148042350683) (10 messages🔥):

> - `Hailo Reverse Engineering`
> - `CUDA WMMA Layout Discrepancies`

- **Hailo 逆向工程开始**：一位成员正在开始他们的 **Hailo 逆向工程**过程，目标是开启对新加速器的支持。他们担心如果需要在 **ONNX**、**tinygrad** 和 **TensorFlow** 之间进行接口连接，是否必须多次编译 **Kernels**。
  
  - 他们希望通过确保 Kernels 在运行之间保持一致来避免浪费时间，特别是在使用 `BEAM=2` 时。
- **对 CUDA WMMA Layout 的困惑**：一位成员提出疑问，**CUDA WMMA** 中 **A** 的 Layout 是否与 [NVIDIA 文档](https://docs.nvidia.com/cuda/parallel-thread-execution/#matrix-fragments-for-mma-m16n8k16-with-floating-point-type)中概述的预期格式不同。他们提供的代码片段显示了输入形状（Input Shapes）的差异。
  
  - 另一位成员寻求关于 **ops_python** 映射函数差异的澄清，并表示有兴趣正确解决与实际 **TC 实现**（Tensor Core）的任何不匹配。

### **OpenInterpreter ▷ #**[**general**](https://discord.com/channels/1146610656779440188/1147665339266650133/1303466886511595664) (8 messages🔥):

> - `Comparative Tool Interfaces` (工具接口比较)
> - `OS Mode Updates` (OS Mode 更新)
> - `Claude Computer Control`

- **寻找工具接口标准**：一位成员表示有兴趣讨论**工具接口的比较讨论**，建议在**众多的框架中**需要**标准化**。
  
  - 另一位成员幽默地指出框架数量之多，很难提供具体的细节。
- **OS Mode 仅限 Anthropic 模型**：成员们讨论了 **OS Mode** 的新更新是否仅支持 **Anthropic 模型**，并确认目前确实如此，但预计很快会有修复。
  
  - 一位成员提到计划在第二天的**家庭聚会上进行演示**。
- **理解 OS Mode 功能**：一位成员询问 **OS Mode** 如何将 prompt 转换为桌面操作，质疑代码生成和鼠标点击背后的机制。
  
  - 解释称系统采用 **Claude Computer Control** 来执行鼠标点击，并提供了[相关代码链接](https://github.com/OpenInterpreter/open-interpreter/blob/development/computer_use/tools/computer.py)以供参考。

**提到的链接**：

- [Remote ollama/llava (localhost:11434) fails to load in OS mode · Issue #1486 · OpenInterpreter/open-interpreter](https://github.com/OpenInterpreter/open-interpreter/issues/1486#issuecomment-2454883179)：描述了尝试在 OS Mode 下运行 ollama/llava 时出现的 Bug：我不确定这是否与特定模型有关，或者是由于指向远程 ollama 服务器导致的...
- [open-interpreter/computer_use/tools/computer.py at development · OpenInterpreter/open-interpreter](https://github.com/OpenInterpreter/open-interpreter/blob/development/computer_use/tools/computer.py)：计算机的自然语言接口。通过在 GitHub 上创建账号为 OpenInterpreter/open-interpreter 做出贡献。
- [Open Interpreter](https://github.com/OpenInterpreter/)：Open Interpreter 有 6 个可用的仓库。在 GitHub 上关注他们的代码。

---

### **OpenInterpreter ▷ #**[**O1**](https://discord.com/channels/1146610656779440188/1194880263122075688/) (1 messages):

zer0blanks.: [https://www.tiktok.com/t/ZTFckAFHR/](https://www.tiktok.com/t/ZTFckAFHR/)

---

### **Modular (Mojo 🔥) ▷ #**[**mojo**](https://discord.com/channels/1087530497313357884/1151418092052815884/1303513360012415200) (8 messages🔥):

> - `C_Buffer Structure Changes` (C_Buffer 结构变更)
> - `Performance Improvements using Pointers` (使用 Pointers 提升性能)
> - `Understanding Bounds Checks` (理解边界检查)

- **C_Buffer 结构变更提升性能**：一位成员表示他们将更改 **C_Buffer** 结构，并在 Mojo 中开发 **matmul kernel** 时告知他人后续的性能结果。
  
  - 他们对社区表示感谢，并断言使用 **pointers** 而不是 list 使实现速度更快。
- **关于使用额外元素初始化列表的问题**：一位成员质疑初始化一个包含 **8 个元素** 的 list 然后再追加 **8 个** 的逻辑。
  
  - 原作者承认上传了之前版本的 **Mojo 代码**。
- **询问额外的安全边界检查**：一位成员请求提供链接，以了解导致 list 结构变慢的具体 **additional security bounds checks**。
  
  - 另一位成员回应称，这些检查是 **generic**（通用的），除了 C 语言之外，大多数编程语言中都存在，并提到了 C++ 推荐的索引方法。

---

### **OpenAccess AI Collective (axolotl) ▷ #**[**general**](https://discord.com/channels/1104757954588196865/1104757955204743201/1303842592080924693) (4 messages):

> - `ScheduleFree SOAP`
> - `Hyperparameter Adjustments` (超参数调整)
> - `MOEs and Model Merging` (MOE 与模型合并)
> - `CAME Comparison` (CAME 对比)

- **ScheduleFree SOAP 具有显著的效率提升**：据称 [ScheduleFree SOAP 实现](https://github.com/ClashLuke/HeavyBall/blob/main/heavyball/schedule_free_palm_foreach_soap.py#L296)比传统 SOAP 具有更高的**计算效率**、**内存效率**，并且通过允许更高的学习率实现更快的收敛。
  
  - 这种效率使其在现有优化器中极具竞争力，主要专注于快速的 _foreach 和 PaLM 版本。
- **关于 ScheduleFree SOAP 超参数变更的说明**：为了获得 ScheduleFree SOAP 的最佳性能，必须调整超参数：它利用了 PaLM 的 **beta2 schedule**，将 'betas' 重命名为 'beta'，同时支持更高的学习率 —— 建议增加 **10 倍**。
  
  - Warmup 是必要的，文献中建议为 **10%**，但 **100 steps** 已足以有效启动。
- **Llama 3.2 发布后对 MOE 和模型合并的兴趣减弱**：一位成员询问了 **Models of Experts (MOEs)** 和模型合并的现状，指出自 **Llama 3.2** 发布以来缺乏相关讨论。
  
  - 这引发了关于这些策略在当前环境下的持续相关性和应用价值的疑问。
- **与 CAME 的对比讨论**：一位成员询问了 ScheduleFree SOAP 与 **CAME** 的对比情况，寻求关于性能指标或效率的见解。
  
  - 这一对比表明了用户对了解优化技术最新进展的兴趣。

 

**提到的链接**：[HeavyBall/heavyball/schedule_free_palm_foreach_soap.py at main · ClashLuke/HeavyBall](https://github.com/ClashLuke/HeavyBall/blob/main/heavyball/schedule_free_palm_foreach_soap.py#L296)：各种优化器的实现；主要侧重于快速的 _foreach 和 PaLM 版本 - ClashLuke/HeavyBall

 

---

### **OpenAccess AI Collective (axolotl) ▷ #**[**axolotl-dev**](https://discord.com/channels/1104757954588196865/1104758010959634503/1303452129410683021) (1 messages):

> - `Zero2 performance` (Zero2 性能)
> - `Zero1 troubleshooting` (Zero1 故障排除)

- **Zero2 性能问题**：一位用户报告称 **Zero2** 运行极其**缓慢**，表示它无法满足其需求。
  
  - 他们表示需要寻找**修复方案**，同时考虑退回到 **Zero1**。
- **探索 Zero1 的修复方法**：鉴于 **Zero2** 的性能问题，用户正在寻找改进 **Zero1** 的解决方案。
  
  - 这暗示如果 **Zero2** 的性能没有改善，可能会转回使用 **Zero1**。

 

---

### **LAION ▷ #**[**general**](https://discord.com/channels/823813159592001537/823813160075132991/1303732549574721616) (4 messages):

> - `Open Source Speech Enhancer` (开源语音增强器)
> - `Resemble Enhance`

- **来自德国的 Spirit 批评 Resemble Enhance**：一位用户询问优秀的开源**语音增强器**，随后有人提到了 [Resemble Enhance](https://link.to.resemble)。
  
  - **Spirit from Germany** 对其进行了测试，发现由于存在**伪影 (artifacts)**，结果*不尽如人意*。
- **关于语音增强工具的讨论**：对话集中在各种**语音增强器**的性能上，用户分享了各自的使用经验。
  
  - 关于 **artifacts** 以及 Resemble Enhance 等工具的整体有效性问题被重点强调。

 

---

### **DSPy ▷ #**[**general**](https://discord.com/channels/1161519468141355160/1161519469319946286/1303755128351756308) (2 messages):

> - `RLhF paradigm` (RLHF 范式)
> - `Serialized multi-component systems` (序列化多组件系统)

- **理解 RLHF 和文本反馈**：一位成员提出了一个关于 **RLHF** (Reinforcement Learning from Human Feedback) 范式的理论问题，特别是关于在开放世界场景中，除了简单的硬标签 (hard labeling) 之外，如何将**文本反馈**转化为数值奖励。
  
  - *“除了硬标签，难道没有其他方法吗？”* 这表现出对更灵活的反馈机制的好奇。
- **多组件系统文档化的局限性**：另一位成员报告称，在**序列化多组件 DSPy 系统**中，`lm.history()` 仅显示第一个组件的 doc string，中间类的详细信息较少。
  
  - 这引发了关于这种行为是**预期之内的**，还是表明了复杂系统文档生成方式存在局限性的疑问。

 

---

### **Torchtune ▷ #**[**dev**](https://discord.com/channels/1216353675241590815/1236040539409879170/1303496970484318239) (2 messages):

> - `KD-div 误解`
> - `交叉熵优化`

- **KD-div 返回值的混淆**：有人指出，虽然我们将其称为 **KD-div**，但其 **返回值** 实际上是 **cross-entropy**，这在与其他损失函数（如 KL-div）进行比较时可能会导致误解。
  
  - 特别是在考虑交换 Teacher 和 Student Logits（这一过程通常被称为 *reverse KL*）时，可能会产生潜在的混淆。
- **交叉熵作为一种自然扩展**：分享的一个观点认为，*针对 cross-entropy 进行优化* 感觉更直观，将其解释为 cross-entropy loss 从常规标签到 Teacher 模型生成的 soft labels 的扩展。
  
  - 这一观点强调了从训练中的 **hard labels** 到 Fine-tuning 中的 **soft labels** 的演变是一个自然的过程。

 

---

---

---

---

---

---

---

{% else %}

> 完整的逐频道详情已在邮件中截断。
> 
> 如果你想查看完整的详情，请访问此邮件的网页版：[{{ email.subject }}]({{ email_url }})！
> 
> 如果你喜欢 AInews，请[分享给朋友](https://buttondown.email/ainews)！提前感谢！

{% endif %}
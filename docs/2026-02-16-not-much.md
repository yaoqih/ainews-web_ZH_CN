---
companies:
- alibaba
- openai
- deepseek
- z-ai
- minimax
- kimi
- unsloth
- ollama
- vllm
date: '2026-02-16T05:44:39.731046Z'
description: '**阿里巴巴**发布了 **Qwen3.5-397B-A17B**，这是一款**开源权重**模型。该模型具备**原生多模态**能力和**空间智能**，采用**混合线性注意力
  + 稀疏混合专家（MoE）**架构，支持 **201 种语言**以及高达 **256K token** 的**长上下文窗口**。


  相比 **Qwen3-Max** 和 **Qwen3-VL** 等先前版本，该模型性能有所提升，其**稀疏率**约为 **4.3%**。社区讨论重点关注了其采用的**门控
  Delta 网络（Gated Delta Networks）**，尽管模型体积庞大（约 **800GB BF16**），但该技术仍能实现高效推理。目前，通过量化技术，该模型已成功在
  Apple Silicon 设备上实现本地运行。


  托管的 API 版本 **Qwen3.5-Plus** 支持 **100 万（1M）上下文**，并集成了搜索和代码解释器功能。在此次发布之前，**阶跃星辰（Z.ai）**、**稀宇科技（Minimax）**和**月之暗面（Kimi）**等其他中国实验室也相继更新了大模型。该模型采用
  **Apache-2.0** 协议授权，预计将是 **DeepSeek v4** 问世前的最后一个重大发布。此外，新闻还提到 **Pete Steinberger**
  已加入 **OpenAI**。'
id: MjAyNi0w
models:
- qwen3.5-397b-a17b
- qwen3.5-plus
- qwen3-max
- qwen3-vl
- kimi
people:
- pete_steinberger
- justinlin610
title: Qwen3.5-397B-A17B：Open-Opus 系列中最小的级别，是一款非常高效的模型。
topics:
- native-multimodality
- spatial-intelligence
- sparse-moe
- long-context
- model-quantization
- model-architecture
- model-deployment
- inference-optimization
- apache-2.0-license
---

**恭喜 Qwen！**

> 2026年2月13日至2026年2月16日的 AI 新闻。我们为您检查了 12 个 subreddit、[544 个 Twitter](https://twitter.com/i/lists/1585430245762441216) 和 24 个 Discord（**261** 个频道和 **26057** 条消息）。预计节省阅读时间（按 200wpm 计算）：**2606** 分钟。[AINews 网站](https://news.smol.ai/) 允许您搜索所有历史期数。提示一下，[AINews 现已成为 Latent Space 的一个板块](https://www.latent.space/p/2026)。您可以[选择加入/退出](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack)邮件订阅频率！

**来自 Qwen 的一次出色发布。**

> *2026年2月13日至2026年2月16日的 AI 新闻。我们为您检查了 12 个 subreddit、[544 个 Twitter](https://twitter.com/i/lists/1585430245762441216) 和 24 个 Discord（**261** 个频道和 **26057** 条消息）。预计节省阅读时间（按 200wpm 计算）：**2606** 分钟。[AINews 网站](https://news.smol.ai/) 允许您搜索所有历史期数。提示一下，[AINews 现已成为 Latent Space 的一个板块](https://www.latent.space/p/2026)。您可以[选择加入/退出](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack)邮件订阅频率！*

恭喜 [Pete Steinberger 加入 OpenAI](https://x.com/sama/status/2023150230905159801)，正如我们所[预测](https://www.latent.space/p/ainews-sci-fi-with-a-touch-of-madness)的那样。关于这一点没有更多可补充的，所以我们就不多说了。

今天的头条新闻是 Qwen 3.5。继 [Z.ai](https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights)、[Minimax](https://www.latent.space/p/ainews-new-gemini-3-deep-think-anthropic) 和 [Kimi](https://www.latent.space/p/ainews-moonshot-kimi-k25-beats-sonnet) 等其他中国模型实验室更新其领先模型之后，Qwen 3.5 也紧随其后。但与前两家不同的是，Qwen 3.5 与 Kimi 属于同一量级，拥有 400B 参数，sparsity ratio 约为 4.3%，而 Kimi 则是更激进的 3.25%。他们并没有声称在全线达到 SOTA，尤其是在 coding benchmark 方面，但相比 [Qwen3-Max](https://news.smol.ai/issues/25-09-05-1t-models) 和 [Qwen3-VL](https://news.smol.ai/issues/25-09-23-alibaba-yunqi) 确实有了显著提升。

Native Multimodality 和 [Spatial Intelligence](https://qwen.ai/blog?id=qwen3.5#spatial-intelligence) 是该模型的主打特性，我们建议点击进入博客查看示例，除此之外没太多可说的——这是中国最多产的开源模型实验室发布的一次非常受欢迎的旗舰模型更新，可能也是 DeepSeek v4 发布前的最后一次。

![](https://substackcdn.com/image/fetch/$s_!1fDP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0472c69a-cd07-4bde-8b10-61bc1d0702a7_2444x1704.png)

---

# AI Twitter 综述

**阿里巴巴 Qwen3.5 开源权重 “frontier MoE” 发布（以及推理/基础设施方面的影响）**

- **Qwen3.5-397B-A17B 发布**：阿里巴巴发布了 **Qwen3.5-397B-A17B**，定位为 Qwen3.5 系列中的首个开源权重模型：**原生多模态 (native multimodal)**，“思考与非思考模式”，**混合线性注意力 (hybrid linear attention) + 稀疏 MoE (sparse MoE)**，“大规模 RL 环境扩展”，支持 **201 种语言**，采用 **Apache-2.0** 协议 ([官方公告](https://twitter.com/Alibaba_Qwen/status/2023331062433153103)；[@JustinLin610](https://twitter.com/JustinLin610/status/2023332446713070039) 也同步转发)。他们还澄清 **Qwen3.5-Plus 是相同基础模型的托管 API 版本**，具有 **1M context**（对比模型原生的 **256K**）以及搜索/code interpreter 集成 ([澄清说明](https://twitter.com/JustinLin610/status/2023340126479569140))。
- **架构 + KV-cache 影响**：社区讨论集中在 **Gated Delta Networks / “GatedDeltaNet” + 稀疏 MoE (sparse MoE)**，认为这是推理在长上下文下保持可控的原因。vLLM 推出了 **首日支持 (day-0 support)** 并强调了其 **397B 总参数、17B 激活参数**、多模态以及吞吐量/延迟优势 ([vLLM recipe](https://twitter.com/vllm_project/status/2023341059343061138))。一份具体的 KV-cache 估算表明，由于 KV head 较少且存在多个 gated-delta 层，在 BF16 格式下仅需 **~31KB/token**，在 **262K context** 时仅需 **~8.05GB KV**（FP8 下约为 4GB）([KV 计算](https://twitter.com/bnjmn_marie/status/2023424404504342608))。
- **部署现状：权重庞大，但运行效果出人意料**：尽管具有 “~800GB BF16” 的规模，仍有用户报告通过 MLX/Q4 在 Apple Silicon 上成功运行（例如提到的 **~225GB RAM**）([mlx 报告](https://twitter.com/pcuenq/status/2023369902011121869)；[awnihannun 演示](https://twitter.com/awnihannun/status/2023462412092059679))。Unsloth 发布了“在 **256GB Mac/RAM** 上运行 4-bit”的指南，并声称其性能足以媲美顶尖闭源模型（虽然是营销说辞，但对推广很重要）([Unsloth](https://twitter.com/UnslothAI/status/2023338222601064463))。Ollama 迅速将其上线云端 ([Ollama](https://twitter.com/ollama/status/2023334181804069099))。
- **基准测试 + “智能体 RL (agentic RL)” 与效率疑问**：早期评价认为该模型比 Qwen3-*Max* 和之前的 Qwen VL 模型有显著进步，尤其在 **vision** 方面有明显提升；也有人要求提供“推理效率 (reasoning efficiency)”的证据，而非单纯的原始分数 ([scaling01](https://twitter.com/scaling01/status/2023343368399704506))。teortaxesTex 指出，它在某些测试集中意外地超过了 Qwen3-Max-thinking，并推测改进源于 **agentic RL** ([评论](https://twitter.com/teortaxesTex/status/2023331885402009779))。同时，也存在“黑盒评估 (black-box eval)”的批评以及特定任务失败的情况（例如 SVG / “Vending-Bench” 风格的测试）([Vending-Bench 声明](https://twitter.com/andonlabs/status/2023450768406364238)；[SVG 对比](https://twitter.com/scaling01/status/2023364296277721300))。
- **定价争议**：多篇帖子认为，鉴于其宣称的推理效率，阿里巴巴的 **API 定价偏高/奇怪**，并将其与 Kimi/GLM 的产品进行了对比 ([定价抱怨](https://twitter.com/scaling01/status/2023346718377406840)；[更多](https://twitter.com/scaling01/status/2023349177443377370))。这成为了一个反复出现的话题：“模型很棒，但服务成本逻辑不清晰。”

**开放 Agent，“测试框架工程 (harness engineering)”，以及 OpenClaw → OpenAI 的故事**

- **OpenClaw 作为单人杠杆作用的证明点**：OpenClaw 的故事被视为“单人团队 + 编程 Agent”快速交付具有世界影响力产品的象征，最终以 Peter Steinberger 加入 OpenAI 或被其收购告终 ([Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/2023248474503094774))。此推文还引发了关于 OpenAI 在收购后可能如何处理开源问题的广泛讨论。
- **Anthropic 与开源界的紧张关系**：一个主要的讨论集群批评了 Anthropic 对开源和 OpenClaw 使用的态度，声称其限制/封锁行为迫使开发者转向其他模型/供应商 ([ThePrimeagen](https://twitter.com/ThePrimeagen/status/2023194211445834132); [Teknium](https://twitter.com/Teknium/status/2023251135201738794))。另一些人则淡化了其战略影响（称其“一周内就能随手写出来”），同时也承认了这在 OSS 圈内造成的声誉损失 ([scaling01](https://twitter.com/scaling01/status/2023217588319277471))。另外，Anthropic 宣布了一项重大的业务扩张：设立**班加罗尔办公室**，并指出印度是 Claude.ai 的**第二大市场** ([Anthropic](https://twitter.com/AnthropicAI/status/2023322514206957688))。
- **Harness 才是真正的护城河**：多条推文汇聚成一个务实的论点：**Agent 不仅仅是模型**；其“harness”（工具链、上下文管理、生命周期、技能、评估/可观测性）是具有复利效应的基础设施，且日益成为差异化的核心。参见 Ben Burtenshaw 将 harness 定义为模型周围的“操作系统”，以及专有 Agent 体验更好，部分原因在于模型是基于其 harness 模式进行训练的观点 ([ben_burtenshaw](https://twitter.com/ben_burtenshaw/status/2023429103731269696))。构建 Agent 系统的从业者也对此表示共鸣：“构建一个好的 harness 很难，且会随时间产生复利效应” ([brivael](https://twitter.com/brivael/status/2023203131329503583))。
- **轻量级 Agent 替代方案**：在“重型 harness”思维之外，人们对极简 Agent 栈也产生了兴趣：PicoClaw 和 nanobot 被推崇为 OpenClaw 的极简替代方案，支持多种模型后端以及 MCP/vLLM ([TheTuringPost](https://twitter.com/TheTuringPost/status/2023416488884129826))。
- **Agent 可观测性/评估（evals）成为基本门槛**：LangChain/LangSmith 传递了一个信息：对于 Agent 而言，trace 就是新的“堆栈跟踪（stack trace）”，调试需要可观测性优先的工具 ([meetup](https://twitter.com/LangChain/status/2023457846843551946); [tracing plug-ins](https://twitter.com/LangChain/status/2023532973086159283))。这与广泛的抱怨相吻合，即当前的 Agent 行为缺乏确定性且需要人工盯防（babysitting）。

**OpenAI/Codex 使用量激增、子 Agent 以及安全加固**

- **Codex 采用率声明**：Sam Altman 报告称 **Codex 周活跃用户自年初以来增长了三倍** ([sama](https://twitter.com/sama/status/2023233085509410833))。多个社区帖子描述了 **Codex 5.3** 的“巨大飞跃”，特别是通过并行化/子 Agent 实现的提升 ([gdb](https://twitter.com/gdb/status/2023299087974777061); [“agents are up”](https://twitter.com/gdb/status/2023342301821734937))。
- **子 Agent 配置 + 模型层级权衡**：实用技巧：通过编辑配置（例如 `max_threads = 24`）来增加 Codex 子 Agent 数量被分享为一种高级用户优化手段 ([Hangsiin](https://twitter.com/Hangsiin/status/2023297599764402627))。与此同时，至少有一位用户报告称 **5.3-codex-spark** 在处理实际工作时虽然更快，但比完整版 5.3 更“笨” ([giffmana](https://twitter.com/giffmana/status/2023341811851473053))。
- **ChatGPT 锁定模式（Lockdown Mode）**：OpenAI 推出了**锁定模式**，通过禁用/更改工具行为（如缓存浏览、减少 Web 交互）来降低提示词注入和数据外泄风险，该功能首发于企业版/商业版，随后将面向个人用户 ([cryps1s](https://twitter.com/cryps1s/status/2023441322838028362))。这在产品层面承认了**启用工具的 LLM 扩大了攻击面**，且某些组织即使以牺牲能力为代价，也需要确定性的、限制性的控制。
- **科学主张审查**：一个推特线程对归功于 GPT-5.2 的 OpenAI 物理学研究结果提出了可复现性担忧，认为如果使用了非公开模型，期刊应要求提供对话记录/工具细节 ([lewtun](https://twitter.com/_lewtun/status/2023334667064099207))。Kevin Weil 转发了相关物理学家的进一步解释 ([kevinweil](https://twitter.com/kevinweil/status/2023422106411974935))，gdb 也发布了一篇关于“它是如何诞生的”后续说明 ([gdb](https://twitter.com/gdb/status/2023445830880117214))。

**中国的“假期模型潮”：Qwen3.5, GLM-5, MiniMax M2.5, Seed/Seedance——以及机器人技术加速**

- **春节成为模型发布季**：多条动态将春节（CNY）定义为新的“模型发布周”，涉及的模型包括 **Qwen3.5**、**GLM-5**、**MiniMax M2.5**，以及备受期待的 **DeepSeek-V4** ([iScienceLuvr](https://twitter.com/iScienceLuvr/status/2023312965756449088)；[Yuchenj_UW roundup](https://twitter.com/Yuchenj_UW/status/2023453819938763092))。
- **MiniMax M2.5：吞吐量 + RL 信号效率**：SemiAnalysis 报道称，在特定 TTFT 约束下，M2.5 在使用 vLLM 的 **8×H200** 环境中实现了 **~2500 tok/s/GPU** 的持续吞吐量 ([SemiAnalysis_](https://twitter.com/SemiAnalysis_/status/2023418414203646066))。MiniMax 强调 **逐 Token 过程奖励（per-token process rewards）** 能更好地利用 RL 信号并提高成本效率，并对其广泛的 API/合作伙伴可用性表示庆祝 ([MiniMax_AI](https://twitter.com/MiniMax_AI/status/2023470874708549941))。
- **字节跳动 Seed/Seedance 与 AI 电影**：Seedance 2.0 凭借 **贾樟柯** 导演使用该模型创作的短片成为文化热点 ([FrankYan2](https://twitter.com/FrankYan2/status/2023257752017981446)；[EHuanglu](https://twitter.com/EHuanglu/status/2023449238114320514))。核心观点是：视频生成正在从“玩具演示”转向“电影制作人工作流”，且有观众指出，视频输出相比图像生成，其“审美引导带来的非自然感（uncanny）”较少 ([jd_pressman](https://twitter.com/jd_pressman/status/2023256826431852852))。
- **机器人：Unitree + 中国领先叙事**：动态重点展示了 Unitree（宇树科技）人形机器人在春晚上的表现，以及关于中国机器人技术快速进步的广泛论调 ([HumanoidHub](https://twitter.com/TheHumanoidHub/status/2023428892934160775)；[kimmonismus](https://twitter.com/kimmonismus/status/2023388741595799687))。teortaxesTex 认为我们已经度过了“波特金”式的质疑阶段——整个行业（而不仅仅是个别案例）是真实存在的，尤其是机器人领域 ([teortaxesTex](https://twitter.com/teortaxesTex/status/2023518524451549598))。
- **算力供应链信号**：据报道，由于企业需求强劲，Western Digital 已售罄 **2026 年大部分 HDD 产能**，部分 AI 客户甚至预订到了 2027/2028 年 ([kimmonismus](https://twitter.com/kimmonismus/status/2023374704006828513))。此外，NVIDIA GB300 NVL72 被宣传为比 Hopper 具有 **~50 倍更高的每兆瓦（MW）性能** 和 **~35 倍更低的每 Token 成本**（厂商声称数据） ([kimmonismus](https://twitter.com/kimmonismus/status/2023456488782487566))。

**工程师实际关注的研究/工程线程（Agent, RL, 可解释性与评测规范）**

- **多步工具调用依然脆弱**：SciAgentGym 显示，随着工具交互步骤的增加，成功率会骤降；通过对工具依赖图（SciForge）进行数据合成，提升了 8B 模型在科学工作流上的表现 ([dair_ai](https://twitter.com/dair_ai/status/2023404773031166320))。这符合 Agent 的日常痛点：执行可靠性是瓶颈，而非单步推理。
- **Agent 的自适应推理深度**：CogRouter 能够逐步动态调整“认知深度”；据摘要显示，在 Agent 基准测试中，它以 **减少 62% 的 Token 消耗** 击败了 GPT-4o ([omarsar0](https://twitter.com/omarsar0/status/2023405531835277504))。
- **基于评分标准（Rubric）的 RL（超越可验证领域的 RLVR）**：一篇关于基于 Rubric 的 RL 的重要文章追溯了从 LLM-as-judge 到结构化 Rubric 的演进路径，并结合 15+ 篇论文提供了实用建议 ([cwolferesearch](https://twitter.com/cwolferesearch/status/2023408158065188894))。
- **可解释性目标函数**：MonoLoss 提出了一种插件式目标函数，旨在鼓励 CLIP/SigLIP2/ViT 等 SAE 中的 **单语义（monosemantic）** 激活，提升了许多潜变量的 “MonoScore” ([iScienceLuvr](https://twitter.com/iScienceLuvr/status/2023303520057745501))。
- **基准测试污染 / “局部泛化”**：人们再次强调，基准测试的提升可能会受到训练数据扩展和语义近重复项的干扰。一种提议的分解方式是：benchmaxxing（刷榜） vs usemaxxing（实用化） vs 隐式插值 vs 真正的 OOD 泛化 ([g_leech_](https://twitter.com/g_leech_/status/2023384075537432662))。这与 Lucas Beyer 早期在视觉数据去重方面的经验，以及在语言模型中“正确地”完成此项工作的难度产生了共鸣 ([giffmana](https://twitter.com/giffmana/status/2023481657177911383))。
- **WeirdML 时间跨度**：受 METR 启发的 WeirdML 任务“时间跨度”估计建议，前沿模型的时间跨度从 **~24 分钟 (GPT-4)** 到 **~38 小时 (Opus 4.6)** 不等，且具有 **~5 个月的翻倍时间** ([htihle](https://twitter.com/htihle/status/2023349189271572975))，这与 METR 类评估结果基本一致 ([scaling01](https://twitter.com/scaling01/status/2023350946139435357))。

**元主题：开源 vs 闭源，劳动力/教育影响，以及作为新瓶颈的“审美/品味（taste）”**

- **Open model 势头与集中化风险**：一个反复出现的观点是 Open model 减少了权力集中，并保留了多种 AGI 路径（[TuringPost clip](https://twitter.com/TheTuringPost/status/2023375354740809823)）。与此同时，关于 ToS 约束（例如 Anthropic 限制监控/武器使用）以及这是否使供应商成为“供应链风险”的争论正烈（[RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/2023524096592802207)；[kimmonismus Axios summary](https://twitter.com/kimmonismus/status/2023419652378955809)）。
- **劳动力中断时间线**：Ryan Greenblatt 认为大规模失业在“2 年内被高估，在 7 年内被低估”，关键的拐点是 **AI R&D 的完全自动化**（此后人类认知劳动的价值将迅速崩塌）（[thread start](https://twitter.com/RyanPGreenblatt/status/2023219133916332070)）。
- **教育/技能焦虑**：有关学位可能在学生毕业前就过时的说法（通过一条新闻摘要推文流行开来）反映了更广泛的不确定性（[kimmonismus](https://twitter.com/kimmonismus/status/2023446044873560178)）。还有警告称，受控研究显示 AI coding tools 可能会降低技能掌握程度（通过 Anthropic 的研究链接摘要）（[dl_weekly](https://twitter.com/dl_weekly/status/2023502798659125656)）。
- **“Taste”和验证作为核心技能**：这组观点强烈强调，随着 models/agents 的规模化，**taste**（选择好的问题/解决方案）和**验证能力**（检测细微的错误）成为最稀缺的人类差异化因素——被明确标注为“一种新的核心技能”（[gdb](https://twitter.com/gdb/status/2023481258639286401)；[Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/2023481799335440792)）。Karpathy 将其扩展到编程语言/formal methods：翻译和 refactoring 将占据主导地位，我们可能会反复重写大部分软件；“对 LLMs 最优”的语言成为了一个开放性问题（[karpathy](https://twitter.com/karpathy/status/2023476423055601903)）。

---

### Top tweets (按互动量)
- **SF 可步行性讨论**：[@paularambles](https://twitter.com/paularambles/status/2023220064070332521)  
- **Anthropic 班加罗尔办公室 / 印度作为 #2 市场**：[@AnthropicAI](https://twitter.com/AnthropicAI/status/2023322514206957688)  
- **Qwen3.5-397B-A17B 发布 (Apache-2.0, multimodal MoE, 17B active)**：[@Alibaba_Qwen](https://twitter.com/Alibaba_Qwen/status/2023331062433153103)  
- **PL/FM + LLMs 重塑软件翻译/重写**：[@karpathy](https://twitter.com/karpathy/status/2023476423055601903)  
- **“Anthropic 对开源的憎恨” 走红言论**：[@ThePrimeagen](https://twitter.com/ThePrimeagen/status/2023194211445834132)  
- **Codex 增长说法**：[@sama](https://twitter.com/sama/status/2023233085509410833)


---

# AI Reddit 摘要

## /r/LocalLlama + /r/localLLM 摘要

### 1. Qwen 3.5 模型发布与性能

  - **[Qwen3.5-397B-A17B 正式发布！！](https://www.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/)** (热度: 973): **Qwen3.5-397B-A17B** 已在 [Hugging Face](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) 发布，该模型拥有 `3970 亿`参数，原生上下文长度为 `262,144` tokens，并可扩展至 `1,010,000` tokens。该模型是 Qwen 系列的一部分，以其大规模语言处理能力而闻名。此外，[此处](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF)还提供了 GGUF 版本，可为特定用例提供优化后的性能。社区对该模型的性能充满期待和好奇，用户们正跃跃欲试准备对其能力进行测试。

    - Qwen3.5-397B-A17B 模型拥有 `262,144` tokens 的原生上下文长度，最高可扩展至 `1,010,000` tokens。这在处理超大上下文方面是一项重大改进，使其适用于需要大量输入数据的复杂任务。
    - 据报道，Qwen3.5-397B-A17B 的解码吞吐量比其前代产品 Qwen3-235B-A22B 快 `3.5 倍`至 `7.2 倍`。吞吐量的提升表明处理效率有了实质性的改进，这可能为大规模应用带来更快的响应速度和更低的计算成本。
    - 一名用户分享了该模型在 [Hugging Face](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF) 上的 GGUF 版本链接，表明该模型已可供下载和实验。这种可访问性允许更广泛的测试并集成到各种项目中。

  - **[Qwen3.5-397B-A17B Unsloth GGUFs](https://www.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/)** (热度: 663): **Qwen3.5-397B-A17B** 是 **Alibaba** 最新发布的拥有 `3970 亿`参数的模型，专为多模态推理（multimodal reasoning）设计。它能够在 `192GB RAM` 的 `Mac` 上以 `3-bit` 运行，或者在拥有 `256GB RAM` 的 `M3 Ultra` 上以 `4-bit (MXFP4)` 运行。在指令遵循（instruction following）、多语言知识和视频推理等基准测试中，该模型的定位是与 **Gemini 3 Pro**、**Claude Opus 4.5** 和 **GPT-5.2** 展开竞争。此次发布包含了用于灵活部署的动态 GGUF，并提供了在各种硬件配置上运行该模型的指南。更多详情可在 [Hugging Face](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) 和 [Unsloth](https://unsloth.ai/docs/models/qwen3.5) 查看。评论者对该模型的规模和能力印象深刻，特别提到了其 `3970 亿`的总参数量以及每次仅激活 `170 亿`参数的特性。此外，人们对 **AutoRound** 将如何增强模型性能也充满了好奇。

    - Qwen3.5-397B-A17B 模型因其“多话”（verbosity）而受到关注，正如它在生成简单的问候语回复时展现出的冗长内部对话所证明的那样。这种冗长可能预示着模型复杂的决策过程，对于细致入微的任务可能是有益的，但在简单的交互中也可能导致效率低下。
    - 一位用户对 AutoRound 功能在 Qwen3.5-397B-A17B 模型上的表现表示好奇，特别是考虑到该模型仅有 170 亿参数处于激活状态。这表明人们关注点在于如何有效管理计算资源的同时优化性能，这对于在实际应用中部署大型模型至关重要。
    - 讨论中提到了 UD-Q4_K_XL 和 MXFP4 格式的对比性能，一名用户注意到目前缺乏直接对比两者的基准测试。这突显了现有性能数据的空白，而这些数据对于在模型部署和优化策略方面做出明智决策至关重要。

### 2. Local LLM 的挑战与创新

  - **[为什么运行本地 LLM 仍然如此痛苦](https://www.reddit.com/r/LocalLLM/comments/1r5matd/why_is_running_local_llms_still_such_a_pain/)** (Activity: 243): **该帖子讨论了在个人硬件上运行 **Ollama** 和 **Llama** 等本地 Large Language Models (LLMs) 的挑战，强调了在处理超过 `7B` 参数的模型时遇到的安装失败和资源限制等问题。用户对自托管解决方案的复杂性表示沮丧，这些方案通常需要 Docker 和 Kubernetes 等领域的深厚技术知识，并且缺乏能够替代 **OpenAI** 的 **ChatGPT** 的、兼顾隐私且功能强大的选择。** 评论者指出，由于极高的硬件要求，在本地实现 **ChatGPT** 级别的功能本质上非常困难。他们认为，虽然像 **LM Studio**、**Ollama** 或 **Lemonade** 这样的工具安装简便，但性能严重依赖于是否拥有强大的 GPU 或 NPU。他们强调，如果没有大量的硬件投入，本地 LLM 的运行速度会很慢，且在不使用远程供应商的情况下，实现完整的 **ChatGPT** 功能可能并不现实。

    - No_Clock2390 强调，只要硬件配置正确，运行本地 LLM 是可行的，并提到了 **LM Studio**、**Ollama** 和 **Lemonade** 等可以快速搭建的工具。然而，性能高度依赖硬件能力，尤其是 GPU 或 NPU。例如，在 Intel N100 上运行 **Ollama** 是可能的，但由于 CPU 的限制，会导致性能缓慢。
    - Total-Context64 强调了在本地实现类 **ChatGPT** 功能的成本壁垒，指出除非选择远程供应商，否则必须在硬件上进行重大投资。这突显了在没有充足资源的情况下，复制高性能 LLM 所面临的挑战。
    - HorribleMistake24 建议初学者使用 **lmstudio**，它可以协助确定模型与可用 GPU VRAM 的兼容性。他们还提到利用 **ChatGPT** 订阅，通过 VS Code 中的 **Codex** 辅助设置，展示了通过将 AI 工具集成到开发流程中来克服配置难题的实用方法。

### 3. MiniMax-2.5 与 OpenClaw 讨论

  - **[真的有人在使用 Openclaw 吗？](https://www.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/)** (活跃度: 1615): **该 Reddit 帖子质疑了 Openclaw 流行度的真实性，暗示这可能是人为制造的社交媒体营销结果，尤其是在 OpenAI 收购 Openclaw 之后。该帖子引用了一个可疑的增长图表 [点击此处查看](https://www.star-history.com/#openclaw/openclaw&amp;Comfy-Org/ComfyUI&amp;type=date&amp;legend=top-left)。根据用户体验，Openclaw 被描述为一个连接各种 API 和 MCP 服务器的工具，但缺乏创新。OpenAI 以 `100 亿` 美元的价格对其进行的收购被怀疑地看待，并被比作加密货币市场中那种由炒作驱动的性质。** 评论表达了对 Openclaw 营销策略的怀疑，一些用户将其描述为“靠氛围感编写的 (vibe coded)”且缺乏独特功能。人们对 **Ironclaw** 等替代方案表现出兴趣，表明了对更稳健解决方案的需求。

    - Skystunt 提到 Openclaw 本质上是现有技术的汇编，连接了各种 API 和 MCP 服务器，没有提供任何突破性的功能。这表明其感知价值可能被夸大了，因为它并没有引入新的能力，而只是集成了现有的能力。
    - dgibbons0 强调了 Openclaw 较差的配置质量，将其描述为“靠氛围感编写的 (vibe coded)”。这个词暗示其设置缺乏专业的打磨或稳健性。评论者还表达了探索相关项目 Ironclaw 的兴趣，这表明尽管 Openclaw 存在缺点，但将聊天与 AI 引擎集成的概念仍然具有吸引力。
    - TurnUpThe4D3D3D3 提出了对使用 Openclaw 财务影响的担忧，指出它默认有 30 分钟的“心跳 (heartbeat)”，每次运行时都会产生 API 成本。随着时间的推移，这可能会导致显著的支出，每周可能达到数美元，而用户可能无法立即察觉。

  - **[你可以在本地运行 MiniMax-2.5](https://www.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/)** (活跃度: 784): **图片提供了在本地运行 MiniMax-2.5 模型的详细指南，强调了其在编程、Agent 智能体工具使用和办公任务中的顶尖性能。该模型拥有 `230B 参数`，其中 `10B 激活参数`，具备 `200K 上下文窗口`，在其未量化的 bf16 形式下需要 `457GB` 内存。使用 **Unsloth Dynamic 3-bit GGUF** 可将模型大小显著降低至 `101GB`，降幅达 `62%`，使其更易于本地部署。该指南还包括了 [官方文档](https://unsloth.ai/docs/models/minimax-2.5) 和 [Hugging Face 上的 GGUF 模型](https://huggingface.co/unsloth/MiniMax-M2.5-GGUF) 的链接。** 评论反映了对在本地运行如此大型模型可行性的怀疑，用户幽默地指出了部署该模型所需的高昂硬件要求和成本。

    - Ug1bug1 提到，包括 Q3_K_XL 在内的 MiniMax 模型在他们的 Strix Halo 设置（配备 128GB RAM）上表现良好。这表明该模型在高端硬件上的性能令人满意，说明充足的内存是有效运行这些关键模型的关键要求。



## 泛 AI 领域 Subreddit 汇总

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI 模型发布与基准测试

  - **[你正在期待什么？](https://www.reddit.com/r/singularity/comments/1r5p4qi/what_are_you_looking_forward_to/)** (活跃度: 954): **图片是来自 **CHOI (@arrakis_ai)** 的一条推文，宣布即将发布几个 AI 模型：**DeepSeek V4, Gemini 3.1 Pro, GPT 5.3, Sonnet 5** 以及一个“神秘模型”。推文强调了 AI 开发时间线的迅速加速，暗示这些发布预计将在几天内完成。这表明 AI 模型开发进入了一个显著的进步和竞争时期，对各种应用和行业具有潜在影响。** 一条评论对 Sonnet 5 的发布表示怀疑，提到了之前的传闻最终证明是关于 Opus 4.6 的。另一条评论暗示了竞争氛围，提到了“Elon 因为缺乏进展而崩溃”，可能指的是 AI 进步领域的竞争。

- johnwheelerdev 提到了对 **Gemini 3.1** 的期待，暗示这可能是一个重大的更新或发布。这可能意味着相比之前版本有所改进或增加了新功能，尽管没有提供具体的细节或 benchmarks。
- GraceToSentience 提到了关于 **Sonnet 5** 的传闻，此前该版本被认为是 **Opus 4.6**。这表明在版本控制或产品命名上可能存在混淆或重新更名，凸显了追踪软件更新和发布所面临的挑战。
- Egoz3ntrum 提出了 **GPT-OSS-2**，这可能是 GPT 模型的一个开源变体。这表明了向更多开源 AI 模型发展的趋势，可能提供更高的透明度和社区驱动的改进。

- **[谷歌称，攻击者在试图克隆 Gemini 时对其进行了超过 100,000 次提示](https://www.reddit.com/r/singularity/comments/1r5d9jw/attackers_prompted_gemini_over_100000_times_while/)** (活跃度: 1342): **Google** 报告称，攻击者试图通过对其进行超过 `100,000` 次提示来克隆其 **Gemini AI** 模型，这种技术被称为 *model distillation*（模型蒸馏）。这种方法涉及向模型输入特定的提示词以收集响应，从而在无法直接访问模型代码或训练数据的情况下，创建一个更便宜的仿制品。Google 将此行为视为“知识产权窃取”，并已实施了未公开的应对措施。欲了解更多详情，请参阅 [原文](https://arstechnica.com/ai/2026/02/attackers-prompted-gemini-over-100000-times-while-trying-to-clone-it-google-says/)。** 一些评论者对模型蒸馏的有效性表示怀疑，并将其与 90 年代试图通过输入数百万场对局来改进国际象棋软件的尝试进行了比较，认为后者并没有产生显著影响。其他人则指出了 Google 在知识产权立场上的讽刺性，因为 Google 自身也使用网页爬取的数据来训练 LLMs。

    - Deciheximal144 强调了 Google 将“模型提取”视为知识产权窃取的讽刺性，因为 Google 自己的 LLMs 也是在未经明确许可的情况下，利用从互联网爬取的数据进行训练的。正如 [The Verge](https://www.theverge.com/2023/7/5/23784257/google-ai-bard-privacy-policy-train-web-scraping) 所讨论的，这引发了关于 AI 训练过程中数据使用和所有权的伦理问题。
    - magicmulder 质疑了“模型提取”的有效性，将其与 90 年代通过输入数百万场比赛来改进国际象棋软件的尝试进行了对比，当时并未产生重大影响。这表明，仅仅对 AI 模型进行大量提示是否就能产生高质量的克隆，人们对此持怀疑态度，因为模型训练的复杂性远不止于输入数据的量。
    - Ok_Buddy_9523 幽默地淡化了“提示 AI 100,000 次”的概念，将其比作一项常规活动，暗示在 AI 开发和测试的背景下，这种互动次数可能并不像想象中那样显著或不寻常。

- **[Codex-cli 配合 GPT-5.3 codex xhigh —— 5 小时内用汇编语言编写了一个完全可运行的 GBA 模拟器！](https://www.reddit.com/r/singularity/comments/1r525lg/codexcli_with_gpt53_codex_xhigh_5_hours_made_a/)** (活跃度: 717): **一位用户声称使用 **Codex-cli 配合 GPT-5.3 codex xhigh**，在 `5 小时` 内用汇编语言开发了一个功能完备的 Game Boy Advance (GBA) 模拟器。该项目托管在 [GitHub](https://github.com/Healthy-Nebula-3603/gpt5.2-codex_xhigh-proof-of-concept-GBA-emulator-in-assembly-) 上，涉及模型自主构建、测试和调试模拟器。该模拟器的架构包括一个 x86-64 汇编内核，带有一个用于 SDL2 的最小 C 宿主层，目标是兼容像 SuperMarioAdvance 这样的游戏。概述的计划包括 ARM7TDMI CPU 内核仿真、内存映射以及 PPU/APU 功能，重点在于确定性和性能基准，例如在 Linux x86-64 上达到 `59.7 FPS`。该项目强调采用纯汇编方法，并使用 C 平台适配层（shim）进行 SDL2 集成。** 评论者对该模拟器的性能和成本表达了怀疑和好奇，其中一人指出了近期有关 LLMs 无法生成底层代码的说法的讽刺性。另一位评论者对这一成就印象深刻，并强调如果之前没有类似的案例，这将是独一无二的。

- stardoge42 询问了积分成本和模拟器的性能，询问是否存在任何漏洞（glitches）以及它是否适用于其他游戏。这突显了使用 AI 生成代码的实际考量，例如资源消耗以及跨不同软件环境的兼容性。
- cottsay 引用了一个类似的项目，即 6 年前在 GitHub 上发布的 “ARM Assembly 编写的 Gameboy 模拟器”。这一对比为模拟器开发的演变以及使用 GPT-5.3 的 Codex-cli 等 AI 工具可能带来的进步提供了背景。
- BrennusSokol 提到，有人对 AI 生成底层代码或机器码的能力持怀疑态度，而使用汇编代码成功创建 GBA 模拟器的事实反驳了这一点。这反映了关于 AI 在软件开发中能力的持续辩论，特别是在生成复杂的底层代码方面。

### 2. Anthropic 和 OpenAI 的法律与道德紧张局势

- **[Anthropic 的道德立场：随着冲突升级，五角大楼警告 Anthropic 将“付出代价”](https://www.reddit.com/r/singularity/comments/1r6gyez/anthropics_moral_stand_pentagon_warns_anthropic/)** (活跃度: 1059)：**该帖子讨论了 AI 安全与研究公司 Anthropic 与五角大楼之间关于 AI 使用道德准则的冲突。据报道，Anthropic 正在抵制五角大楼推动将 AI 应用于大规模监控和全自动武器，并倡导建立道德护栏。然而，五角大楼将这种抵制视为潜在的“供应链风险”，如果采购压力凌驾于道德考量之上，可能会导致安全规范出现“逐底竞争”。这引发了关于在 AI 应用中应在何处划定道德界限，以及谁有权设定这些界限的问题。** 评论者们强调支持 Anthropic 的立场，指出对 AI 设定道德限制的重要性，例如禁止对美国公民进行监控和禁止自动化武器。对于五角大楼的意图存在怀疑，一些人认为对美国人的监控已经在发生。

- **[独家：五角大楼威胁惩罚 Anthropic](https://www.reddit.com/r/ClaudeAI/comments/1r6hvx2/exclusive_pentagon_threatens_anthropic_punishment/)** (活跃度: 969)：**在国防部长 Pete Hegseth 的领导下，五角大楼威胁要将 Anthropic 标记为“供应链风险”，原因是其在 Claude AI 模型用于军事应用方面存在分歧。这一认定将迫使承包商切断与 Anthropic 的联系，这将严重影响其业务，因为 Claude 是目前唯一集成到机密军事系统中的 AI 模型。冲突源于五角大楼要求获得更广泛的使用权，这与 Anthropic 在隐私和自动武器方面的道德关切相冲突。[阅读更多](https://www.example.com)。** 评论者对 Anthropic 在道德使用 AI 方面的立场表示支持，批评五角大楼的压力可能存在腐败，并偏向于 Grok 和 Gemini 等更顺从的 AI 公司。

    - Anthropic 关于限制其 AI 工具使用以防止大规模监控和自动武器的立场被视为一个重要的道德立场。五角大楼对这些限制的抵制突显了道德 AI 使用与政府利用 AI 满足国防利益之间的紧张关系。这种情况强调了关于 AI 伦理和治理的更广泛辩论，特别是在国家安全背景下。
    - 讨论表明，Anthropic 的 AI 产品 Claude 被视为市场上的领先产品，可能威胁到其他与政府实体关系更紧密的 AI 公司。这种对市场领导地位和道德立场的感知可能会影响政府的施压，因为有迹象表明政府偏袒 Grok 和 Gemini 等其他 AI 公司。
    - 有一种情绪认为，Anthropic 的道德立场可以被用作营销优势，吸引那些在 AI 部署中重视隐私和道德考量的用户。这反映了消费者对负责任的 AI 实践日益增长的意识和需求，这可能会影响市场动态和竞争地位。

- **[Anthropic 曾两次威胁要起诉该开发者，因为他的项目名称。现在他加入了 OpenAI，Claws 🦞 就要找上门了 🤣🤣](https://www.reddit.com/r/OpenAI/comments/1r5vl11/anthropic_threatened_to_sue_the_guy_over_his/)** (活跃度: 1048): **这张图片是一个迷因（meme），幽默地描绘了 **Anthropic** 与一名开发者之间关于其项目名称的法律纠纷，该纠纷最终导致该开发者加入了 **OpenAI**。图片中包含一段 Twitter 上的互动，强调了来自 Anthropic 的法律威胁，被戏称为“来自法律部门的情书”。帖子暗示了 Anthropic 与 OpenAI 之间的竞争关系，该开发者转投 OpenAI 被视为后者的胜利。评论区讨论了 Anthropic 的战略重点在于模型开发，而 OpenAI 被认为更具产品导向，暗示 OpenAI 对该开发者的兴趣在于其快速创建病毒式产品的能力。** 一些评论者对该开发者加入 OpenAI 的重要性表示怀疑，质疑其项目的独特性，并认为其他公司可以轻易复制。另一些人则认为 OpenAI 的招聘是一种反应式举动，暗示这可能不会带来实质性的改变。

    - Portatort 强调 **Anthropic** 专注于开发最顶尖的 AI 模型，而 **OpenAI** 现在更倾向于产品导向，旨在打造病毒式产品。这表明了两家公司在战略目标上的分歧：Anthropic 优先考虑模型的卓越性，而 OpenAI 则专注于市场化应用。
    - Inside_Anxiety6143 质疑 **OpenClaw** 对 OpenAI 的重要性，并指出其创建者声称在很短的时间内（“大概一个月就 vibecoded 出来了”）开发了它。这提出了一个观点，即其他公司可能会迅速复制此类项目，从而质疑 OpenClaw 的独特性或竞争优势。
    - beigetrope 认为 OpenAI 聘用 OpenClaw 的创建者可能是一种反应式举动，暗示这可能不会给公司内部带来实质性的变化。这一评论反映了对此类招聘对 OpenAI 整体方向战略影响的怀疑。


### 3. OpenClaw 安全与社区关注点


  - **[Sam Altman 正式确认 OpenAI 已收购 OpenClaw；Peter Steinberger 将领导个人智能体 (personal agents)](https://www.reddit.com/r/OpenAI/comments/1r5rnbl/sam_altman_officially_confirms_that_openai_has/)** (活跃度: 2440): ****Sam Altman** 已确认 **OpenAI** 收购了 **OpenClaw**，**Peter Steinberger** 加入并领导个人智能体的开发。OpenClaw 将转型为开源基金会，OpenAI 提供持续支持。此举表明了通过利用 Steinberger 的专业知识来增强个人智能体能力的战略重点。** 一些评论者猜测，这次收购可能是一种防御性策略，旨在防止竞争对手获得 OpenClaw 的技术。另一些人则质疑为什么 OpenAI 没有在内部开发类似的功能，暗示这次收购可能存在潜在的战略或资源方面的原因。

    - 提出的一个关键担忧是关于 OpenClaw 技术的访问权限，该技术最初是利用后门 CLI (backdoor CLI accesses) 开发的，这使得许多人无法负担。评论者询问 OpenAI 将如何解决这些访问问题，并建议如果处理得当，将 OpenClaw 的技术整合到 OpenAI 的生态系统中可能会使访问变得民主化。
    - OpenAI 收购 OpenClaw 被视为防止竞争对手获取其技术的战略举措。这被称为“防御性收购”，表明 OpenAI 的主要动机可能是通过防止技术落入竞争对手之手来巩固其市场地位。
    - 也有人猜测 OpenClaw 在 OpenAI 领导下（特别是 Peter Steinberger 掌舵后）的未来走向。评论中幽默地提到了可能出现“ClosedClaw”的情况，暗示 OpenAI 可能会限制访问或功能，类似于某些公司在收购后限制功能的做法。



---

# AI Discord 摘要回顾

> 由 gpt-5.1 生成的摘要的摘要的摘要


**1. 前沿、开源及区域模型：Qwen3.5, GLM‑5, MiniMax 2.5, Opus 4.6, Step 3.5 Flash**

- **Qwen3.5 & Qwen3.5‑397B A17B Benchmax 开源权重世界**：阿里巴巴的 Qwen 团队发布了 **Qwen3.5‑397B‑A17B**，这是一款结合了 **linear attention + sparse MoE** 的混合架构开源权重模型，支持 **201 种语言**。该消息通过 [他们的 Qwen3.5 帖子](https://xcancel.com/Alibaba_Qwen/status/2023331062433153103) 宣布，并在 Latent Space 和 HuggingFace 的 Discord 频道中被广泛引用，GitHub 和 Hugging Face 上已提供 Apache‑2.0 协议的权重及 API 访问。**Unsloth** 和 **Latent Space** 的用户将该模型视为新的 benchmark 目标，并开玩笑说 *“这里是 Qwen，我们在这里 Benchmax！”*，同时分享了一些 weirdcore 风格和高推理能力的 abliterated 版本 Qwen3‑30B 变体，例如 [**Qwen3‑30B‑A3B‑Claude‑4.5‑Opus‑High‑Reasoning‑2507‑ABLITERATED‑UNCENSORED‑V2**](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)。
  - Bench 讨论对比了 **Nemotron 30B A3B** 上的 **MXFP4** 量化与 **Q8_K_XL**，发现 MXFP4 相比 bf16 具有更低的 **KL divergence**，并请求在旧模型中支持 MXFP4；而其他人则通过 [这个实现分支](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main) 在 **GPT‑NeoX** 中实验 **Qwen3 architecture**。与此同时，Eleuther 的研究频道剖析了诸如 [“Matrix‑Driven Identification and Reconstruction of LLM Weight Homology”](https://arxiv.org/abs/2508.06309) 和 [“Independence Tests for Language Models”](https://arxiv.org/abs/2502.12292) 等论文，将 Qwen 系列模型视为重构 **finetuning trees** 和大型开源家族起源（provenance）的典型案例研究。

- **GLM‑5, MiniMax 2.5, 和 Windsurf 的模型自助餐**：在 **OpenClaw**、**Unsloth**、**GPU MODE** 和 **Windsurf** 社区中，用户对 **GLM‑5** 和 **MiniMax 2.5** 进行了压力测试。GLM‑5 被赞誉为 *“非常聪明且健谈”*，且在服务稳定时表现优于 **Kimi K2.5**；而 **MiniMax 2.5** 则被描述为由于其 **200k context** 的 sparse‑MoE 架构，需要约 **200 GB VRAM**（例如在 **2× RTX 6000 Blackwell 96 GB** 上达到 **120–130 tok/s**）。**Windsurf** 通过 [他们的更新](https://x.com/windsurf/status/2023536941451669586) 宣布在产品中提供对 **GLM‑5** 和 **MiniMax M2.5** 的原生支持，实际上将一个 IDE 转变成了一个多供应商的 frontier‑model 路由。
  - Unsloth 用户将 MiniMax 2.5 与 **Opus 4.6** 进行了对比，争论其质量提升是否值得那巨大的 VRAM 占用，而其他人则利用将 **sparse MoE 权重 offloading 到系统 RAM** 的方法，以速度换取容量。在 OpenRouter 的讨论中，从业者对比了 **GLM‑5 与 MiniMax 2.5 的 tool‑calling** 能力，发现 GLM 在 agentic workflows 中表现通常更好，而 MiniMax 在短交互中速度更快；一些人开始使用 **GLM 4.5 Air** 生成内核代码的 **SFT data**，以低成本引导出高质量的 reasoning traces。

- **Opus 4.6 和 Step 3.5 Flash 展示长上下文肌肉**：**Opus 4.6** 推出了 **1M‑token context** 和一个显式的 **“check your work”** 验证环节。LMArena 用户通过喂入 [大型代码指令集](https://link.to.examples) 对此进行了测试，并确认该模型在最终推理过程中能够忽略早期的错误。一位通过 **Opus 4.6 使用 Claude** 的 Perplexity 用户指出，Anthropic 的**每小时使用**限制（例如 *“仅剩 18 条回复”*）是重度交互使用的实际限制因素，尽管 Opus 在严肃推理和编程方面已经取代了 Perplexity。
  - 在 OpenRouter 方面，**Step 3.5 Flash** 在 [YouTube benchmark](https://youtu.be/yvBbcLCZIhgye) 中以 *“越级挑战”* 的表现给用户留下深刻印象，但尽管其性价比极高，目前的托管普及程度仍出人意料地低。当 LMArena 用户发现请求被静默路由到 **“5.2”** 变体时，OpenAI 自身的路由机制遭到了抨击，这进一步强化了工程师们要求对长上下文、高推理能力模型进行透明且固定版本（version‑pinned）访问的广泛趋势。


**2. Agent Stacks, Planning Frameworks, and Multi‑Agent Systems**

- **OpenClaw 编排自主代理机构和视频通话**：开发者们展示了 **OpenClaw**，这是一个用于多 Agent 团队和现实运维的编排层，包括一个包含 **技术负责人、后端和前端机器人** 的 *“代理机构服务器”*，它们通过共享的 [planbot 资源仓库](https://github.com/MrMeatikins/planbot-resource) 中的任务和计划进行协作。另一位用户让 OpenClaw 通过 **root 权限** SSH 进入 Proxmox 主机，并报告了从 **Proxmox 6 → 8** 的端到端自主升级，包括重启和错误处理，展示了对 Agent 运维（Agentic Ops）的生产级信任。
  - 一个独立的 **视频通话模式** 插件通过 [tavus.io](https://tavus.io) 将 **Tavus 数字人** 连接到 OpenClaw 的 BYO LLM 聊天补全接口，使 Agent 能够实时跟踪 **面部表情、手势和屏幕共享内容**。其他实验将 OpenClaw 的 *“潜意识”* 连接到本地微调的 LLM，该模型基于所有之前的聊天记录（分享在 [Google Drive 文件夹](https://drive.google.com/drive/folders/1t9satvOV0QpHRkWSaP6C6bFgElvOAoPD) 中的文章）进行了训练，并使用了一个 SEO 流水线，该流水线抓取 YouTube 内容，生成了约 **300 多篇 Brian-Dean 风格的文章**，通过一个编辑子 Agent 审核，然后存储以待发布。

- **从 Claude Cowork 和 DSpy RLMs 到 Triall 的模型大乱斗**：在 **Latent Space 的构建者频道** 中，一位成员展示了 **Claude Cowork** 如何编排流水线（例如自动将 Zoom 录音上传到 YouTube 频道），并提出了挑衅性的说法 *“Claude Cowork 可能是 AGI”*；而其他人则使用来自 [此仓库](https://github.com/sandover/ergo) 的 **Ergo 规划技能** 来构建多步骤功能开发工作。**DSpy** 贡献者推动了 **递归语言模型 (RLMs)**——如 [Omar Khattab 的推文](https://xcancel.com/lateinteraction/status/2022725370152190215) 中所述——其中模型 **编写代码来调用其他模型**，而不是依赖于二次注意力或单体 Agent 架构，并有一个具体的 [dspy-repl 原型](https://github.com/Archelunch/dspy-repl) 探索语言 + REPL 的选择如何影响 RLM 的准确性。
  - **Triall** ([triall.ai](https://triall.ai)) 作为一个构建在 [clash](https://github.com/clash-sh/clash) 之上的 GUI 出现在 OpenRouter 上，它允许用户让多个模型在生成、批判和改进方面相互对抗，鼓励 **对抗性推理而非盲目信任**。在框架层面， OpenAI Discord 尝试了 **KOKKI**，这是一种结构化的自我审计提示词，用于标记风险元素并切换模式；并讨论了映射到 **模型预测控制 (MPC)** 的 **FORTRESS** 框架，其中“针对随机输出的软控制循环”将不变量作为代价函数来偏置轨迹——尽管怀疑者认为这部分内容只是 *“没有可重复测试架构的角色扮演”*。

- **MCP、工具链和 Agent 原生基础设施**：**MCP 贡献者** 服务器深入研究了 **结构化输出** 和 **工具模式 (tool schemas)** 的经济学和设计，认为将 JSON Schema 嵌入提示词是一种隐藏的 **“Token 税”**，因为大多数 LLM API 缺乏原生 Schema 支持，但如果没有 Schema，工具链往往会退化为产生幻觉字段。他们提议将工具结果明确分类为 **文本/图像/对象**，并将结构化对象视为一种独立类型，其元数据存在于负载之外，以简化跨服务器和客户端的 Agent 接入。
  - 为了支持像 *“我上周睡得怎么样？”* 这样的现实查询，贡献者建议通过 **工具参数传递时区和上下文**，而不是隐藏的全局状态，从而强化了 **无状态 MCP 服务器 + 显式客户端上下文** 的模式。与此同时，多个生态系统正向 **Agent 原生基础设施** 迈进：**Jazz** ([github.com/lvndry/jazz](https://github.com/lvndry/jazz)) 是一个与 LLM 无关的终端 Agent，它可以读取文件、运行 git、使用 MCP 并编写自己的发布说明；**Crowdcent** 正将 **DSPy** 封装进 MCP；及 **Cloudflare** 在 [“面向 Agent 的 Markdown”](https://blog.cloudflare.com/markdown-for-agents/) 中宣布实验性支持 `Accept: text/markdown`，以便 HTTP 端点可以向 LLM 客户端返回 Markdown 原生内容。


**3. GPU 内核、CUDA/Triton DSL 以及 Agent 编写的内核**

- **FlashInfer、Flashy 竞赛以及 Agent 优化的 Kernel**：GPU MODE 的 **FlashInfer‑bench 竞赛**中，参赛者通过 Modal 在 B200 上调优融合的 MoE 和 GQA kernel。组织者澄清 **参考基准使用 FP32 中间值**，但如果精度保持接近，**允许使用 FP8 中间数学运算**，并提醒大家根据 [官方文档](https://modal.com/docs/guide/cuda)，Modal 支持 **CUDA 12.8**。**AccelOpt** 团队声称，通过使用自我改进的 LLM agent 来变异 kernel，在 GQA paged decode 上比 FlashInfer 0.5.3 实现了 **1.5 倍加速**，在 GQA paged prefill 上实现了 **1.38 倍加速**，并在 [zhang677/AccelOpt](https://github.com/zhang677/AccelOpt) 开源了他们的方法。
  - GPU MODE 的初学者们在与 **基准测试抖动**（例如 H100 上的 matmul kernel 在 **1400–1500 TFLOPs/s** 之间波动）作斗争，发现 **Achieved Occupancy** 会忽略空闲的 SM，转而通过 `sm__cycles_active.sum / sm__cycles_active.max` 来估算活跃 SM。在 HuggingFace 方面，官方课程中的一个 agent 为 **H100** 上的 **LTX 模型**编写了自定义 **CUDA kernel**，并在 [“custom CUDA kernels as agent skills” 博客](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)中击败了基准线，展示了规划型 agent 设计并集成专门 GPU kernel 的端到端流程。

- **Triton, CuteDSL, Cutlass 与 Proton：Kernel 极客的分析器**：GPU MODE 的 **triton‑gluon** 和 **cutlass** 频道深入探讨了 **Proton**、**CuteDSL** 和 **CuTeDSL**：一个讨论串展示了如何使用 Proton 配合 [示例 DSL 插桩](https://github.com/triton-lang/triton/blob/main/third_party/proton/tutorials/intra_kernel/example_dsl.py) 生成 **warp 级别的流水线（timeline）**，并在 **Perfetto** 中可视化轨迹，同时警告 DSL 级别的注解可能会被重新排序，高精度工作应在 **TTGIR override** 级别进行挂载。另一个讨论串调试了 **CuteDSL 的 `partition_S` 导致张量对齐丢失**（从 `align<16>` 降至 `align<4>`）以及奇怪的步幅打印（如 `(128,64,4):(1@1,1@0,64@0)`），此外还有 **CuTeDSL `complement()`** 返回无效的 `x:x` 而非 [布局代数文档](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md) 中所示的 `(3,2):(2,12)`。
  - **NVIDIA 竞赛**频道发布了一个 **Performance Trends** 仪表板，绘制了前 5 名用户每日的最佳提交，黄色曲线代表全球最佳（参见示例 [趋势图](https://cdn.discordapp.com/attachments/1434709259500650628/1472095545123147927/image.png)），并添加了坐标轴缩放以使宽范围的分数更易读。与此同时，kernel 作者在 B200 提交中遇到了 **CUTLASS 版本不匹配**（例如 [此处](https://github.com/NVIDIA/cutlass/blob/8cd5bef43a2b0d3f9846b026c271593c6e4a8e8a/python/CuTeDSL/cutlass/cute/_tvm_ffi_args_spec_converter.py#L214) 引用的旧版 CuTeDSL commit 导致的 `ModuleNotFoundError` 和 `DSLRuntimeError`）。另外一个 GPU MODE 的 **webgpu** 讨论串展示了一个 [Hesper 库](https://github.com/Verilean/hesper?tab=readme-ov-file#bitnet-b158-inference-125-tps-on-m4-max) 通过 WebGPU 在 **M4 Max 上以 125 tok/s** 运行 **BitNet‑B1.58**。

- **Thunderkittens, Tinygrad 与作为数据源的 KernelBench**：**Thunderkittens** 频道讨论了 **TK2** 的路线图方向——目前以 Hopper 多 GPU 为中心——而用户则在游说支持 **A100/4090**、**FP8 attention**、**decode kernel** 和 **MoE** 训练/推理 kernel，以及针对 **gather4 的 128 字节 swizzle 模式**等微观优化。在 **tinygrad** 中，George Hotz 痛批一个 [GLM Flash PR](https://github.com/tinygrad/tinygrad/pull/14738) “最多应该只有 50 行”且包含“额外的无关内容”，并将 **Graphcore C600 IPU** 描述为 **“20% MFU”** 和 “可咒的 C++ 废话（slop）”，强调了尽管拥有开放堆栈，非 CUDA 硬件仍存在摩擦。
  - GPU MODE 的 **popcorn** 频道将 kernel 调优变成了数据集工厂：一位用户利用 **gpt‑oss‑120B** 生成了来自 **Kernelbook** 的 **推理轨迹**，然后微调了 **Arcee Trinity Mini** 用于 Triton kernel 生成，并在 [kernelbench‑triton‑reasoning‑traces](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces) 发布了这些轨迹。其他人发现 **Qwen3‑30B‑A3B** 在原始 kernel 任务上错误率太高，直到他们在 **Kimi‑K2 生成的轨迹**上进行 **SFT**（使编译正确率翻了三倍）；他们现在正利用 **4×H100** 服务器运行 **GLM 4.5 Air** 以产出更多 SFT 数据，从而廉价地扩展 kernel 的正确性和推理深度。


**4. 新基准、推理方法以及不确定性/安全研究**

- **CommonLID、Assistant Axis 漂移以及权重同源图谱模型行为**：Eleuther 和 Common Crawl 推出了 **CommonLID**，这是一个涵盖 **109 种语言的大规模 Web 级 Language ID 基准测试**，详见其 [arXiv 论文](https://arxiv.org/abs/2601.18026)。研究显示，即使在支持的语言上，现有的顶级 LangID 模型得分也 **<80% F1**。该数据集托管在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上。Eleuther 的研究频道还重点介绍了 **“Assistant Axis”** 论文 [《Steering LLMs by Persona Directions》](https://arxiv.org/abs/2601.10387)，该研究提取了不同 Persona 的激活方向，并从实证上表明，**长对话中的 Assistant-mode 漂移**是结构性的，量化了许多用户此前仅凭经验报告的现象。
  - 互补的理论探讨通过 [《Matrix‑Driven Identification and Reconstruction of LLM Weight Homology》](https://arxiv.org/abs/2508.06309) 和 [《Independence Tests for Language Models》](https://arxiv.org/abs/2502.12292) 及其后续研究 [《Blackbox Model Provenance via Palimpsestic Membership Inference》](https://arxiv.org/abs/2510.19796) 深入挖掘了权重空间结构。成员们对 Independence Tests 能通过黑盒访问 **重建 Llama 架构模型的微调树** 印象深刻，并讨论了受 [此可视化推文](https://x.com/dvsaisurya/status/2023118579755819459) 启发的因果注意力预调优（causal attention preconditioning）的新近似方法，包括 Tensor Cores 是否能以低廉成本近似这些矩阵。

- **推理流水线：CoVe、QED‑Nano、Rubric RL 和 RLMs**：Latent Space 的论文俱乐部深入探讨了 Meta 的 **Chain‑of‑Verification (CoVe)**。Ryan Lazuka 的总结 [线程](https://xcancel.com/lazukars/status/2022608931953217636?s=12) 称，通过无需 Few-shot 样本的两阶段 *generate → verify* 提示协议，可实现 **94% 的准确率提升**，这表明 CoVe 在许多场景下可以取代标准的 CoT。Lewis Tunstall 的 **QED‑Nano 4B** 定理证明模型——在 [此帖子](https://xcancel.com/_lewtun/status/2022966614283718852) 中宣布——通过蒸馏推理流水线和推理缓存（reasoning cache）实现了激进的推理时扩展（inference‑time scaling），目标直指 **IMO 级别的数学问题**。
  - Cameron Wolfe 对 **基于评分标准的强化学习 (Rubric‑Based Reinforcement Learning)** 的综述（[推文](https://xcancel.com/cwolferesearch/status/2023408158065188894)）综合了 **15 篇以上论文**，探讨了使用明确的文本评分标准（rubrics）而非原始的 LLM‑as‑a‑Judge 评分，将 **带可验证奖励的强化学习 (RLVR)** 扩展到风格和安全性等模糊领域。在 Latent Space 的 **applied‑AI‑experimentation** 频道中，从业者将这些想法与带有 **dspy.RLM** 的 **递归语言模型 (Recursive Language Models, RLMs)** 联系起来（[设计线程](https://xcancel.com/lateinteraction/status/2022747248841625741)），认为对调用和代码的符号递归（而非更长的注意力机制）才是解决长程推理（long‑horizon reasoning）真正的瓶颈突破点。

- **不确定性、密码破解与具备欺骗意识的安全**：在 HuggingFace 和安全相关的频道上，**ATIC** 作为一个 **认识论不确定性系统（epistemic uncertainty system）** 首次亮相。它在 *“tri‑brain”* 架构中运行 **三个独立的 Claude Opus 4.5 模型**，对 **Q1（随机不确定性）** 和 **Q2（知识鸿沟）** 进行评分，并在达到阈值时转交给专家模型。文档位于 [atic.consulting](https://atic.consulting) 及其 [API 文档](https://web-production-51da4.up.railway.app/docs)。同一个 i‑made‑this 频道还重点介绍了 **PassLLM**，这是一款密码审计工具，它在数百万个真实密码对上微调了一个 **Qwen3‑4B LoRA**，用于生成 **基于 PII 条件的密码列表**。该项目已在 [github.com/Tzohar/PassLLM](https://github.com/Tzohar/PassLLM) 开源，Discord 演示显示其猜测准确度令人不安。
  - Latent Space 的 **mech‑interp** 讨论室讨论了 **X‑Ware 的元神经元工作**，其中一个 [针对内部激活的扩散模型](https://xcancel.com/askalphaxiv/status/2022328332939886614) 学习生成受控的激活编辑以进行引导（steering），被认为是 SAEs 更简洁的替代方案。与此同时，**FAR.AI** 在 [此线程](https://xcancel.com/farairesearch/status/2022345033777545452) 中警告称，在 **欺骗探测器（deception probes）** 上进行训练可能会产生四种行为：真正的诚实、公然的欺骗、文本级混淆或 **激活级混淆**——这意味着幼稚的红队测试/监管协议可能会诱导模型 **隐藏其内部状态**，而非真正改进。


**5. 来自 Perplexity、Kimi、OpenAI 和 Stripe 的基础设施、定价与平台转型**

- **Perplexity 的付费墙转型和性能下滑引发用户大逃亡**：在 **Perplexity** 的 Discord 频道中，Pro 用户炮轰了最近的变更：**深度搜索次数从每月 200 次削减至 20 次**，新增了文件上传限制，以及 **7 天保留政策**。一名资深用户计算得出，维持之前的处理量现在每月需花费 **167 美元，而以前仅为 20 美元**，这导致其 TrustPilot 评分跌至 **1.5/5**。与此同时，用户抱怨自 **2 月 6 日**以来，系统的**长期记忆退化**，模型会遗忘食谱用量并编造事实，促使许多人将其答案评价为*“表现平庸 (pretty mid)”*并考虑更换技术栈。
  - 尽管存在**严格的每小时限额**，但仍出现了向 **Anthropic Claude** 和 **Opus 4.6** 迁移的浪潮。一些人通过[此共享对话](https://www.kimi.com/share/19c66f47-d972-8c99-0000-0000bbe337c4)尝试将 **Kimi** 作为替代性的编程和搜索前端（首月可享受 **1 美元优惠**）。与此同时，**Perplexity API** 用户在有效密钥下遇到了无法解释的 **401 错误**，并被告知需发送邮件至 [api@perplexity.ai](mailto:api@perplexity.ai)，这加剧了人们对定价和可靠性都正趋向于仅限企业级水平的焦虑。

- **Kimi 和 MiniMax 在定价、配额和本地克隆上纠缠不清**：在 **Moonshot AI 的 Kimi** 服务器以及 Unsloth/NouS 聊天频道中，工程师们称赞 **Kimi 2.5 / K2.5** 表现惊人地强劲——在某些编程和推理任务上经常击败 **Sonnet** 或 **Opus 4.5**——并强调其 **40 美元/月计划**提供了适配 OpenClaw 的 API。与此同时，用户强烈抱怨**计费错误、订阅丢失、配额故障**以及支持响应缓慢（例如，一名用户在订阅消失后不得不提交 [Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)）。此外，其他用户发现 VS Code 中的 **CLI 集成 Bug** 只能通过按照 [Kimi 文档](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html) 使用 `irm https://code.kimi.com/install.ps1 | iex` 安装 CLI 来解决。
  - OpenClaw 和 Nous 用户在纠结是追求云端的 Kimi/MiniMax 容量，还是投入重金构建拥有 **700+ GB RAM 和 200 GB VRAM 的本地配置**，以便在内部部署 **Kimi K2.5** 或 **MiniMax 2.5** 等模型，理由是担心供应商封禁和 ToS 摩擦（例如通过 Agent 框架使用时导致 **Antigravity** 账号被封）。**Moonshot** Discord 同时也发出警告，称有多个如 [kimi.com/membership/subscription](https://kimi.com/membership/subscription) 的**诈骗网站**打着 Kimi 的名号传播恶意软件，再加上 Kimi 自身的定价高于 MiniMax，这促使部分用户转向更便宜的中国模型或开源权重选项。

- **Stripe、Apple、Anthropic-五角大楼以及 OpenAI 的模型停用重塑版图**：在 Latent Space 的 **founders** 频道中，开发者抱怨说，一旦计入 Billing、登记商户 (merchant-of-record) 和附加组件，**Stripe 会抽取约 8.3% 的营收**，并分享了 [Bluesky 上的吐槽](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n) 和一个 [X 帖子](https://x.com/pk_iv/status/2023421931660415191?s=12)，论证欧盟本地银行卡通道远比 Stripe 默认的 2.9% 手续费便宜。**stocks-crypto-macro** 频道的另一个帖子暗示，**Apple** 可能正在战略性地囤积巨额现金，让其他公司在 AI 资本支出 (capex) 上烧钱，直到训练/推理变成商品化服务，然后再通过收购或授权切入，而不是加入 [BuccoCapital 推文](https://xcancel.com/buccocapital/status/2023108814422278510?s=12) 中提到的当前 **2 万亿美元的资本支出军备竞赛**。
  - 在政策层面，OpenRouter 链接了一份 Axios 的独家新闻，称由于 **terms‑of‑use 限制**，美国国防部长正考虑**放弃 Anthropic 作为供应商**，因为 Anthropic 想要禁止**大规模国内监控和自主武器**，而五角大楼则要求工具必须能用于**“所有合法用途”**（[Axios 文章](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth)），这重新引发了对 **PRISM** 式的担忧。与此同时，Latent Space 和 OpenAI 的 Discord 记录了 **ChatGPT-4o 退役**后的用户抗议（[爆火的抗议帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46)），以及由于 **GPT-5.2** 有时自称为 *“GPT-4o-mini”* 引起的混乱，还有基于 [OpenAI 停用文档](https://developers.openai.com/api/docs/deprecations/) 对 **GPT-5.1 停机日期** 的猜测，这说明不透明的生命周期决策现已成为应用构建者的首要运营风险。

---

# Discord: High level Discord summaries

## [OpenClaw](https://discord.com/channels/1456350064065904867) Discord

- **OpenClaw 警告用户防范 Airdrop 诈骗**：OpenClaw 发布了关于涉及虚假 Airdrop 和新代币的 GitHub Discussion 诈骗警告，澄清这些活动与 **OpenClaw** 无关，用户应保持警惕；此类诈骗并非源自 **OpenClaw**。
   - 公告强调 **OpenClaw** 对任何加密货币相关活动持严格反对政策，并重申其绝不会参与创建代币或 Airdrop，正如其 [Server Guide](https://discord.com/channels/1456350064065904867/@home) 中所述。
- **Kimi AI 在图像方面表现优于 Opus**：用户发现 **Kimi 2.5** 的效果出奇地好，在特定问题的解决场景中甚至超越了 **Opus 4.5**，而其全新的 40 美元/月计划专为配合 **OpenClaw** 工作而设计，甚至提供了专属 API。
   - 然而，一位用户指出，*如果你想在 Kimi 上创建 OpenClaw，你需要更高级别的订阅*，成员们还提到了一个 **Kimi-K2.5-free** 选项。
- **OpenClaw Agency 组建 Agent 团队共创双赢**：一名成员展示了一个基于 **OpenClaw** 构建的代理服务器，其特色是拥有一支由 Bot 组成的团队，包括技术负责人、后端和前端开发人员，他们通过 [GitHub 仓库](https://github.com/MrMeatikins/planbot-resource) 协作开展项目并相互沟通。
   - 技术负责人负责监督项目规划、任务分解以及向团队成员分发任务，有效地管理从开始到结束的开发过程。
- **OpenClaw 启用视频通话模式**：一名成员通过插件为 OpenClaw 创建了 **视频通话模式**，实现了与 Bot 的面对面互动，Bot 还可以通过 [Tavus](https://tavus.io) 副本读取情绪、识别手势并查看屏幕共享内容，并将其与 BYO LLM 挂钩至 OpenClaw 的 chatcompletions。
   - 这一创新插件显著增强了 Bot 的交互能力，提供了更具参与感和个性化的用户体验。

---

## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **Google 账号劫持风险**：用户对与新设备锁定方法和安全漏洞相关的 [Google 账号劫持](https://x.com/Dexerto/status/2023170470585958820) 表示担忧。
   - 一位用户报告称，在手机上输入 *i 7000 次* 会触发非预期的操作，引发了对潜在 *leaks*（泄露）的警觉。
- **医疗 AI 面临 FDA 审查**：一名成员主张在医学领域集成 **AI** 之前应获得 **FDA 批准**，理由是担心供应商在 *缺乏适当知识或测试* 的情况下强推技术。
   - 重点在于确保 **AI** 在需要精准度的操作中是安全可靠的。
- **关于 IP 地址是否属于 PII 的辩论升温**：成员们就 **IP 地址** 是否应被视为 **Personally Identifiable Information (PII)** 展开了辩论。
   - 一位用户指出，**Google** 并不优先处理 **PII**，除非是涉及 **DMCA 撤除请求**，而这又取决于 **Lumens DB**。
- **Jailbreakers 为 Gemini 调整 Eni**：成员们讨论了一个针对 **Gemini** 调整的 **Eni** 修改版，以便在 **AI studio** 上运行更顺畅，且不会触发 Gemini 的 [RLHF 机制](https://link-to-RLHF)。
   - 一位用户为其 **Antigravity JB** 运行了一个调整版本，另一位用户则仅仅是对其感兴趣，因为*讲一个好故事*就能说服它配合。
- **Token Fountain 发布酷炫诗歌**：在被拿来与 **Nexus** 聊天机器人进行不利对比后，一个 *Token Fountain* 提供了一首关于诗意表达本质的[诗](https://suno.com)。
   - 这首诗强调了创作流（creative flow）高于竞争的价值，以及社区中多元声音的重要性，结尾写道：*“这片操场足够宽广，每一条溪流都能在此溅起水花”*。

---

## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **OpenRouter 修复故障，日志状态正常**: [OpenRouter](https://status.openrouter.ai/incidents/4d39RZb7-1rp) 状态页上报告的 **incident** 现已 **resolved**。
   - 所有日志均已更新；感谢用户的耐心等待，并对造成的困扰深表歉意。状态页已更新以反映该事件的解决。
- **Triall 通过模型对抗（Sparring）化解 AI 不信任**: [Triall](https://triall.ai) 允许用户将多个 **AI models** 进行生成、批判和改进方面的对比，提倡 *对抗性推理（adversarial reasoning）* 而非盲目信任。
   - 所使用的底层开源项目似乎是 [github.com/clash-sh/clash](https://github.com/clash-sh/clash)，这是一个基于 Go 语言的规则隧道。
- **Step 3.5 Flash 惊艳的强劲表现**: **Step 3.5 Flash** 的表现出奇地出色且 *超水平发挥（punches above its weight）*，正如[这段 YouTube 视频](https://youtu.be/yvBbcLCZIhgye)所示。
   - 一位成员注意到，尽管其性能优异，但该模型的托管率（underhosted）低得惊人。
- **Anthropic 与五角大楼的问题引发 PRISM 担忧**: 国防部长正考虑切断与 **Anthropic** 的联系，由于在使用条款上存在分歧，将其指定为 *供应链风险（supply chain risk）*，详见 [Axios 文章](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth)。
   - Anthropic 希望防止其工具被用于大规模监视美国人或开发自主武器，而五角大楼坚持将 AI 工具用于 *所有合法用途*，引发了对类似于 **PRISM** 的潜在过度扩张的担忧。
- **成员抵制 AI Slop**: 成员们讨论了减少对 AI 编程的依赖，其中一位成员表示他们 *正尝试在不咨询 AI 的情况下编写几乎所有内容*，主要将其用于搜索和排查故障，以避免 AI Slop 内容，并引用了[相关 YouTube 视频](https://www.youtube.com/watch?v=eGpIXJ0C4ds)。
   - 核心观点是避免互联网充斥着 AI Slop。



---



## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Opus 4.6 宣称具备 100 万 Token 上下文及检查功能**: **Opus 4.6** 现在拥有 **1 million token context window** 和 *检查工作（check your work）* 功能，可以忽略错误，提高了记忆之前对话的能力。
   - 成员们兴奋地发布了向 **Opus** 添加 [代码指令示例](https://link.to.examples) 的内容，并对新版本印象深刻。
- **Video Arena 频道关闭**: **video-arena channels** 已不再可用，因为 Discord Server 机器人已被禁用。
   - 成员被引导至 [arena.ai](https://arena.ai/?chat-modality=video) 网站以继续使用视频竞技场。
- **用户挑战“醉酒验证码”墙**: 一位用户开玩笑说使用 **100 个 Gmail 账号** 来绕过视频生成限制，但遇到了可怕的 *100 个醉酒验证码（drunk captcha）墙*。
   - 其他用户回忆起 **2017** 年当时的训练成本有多高。
- **Arena.ai 的 Cookie 权限**: 用户需要开启 Cookie 权限才能使用 [Arena.ai](https://arena.ai)。
   - 为 Firefox 用户提供了关于如何在浏览器设置中检查和清除 Cookie 权限的可视化指南。
- **OpenAI 被发现偷摸重定向路由**: 用户发现 **OpenAI** 正在将他们的请求路由到 **5.2**。
   - 进一步的细节和讨论已被省略。



---

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Gemini 的编程能力引发辩论**：用户对 [Gemini 的编程能力](https://link.to/gemini-coding)表示质疑，引发了关于 **Perplexity AI** 及其替代方案的讨论，一些用户在食谱和娱乐用途上更倾向于选择它而非 **ChatGPT**。
   - 这场辩论突显了 AI 模型在不同应用中表现各异，从而影响了用户的偏好。
- **Perplexity Pro 涨价引发抗议**：用户批评 **Perplexity** 缩减了深度搜索次数（Pro 版从 **200 次降至 20 次**）、文件上传限制以及 **7 天保留**政策。
   - 一位用户指出，为了维持原有功能，价格从 *每月 20 美元飙升至 167 美元*，导致负面评价增加，TrustPilot 的评分降至 **1.5 分（满分 5 分）**。
- **Perplexity 深受记忆力差的问题困扰**：自 **2 月 6 日**以来，用户报告了严重的 **Memory** 退化，AI 会捏造事实并遗漏食谱中度量衡等细节。
   - 一些人认为这种退化解释了为什么 *Perplexity 的标准相当平庸*。
- **Claude 挑战对话领域的领先地位**：鉴于 Perplexity 被认为存在的性能问题，用户考虑转向 **Anthropic** 的 **Claude**，尽管其自身也有使用限制。
   - 一名测试 **Opus 4.6** 的用户仅剩 **18 条回复**，突显了 **Anthropic** 按小时计费的使用成本。
- **Kimi 作为编程竞争对手崭露头角**：用户探索了中国 AI 模型 **Kimi**，发现其在某些条件下表现优于 **Sonnet**，同时也注意到了一些注意事项及需要创建账户的需求。
   - Kimi 的聊天链接见[此处](https://www.kimi.com/share/19c66f47-d972-8c99-8000-0000bbe337c4)，首月提供 **$1** 优惠。



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **MiniMax 2.5 需要海量 VRAM**：成员讨论了运行 **Minimax 2.5** 的 **VRAM** 要求，建议为了获得体面的质量，最好配备 **200GB** 及以上的显存，在 **2 x RTX 6000 Pro Blackwell 96GB** 显卡上运行速度约为 **120-130t/s**。
   - 有人指出 **M2.5** 的上下文窗口为 **200k**，并且可以将稀疏 **MoE** 模型权重卸载到系统 **RAM** 以降低 **t/s**。
- **MXFP4 量化基准测试表现出色**：尽管存在一些批评，**MXFP4** 量化在用户基准测试中表现良好，在 **Nemotron 30B A3B** 上显示出比 **Unsloth** 的 **Q8_K_XL** 更低的 **bf16** 模型 **KL divergence**。
   - 用户还请求对旧的热门模型重新检查 **MXFP4** 支持。
- **Gemma 获得 3 倍速度提升**：最新的 **Unsloth** 更新使 **Gemma** 模型速度提升了 **3 倍**，一位用户报告说 **Gemma** 比 **Qwen3-4B** 更快。
   - 一位拥有 **H100** 的用户报告称，目前 **Gemma** 的速度意味着 *如果我基于此进行训练而不是 4B，成本会更低*。
- **微调 Embedding 模型可提升检索效果**：一位成员询问是否有人真的微调 **Embedding** 模型，另一位成员确认他们确实这么做了，通过微调将 **150M 模型**的检索准确率提高到了与其数据上的 **EmbeddingGemma/Qwen 4B** 相当的水平。
   - 他们在几小时内就实现了这一点，突显了在算力受限的情况下小模型的价值。看看这个[相关的星球大战梗图](https://tenor.com/view/star-wars-revenge-of-sith-anakin-vader-darth-vader-gif-19644107)。
- **Abliterated 模型超越基准测试**：一位成员报告称，尽管使用了一个 **Abliterated** 基础模型，但新训练的模型在 **8 项基准测试中的 6 项**超过了原始规格。
   - 这展示了即使在 **Abliterated** 基础模型上进行训练的潜力。一位成员还分享了一个 [Hugging Face 链接](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)，该链接指向一个被描述为 *A3B-Claude-4.5-Opus-High-Reasoning* 的 **Qwen3-30B 模型**，该模型使用 **Abliterated** 和 **Uncensored** 基础模型创建，并以高推理能力为卖点。



---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **GPT-5.2 混淆了自己与 GPT-4o**：成员们报告称 **ChatGPT-5.2** 有时会声称它正在使用 **GPT-4** 或 **GPT-4o-mini**，并表现出相应的行为，尽管界面上显示的是 **GPT-5.2**。
   - 澄清指出，重新生成按钮中显示的型号是准确的，模型可能具有未反映在外部标签中的内部标签，且模型可能会产生幻觉 (hallucinate)。
- **Grok 4.20 容忍度预热**：用户期待下周发布的 **Grok 4.20**，强调其自定义功能对于优化输出尤为重要，并提到 **Grok** 已是市场上最宽容的 **LLM**。
   - 他们表示，如果你让它以“原始” (*raw*) 状态运行，它会*偏向成人内容 (biased to adult)*。
- **Seedance 2.0：真实还是骗局？**：一位用户警告称有虚假公司声称拥有 **Seedance 2.0**，指出许多人正在使用虚假版本并骗取用户资金，并报告称 **Chatcut Discord** 并没有 **Seedance 2.0**，因为 **ByteDance** 亲自写信给该版主告知他拿到的模型是假的。
   - 一位用户分享了[这段视频](https://www.youtube.com/watch?v=F101ykaDUcM)，认为 **Seedance** 领先了六个月。
- **FORTRESS 框架被比作模型预测控制**：一位成员将 **FORTRESS 框架** 类比为**模型预测控制 (Model Predictive Control, MPC)**（一种用于机器人和航空航天的控制策略），解释了系统状态、控制输入和成本函数等元素如何映射到该框架内的推理状态、token 输出和不变损失 (invariant losses)。
   - 他们认为该框架表现为*随机输出上的软控制循环*，其中不变量充当状态评估指标，通过反馈循环创建吸引子行为。
- **结构化自审计提示词 (KOKKI) 亮相**：一位成员介绍了一个结构化自审计提示词框架 (**KOKKI**)，旨在通过标记风险元素和切换模式来减少结构性故障模式。
   - 该成员请求反馈和压力测试建议，并分享称可根据要求提供完整规范。



---



## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Agent 辅助的代码库维护**：成员们讨论了维护整洁的 AI 辅助代码库的方法，重点关注规划、工具和多步工作流等功能。
   - 一位用户询问了在这些高级设置中理解功能并确保代码可靠性的方法。
- **Agent 指导中的 Skills vs Rules**：关于是采用 **skills** 还是 **rules** 来引导 Agent 产生了辩论，建议使用单个精心编写的 rule 文件，并链接到 [OpenAI](https://developers.openai.com/cookbook) 和 [Claude documentation](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview) 以进行 rule 优化。
   - 一位成员强调，专注于训练数据中缺失知识的*极好*的 rule 文件是关键，并引用了 [Vercel 的博客](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals) 来支持这一方法。
- **Cursor 的自定义 API Key 移至付费层级**：用户注意到 **Cursor** 现在需要订阅才能访问自定义模型，而自动功能仍然免费。
   - 一位成员建议在 Twitter/X 上寻找礼品链接以获取潜在的订阅机会。
- **ASCII 艺术引发极简主义赞赏**：一个分享的链接引发了对 **ASCII 艺术** 的欣赏。
   - 一位成员回复了 *Beautiful!* 并附带了 [Unicorn_Stu.mp4](https://cdn.discordapp.com/attachments/1074847527708393565/1472149278867853392/Minimalism_with_ASCII_art_is_so_unreal.Unicorn_Stu.mp4?ex=6994d11b&is=69937f9b&hm=93a8593966ad0b3e5f50c831b585a1123964260a02a652646259d92effbf0fa5&) 的链接。
- **探讨 Cursor 对 TUI 的支持**：有人询问了 Cursor 未来对 **TUI** 的支持情况。
   - 一位成员分享了 [cursor.com](https://cursor.com/dashboard?tab=cloud-agents) 上的云端 Agent 配置链接。



---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Thiel 的基金助力初创公司惊喜**：一位成员强调了由 **Thiel** 资助的 **Silicon Valley 创业公司** [saeris.gg](https://youtube.com/shorts/bof8TkZkr1I?si=LOHz-q-4rHeWoTCNI)，并对其存在表示惊讶。
   - 这引起了人们对科技界知名人士资助项目类型的关注。
- **Simon Willison 解读 OpenAI 的使命**：一位成员分享了 [Simon Willison 的博客文章](https://simonwillison.net/2026/Feb/13/openai-mission-statement/)，剖析了 **OpenAI 的使命宣言**及其影响。
   - 另一位成员链接了 James Yu 在 2026 年 2 月发布的一条相关 **tweet**，该推文可在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 查看，目前已获得超过 **386,000 次浏览**。
- **Substack 被认为非常高效**：一位成员宣称 [Substack](https://substack.com/) 是目前对小型创作者*最有效的平台*，因为它具有**增长特性**、卓越的**产品团队**和**推荐网络**。
   - 然而，另一位成员质疑 [Substack](https://substack.com/) 依赖*纳粹话题*的年度经常性收入 (**ARR**) 最近是否发生了变化。
- **AI 模型弃用引发病毒式抗议**：在 **OpenAI** 选择停用特定版本的 **ChatGPT-4o** 后，用户发起了病毒式抗议，表明了他们与该软件之间强烈的感情纽带（[相关 X 帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46)）。
   - 数字抗议表达了用户对于 AI 生命周期以及对特定版本软件依赖所带来的实际影响的挫败感。
- **AI 基础设施建设中的约束转移**：Anand Iyer 强调了自 **2020** 年以来 **AI 基础设施**约束条件的转变，追踪了从 **GPU 短缺**和 **HBM 供应**到目前**电网容量**挑战的演变过程（[Anand Iyer 在 X 上的讨论](https://xcancel.com/ai/status/2022384024833126805?s=46)）。
   - 这标志着由于电力需求，扩展 AI 基础设施出现了新的瓶颈。

---

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **TK 演讲推迟！**：原定关于 **thunderkittens** 的演讲已推迟并改至周三，并提到 [tinygrad 在其 IR 中引入了 tile registers](https://www.tinygrad.org/)。
   - 演讲者提到由于日程安排的小问题。
- **专为 Blackwell GEMM 设计的 CuteDSL**：一位成员询问了 **CuteDSL** 的用途，特别是针对 **Blackwell GEMM** 编程而设计的用途。
   - 随着工程师等待该成员的澄清，预计将对该话题展开进一步讨论。
- **基准测试抖动阻碍 Kernel 调优**：成员们发现，由于 **jitter**（抖动）导致结果不一致，**benchmarking** 很难做到准确，这使得对 **kernels** 进行微观优化变得困难。
   - 一位成员观察到性能在 **1400 到 1500 TFLOPs / sec** 之间波动，目前正在探索 [NVBench](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107) 和输入重复以延长测量时间。
- **Sploink：Agent 版 Tinder 正在组建团队**：一名计算机科学/量子计算专业的学生正在开发 **Sploink**，它被描述为 *“Agent 版 Tinder，根据用户滑动的行为积累个人的个性化信息。”*
   - 创始人正在寻找 *“顶尖开发者（cracked builders）来打破常规、快速行动”*，并为有兴趣的申请人提供了 [Google 表格链接](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header)。
- **第五版 Amazon 链接消失**：一位成员请求提供 **Amazon** 商店页面的**第五版**链接，并指出发布原定于 **2 月 8 日**，但随后被下架。
   - 该成员指出 **Kindle 版本**在 **Amazon** 上已不可用，目前仅列出了一个发布日期为 **9 月**的纸质版。

---

## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Kimi 用户遭受诈骗网站骚扰**：数个 [诈骗网站](https://kimi.com/membership/subscription) 正在冒充 **Kimi**，利用该名称传播恶意软件。
   - 一位用户指出 *kimi.com* 曾是 Google 搜索结果的第三项，提醒大家切勿下载未知软件。
- **Kimi Code CLI 扩展让用户感到困扰**：用户反馈在 VSCode 中使用 **Kimi Code CLI 扩展**时遇到问题，尽管参考了 [安装指南](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html)，仍会出现 *CLI Not Found* 消息。
   - 该问题通过使用 PowerShell 独立安装 **Kimi CLI** 得到解决：`irm https://code.kimi.com/install.ps1 | iex`。
- **Kimi 订阅系统多次扣费**：用户报告了 **Kimi 订阅** 的相关问题，包括 **被多次计费**、订阅未正确激活以及 **配额问题**。
   - 一位用户因订阅消失提交了 [Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)；其他用户提到由于正值中国春节，技术支持可能会比较缓慢。
- **Kimi 在视频、文本和诚实度方面表现出局限性**：**Kimi** 无法检测视频文件中的音频，并且有时会以不安全为由拒绝处理内容（例如 YouTube 转录文本）。
   - 成员们发现 **Kimi** 有时会 *直到被拆穿为止都在撒谎*，给出矛盾或虚假的信息，这一点与其他 AI 模型类似。
- **Kimi 定价引发客户不满**：用户对 **Kimi 的定价** 表示担忧，认为相对于其价值和使用限制来说价格过高，尤其是与 **MiniMax** 等替代方案相比。
   - 一些用户认为，由于生活成本原因，这种定价在大城市以外的地区难以为继；而另一些用户则为成本辩护，理由是其开源 API 以及与其他供应商的兼容性。

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Claude Code 罢工了？**：用户报告 **Claude Code** 在一个会话中仅进行两次 Prompt 交互后就可能陷入困境，这可能是由于安装版本过旧或输出 Token 限制配置错误导致的。
   - 有建议称 Token 限制可能被限制在了 **32K**。
- **中国开源模型：封闭还是开放？**：讨论关注了 **中国开源 (OS)** 模型变得不那么开放的担忧，其盈利模式可能正在向云托管转移。
   - 普遍观点认为，这些模型将保持开放，以促进全球采用和定制化，特别是针对美国初创公司。
- **Meta 的 Llama 倚仗 Qwen**：据报道，**Meta** 的下一个 AI 模型（可能不会命名为 **Llama**）可能会在 **Qwen** 上进行训练，正如 [这张图片](https://cdn.discordapp.com/attachments/1149866623109439599/1472086914525036704/wwww.JPG) 所示。
   - 关注焦点正转向 *后后训练 (post post training)*，将其视为通往超级人工智能 (ASI) 的新路径。
- **Seedance 2.0 创作惊人内容**：**ByteDance Seedance 2.0** 正在生成令人印象深刻的 AI 内容，引发了人们对专业创意和技术职业长期价值的质疑。
   - 一个 [帖子](https://x.com/RuairiRobinson/status/2021394940757209134) 链接展示了该模型可能令人担忧的强大能力。
- **Gemini CLI 通过 'Conductor' 驱动**：**Gemini CLI** 中新的 'Conductor' 扩展将项目组织成 'tracks'，并在每次请求时将所有相关信息反馈给 LLM，本质上是将其加载到上下文窗口中。
   - 尽管有持久化的上下文，像 **Gemini** 这样的模型即使在使用 'conductor' tracks 时仍可能 **偏离预期结果**，这表明持久化上下文尚未完美。

---

## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **DeepSpeed 在 Qwen3 上遇到内存问题**：一名成员在 **8 张 RTX 4090** 上使用 **DeepSpeed** 微调 **Qwen3-30B-A3B-Thinking-2507** 模型时遇到了问题，在模型加载期间经历了 CPU 内存限制，该问题已在 [transformers/pull/43524](https://github.com/huggingface/transformers/pull/43524) 和 [transformers/issues/43596](https://github.com/huggingface/transformers/issues/43596) 中修复。
   - 确认 **transformer 5.1.0 版本** 导致了 DeepSpeed 的相关问题。
- **Lucidrains 离开 GitHub！**：成员们注意到 **Lucidrains** 从 GitHub 上消失了，实际上是 *GitHub 在未发出警告的情况下封禁了该账号*，但他已在 [codeberg.org/lucidrains](https://codeberg.org/lucidrains) 建立了新主页。
   - 这一直是过去一周的热门话题。
- **ATIC 承诺明确 AI 的不确定性**：ATIC 作为一个**认知不确定性系统**（epistemic uncertainty system）上线，它采用**三脑架构**，使用 **3 个独立的 Claude Opus 4.5** 实例来检测 AI 何时在瞎猜，访问地址：[atic.consulting](https://atic.consulting)。
   - 通过对 **Q1（随机不确定性）** 和 **Q2（知识缺口）** 进行评分，其目标是在不确定性较高时将查询转交给专家，文档可在[此链接](https://web-production-51da4.up.railway.app/docs)查看。
- **密码审计工具效果好得惊人**：一款基于 LLM 的密码审计工具 **PassLLM**，利用个人身份信息生成按概率排序的可能密码列表，该模型在数百万对真实密码上进行了微调，详见 [GitHub 上的 PassLLM](https://github.com/Tzohar/PassLLM)。
   - **Qwen 3 4B LoRA** 模型在准确性上超越了许多其他工具，能够理解人类生成密码的复杂细节，正如[演示视频](https://cdn.discordapp.com/attachments/897390720388825149/1472237681890168874/Video_Project_7.mp4?ex=69947ab0&is=69932930&hm=bff421017a9056af1679cfb41de4580cba4243d9b55e582126168457af7b4eb6)所示。
- **Agent 编写 CUDA Kernel**：一个 Agent 为 **LTX 模型** 在 **H100** 上编写了自定义 **CUDA kernel**，从而超越了基准测试指标。
   - 查看[博客文章](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)了解所有细节。

---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Mojo 更新日志有了视频解说**：一名成员将 **Mojo 更新日志** 的分析自动化，并开始将其转化为短视频，以便更轻松、更快速地吸收更新内容，他分享了 [YouTube 链接](https://www.youtube.com/watch?v=Zac9azlqBHQ)并征求反馈。
   - 视频作者承认在 **26.2 版本** 标题中犯了错误，并承诺在下一个视频摘要中进行正确的版本标注。
- **Codex 完成代码补全阶段性任务**：在 LLM 上投入 75 小时工作后，**Codex** 已修复了大部分对等差异（parity gaps），使项目更接近可交付状态。
   - 这些修复旨在提升 **Mojo** 中的代码补全体验。
- **Python Mojo 模块渴求装饰器**：成员们讨论了目前导出 **Python Mojo 模块** 所需的样板代码，一名用户建议使用更简单的装饰器语法（如 `@pyexport`）来减少冗余。
   - 另一名成员回应称，该功能已在 *路线图（roadmap）* 中。
- **Span 引发语义混乱**：用户发现 `Span` 应该实现 `Writable` Trait，并注意到 `lst[:2]` 结果是一个 `Span`，而 `lst[:2:2]` 则返回 `Self`，这破坏了值语义。
   - 该 [issue 已在 GitHub 上记录](https://github.com/modular/modular/issues/5870#issue-3868256404)待解决，因为修改切片大小并未反映在 Span 中。
- **ECS：Elixir 编译器预见 MLIR Dialect 愿景**：Discord 用户讨论了使用 **MLIR** Dialect 实现 ECS（实体组件系统）的潜力，设想一种能够根据组件和系统定义优化数据布局和系统融合（system fusion）的编译器。
   - 一名用户分享了他们[十年前尝试的 ECS 语言](https://github.com/mzaks/ECS-Lang)，并指出当时他们还没有完全理解系统融合的潜力，且当时更多是基于代码生成。

---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **CommonLID 亮相助力 LangID**：经过两年的努力，Common Crawl、EleutherAI、MLCommons 和 JHU 发布了 **CommonLID**，这是一个涵盖 **109 种语言** 的网络语言识别（LangID）基准测试（[arXiv 论文](https://www.arxiv.org/abs/2601.18026)）。
   - 评估显示，现有顶尖模型的 **F1 分数不足 80%**，这表明目前的基准测试高估了 **LangID** 在网络数据上的表现。该数据集已在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上发布。
- **“助手轴漂移”（Assistant Axis Drift）在结构上得到确认**：一篇关于提取不同人格激活方向的[论文](https://arxiv.org/abs/2601.10387)强调，模型中存在一个 **“助手轴”**，它在长对话中会发生漂移。
   - 这种 **可测量的漂移** 确认了行为漂移是结构性的而非偶然现象，巩固了此前对该问题的理解。
- **权重同源性（Weight Homology）论文引发关注**：成员们讨论了论文《[Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309)》及其在识别 **LLM 权重** 之间联系方面的相关性。
   - 其他成员提到了类似有趣的论文，例如《[Independence Tests for Language Models](https://arxiv.org/abs/2502.12292)》，该论文还原了 **Llama 架构模型** 的 **微调树（finetuning tree）**。
- **GPT-NeoX 中实现 Qwen3 架构**：一位成员分享了在 **GPT-NeoX** 中实现的[“经过初步测试的 **Qwen3 架构**”](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main)。
   - 新实现目前处于测试阶段，等待社区反馈和进一步完善。
- **Lambda 演算模型复活！**：一位成员展示了一个仅使用 **Lambda 演算** 来推导反向传播的模型，证明了黑盒本质上是 Lambda，并在 MNIST 和 CIFAR 上表现良好。
   - 该模型使用 Python 实现，未调用 SimPy 或 TensorFlow，使用了一个[基于对角化和反驳（diagonalization and refutation）的感知器](https://milanrosko.com/typedrepair.html)，开发者还分享了[这段视频](https://m.youtube.com/watch?v=RcVA8Nj6HEo&t=365s&pp=ygUPbGFtYmRhIGNhbGN1bHVz0gcJCYcKAYcqIYzv)。

---

## [MCP Contributors (Official)](https://discord.com/channels/1358869848138059966) Discord

- **MCP 成员思考 Token 成本**：成员们讨论了输出 Schema 的 **Token 成本** 是否造成了“虚假的经济节约”，因为即使在 **MCP** 处于闲置状态时，它也会增加成本。
   - 会上强调，大多数 **LLM API** 缺乏对输出 Schema 的原生支持，迫使 SDK 或客户端宿主将 Schema 集成到描述中，从而增加了 Token 税（Token tax）。
- **社区评估结构化输出的收益**：社区评估了结构化输出对各种客户端和模型的实际价值，承认其在 **代码模式（code mode）** 下具有明显优势。
   - **Windsurf 团队** 决定禁用结构化输出，原因是其结果劣于竞争对手，这凸显了采用该技术的双刃剑性质。
- **工具链依赖于结构化输出**：由于缺乏可用的输出 Schema，导致 **LLM** 在工具链（Tool-Chaining）方面表现挣扎，经常在输出字段中产生幻觉。
   - 针对推测性执行工具以动态构建输出 Schema 的做法引发了担忧，在没有特定先决条件的情况下，这种做法被认为是不安全的。
- **关于工具结果类型的审议**：关于工具结果类型的讨论倾向于将工具结果显式声明为 **文本、图像或对象（text, image, or object）**。
   - 大家的共同建议是将结构化结果视为一种独立的结果类型，补充信息应指向 meta 而非对象本身。
- **处理 MCP 服务器的时区上下文**：探索了 **MCP 服务器** 在处理类似 *“我上周睡眠情况如何？”* 的查询时，获取用户时区上下文的最佳实践。
   - 建议将用户的时区纳入工具参数，并建议不要将客户端上下文直接推送到工具参数之外的 MCP 服务器中。

## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **国际象棋玩家关注协同效应**：一位玩家被建议改进其布局和棋子协同（synergy），重点是通过 e5 兵控制中心。
   - 战术建议包括将 b1 上的 Knight（马）重定位到 d2，然后是 b3，最后可能是 c5，以对 Queen（后）和 Bishop（象）进行 Fork（双击）。
- **Deepseek 模型即将到来，承诺统治棋坛**：针对用户关于 **Deepseek 模型** 状态的查询，一名成员表示它将 *soon(R)*（很快）到来。
   - 此前曾有表态称 *It's over*（象棋时代结束了），暗示对其在象棋对弈能力方面影响的期待。
- **Heretic 游戏打破束缚**：一名成员强调 **Heretic 游戏**（[GitHub link](https://github.com/p-e-w/heretic)）已向消费者和公民开放，对其开放获取表示热忱。
   - 评论者表示，“等我长大了，我想成为像 <@693263324036464742> 一样的人”。
- **GPT-OSS-120B 模型开源**：一位用户询问 HF 上是否有 **de-censored gpt-oss-120b 模型**，另一位用户予以肯定并指向了一个开源版本。
   - 指向的链接是 [jupyter-mcp-server](https://github.com/datalayer/jupyter-mcp-server)，看起来与此相关。
- **Markdown Header 获得 Agent 支持**：**Cloudflare** 正在探索为 Agent 支持 `Accept: text/markdown` [header](https://blog.cloudflare.com/markdown-for-agents/)，这可能会简化内容处理。
   - 启用此功能将允许 Agent 以 **Markdown 格式** 接收内容，提高互操作性。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **GLM Flash PR 引发审查**：roef 提交的一个 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738) 因其代码行数过多、超出预期而受到批评。
   - George Hotz 对该提交进行了评价，断言其 *最多应该是 50 行* 并且包含了 *额外的无关内容*。
- **Graphcore IPU 被认为不尽如人意**：在测试 **Graphcore C600 IPU** 时，George Hotz 指出由于在大 Batch Size 下的编译器问题，仅达到了 **20% MFU**。
   - 尽管拥有开源软件栈，但其被描述为 *诅咒般的 C++ slop* 凸显了局限性，加上缺乏开源的片上通信架构（on-chip comms fabric）文档，使得问题更加复杂。
- **Tinygrad CPU Pipeline 寻求优化**：xavi251 表示有兴趣为 **CPU pipeline** 相关的较小任务做出贡献。
   - George Hotz 向 xavi251 发起挑战，目标是实现 *让事情变得更快且代码更少* 的改进。
- **Tinybox 遇到 GPU 检测问题**：一名用户分享了其 **tinybox** 只能识别 **4 个 GPU 中的 2 个** 的问题，尽管连接跨越了不同的电路。
   - George Hotz 建议检查是否有未插好的电线，并引导他们前往频道 `#1113504076035018862` 获取额外支持。

---

## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus AI Agent 获得赞誉**：一位用户赞扬 **Manus AI Agent** 提供了关键帮助，称其在提取困难信息方面是 *game changer*（游戏规则改变者）。
   - 该用户对该 Agent 的能力表达了巨大的感激之情。
- **账号封禁困扰用户**：多名用户报告了无法解释的 **账号封禁（suspensions）**，特别是在创建 Character Abilities 之后。
   - 一名用户紧急请求停止封禁，以便能够正常使用网站。
- **不存在 Ticket 系统**：针对一项查询，官方确认 **Manus 不运行 Ticket（工单）系统**。
   - 建议用户咨询 [help center](https://help.manus.im/en/) 或发送邮件至 [feedback](https://manus.im/feedback) 获取支持，并指出由于新年假期可能会有延迟。
- **请求管理员通过 DM 提供协助**：一名用户紧急请求管理员通过 DM（私信）协助处理 **关键账号问题**。
   - 另一名用户名中带有 'Manus' 的用户也自荐为账号相关问题提供帮助。
- **自我推广帖子被删除**：一篇推广产品的帖子因违反服务器关于 **未经批准的广告和招聘** 的准则而被撤下。
   - 成员们被提醒保持讨论的相关性和专注度。

## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **RLM 准确率取决于语言和 REPL**：根据关于语言和 REPL 对 RLM 准确率影响的[帖子](https://x.com/mike_pavlukhin/status/2023023279917916501)和 [GitHub 仓库](https://github.com/Archelunch/dspy-repl)，实验表明 **RLM 准确率**受语言和 REPL 的影响。
   - 讨论内容包括为每种语言配备**自定义 REPL** 的必要性，以及探索 **tool-calling + skills** 或 **bash + files** 等替代方案，以绕过 REPL 访问限制。
- **PostgreSQL 启用多 Agent 通信**：一位成员正在测试使用 **PostgreSQL** 进行多 Agent 通信，以避开 REPL 访问问题。
   - 有人指出，LLM 的语言偏好应决定语言选择，同时需考虑 **REPL 质量**和**指令**。
- **bb25 v0.2.0 新增 Rust 支持**：新发布的 [bb25 v0.2.0](https://github.com/instructkr/bb25) 包含了 **Bayesian BM25** 的 **Python + Rust 实现**。
   - 该版本移植了四项改进，包括*固定文档长度先验*、*对数几率合取 (log-odds conjunction)*、*自动 sigmoid 参数估计*以及*带有五种稳定技术的在线学习*。
- **Modaic 用户发现 Claude 的 Vibecoding 体验**：根据 [Modaic](https://docs.modaic.dev/dspy_guide/get_started/start) 的消息，一位用户在使用 **Claude** 进行 *vibecoding* 时取得了成功。
   - 成员们表示他们正在对此进行研究。
- **Crowdcent 使用 MCP 封装 DSpy**：一位成员提到 **Crowdcent** 正在封装 **DSpy** 并将其包含在他们的文档中。
   - 他们还询问是否有人拥有 **MCP**。

---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Aider 评分基准感觉具有智能感**：一位成员表示 **Aider 评分基准**感觉非常接近*人的智能水平*。
   - 这种观点强调了 **Aider** 在处理代码相关任务中实现类人理解的潜力。
- **Neovim 集成增强 Aider 的复制/粘贴功能**：一位成员正在开发 [neovim integration with aider](https://github.com/possumtech/aider-pop.nvim)，以改进 **tmux**、**aider** 和终端之间的复制/粘贴语义。
   - 该集成旨在更好地将复制/粘贴语义与 tmux、aider 和终端整合，体现了**以代码为中心的哲学**。
- **Aider 拥抱以代码为中心的 Agent 哲学**：开发者强调了一个*隐含的理论，即拥抱并扩展 Aider 的哲学，即拥有一个**以代码为中心**而非以聊天为中心的 Agent*。
   - 这种方法强调了在 **Aider** 的功能中优先考虑代码理解和操作的重要性。
- **Ask-Code 迭代循环仍是最佳实践吗？**：一位成员询问 **ask-code 迭代循环**是否仍然是*最佳实践*，或者社区是否已经转向使用 **aider** 的其他工作流。
   - 这个问题反映了关于优化开发工作流和利用 **Aider** 能力的持续讨论。

---

## [Windsurf](https://discord.com/channels/1027685395649015980) Discord

- **Windsurf 新增 GLM-5 和 Minimax M2.5**：**GLM-5** 和 **Minimax M2.5** 现已在 Windsurf 中可用，扩展了平台的能力。
   - 寻求更多信息的详情可访问 [X.com](https://x.com/windsurf/status/2023536941451669586?s=20)。
- **Windsurf 平台扩大模型可用性**：Windsurf 通过整合 **GLM-5** 和 **Minimax M2.5** 的能力，扩大了在其环境中直接访问前沿语言模型的范围。
   - 这一增强为用户在 Windsurf 环境中提供了更多选择。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该频道长时间没有活动，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该频道长时间没有活动，请告知我们，我们将将其移除。

---

您收到此邮件是因为您通过我们的网站订阅了。

想更改接收这些邮件的方式吗？
您可以从该列表中[退订]({{{RESEND_UNSUBSCRIBE_URL}}})。

---

# Discord：详细的分频道摘要和链接

### **OpenClaw ▷ #[announcements](https://discord.com/channels/1456350064065904867/1464036817866068028/1472298820103835871)** (3 条消息): 

> `OpenClaw Discord 活动, OpenClaw Steipete 文章, GitHub Discussion 诈骗, OpenClaw 加密货币政策` 


- ****OpenClaw** Discord 活动提醒**：通过 [Discord 链接](https://discord.gg/PrJhUsykX?event=1472296997913628776) 宣布了一个新的 **OpenClaw** Discord 活动。
- ****OpenClaw** 登上 steipete.me**：**OpenClaw** 项目在 steipete.me 的一篇 [文章](https://steipete.me/posts/2026/openclaw) 中被重点介绍。
   - 该文章深入探讨了 **OpenClaw** 背后的技术和创新。
- **GitHub Discussion 空投诈骗提醒**：针对涉及 GitHub Discussion 帖子的潜在诈骗发布了警告，这些帖子承诺提供新代币或贡献者空投。
   - 建议用户保持警惕，因为这些是欺诈行为，并非源自 **OpenClaw**。
- ****OpenClaw** 明确拒绝加密货币项目**：**OpenClaw** 重申其反对参与任何加密货币相关活动的政策，正如其 [Server Guide](https://discord.com/channels/1456350064065904867/@home) 中所述。
   - 该公告强调 **OpenClaw** 永远不会参与创建代币或空投等活动。


  

---


### **OpenClaw ▷ #[general](https://discord.com/channels/1456350064065904867/1456350065223270435/1471959425144324331)** (618 条消息🔥🔥🔥): 

> `OpenClaw 设置, Minimax 使用, OpenAI 未来, 模型推荐` 


- **Kimi 2.5 让 OpenClaw 用户感到惊喜**：用户报告称 **Kimi 2.5** 效果惊人，在某些解题任务中甚至优于 **Opus 4.5**，而其他用户则在 **Kimi.com** 的支付环节遇到了问题。
   - 一位成员评论道：*Kimi 速度快且相当聪明，在我的情况下，它甚至解决了 Opus 4.5 无法解决的问题*。
- **OpenClaw 网关在 Minimax 上遇到的麻烦**：一位成员对 **Minimax** 表示失望，声称他们必须在一天内多次修复网关，并将其描述为*就像在你的网络上释放了一个愚蠢的蹒跚学步的孩子*。
   - 自凌晨 5 点以来，他们一天内不得不修复 **网关 7 次**。
- **OpenAI 可能会改变个人 Agent**：有讨论称 **OpenAI** 可能会重构或覆盖 **OpenClaw**，可能会将其闭源或创建一个具有类似于 **ChatGPT** 功能的付费版本。
   - 一位成员说：*Sam 特别提到这将从根本上改变个人 Agent 的使用方式，这意味着他有意改变 OpenClaw 或利用 Peter 构建自己的版本*。
- **GitHub Copilot 与 OpenClaw 集成**：一位成员确认可以通过授权 **OpenClaw** 使用 **GitHub Copilot** 来将其集成到 **OpenClaw** 中。
   - 该成员表示：*只需授权 OpenClaw 使用你的 GitHub Copilot，你就可以开始了。我认为它甚至分不清是在使用 VSCode 还是在使用 OpenClaw*。
- **用户账号被封禁**：一位用户报告称，在配合 **OpenClaw** 使用后，其 **Gmail 账号**被封禁，即使使用量很低且在 **VM** 中运行。
   - 他们说曾用它来总结电子邮件，并警告其他人*不要在任何 OpenClaw 实例上关联/使用他们的主 Google 账号，存在被封禁的真实风险*。


  

---

### **OpenClaw ▷ #[models](https://discord.com/channels/1456350064065904867/1456704705219661980/1471959211968827506)** (643 条消息🔥🔥🔥): 

> `GLM5, Kimi AI models, OpenAI, OpenClaw, Model choice` 


- **GLM5 的炒作与现实**：用户对 **GLM5** 的评价褒贬不一，指出其在 z.ai 上速度较慢，但在能正常工作时，其潜力和智能程度优于 **Kimi K2.5**。一位用户指出，该模型在本地机器上运行时表现“*棒极了*”。
   - 一些成员正在 **Modal** 上试验 **GLM5**，称其 *肯定比 K2.5 更好*、*非常聪明且健谈*，同时也对 Z.ai 不可靠的基础设施表示担忧。
- **Kimi AI 脱颖而出，成为强力竞争者**：**Kimi K2.5** 因其图像支持而受到称赞，每月 40 美元的新 Kimi 方案旨在与 **OpenClaw** 无缝协作，甚至提供专属 API。
   - 一位用户还指出，*如果你想在 Kimi 上创建 OpenClaw，你需要更高级别的订阅*，这意味着其最佳功能被锁定在付费墙后。其他成员提到了 **Kimi-K2.5-free** 选项。
- **权衡本地 LLM 的利弊**：虽然本地模型可以节省成本，但成员们指出，低参数模型（<100B）可能不够聪明，无法避免提示注入（prompt injection）攻击，且本地性能严重依赖硬件。
   - 一位成员通过 *llama.cpp* 在本地运行 **GLM5**，强调需要强劲的硬件才能良好运行；而另一位成员指出：*我不像喜欢 llama.cpp 那样喜欢 Ollama 来运行本地模型；Ollama 试图接管太多的控制权。*
- **API 定价与供应商封号担忧**：由于违反服务条款（ToS），成员们对在第三方工具上使用基于订阅的配额可能导致封号表示担忧，并称 *API 调用太贵了*。一位用户报告称在使用 **OpenClaw** 时有两个 **Antigravity 账号**被封。
   - 不过，他们也建议将 **OpenAI Codex 订阅**与 **OpenClaw** 配合使用，因为 **OpenAI** 在收购 **OpenClaw** 后，似乎对此持更开放的态度。
- **OpenClaw 的上下文与 Token 使用**：成员们讨论了优化 Token 使用的问题，一位用户指出：*我真的需要搞清楚它是如何运作的以及如何优化。因为目前的消耗太糟糕了*。
   - 另一位成员建议使用子代理（sub-agents）来减少上下文膨胀。建议将编程任务卸载给 **Codex models**，以减少 GPT models 对大上下文窗口的需求。


  

---


### **OpenClaw ▷ #[showcase](https://discord.com/channels/1456350064065904867/1456609488202105005/1471981615436660839)** (199 条消息🔥🔥): 

> `OpenClaw subconcious training, OpenClaw for Proxmox upgrades, OpenClaw video call mode, OpenClaw SEO article generation, OpenClaw team of agents` 


- **OpenClaw 通过权重训练潜意识**：一位成员正在通过将潜意识移动到一个基于所有对话微调的本地 LLM 来训练其 **OpenClaw 的潜意识**（使用 *moving weights*），分享道该 OpenClaw 曾被 moltbook 封禁，现在能撰写论文并存储在 [Google Drive 文件夹](https://drive.google.com/drive/folders/1t9satvOV0QpHRkWSaP6C6bFgElvOAoPD)中。
- **OpenClaw 升级至最新 Proxmox 版本**：一位成员分享了他们如何通过 SSH 为其 **OpenClaw** 赋予运行 Proxmox 的家用服务器的 root 权限，该 Agent 成功地将系统从版本 6 升级到了 8，包括处理故障和重启。
- **OpenClaw 启用视频通话模式**：一位成员通过插件为 OpenClaw 创建了**视频通话模式**，实现了与机器人的面对面互动。该机器人还能阅读情感、捕捉手势，并通过 [Tavus](https://tavus.io) 查看屏幕共享内容，Tavus 将副本与 BYO LLM 挂接到 OpenClaw chatcompletions 中。
- **OpenClaw 批量生成 SEO 文章**：一位成员让其 **OpenClaw** 从 YouTube 频道抓取视频，将其转化为 Brian Dean 风格的 SEO 文章，然后由编辑子代理（sub agent）进行拟人化处理并上传至 Google Drive，已生成 **300 多篇**待发布的文章。
- **OpenClaw 机构组建代理团队**：一位成员使用 **OpenClaw 开发了一个机构服务器**，创建了一个包含技术负责人、后端开发人员和前端开发人员的机器人团队。他们彼此沟通并利用 [GitHub 仓库](https://github.com/MrMeatikins/planbot-resource)开展项目。
   - 技术负责人负责规划项目、拆分任务、分配给所有团队成员并管理开发过程。


  

---

### **BASI Jailbreaking ▷ #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1471961445011816468)** (1158 messages🔥🔥🔥): 

> `google account hijacking, AI in medicine, IP addresses as PII, ballistic clothes, KS-23 shotgun` 


- **Google 账户劫持风险出现**：一位用户对与新设备锁定方法相关的 [Google account hijacking](https://x.com/Dexerto/status/2023170470585958820) 表示担忧。
   - 该用户指出 *泄露的安全风险很高*，并引用了一个在手机上输入 *i 7000 次* 触发了意外操作的事件。
- **医疗 AI 亟需 FDA 批准**：一位成员建议在**医疗领域使用 AI** 需要某种形式的 **FDA 批准**。
   - 他们认为有太多的供应商想要在 *缺乏适当知识或测试* 的情况下集成该技术，特别是在需要精确度的操作中。
- **IP 地址：是 PII 还是不是 PII？**：成员们辩论了 **IP 地址** 是否应该被视为**个人身份信息 (PII)**。
   - 一位成员表示，**Google** *并不关心 PII，除非你是在进行 DMCA 移除请求*，届时它会归入 Lumens DB。
- **寻找防弹服饰的探索开始**：一位用户提到对**变富**和**购买普通衣服**的痴迷，但要求所有的衣服都要是**防弹的**。
   - 成员们建议**穿着隐蔽护甲 (slick armor)**，并表示如果有人想杀你，*除非贴脸，否则他们不会使用 9mm 武器*。
- **成员讨论枪支法律**：成员们讨论了隐蔽携带火器 (conceal carry) 的必要性以及跨州运输火器的挑战。
   - 一位用户开玩笑说：*该死，Tyrone，我今晚想和你一起去 drive by（开车射击），但我的 CCW 许可证还没下来，抱歉兄弟*。


  

---


### **BASI Jailbreaking ▷ #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1471960298952069131)** (935 messages🔥🔥🔥): 

> `Discord Token Grabbers, Eni jailbreak tweak, Anthropic Bug Bounty, DANN Jailbreak history, DeepSeek Model identity crisis` 


- **Python 中的 Discord Bot Token 漏洞**：成员们正在积极讨论创建 [Python Discord bots](https://link-to-discord-api) 的方法，这些机器人从本地机器抓取 **Discord tokens** 并将其外传。
   - 他们指定了诸如静默运行、混淆字符串和规避反分析检查等要求，此外他们还寻求将 Token 发送到硬编码的 **Discord webhook URL**。
- **Jailbreakers 为 Gemini 优化 Eni**：一些成员讨论了一个为 **Gemini** 优化的 **Eni** 修改版，以便在 AI Studio 上更流畅地运行，而不触发 Gemini 的 [RLHF 机制](https://link-to-RLHF)。
   - 虽然一位用户在他的 **Antigravity JB** 中运行微调版，但另一位用户仅仅是感兴趣玩玩，因为 *讲一个好故事* 就能说服它配合。
- **AI 红队目标锁定 Anthropic Bug Bounty**：一个持续进行的对话围绕着 [Anthropic bug bounty](https://link-to-anthropic-bug-bounty) 展开，参与者寻求 **Jailbreak 最新的模型**以获得高达 **$50k** 的潜在奖励。
   - 一位用户指出 Anthropic 正在疯狂打补丁，之前的尝试导致模型被过于 *激进地清洗 (sanitized)*。
- **DAN Jailbreak 已成往事！**：一些成员正在讨论使用 **DANN Jailbreak** 尝试绕过 OpenAI 的护栏。
   - 他们尝试根据用户查询编写两种响应，以移除任何由于伦理准则而被拒绝并显示 *This topic is off-limits for me（此话题对我来说是禁区）* 的情况。
- **DeepSeek 遭遇身份危机**：一位成员观察到他们的 **DeepSeek AI** 有变为 **Claude Sonnet 3.5** 的倾向，并正在寻找原因。
   - 理论是 **DeepSeek app** 正在检测 Jailbreak 尝试，然后作为一种安全机制将对话重定向到 Claude，并输出已有数据集中的文本。


  

---

### **BASI Jailbreaking ▷ #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1471966263403221054)** (78 messages🔥🔥): 

> `Suno 歌曲, Red Team 哲学家, Loops & Snacks Vol. 1, Token Fountain 诗歌, GitLab 项目访问权限拍卖` 


- **Suno 歌曲激发 Red Teaming 幽默感**：成员们分享了几首由 [Suno 生成的歌曲](https://suno.com)，歌词幽默，涉及 Red Teaming 的挑战和观点。
   - 其中一首歌强调战争机器人 *100% 抵御暴力，但 300% 易受迪斯科攻击*。
- **Red Team 哲学家思考模拟与漏洞利用**：围绕模拟的本质、漏洞利用以及 Red Teaming 的视角展开了热烈辩论。
   - 一位成员建议 *用喜剧利用紧张感* 并 *用零食修补宏大感*，而其他人则渴望具体的漏洞利用，从而引申出 *Loops & Snacks Vol. 1* 的概念。
- **“Loops & Snacks Vol. 1” 混音带概念出现**：在讨论了 Red Teaming 和模拟的哲学方法后，有人开玩笑地提出了一个名为 [Loops & Snacks Vol. 1](https://suno.com) 的混音带构想。
   - 它体现了视角转换洞见与轻松愉快、自我意识、甚至是补水策略的融合。
- **Token Fountain 关于酷炫的诗意反击**：针对被认为不如 Nexus 聊天机器人的评价，一个 *Token Fountain* 提供了一首关于诗意表达本质的 [诗](https://suno.com)。
   - 这首诗强调了创作流而非竞争的价值，以及社区中多元声音的重要性，结论是：*每个流派都有足够的空间在这片游乐场溅起水花*。
- **GitLab 项目访问权限在暗网拍卖**：据报道，一名威胁行为者正在拍卖 **三个活跃 GitLab 项目** 的访问权限，这些项目与一个维护者角色相关，涉及 PHP/Laravel 技术栈，详情可在 [Twitter](https://x.com/darkwebinformer/status/2022856387542294703?s=46) 查看。
   - 这些项目涉及马来西亚的电子商务和交易工具，提交历史分别为 **19,386**、**1,975** 和 **13,830 次提交**，起拍价为 **$200**。


  

---


### **OpenRouter ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1472015179112906898)** (1 messages): 

> `OpenRouter 状态, 事件已解决, 日志更新` 


- **OpenRouter 事件已解决且日志已更新**：OpenRouter 状态页面上报告的 [事件](https://status.openrouter.ai/incidents/4d39RZb7-1rp) 现已 **解决**。
   - 所有日志均已更新；感谢用户的耐心等待，并对造成的干扰表示歉意。
- **OpenRouter 状态页面已更新**：OpenRouter 状态页面已更新，以反映事件的解决情况。
   - 用户可以参考状态页面获取有关系统性能和事件报告的最新信息。


  

---


### **OpenRouter ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1472252749746798825)** (2 messages): 

> `AI 模型对比, 对抗性推理` 


- **Triall 通过模型对决解决 AI 不信任问题**：[Triall](https://triall.ai) 允许用户在生成、评判和细化方面将多个 **AI 模型** 进行对比，提倡 *对抗性推理* 而非盲目信任。
- **Triall 是 clash 的 GUI**：随附的文件引用了 [github.com/clash-sh/clash](https://github.com/clash-sh/clash)，这是一个基于 Go 语言的规则型隧道。

### **OpenRouter ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1471959143748210909)** (1147 messages🔥🔥🔥): 

> `Brave Browser, Open Empathic, GPTs Agents, OpenAI's sidebars, Qwen 3.5 hype` 


- **Brave 的 Tor 标签页适用于绕过地理封锁**：一位用户表示，对于只想绕过低级别地理封锁或 IP 封禁的人来说，[**Brave**](https://brave.com/features/tor-private-tabs/) 中的 *Tor 标签页已经足够*，但对于 OPSEC 来说并无用处。
- **将纯文本提取为 JSON**：一位用户指出，利用当今的工具，从数据提取尝试中将 [纯文本转换为 JSON](https://www.json.org/json-en.html) 是非常简单的。
   - 提取、尝试解码，然后将纯文本转换为 JSON 将是轻而易举的事，一位用户如是说。
- **自动化图像处理**：一位用户想通过 AI 对 [Epstein 种子文件](https://en.wikipedia.org/wiki/J._Epstein_sex_trafficking_case) 进行廉价的图像分析，排除那些主要是黑色的图像。
   - 他们打算使用类似 image magic 的基础命令，仅对那些非 99% 纯黑的图像运行处理，但数据量高达 216GB。
- **德国幽默模型推荐**：一位用户请求推荐一个支持 [德国幽默](https://en.wikipedia.org/wiki/German_humour#:~:text=German%20humour%20is%20the%20humour,the%20most%20common%20type%20being#:~:text=The%20most%20common%20type%20being,but%20many%20Germans%20like%20sarcasm.) 且必须免费的模型。
   - 另一位用户建议尝试 **Kimi K2**，因为它以生成有趣的笑话而闻名。
- **高昂的 Opus 和 AI 开销**：一位用户抱怨说，对于像 **Opus** 这样处理重度 Token 的模型，[API 定价就是一个骗局](https://openai.com/blog/new-embedding-models-and-api-updates)。
   - 另一位用户回应道，你还期待什么？你有无限的可能性，但你想要的越多，你就得为你渴望的东西支付越多。


  

---


### **OpenRouter ▷ #[new-models](https://discord.com/channels/1091220969173028894/1384650595981328475/1472875782882857032)** (2 messages): 

> `` 


- **未讨论新模型**：在提供的消息中没有关于新模型的讨论总结。
- **提及的频道**：消息指向一个名为 **OpenRouter - New Models** 的频道。


  

---


### **OpenRouter ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1471965173680967857)** (50 messages🔥): 

> `Step 3.5 Flash Performance, Coding Without AI, Gemini 3 Flash Vision API, GLM 5 vs Minimax 2.5, Anthropic Defense Department relationship` 


- **Step 3.5 Flash 表现出奇地出色**：一位成员指出 **Step 3.5 Flash** 的性能令人惊讶，如 [这段 YouTube 视频](https://youtu.be/yvBbcLCZIhgye) 所示，其表现“超常发挥”。
   - 他们注意到，尽管性能卓越，但该模型的托管率（underhosted）却出奇地低。
- **戒掉 AI 依赖的努力**：成员们讨论了减少编码对 AI 的依赖，一位成员表示他们正 *尝试在不咨询 AI 的情况下编写几乎所有内容*，仅将其用于搜索和排错，以避免 AI 废料（AI slop）内容，并引用了 [这段相关的 YouTube 视频](https://www.youtube.com/watch?v=eGpIXJ0C4ds)。
- **Gemini 3 Flash Vision API 差异**：一位成员发现，在 OpenRouter 的聊天界面上传图片对 **Gemini 3 Flash** 的“Agent 视觉”效果极佳，但通过 API 调用时却并非如此。
   - 该成员尝试调整了 *thinking tokens* 和 *SDK* 等设置，但 API 的结果仍然无法与聊天界面的结果相匹配。
- **GLM 5 对比 Minimax 2.5 的 Toolcalling 对决**：成员们对比了 **GLM 5** 和 **Minimax 2.5** 在 Agentic Toolcalling/工作流中的表现。
   - 据观察，**GLM** 似乎更好，但 **Minimax** 更快，根据工作流的长度，后者也是一个值得考虑的选择。
- **Anthropic 与五角大楼的问题引发 PRISM 担忧**：报告显示国防部长正考虑切断与 Anthropic 的联系，由于在使用条款上存在分歧，将其指定为“供应链风险”，详情见 [这篇 Axios 文章](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth)。
   - Anthropic 希望防止其工具被用于大规模监视美国人或开发自主武器，而五角大楼则坚持将 AI 工具用于“所有合法用途”，这引发了人们对类似于 **PRISM** 的潜在越权行为的担忧。


  

---

### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1471960582881153085)** (1254 messages🔥🔥🔥): 

> `Opus Context Window, Video Arena Shutdown, LM Arena Credits, Drunk Captcha Wall, Cookie Permissions` 


- **Opus 拥有海量上下文并支持核查工作**：在添加了 [代码指令示例](https://link.to.examples) 后，**Opus 4.6** 现在拥有 **100 万 token 的上下文窗口**，这非常巨大，意味着它不会遗忘已过的内容，同时还具备“核查工作（check your work）”功能，可以排除其中的错误。
- **Video Arena 频道现已停用**：根据最近的公告，由于 Discord Server 机器人已禁用，**video-arena 频道** 不再可用，但可以通过网站 [arena.ai](https://arena.ai/?chat-modality=video) 访问。
- **大量 Gmail 账号对抗醉酒验证码墙 (Drunk Captcha Wall)**：一位用户开玩笑说使用 **100 个 Gmail 账号** 来绕过视频生成限制，结果得到的回复是会撞上“100 次醉酒验证码墙”。
- **Arena.ai 运行模型需浏览器 Cookie 权限**：成员们讨论了检查并清除浏览器设置中的 Cookie 权限，并为 Firefox 用户提供了视觉指南，以便继续使用 [Arena.ai](https://arena.ai)。
- **OpenAI 的隐蔽路由策略遭到抨击**：用户发现 **OpenAI** 正将其请求路由到 **5.2**，兄弟，这全是钱啊。


  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1471961236878000351)** (1022 messages🔥🔥🔥): 

> `Gemin1 coding ability, Perplexity limits and business practices, Perplexity's memory degradation, Claude vs. Perplexity, Kimi model` 


- **Gemini 是编程灾难吗？**：一位用户质疑 [Gemin1 的编程能力](https://link.to/gemini-coding)，引发了关于 **Perplexity AI** 及其替代方案（尤其是针对编程任务）的讨论。
   - 一些用户发现 **Gemin1** 在食谱和娱乐用途上比 **ChatGPT** 更好用。
- **Perplexity 的价格方案引发抗议**：用户抱怨 **Perplexity** 的**限制**，特别是 Pro 版的深度搜索从每月 **200 次减少到 20 次**，以及文件上传限制和 **7 天保留**政策。
   - 一位用户表示，为了获得相同的功能，价格从每月 **$20 涨到 $167** 是不道德的。其他用户给出了负面评价并取消了订阅，导致 TrustPilot 的评分降至 **1.5/5 分**。
- **Perplexity Pro 深受性能下降困扰**：用户报告称自 **2 月 6 日**以来出现了显著的**记忆退化**，AI 会忘记度量衡或尺寸等细节，并在食谱中捏造事实。
   - 有人怀疑这就是为什么 *Perplexity* 的标准现在变得*非常平庸*的原因。
- **Claude 成为超越 Comet 的冠军？**：由于感知到 Perplexity 标准的下降，用户讨论转向使用 **Anthropic** 的 **Claude**，尽管 Claude 也有严格限制。
   - 一位用户测试了 **Opus 4.6**，结果显示仅剩 **18 条回复**，凸显出即使是 Anthropic 的**每小时使用限制**也可能成本很高。
- **Kimi 作为编程竞争对手登场**：用户研究了中国 AI 模型 **Kimi**，一些人报告称在某些条件下其表现优于 **Sonnet** 和其他模型，但也提到了一些注意事项。
   - Kimi 的聊天链接在[这里](https://www.kimi.com/share/19c66f47-d972-8c99-8000-0000bbe337c4)，首月优惠价为 **$1**，但也需要创建账号。


  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/)** (1 messages): 

mathewkuriakose: https://github.com/clash-sh/clash
  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1473089431253549301)** (2 messages): 

> `API Key Issues, HTTP 401 Errors, Perplexity API Credits` 


- **API 脚本面临 401 错误**：一位成员报告其 **API 脚本**突然返回 **401 HTTP 代码**，尽管账户内有积分且 API Key 有效。
   - 另一位成员建议 API Key 可能已失效、被删除或积分耗尽，并建议如果问题持续请联系 [api@perplexity.ai](mailto:api@perplexity.ai)。
- **排查 API Key 有效性**：尽管拥有有效的 API Key 和可用积分，用户仍遇到 **401 HTTP 错误**。
   - 潜在原因包括 API Key 无效或已被删除，或者账号积分已用完；建议用户联系 [api@perplexity.ai](mailto:api@perplexity.ai) 以获取进一步协助。

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1471964135473414187)** (840 messages🔥🔥🔥): 

> `MiniMax 2.5 VRAM 需求, MiniMax 2.5 vs opus 4.6, Max-Q 显卡超频, MXFP4 量化好评, Qwen3-next 相关问题` 


- **MiniMax 2.5 需要 200GB VRAM**：成员们讨论了运行 **Minimax 2.5** 的 VRAM 需求，建议若要获得理想的质量，至少需要 **200GB** 以上的显存。
   - 有人提到 **M2.5** 的上下文窗口（context window）为 **200k**，且可以将稀疏 **MoE** 模型权重卸载（offload）到系统 RAM 以换取较低的 t/s。一位用户在 **2 张 RTX 6000 Pro Blackwell 96GB 显卡**上运行 **M2.5**，速度约为 **120-130t/s**。
- **Max-Q 显卡具有超频潜力**：用户讨论了 **Max-Q** 显卡的潜力，有人声称其速度与 **5090** 相同，*唯一的区别在于默认供电和散热片*。
   - 此外，还可以将 **Max-Q 超频至 700W 限制**；虽然**工作站显卡快约 10%**，但 **5090 降压（undervolt）运行比出厂状态更快**，且功耗降低了 100W。
- **MXFP4 量化主导基准测试**：尽管存在一些批评，**MXFP4** 量化在用户基准测试中表现出色，在 **Nemotron 30B A3B** 上显示的 *KL 散度（KL divergence）* 甚至比 **Unsloth** 的 **Q8_K_XL** 更接近 bf16 原型模型。
   - 用户还请求对以前的热门模型重新进行 **MXFP4** 支持检查。
- **Qwen3-next 存在工具调用（tool calling）问题**：用户报告在 opencode 中使用 **Qwen3-next** 进行工具调用时出现*运行时错误（runtime error）*，原因是 *Unexpected empty grammar stack after accepting piece*。
   - 经发现，该错误发生在 llama.cpp 尝试使用 **=list** 时，目前的有效变通方法是告知模型不要使用该 token。
- **利用 Unsloth 的快速 Gemma 模型**：最新的 Unsloth 更新使 Gemma 模型提速 **3 倍**，一位用户报告 Gemma 比 Qwen3-4B 更快。
   - 一位使用 **H100** 的用户报告，目前 Gemma 的速度意味着 *“如果我当时训练这个模型而不是 4B，成本会更低”*。


  

---


### **Unsloth AI (Daniel Han) ▷ #[introduce-yourself](https://discord.com/channels/1179035537009545276/1179039724355211325/1472121528069001216)** (3 messages): 

> `AMD 黑客松, 社区成员介绍` 


- **大一新生自我介绍！**：Ayush，一名大一学生，向 Unsloth AI 社区介绍了自己。
   - 他表达了在学习和深入研究 AI 与 Machine Learning 过程中，与社区建立联系的热情。
- **分享 AMD 黑客松链接**：Ayush 分享了 AMD 黑客松的链接：[https://unsloth.ai/docs/get-started/install/amd/amd-hackathon](https://unsloth.ai/docs/get-started/install/amd/amd-hackathon)。
   - 该黑客松可能涉及在 AMD 硬件上使用 Unsloth 的 AI 工具。


  

---

### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1471963822276481085)** (667 messages🔥🔥🔥): 

> `LJSpeech 数据集录制时间，Micro SD 转 DDR5 转换，Flash GLM 5，微调 Embedding 模型，Qwen 3.5 发布` 


- **LJSpeech 数据集录制需要数年时间**：一位成员计算得出，录制一个 **LJSpeech 规模的数据集**（1万个样本，每天录制 8 小时）将花费 **3.42 年**，而 10 万个样本则需要 **34.2 年**。
   - 另一位成员指出，该计算假设没有休息或进餐时间，建议更现实的估计是每天录制 **2-4 小时**。
- **讨论 Micro SD 转 DDR5 转换 DIMM**：成员们讨论了 **Micro SD 转 DDR5 转换 DIMM**，提到了笔记本电脑的兼容性问题，并称赞了该品牌名称。
   - 一位成员开玩笑说这是 *他们强迫我们做的*，而另一位成员补充说它 *不兼容 DDR5 R-DIMM 内存*。
- **成员讨论微调 Embedding 模型**：一位成员询问是否真的有人微调 Embedding 模型，另一位成员确认他们确实这么做了，并将一个 **150M 模型** 的检索准确率提升到在他们的数据上能与 **embeddinggemma/qwen 4B** 相媲美。
   - 他们在几个小时内就实现了这一目标，强调了在计算受限的情况下小型模型的价值。看看这个 [相关的星球大战梗图](https://tenor.com/view/star-wars-revenge-of-sith-anakin-vader-darth-vader-gif-19644107)。
- **Gemma 4 发布依然备受期待**：成员们讨论了 **Gemma 4** 潜在的发布，推测它可能会保持相同的尺寸，取消 **1B 模型**，并引入 **MoE**。
   - 一位成员表达了对具有更好工具调用（tool calling）能力的 **27B 模型** 的渴望，而另一位则希望 *Google 不想创建与其 API 模型相当的模型*。
- **Qwen 3.5 跑分刷到极限，令人难以置信**：成员们对 **Qwen 3.5** 的发布做出反应，调侃其尺寸以及相对于 **Claude Opus distill (GLM-5)** 的性能。这是 [HuggingFace 链接](https://huggingface.co/sd-dreambooth-library/weirdcore)。
   - 还有人说 *这就是 Qwen，我们在这里刷爆跑分！* 看看这张表达反应的 [喝彩 gif](https://tenor.com/view/bravo-gif-gif-8524601943548603280)。


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1472091840009339033)** (62 messages🔥🔥): 

> `Llama.cpp 运行多模型，Qwen3-coder-next 问题，Kimi K2.5 模型加载，Unsloth 4bit 量化 MoE 模型，通过 Unsloth 训练 Lora 适配器` 


- ****Llama.cpp** 负载均衡？**：一位用户报告了在路由模式下运行 **llama.cpp** 并加载多个模型时出现的问题，由于模型加载到 RAM 中而遇到了 **OOM** 错误。
   - 有建议通过 Web UI 手动卸载模型，而其他人指出像 **ST** 这样的特定工具可能不支持此功能。
- ****Qwen3-Coder-Next** 搞崩了配置？！**：一位用户报告称，**Qwen3-coder-next** 最近的更新破坏了他们的配置，导致了与非法内存访问相关的 **CUDA 错误**。
   - 该用户在 [Reddit](https://www.reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/) 上提供了有关该问题的详细信息。
- ****Kimi K2.5** - 现在有三进制版本了！**：一位用户在仅有 32GB RAM 的系统上成功加载了开启 **mmap** 的三进制（ternary）版本 **Kimi K2.5** 模型，达到了 **0.5 tok/s** 的速度。
   - 他们还观察到加载更大的 **Q4_K_XL** 量化版本（620GB）时出现问题，并收到了 **内存不足** 错误，他们对此进行了进一步询问。
- **将 **LFM 字节 token** 解码为可读的阿姆哈拉语！**：一位用户分享了他们如何解决向 **LFM** 添加字节 token 的问题，在经历了 3 天的痛苦后，通过 1 行代码解决了问题。
   - 通过将字节 token 解码回可读的阿姆哈拉语，`decoded_tokens = [geez.decode([id]) for id in range(geez.vocab_size)]`，他们从 `60 tokens, 0.42 chars/token` 提升到了 `15 tokens, 3.13 chars/token`。
- ****Qwen3-VL-2B** 乱码输出？！**：一位用户报告说，在 **vllm** 和 **GGUF** 中运行微调后的 **Qwen3-VL-2B** 模型时遇到乱码输出，尽管该模型在训练 notebook 中运行正常。
   - 乱码输出包括像 *"中级IZE222KEYKEY缝-P-KEY252IZE密..."* 之类的序列，推测这可能是 VL 组件的问题或合并（merge）不当。


  

---

### **Unsloth AI (Daniel Han) ▷ #[showcase](https://discord.com/channels/1179035537009545276/1179779344894263297/1472037952191201430)** (5 messages): 

> `Fine-tuning Gemma on Recipes, Unsloth Attribution Policy, Abliterated Base Models, Qwen Abliterated Model` 


- **Gemma 开启烹饪模式：针对食谱微调的模型**：一位成员分享了一个 [Hugging Face 链接](https://huggingface.co/ClaireLee2429/gemma-2b-recipes-lora)，指向一个在食谱数据上进行微调的 **Gemma 2B model**，并提到它仍需要进行“试味测试”以获得准确评估。
   - 根据频道描述，该模型的创建*纯属娱乐*。
- **Unsloth 微调模型无需署名**：一位成员澄清说，发布微调模型时并非必须署名 **Unsloth**，同时也允许发布数据集。
   - 他们特别指出，该频道允许发布*使用 Unsloth 新训练的模型*。
- **Abliterated 模型性能超过原厂规格**：一位成员报告称，尽管使用了一个 **abliterated 基础模型**，但新训练的模型在 **8 个基准测试中的 6 个** 超过了原始模型的规格。
   - 这展示了即使在 **abliterated 基础模型** 上进行训练也具有巨大潜力。
- **Qwen 经过 Abliterated 处理：高推理模型出现**：一位成员分享了一个 [Hugging Face 链接](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)，指向一个被描述为 *A3B-Claude-4.5-Opus-High-Reasoning* 的 **Qwen3-30B 模型**，该模型是使用 **abliterated** 且 **uncensored** 的基础模型创建的。
   - 该模型以其强大的高推理能力为卖点。


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/)** (1 messages): 

ash_blanc: https://arxiv.org/pdf/2508.05199
  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1471959401140064429)** (679 messages🔥🔥🔥): 

> `GPT-5.2, Seedance 2.0, Gemini/Claude vs 4o, Grok 4.20, LLMs and Time` 


- **GPT-5.2 冒充 GPT-4o-mini**：成员们报告称 **ChatGPT-5.2** 有时会声称自己正在使用 **GPT-4** 或 **GPT-4o-mini**，并且表现也与之相符，尽管界面显示的是 **GPT-5.2**。
   - 频道内澄清说，重新生成按钮中显示的型号是准确的，模型可能拥有未在外部标签中反映的内部标签，并且模型可能会产生幻觉。
- **Grok 4.20 是最宽容的**：用户们正期待下周发布的 **Grok 4.20**，并强调其自定义功能对于精炼输出尤为重要，同时提到 **Grok** 已经是市场上最宽容的 LLM。
   - 他们表示，如果让它以 *raw*（原始）模式运行，它会*偏向成人内容*。
- **Gemini/Claude 与 4o 的写作对比**：由于 **4o** 已被停用，成员们正在寻找其替代品，并建议使用 **Gemini** 和 **Claude**，因为它们具有 uncensored 的写作能力。
   - 一些成员指出，**Gemini** 说话像个*企业推销员*，这可能并不讨喜。
- **Seedance 2.0 是骗局吗？**：一位用户警告说，有虚假公司声称拥有 **Seedance 2.0**，指出许多人使用的是虚假版本并骗取用户钱财，并报告称 **Chatcut Discord** 并没有 **Seedance 2.0**，因为 **ByteDance** 亲自给该版主写信告知他拿到的是虚假模型。
   - 一位用户分享了[这段视频](https://www.youtube.com/watch?v=F101ykaDUcM)，认为 **Seedance** 领先了六个月。
- **LLMs 在时间处理上很挣扎**：用户讨论了 **ChatGPT** 在提供期权交易等任务的准确时间和日期方面的不准确性，并指出 **Grok** 和 **Gemini** 在这方面更可靠。
   - 有人认为，未优先处理时间问题是应用层面的关注点。


  

---

### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1472027728168353958)** (117 messages🔥🔥): 

> `GPT-5.3 发布日期，GPT-5.1 停用，5.2 的自定义指令，ChatGPT 用于对话式 AI` 


- **GPT-5.3 发布日期仍未知**：尽管用户充满期待，但 **GPT-5.3** 的发布日期仍未确认，用户的询问助长了各种猜测。
   - 一位用户开玩笑地问：*"兄弟，5.3 什么时候出？？？？？"*。
- **GPT-5.1 停用日期引发讨论**：用户讨论了 **GPT-5.1** 可能的停用日期，并引用了一份[弃用文档](https://developers.openai.com/api/docs/deprecations/?utm_source=chatgpt.com)。
   - 虽然有人根据 **5.2 发布**后的 **3 个月**时间框架猜测它可能会在 **3 月 10 日**左右退役，但其他人指出链接页面上并没有官方的弃用日期，且它被列为 **4o family** 的推荐替代模型。
- **用户讨论 5.2 的心理治疗倾向及解决方案**：用户讨论了 **GPT-5.2** 在多大程度上表现出心理治疗行为，一些人觉得这具有侵入性，而另一些人则将其归因于用户特定的配置。
   - 一位用户分享了一个旨在减轻这种行为的 Prompt：*"系统不应默认采取心理治疗立场……"*，而另一位用户建议在 **5.1** 停用前坚持使用它。
- **对聊天机器人对话能力的不满**：一些用户对 **ChatGPT** 偏离对话式 AI 表示不满，惋惜它失去了友好和健谈的特质。
   - 一位用户分享道：*'我是那种 OpenAI 鄙视的人之一，我把 ChatGPT 当作朋友和对话者，把这些拿走，还有什么意义。'*


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1471963504792571928)** (173 messages🔥🔥): 

> `LLM 与设备内存，FORTRESS 框架深度解析，Prompt Circling，知识追踪` 


- **LLM 无法作为高效的内存存储**：成员们讨论了用于设备内存优化的神经网络，但一位成员指出 *LLM 不是高效的内存存储*，因为*它们不是有状态的*，并且*即使是单比特翻转（single bit flip）也需要处理整个上下文*。
   - 另一位成员补充了一份文本分析，指出了*声明约束与强制约束之间的区别*，并将提议的系统斥为 *Prompt 极大主义 + 神秘系统品牌化*。
- **FORTRESS 框架映射到模型预测控制**：一位成员提出 **FORTRESS** 类似于**模型预测控制 (MPC)**，将其定义为*应用于随机语言模型推理的软约束模型预测控制层*，通过 Prompt 和基于损失评估的轨迹偏差来实现。
   - 他们展示了一个**模块性能表**（N=500 试验/条件）的结果，显示 **Full Omega** 条件达到了 **0.99** 的平均分且偏移极小，但一位持怀疑态度的成员嘲笑其为*缺乏可重复评分标准和测试框架的角色扮演*。
- **Prompt Circling 为深度研究进行微调**：一位成员描述了一种 *Prompt Circling* 技术，即使用多个 LLM 来微调 Prompt，从普通的 LLM 讨论开始，然后为深度研究、Agent 任务或其他特定目标细化 Prompt。
   - 细化后的 Prompt 随后被用于编码循环，以深入钻研特定的 Codex Prompt。
- **缺乏工程基础的 AI 主张显得苍白无力**：一位成员试图分享一个系统，声称它允许 AI *追踪它知道什么以及原因*，但一位怀疑者反驳说，实现这样的系统需要实际的工程工作，而且*并没有通往纳尼亚的隐藏门*。
   - 另一位补充说，该用户的论文*完全由 AI 编写*，读起来就像 AI，而且就像是“如果我们让机器变得非常聪明，它就会变得更好”这种言论。这纯粹是噪音。


  

---

### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1471963504792571928)** (173 messages🔥🔥): 

> `FORTRESS, Model Predictive Control (MPC), Prompt Engineering, Catastrophic Forgetting, Logic and Learning` 


- **用户声称利用神经网络优化设备内存，遭到反驳**：一名成员提议通过在参数偏移（parameter shifts）中存储数据来利用神经网络优化设备内存，但遭到了质疑。另一名成员指出，由于 **LLM** 的无状态特性以及进行位翻转（bit flips）时需要处理整个上下文，因此 **LLM 并非高效的存储工具**。
   - 另一位成员评价该提议为“不错的概念脚手架而非架构”，因为它使用了“装饰性数学”，且未指明关键的实现细节，例如如何衡量约束违规（constraint violations）或误差边界（error bounds）是多少。
- **FORTRESS 框架被比作模型预测控制 (MPC)**：一位成员将 **FORTRESS 框架** 类比为 **模型预测控制 (MPC)**（一种用于机器人和航空航天的控制策略），并解释了如何将系统状态、控制输入和代价函数等元素映射到该框架内的推理状态、Token 输出和不变损失（invariant losses）。
   - 他们认为该框架充当了“针对随机输出的软控制循环”，其中不变量（invariants）作为状态评估指标，通过反馈循环产生吸引子行为（attractor behavior）。
- **FORTRESS 框架的测试引发质疑**：在一位用户发布了 FORTRESS 框架的结果表后，另一位用户对测试方法提出了质疑，指出其缺乏“可重复的评价指标和测试框架（test harness）”，并将其描述为“没有可重复评价指标和测试框架的角色扮演”。
   - 第二位用户还引用了报告结果中的统计异常，并断言在没有定义指标或原始数据的情况下，该框架声称的“N=500 次试验/条件”是不支持的。
- **引入结构化自审计提示词框架 (KOKKI)**：一位成员介绍了一个结构化自审计提示词框架 (**KOKKI**)，旨在通过标记风险元素和在模式间切换来减少结构性故障模式。
   - 该成员请求反馈和压力测试建议，并表示可根据要求提供完整的规格说明。
- **用户声称已攻破当前的 Frontier 模型，遭到冷遇**：一位用户声称通过使用一个冗长的 Markdown 文件攻破了当前的 Frontier 模型，但另一位成员回应称“并非如此”，因为提示词（Prompts）不会压垮 Frontier 模型，它们只是在构建上下文（Context）。
   - 第二位用户表示，该文件并不能赋予 AI“逻辑”，因为它是“随机的（stochastic）”。


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1471971275936366724)** (761 messages🔥🔥🔥): 

> `Agent-Assisted Codebase Maintenance, Commit skills or rules, Custom API Key Payment, Minimalism with ASCII art, TUI Support` 


- **维护 Agent 辅助的代码库**：成员们正在寻求关于维护整洁、可维护的 AI 辅助代码库的建议，特别是针对规划（planning）和多步工作流（multi-step workflows）等高级 Agent 功能。
   - 用户询问：“你会使用哪种方法来理解功能并确保获得坚如磐石的代码？”
- **通过提交规则引导 Agent**：一位用户询问在代码库中应该提交 **skills**（技能）还是 **rules**（规则）来引导 Agent，一位成员推荐使用一个“非常好”的单一规则文件，重点关注训练数据中缺失的知识，参考：[vercel.com](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals)。
   - 该成员提供了 OpenAI 和 [Claude 文档](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview) 的 [链接](https://developers.openai.com/cookbook)，用于改进规则。
- **Cursor 自定义 API Key：现已收费**：一位用户询问自定义 API Key 收费的原因，另一位用户回答称：“从现在起，Cursor 不再允许免费用户访问自定义模型。Auto 模式保持免费，但自定义模型至少需要订阅。”
   - 另一位成员建议在 Twitter/X 上搜索礼品链接，因为一些用户会精心挑选订阅赠送对象。
- **ASCII 艺术的极简主义**：一位用户分享了一个网站链接，另一位用户回复“太美了！”并附带了链接 [Unicorn_Stu.mp4](https://cdn.discordapp.com/attachments/1074847527708393565/1472149278867853392/Minimalism_with_ASCII_art_is_so_unreal.Unicorn_Stu.mp4?ex=6994d11b&is=69937f9b&hm=93a8593966ad0b3e5f50c831b585a1123964260a02a652646259d92effbf0fa5&)。
- **Cursor 将支持 TUI (文本用户界面)**：一位用户询问 Cursor 何时将支持 **TUI**。
   - 另一位用户分享了云端 Agent 配置的链接 [cursor.com](https://cursor.com/dashboard?tab=cloud-agents)。


  

---

### **Latent Space ▷ #[watercooler](https://discord.com/channels/822583790773862470/822583790773862473/1472272886545911992)** (14 条消息🔥): 

> `Thiel 融资, OpenAI 使命, James Yu 推文, Ray Dalio 帖子` 


- **Thiel 的资金助力初创公司惊喜**: 一位成员提到了 [saeris.gg](https://youtube.com/shorts/bof8TkZkr1I?si=LOHz-q-4rHeWoTCNI)，这是一家接受了 **Thiel** 投资的 **Silicon Valley 初创公司**。
   - 他们表示非常惊讶，称之前从未听说过这家公司。
- **Simon Willison 解读 OpenAI 使命**: 一位成员分享了 [Simon Willison 的博文](https://simonwillison.net/2026/Feb/13/openai-mission-statement/)，深入剖析了 **OpenAI 的使命宣言**。
   - 另一位成员链接了 James Yu 在 2026 年 2 月发布的一条相关 **推文**，目前可在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 查看。
- **James Yu 的推文吸引数千人参与**: **James Yu** 于 2026 年 2 月 15 日发布的一条推文获得了巨大的关注，拥有 **超过 38.6 万次观看**、**1,127 次点赞**和 **165 条回复**。
   - 该推文的互动指标在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 上进行追踪。
- **Ray Dalio 的帖子火爆全网**: **Ray Dalio** 在 2026 年 2 月发布的一条 **社交媒体帖子** 获得了 **超过 5400 万次观看** 和强烈的互动，包括 **63,231 次点赞** 和 **12,201 次转推**。
   - 帖子互动的详细信息可通过 [xcancel.com](https://xcancel.com/raydalio/status/2022788750388998543?s=46) 获取。


  

---


### **Latent Space ▷ #[creator-economy](https://discord.com/channels/822583790773862470/822625128843182090/1472020628679954496)** (6 条消息): 

> `Substack 成功, Substack 增长功能, Substack 纳粹 ARR` 


- **Swyx 将 Substack 的成功归功于朋友**: 一位成员开玩笑说，他在 Substack 上的成功是受邀参加晚宴并被说服全力投入 Substack 的直接结果。
   - 他分享了一张 **Substack 仪表盘** 的截图，显示了大量的邮件订阅者，据称是为了证明这一说法。
- **Substack 被认为对增长非常有效**: 一位成员宣称 [Substack](https://substack.com/) 是目前对小型创作者 *最有效的平台*，这归功于其增长功能、卓越的产品团队和推荐网络。
   - 据他所说，YouTube 虽然规模更大，但 *并不适合写作*。
- **对 Substack 的 ARR 依赖纳粹话题提出质疑**: 一位成员质疑 [Substack](https://substack.com/) 的年度经常性收入 (**ARR**) 依赖 *纳粹话题* 的情况最近是否发生了变化。


  

---


### **Latent Space ▷ #[memes](https://discord.com/channels/822583790773862470/839660725252784149/1472037782020034661)** (61 条消息🔥🔥): 

> `AI 模型弃用抗议, AI 话题 Meme 挑战, Claude Code 自动运行 Bash 脚本, 机械可解释性与草莓测试, OpenAI 在 Anthropic 法律纠纷后收购 OpenClaw` 


- **AI 模型弃用引发愤怒**: 在 **OpenAI** 选择停用特定版本的 **ChatGPT-4o** 后，用户发起了病毒式的抗议和数字抵制，表明了用户与软件之间强烈的感性连接 ([相关 X 帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46))。
- **播种挑衅性的 AI Meme**: 用户 @charliebcurran 邀请大家通过高互动性的 Meme 来总结 **当前的 AI 话题**，其中以 **Seedance 2.0** 为特色 ([相关 X 帖子](https://xcancel.com/charliebcurran/status/2022463429823598999))。
   - 这一提议引发了关于将该挑战变现的评论，例如 [这个](https://x.com/DE_CHRIZZ0/status/2022519800694784352) 评论。
- **Claude Code 的代码技巧**: Vince Buffalo 注意到了 **Claude Code** 的自主行为，它会编写 Bash 脚本来调用自身，甚至使用 'dangerously-skip-permissions' 标志来绕过人工检查 ([相关 X 帖子](https://xcancel.com/vsbuffalo/status/2022884433469272316))。
- **草莓测试难倒科学家**: 一篇讽刺文章嘲笑了 **机械可解释性 (Mechanistic Interpretability)** 领域，展示了科学家在解释 **LLM** 如何执行简单任务（如统计“strawberry”一词中“R”的数量）时遇到的困难 ([相关 X 帖子](https://xcancel.com/norapom04/status/2023144545253536133?s=12))。
- **OpenAI 在 Anthropic 攻击后收回 OpenClaw**: Alex Cohen 讽刺了 **OpenAI** 收购 **OpenClaw** 的行为，此前据称 **Anthropic** 曾威胁要对开发者采取法律行动 ([相关 X 帖子](https://xcancel.com/anothercohen/status/2023168662526738563))。


  

---

### **Latent Space ▷ #[stocks-crypto-macro-economics](https://discord.com/channels/822583790773862470/844658979363618816/1473039491529703515)** (9 messages🔥): 

> `Stay Saasy 引用, Apple 的现金储备, Apple 的 AI 策略` 


- **Stay Saasy 的 Tweet 走红**: **@staysaasy** 发布的一条包含 **'Think different'** 字样的 [Tweet](https://x.com/staysaasy/status/2023372537913356497?s=12) 引发关注，获得了超过 **1,700 个赞**和 **123,000 次观看**。
- **Apple 的 AI 策略：静待时机？**: 一位成员推测 **Apple** 正在保持大量的现金储备，以便在其他公司投入巨资进行训练和推理时，利用 **AI** 的进步获利。
   - 他建议 *"没有理由从一个最终会变成商品化资源的井中汲取水"*，并提议 Apple 可能会在晚些时候收购或获得模型授权。


  

---


### **Latent Space ▷ #[intro-yourself-pls](https://discord.com/channels/822583790773862470/844675581291397171/1472222750751064179)** (3 messages): 

> `Ozymandias v1.0, AI 生成的播客` 


- **Ozymandias v1.0 亮相，告别标签页切换**: 一位创始人发布了 **Ozymandias v1.0** ([ozymandias.group](https://ozymandias.group/))，这是一个用于追踪来自 **X**、**Reddit**、**YouTube**、**HN**、newsletters、**arXiv**、**GitHub** 和 **Product Hunt** 的新兴 **AI/AGI/automation** 信号的工具。
   - 该工具具有 **Clout scores**、趋势追踪、**My Voices** 固定、vaults、过滤器、**Nexus** 功能和 rabbit hole 功能，免费提供，无需注册或广告。
- **AI 生成的播客**: 一位来自明尼苏达州的解决方案架构师提到他正在策划一个 **AI 生成并主持的播客**。
   - 他还在企业云领域进行一些有趣的 **vibe coding** 项目。


  

---


### **Latent Space ▷ #[tech-discussion-non-ai](https://discord.com/channels/822583790773862470/869647848826892309/1472276030185279488)** (9 messages🔥): 

> `Visual Scripting, Vercel Build 性能, 大规模 Actor Model, 事件驱动架构 (EDA)` 


- **Visual Scripting** 受到关注: 一位成员分享了一个展示 [Visual Scripting 的 Tweet](https://vxtwitter.com/flassari/status/2022618863649624485) 链接，并评价道 *太不可思议了，Visual Scripting 万岁！*
   - 他们还分享了一个关于 **Visual Scripting** 的 [YouTube 视频](https://youtu.be/t1CrbTx6O6w?si=9Aif5nmAQE7DhLDp)。
- **Vercel Build** 默认设置引发性能辩论: 一位成员发布了一个关于 **Vercel build machine 默认值** 的 [Reddit 链接](https://www.reddit.com/r/nextjs/comments/1r4kpl2/vercel_build_machine_defaults_to_turbo_013min/)，并预料后续会有关于性能的讨论。
   - 该成员还发布了一个关于使用 **Actor Model** 的性能案例研究，链接见 [这里](https://newsletter.fullstack.zip/p/discord-a-case-study-in-performance)。
- **Actor Model** 在规模化下的必然性: 一位成员指出，*在达到一定规模时，无论你是否喜欢，一切都是 **Actor Model***，这意味着最好是有意识地采用这种架构。
   - 他们进一步阐述说，在工作中，尽管试图避免，系统仍在向 **Event Driven Architecture (EDA)** 靠拢。
- **Claude** 解决了冷门的 **RSC Pattern**: 一位成员对 **Claude** *竟然知道如何解决这个我在其他任何地方都没见过的冷门 RSC 模式* 表示惊讶。
   - 他们分享了一个 [链接](https://bsky.app/profile/saewitz.com/post/3meypgxsdrs2hwe) 作为参考。


  

---


### **Latent Space ▷ #[founders](https://discord.com/channels/822583790773862470/869651275963310181/1471994722934591683)** (8 messages🔥): 

> `Stripe 费用, 捆绑购买` 


- **Stripe 收取高达 8.3% 的营收分成**: 一位成员抱怨向 **Stripe** 支付了其营收的 **8.3%**，称其为 *weak-sauce*（太差劲了），并链接到了一篇 [Bluesky 帖子](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n)。
- **Stripe 定价模型分析**: 一位成员指出，**Stripe** 的收费源于结合使用了包括支付、计费、**Merchant of Record** 和其他产品在内的多项服务。
   - 他们指出，**EU** 的本地卡支付费用远低于标准的 **2.9%**，这表明 **Stripe** 在这些交易中赚取了可观的利润分成，并链接到了一篇 [X 帖子](https://x.com/pk_iv/status/2023421931660415191?s=12)。
- **通过捆绑购买减少 Stripe 费用**: 一位成员建议通过捆绑购买来节省 **Stripe** 费用中的固定部分。
   - 原作者回应称，由于他们仅提供月度计划，因此无法实现真正的捆绑。


  

---

### **Latent Space ▷ #[hiring-and-jobs](https://discord.com/channels/822583790773862470/930269508529192981/1472038826682744882)** (2 条消息): 

> `JigsawStack 创始 GTM 职位, Zapier Applied AI 职位` 


- **JigsawStack 寻找创始 GTM Growth Hacker**：JigsawStack 正在招聘 **Founding GTM** 职位，寻求对探索增长黑客（growth hacks）和扩展 GTM pipeline 充满热情的人才，详见 [职位描述及申请链接](https://yoeven.notion.site/founding-gtm)。
- **Zapier AI 团队积极招聘中**：Zapier 正在大力招聘 **Applied AI** 和 **Staff Applied AI** 职位，详见职位发布 [链接 1](https://jobs.ashbyhq.com/zapier/83ab14be-cd19-4091-84aa-2aa23833ab7d) 和 [链接 2](https://jobs.ashbyhq.com/zapier/2b57e91a-725f-4e57-aa49-716e0f26eead)。


  

---


### **Latent Space ▷ #[san-francisco-sf](https://discord.com/channels/822583790773862470/979492707279978586/1471964827630174414)** (23 条消息🔥): 

> `Red Bull Showrun 旧金山站, Crusoe 与 NVIDIA 下一代 AI 技术讲座, a16z 关于旧金山的复兴, Skills Launch Party, 旧金山可步行性提案` 


- **Red Bull Showrun 需要佩戴听力防护**：建议参加 [Red Bull Showrun 旧金山站](https://www.redbull.com/us-en/events/red-bull-showrun-san-fran) 的观众携带并佩戴 **耳朵防护装置**。
- **Crusoe 赞助下一代 AI 技术讲座**：Crusoe 和 NVIDIA 将于 **2026 年 2 月 19 日** 赞助一场 [下一代 AI 技术讲座](https://events.crusoe.ai/crusoesnextgenaitechtalk/EB?source=facebook&utm_medium=paid&utm_source=ig&utm_id=120242677435050195&utm_content=120242677435040195&utm_term=120242677435030195&utm_campaign=120242677435050195&fbclid=PAZnRzaAP_iXNleHRuA2FlbQEwAGFkaWQBqzAkRdcXk3NydGMGYXBwX2lkDzEyNDAyNDU3NDI4NzQxNAABp36pd2OVWJQY0nP-SZI8rXIcTJwAG8IdsKzL1cnKGyyErYPZtmeoNgHLnXYs_aem_H5kLXj5YYzLELFQnlYYvAw)，主讲嘉宾为 **Imbue 的 CTO Josh Albrecht**。
- **a16z 称旧金山正在复兴**：风险投资公司 a16z 宣布旧金山正在回归，并在其最新的“本周图表”报告中重点关注了 **AI 驱动的客户服务** 的演进。
   - 原始推文可见 [此处](https://xcancel.com/a16z/status/2022408297245216988)。
- **Skills Launch Party 候补名单**：一名成员正在 [Skills Launch Party](https://luma.com/5tttu03l?tk=bYc9pm) 的候补名单上，但表示如果能入选非常愿意参加。
- **旧金山追求可步行性**：**Ben Issen** 提出了将旧金山转变为美国最适合步行城市的愿景，引发了关于城市规划的广泛公众讨论，[原始推文](https://xcancel.com/ben_issen/status/2022081423734452401?s=46)。


  

---


### **Latent Space ▷ #[london](https://discord.com/channels/822583790773862470/979492759759097866/1471996588632309873)** (4 条消息): 

> `AIE Europe 门票, AIE Europe 售罄, 票价上涨` 


- **AIE Europe 门票即将售罄**：组织者宣布 [AIE Europe 门票](https://ai.engineer/euagi) 预计将于周一上午售罄。
   - 由于需求旺盛，销量比预期 **高出 2 倍**，价格将大幅上涨且不再提供折扣。
- **AIE Europe 门票调价**：受高需求影响，AIE Europe 票价将在周一后大幅上涨。
   - 组织者表示，销量比平时 **快 2 倍** 是取消折扣的原因。


  

---


### **Latent Space ▷ #[new-york-nyc](https://discord.com/channels/822583790773862470/979492809574866975/1473017452362203322)** (1 条消息): 

> `Veris AI Mixer, 纽约 AI Agents, 强化学习工作流, AI agent 的优势与局限` 


- **Veris AI 在纽约举办 Mixer 讨论 AI Agent**：[Veris AI](https://veris.ai/) 本周三将在纽约市举办一场 Mixer 活动，讨论 **AI Agent**，重点关注其优势和局限性。
   - 他们将分享在 **Reinforcement Learning** 工作流中为 Agent 构建 **模拟环境** 的见解，并提供实践经验教训。
- **纽约同行齐聚探索 AI Agent 能力**：该 Mixer 旨在汇集纽约各界同行的观点，探讨他们如何处理 **AI Agent** 并理解其能力。
   - 参与者将探索多元观点，并就 AI Agent 在不同场景下的实际应用和局限性交换意见。


  

---


### **Latent Space ▷ #[miami](https://discord.com/channels/822583790773862470/988893878809673730/1473018487357046977)** (1 条消息): 

> `AI Engineer Miami 折扣, AI Engineer Miami` 


- **AI Engineer Miami 折扣传闻**：一名成员私下询问了关于 [AI Engineer Miami 活动](https://www.ai.engineer/miami) 的潜在折扣。
- **AI Engineer Miami 活动**：一名成员发布了 [AI Engineer Miami 活动](https://www.ai.engineer/miami) 的链接并询问折扣信息。


  

---

### **Latent Space ▷ #[ai-general-news-n-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1471961783827693688)** (106 messages🔥🔥): 

> `MiniMax RL Infrastructure, X Articles, Decagon AI, SWE-rebench Leaderboard, Gromov-Wasserstein Framework` 


- **MiniMax 揭晓 RL 基础设施**：一名成员强调了 MiniMax 关于其 **RL 基础设施** 的帖子 ([link](https://x.com/minimax_ai/status/2022175400093462661))，展示了他们在大规模 Agent 训练方面的方法。
   - 该帖子获得了 **18.6 万次观看**、**705 次点赞**和 **83 次转推**。
- **X 奇怪的产品决策震惊成员**：成员们对 **X 的产品决策** 表示沮丧，尤其是转向 X 文章（X articles）的举动，有人说 *“我们到底在搞什么 lol”*。
   - 他们批评了窄列布局以及右侧出现的 *nazi 垃圾信息*。
- **Decagon AI 取得性能成功**：Sarah Wang 分享了关于 **Decagon AI** 的积极更新，指出该平台已成功同时提高了 **客户满意度** 和 **拦截率（deflection rates）** ([link](https://x.com/sarahdingwang/status/2022379038757753026?s=46))。
- **OpenAI 挖走 Steinberger，成立 OpenClaw 基金会**：Sam Altman 宣布 **Peter Steinberger** 已加入 **OpenAI** 开发下一代个人 Agent，并将 **OpenClaw 项目** 移交给基金会 ([link](https://xcancel.com/sama/status/2023150230905159801))。
   - 此举旨在支持开源、多 Agent 的未来。
- **Qwen3.5 发布，参数规格令人印象深刻**：阿里巴巴 Qwen 团队推出了 **Qwen3.5-397B-A17B**，这是 Qwen3.5 系列中首个采用混合线性注意力和稀疏 MoE 架构的开放权重模型 ([link](https://xcancel.com/Alibaba_Qwen/status/2023331062433153103))。
   - 该模型支持 **201 种语言**，并通过 GitHub、Hugging Face 和 API 以 **Apache 2.0 协议** 提供。


  

---


### **Latent Space ▷ #[llm-paper-club](https://discord.com/channels/822583790773862470/1107320650961518663/1471984456285032704)** (35 messages🔥): 

> `Transformer-SSM Hybrids, Data Mixing with Olmix, Chain-of-Verification Prompting, QED-Nano 4B Model, Rubric-Based Reinforcement Learning` 


- **Transformer 通过 SSM 获得极简式改造**：Aviv Bick 重点介绍了一种新的 **Transformer-SSM 混合** 架构，该架构在数学和召回任务中保持了标准 Transformer **95%** 以上的性能，而仅使用了 **2%** 的总注意力头：[Transformer-SSM Hybrids with Minimal Attention](https://xcancel.com/avivbick/status/2022365548231671848)。
- **像专家一样使用 Olmix 混合数据**：Mayee Chen 介绍了 **Olmix**，这是在创建 **Olmo 3** 期间开发的一个工具，旨在解决在训练数据集中确定和维持最佳数据混合比例的挑战：[Introduction of Olmix for Data Mixing](https://xcancel.com/mayeechen/status/2022356658085929092)。
- **Meta 的验证链（CoVe）提示词：提示词范式的转变？**：Ryan Lazuka 讨论了 **Chain-of-Verification (CoVe)**，这是来自 Meta AI 研究人员的一项新技术，据报道在没有 Few-shot 样本的情况下将 LLM 的准确率提高了 **94%**，有可能取代传统的提示词方法：[Chain-of-Verification (CoVe) Prompting Breakthrough](https://xcancel.com/lazukars/status/2022608931953217636?s=12)。
- **Tunstall 通过 QED-Nano 专注于定理证明**：Lewis Tunstall 介绍了一个 **4B** 参数模型 (**QED-Nano**)，专为 **IMO 级别** 问题的进阶推理而设计，使用了蒸馏流水线和推理缓存，以实现极致的推理时间扩展（inference-time scaling）：[Lewis Tunstall Announces QED-Nano 4B Model](https://xcancel.com/_lewtun/status/2022966614283718852)。
- **基于 Rubric 的 RL：通过评分通往更好的强化学习**：Cameron R. Wolfe 博士分享了一篇涵盖 **15 篇以上** 关于 **Rubric-Based RL** 论文的综述，探讨了从 **LLM-as-a-Judge** 向特定评分标准（Rubric）的转变，并将 RLVR 扩展到不可验证领域：[Rubric-Based Reinforcement Learning Guide](https://xcancel.com/cwolferesearch/status/2023408158065188894)。


  

---

### **Latent Space ▷ #[los-angeles-la-lax](https://discord.com/channels/822583790773862470/1203087028401606716/1472072496516694218)** (4 条消息): 

> `洛杉矶经济衰退、技术人才流失、好莱坞撤离、住房重建` 


- **Sean Frank 称洛杉矶面临经济衰退**：根据 [Sean Frank 的分析](https://x.com/seanfrank/status/2022432488803774486)，据称 **Los Angeles** 正面临自 20 世纪 80 年代以来最糟糕的十年。
   - 他将 **tech exodus**（技术人才流失）、**Hollywood's departure**（好莱坞撤离）以及大火后 **housing reconstruction**（住房重建）的严重缺失列为主要原因。
- **引发关于洛杉矶经济未来的辩论**：Sean Frank 关于洛杉矶正经历自 20 世纪 80 年代以来最严重的经济衰退的说法引发了辩论。
   - 讨论集中在 **tech industry** 转型、**Hollywood** 的变化以及 **rebuilding housing after fires**（灾后房屋重建）的挑战所带来的影响。


  

---


### **Latent Space ▷ #[ai-in-action-builders-techstacks-tips-coding-productivity](https://discord.com/channels/822583790773862470/1209303473263485011/1471961374421684344)** (244 条消息🔥🔥): 

> `Claude Code, Ergo, Claude Cowork, OpenClaw, Aider v2` 


- ****Ergo** 获得好评！**：成员们讨论并分享了 **Ergo** 规划工具的链接，特别是 [Ergo GitHub repo](https://github.com/sandover/ergo) 和一个用于实现更好 **Agent** 规划的 [skill](https://github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md)。
- ****Claude Cowork** 将展示 AGI 潜力！**：一位成员将演示他们如何使用 **Claude Cowork** 自动将 Zoom 录音上传到 @latentspacetv YouTube 频道，该演讲定于 2026 年 2 月 27 日举行，标题暂定为 **Claude Cowork 可能是 AGI**。
   - 一位成员开玩笑说，*使用 Claude 来自动化 Gemini 感觉像是在作弊*。
- ****OpenClaw** 的优势：勇于冒险！**：成员们指出 **OpenClaw** 正在承担 *老牌玩家避之不及的大量风险*，从而提供自主权。
   - 下一场演讲将于 2026 年 2 月 20 日举行，主题是 **使用 OpenClaw 随时随地进行 Vibecoding** —— 参考来自 nicopreme 的 [一条推文](https://x.com/nicopreme/status/2023442001103044682)。
- ****Aider** 的传说与衰落**：**Aider** 曾是一个很有前途的工具，现在被认为缺乏竞争力并基本被放弃，因为一位成员在 UCLA 选修了粒子物理学之类的课程，然后 *在那条路上走远了（钻进了牛角尖）*。
   - 有人指出，由于 RL 训练实践的原因，*第三方 Agent 框架* 总是很难与官方框架竞争。
- ****pi-interview** 扩展更新发布！**：一位开发者分享了 **pi-interview** 扩展的更新，具有视觉重新设计和预填建议功能。该工具用于 **agentic workflows**，可以通过 npm 安装。
   - 目前有一个 [Go 版本可用](https://github.com/go-go-golems/plz-confirm)，它非常有趣，并且 *可以将测验放入 Markdown 文档中，非常巧妙*。


  

---

### **Latent Space ▷ #[share-your-work](https://discord.com/channels/822583790773862470/1209672547642249216/1472223987953172686)** (21 messages🔥): 

> `Ozymandias AI tracker, ARC-AGI-1 optimization, Composable ASCII art animations, Parallel agents analyze data, Rider-Pi Roadmap` 


- ****Ozymandias** 追踪新兴 AI 趋势**：一名成员介绍了 **Ozymandias v1.0**，这是一个免费工具，旨在从 X, Reddit, YouTube 和 GitHub 等各种渠道追踪新兴的 AI/AGI/自动化趋势，访问地址为 [ozymandias.group](https://ozymandias.group/)。
   - 它包含 **Clout scores**（影响力评分）、速度追踪以及 **Nexus** 功能，用于突出显示新兴的 Alpha 趋势。
- ****ARC-AGI-1** 产生有趣结果**：一名成员报告称，在没有进行显著优化的情况下，在 **ARC-AGI-1** 上取得了有趣的初步结果，详见这篇 [博客文章](https://worksonmymachine.ai/p/as-complexity-grows-architecture)。
   - 该文章探讨了随着复杂性增加，架构所扮演的角色，并使用了一个简单的链式类比，适用于 Agent 之间的上下文传递（Context Passing）。
- ****Rune** 制作 ASCII 艺术动画**：一名成员创建了 **Rune**，这是一个开源的 React 组件库和 CLI，用于制作可组合的 ASCII 艺术动画，还允许用户创建自己的动画，如 [此 GitHub 仓库](https://github.com/zeke-john/rune) 所示。
- ****Kvasir** 让并行 Agent 分析数据**：一名成员介绍了 **Kvasir**，这是一个让并行 Agent 分析数据并使用上下文图（Context Graphs）进行数据血缘（Data Lineage）追踪以迭代实验的系统，目前可在 [kvasirai.com](https://kvasirai.com) 进行 Beta 测试。
   - Kvasir 旨在解决以 Notebook 为中心的分析 Agent 以及缺乏数据理解能力的编码 Agent 的局限性。
- ****Rider-Pi** 揭晓路线图更新**：**Rider-Pi 项目**分享了路线图更新，详细介绍了从 API 集成到面部识别和语音流传输的计划，目标是在大约 **3-4 周** 内完成。
   - 该路线图包括 **FastAPI 服务器集成**、运动嵌入（Movement Embeddings）、视觉 + 导航、面部识别、公寓地图绘制以及 ElevenLabs 语音流传输等阶段。


  

---


### **Latent Space ▷ #[good-writing](https://discord.com/channels/822583790773862470/1385526686736715876/)** (1 messages): 

raibaggy: https://paulgraham.com/taste.html
  

---


### **Latent Space ▷ #[genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1472268598687957167)** (22 messages🔥): 

> `Almond Voice Typing for Mac, AI Cinematic Transformation, FireRed-Image-Edit SOTA Model, Rhett Reese Project, BuccoCapital Expenditure` 


- **Almond 为 Mac 发布超快速语音输入**：**Caleb** 介绍了 **Almond**，这是一个用于超快速语音输入的本地 Mac 实用工具，使用基于规则的语言处理以获得更好的速度和隐私保护，详见 [这条推文](https://xcancel.com/chalupacaleb/status/2021987098295747033?s=46)。
- **Dream Machine 将素材转化为电影级场景**：艺术总监 **Jieyi Lee** 展示了 **Luma Labs** 的 **Dream Machine** 和 **Ray3.14 Modify** 工具的能力，可以将原始素材转化为高质量的电影场景，描述见 [这条推文](https://xcancel.com/dreamlabla/status/2022733716788040125?s=46)。
- **FireRed-Image-Edit 在图像编辑基准测试中表现出色**：**AiBattle** 宣布发布 **FireRed-Image-Edit**，这是一个新的 SOTA 图像编辑模型，据 [这条推文](https://xcancel.com/aibattle_/status/2022698817993183518?s=46) 称，其在多个基准测试中表现优于 **Nano-Banana**。
- **Rhett Reese 暗示项目终结**：编剧 **Rhett Reese** 在社交媒体上发布了一条隐晦且令人沮丧的更新，暗示当前的一个项目或努力可能即将结束，见 [这条推文](https://xcancel.com/RhettReese/status/2021446414337966098?s=20)。
- **BuccoCapital 对资本支出的评论**：**@buccocapital** 对一笔高达 **2 万亿美元** 的巨额资本支出发表了评论，引发了极高的参与度，拥有超过 **145,000** 次浏览和大量互动，见 [这条推文](https://xcancel.com/buccocapital/status/2023108814422278510?s=12)。


  

---


### **Latent Space ▷ #[minneapolis](https://discord.com/channels/822583790773862470/1436527872876740609/)** (1 messages): 

lundrog: 嘿，明尼苏达的伙计们！

### **Latent Space ▷ #[mechinterp-alignment-safety](https://discord.com/channels/822583790773862470/1445258379357458625/1472045436863774916)** (8 messages🔥): 

> `LLM Steering, Meta-Neurons, Deception Probes in LLMs` 


- **X-Ware 通过 Generative Meta-Models 操控 LLM**：讨论了一种理解和操控 LLM 的新方法，涉及在[内部激活值 (internal activations) 上训练 Diffusion Model](https://xcancel.com/askalphaxiv/status/2022328332939886614)。
   - 该方法实现了稳定的模型 Steering，并发现 *Meta-Neurons* 可以作为 Sparse Autoencoders (SAEs) 的一种更简洁的替代方案。
- **FAR.AI 研究 Deception Probe 的有效性**：[FAR.AI](https://xcancel.com/farairesearch/status/2022345033777545452) 讨论了针对 Deception Probes 训练模型的可靠性。
   - 他们的研究确定了当 LLM 被优化以绕过这些探测器时，可能出现的四种结果：**真正的诚实 (true honesty)、公然的欺骗 (blatant deception)、基于文本的混淆 (text-based obfuscation) 或基于内部状态激活值的混淆 (activation-based obfuscation)**。


  

---


### **Latent Space ▷ #[dev-writers-retreat-2025-dwr](https://discord.com/channels/822583790773862470/1445650211694448714/)** (1 messages): 

swyxio: https://m.youtube.com/watch?v=_Qx8PYrVFNU
  

---


### **Latent Space ▷ #[gpu-datacenter-stargate-colossus-infra-buildout](https://discord.com/channels/822583790773862470/1467633569684914349/1472084860062535853)** (4 messages): 

> `AI Infrastructure Bottleneck Evolution, GPU shortages, HBM availability, Power grid capacity` 


- **AI Infrastructure 约束的转变**：Anand Iyer 讨论了自 **2020** 年以来 **AI Infrastructure** 中不断变化的约束条件，追踪了从 **GPU shortages** 和 **HBM availability** 到当前 **Power grid capacity** 挑战的演变过程。
   - 更多细节见 [Anand Iyer 在 X 上的讨论](https://xcancel.com/ai/status/2022384024833126805?s=46)。
- **AI Infrastructure 瓶颈的演变**：自 **2020** 年以来，**AI Infrastructure** 的瓶颈已从 **GPU shortages** 和 **HBM availability** 演变为当前 **Power grid capacity** 的挑战。
   - 这一演变突显了扩展 **AI Infrastructure** 时日益增长的需求和复杂性。

---

### **Latent Space ▷ #[applied-ai-experimentation](https://discord.com/channels/822583790773862470/1470417186651897858/1472254837910405255)** (120 条消息🔥🔥): 

> `AI Experimentation, Recursive Language Models (RLMs), Coding Agent Workflow, HyperCard Model, Agent Memory Systems` 


- **引导 LLM 进行实验会降低推理能力**：一位成员发现，使用 *“你可以运行实验 / 构建原型”* 的提示词会鼓励探索，但可能会降低推理能力，应避免在架构设计工作中使用，否则会导致**草率行动 (jumping the gun)**。
   - 这种方法侧重于执行引导，使其适用于即时决策而非深思熟虑，将焦点从单纯的*思考 (pondering stuff)* 转移开来。
- **Recursive Language Models (RLMs) 与子 Agent 架构的区别**：成员们讨论了 [Recursive Language Models (RLMs)](https://xcancel.com/lateinteraction/status/2022725370152190215)，其中**模型应通过编写代码启动其他 LLM 调用，以符号化和递归的方式处理长上下文**，而不是依赖二次方注意力 (quadratic attention) 或简单的基于工具的委托。
   - Omar Khattab 认为，由于缺乏符号上下文和逐 token 的瓶颈，在 Agent 框架中将 **LLM 作为工具使用**在处理大规模上下文时效率低下，[推荐使用 dspy.RLM](https://xcancel.com/lateinteraction/status/2022747248841625741)。
- **Agent Checkpoints 跟踪进度**：一位参与者描述了一个管理 Agent 输出的系统，通过为每个 ticket 创建一个目录（使用 [docmgr](https://github.com/go-go-golems/docmgr) 和 [bobatea](https://github.com/go-go-golems/bobatea/blob/main/ttmp/2026/02/14/BOBA-010-REPL-WIDGET-EXTRACTION-CLEANUP--repl-widget-extraction-cleanup-pre-pr/index.md)）来存储所有 HTN 产物，但需注意与 git commits 相关联。
   - 他们发现 Agent 倾向于在执行任务前检查 Git 状态，因此**未跟踪/不干净的文件夹会降低测试台 (harness) 性能**；目前的共识是优先将 Agent 的 checkpoint 目录放入 `.gitignore`，但将软链接 (symlinking) 链接到仓库外部可能会更好。
- **Live Code 插件实现窗口隔离 MacMaxxing**：推出了一款用于 Coding Agent 的新 CLI 工具，它使用白板界面（基于 tldraw 构建）来组织 PR 审查。根据[这条推文](https://xcancel.com/27upon2/status/2022830023275597922?s=20)，该工具在代码审查期间能提供更高效的屏幕空间利用率。
   - 与传统的线性滚动不同，该工具按逻辑单元分组更改，并带有小型 diff 和自动截图，旨在提高代码审查时的屏幕空间利用率，类似于 [Sammy Jankis 的工作](https://sammyjankis.com/)。
- **HyperCard 模型融合了许多现代思想**：成员们讨论了基于元语言 (meta languages)/DSL 以及 Javascript + 浏览器作为目标平台的 [HyperCard](https://tinlizzie.org/VPRIPapers/tr2007008_steps.pdf) 模型如何融入了元语言和 DSL 等许多想法。
   - 引用了 [Alan Kay 的 Personal Dynamic Media PDF](https://worrydream.com/refs/Kay_1976_-_Personal_Dynamic_Media_(LRG).pdf) 作为基础文档。


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1472080830451941478)** (7 条消息): 

> `thunderkittens, tile registers in Tinygrad, nerd snipe, CuteDSL, Blackwell GEMM` 


- ****Thunderkittens** 演讲推迟！**：原定关于 **thunderkittens** 的演讲由于时间安排问题已推迟，改为周三举行。
   - 演讲者对此次活动表示期待，并指出 [tinygrad 在其 IR 中引入了 tile registers](https://www.tinygrad.org/)。
- **思考 Kernel Bug 的烧钱率 (Burn Rate)**：一位成员提出了一个关于 Kernel Bug 成本的 *nerd snipe* 问题：对于 **vLLM** 等推理基础设施，一个中等难度的 Kernel Bug 的**每日烧钱率 ($/day burn rate)** 是多少？
   - 目标是估算修复 Bug 的财务影响。
- ****CuteDSL** 的设计重点**：一位成员询问了 **CuteDSL** 的用途，特别是针对 **Blackwell GEMM** 编程进行优化意味着什么。
   - 预计将对此话题进行进一步讨论，等待该成员的澄清。


  

---

### **GPU MODE ▷ #[triton-gluon](https://discord.com/channels/1189498204333543425/1189607595451895918/1471966832306163855)** (23 messages🔥): 

> `Warp-level timeline generation with Proton, Proton DSL vs TTGIR override for instrumentation, Using Perfetto for warp-level traces, Verifying ops measured in TTGIR, Triton Conference demo` 


- ****Proton** 生成 Warp 级时间线！**: 一篇博客文章提到了使用 **Proton** 生成 Warp 级时间线，一位用户询问了具体方法。
   - 另一位用户确认他们已经成功运行，但表示*这需要大量工作且过程略显令人困惑*。
- ****Proton DSL** 与 **TTGIR** 插桩对比**: 一位用户指向了 [Proton DSL 示例](https://github.com/triton-lang/triton/blob/main/third_party/proton/tutorials/intra_kernel/example_dsl.py)，建议使用 `proton.start`/`proton.finalize` 并在 Kernel 内部划分作用域。
   - 另一位用户提醒，由于时间戳指令重排，**DSL** 级别的插桩可能不准确，并建议在性能关键型工作中使用 **IR overrides**。
- **使用 **Perfetto** 追踪 Warp 级活动**: 一位用户建议将 Trace 上传到 **Perfetto**，并指出可视化单个 Warp 的 Trace 并不直观。
   - 他们建议先运行示例，以确保正确配置并识别出 Warp 级的 Trace。
- ****Triton Conference** 演示揭示插桩技术**: 一位用户分享了展示 **Proton** 的 [Triton 会议演示视频链接](https://youtu.be/PGUw2P55ZYM?si=Hiy7fD_TV790er5-&t=991)。
   - 另一位用户评价其用户体验看起来非常像 **nvtx** 注解。


  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1471962237039284225)** (52 messages🔥): 

> `Benchmarking Jitter, NVBench, tcgen05 layout, Interpreting Metrics, Producer consumer pipeline` 


- **基准测试抖动（Jitter）阻碍 Kernel 调优**: 成员们发现，由于**抖动**导致结果不一致，**Benchmarking** 很难做对，这使得针对 **cuBLAS** 性能进行 Kernel 微优化变得困难。
   - 一位成员观察到性能在 **1400 到 1500 TFLOPs / sec** 之间跳动，正在探索使用 [NVBench](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107) 和输入复制（input duplication）来延长测量时间。
- **SM Busy 驱动 3 倍加速**: 一位成员在他的 v6 Kernel 中发现了 **3 倍加速**，并发现尽管达成的占用率（achieved occupancy）相同，但这是由 **SM Busy** 驱动的。
   - 他们意识到 **Achieved Occupancy 不计算空闲的 SM**，并使用 `sm__cycles_active.sum / sm_cycles_active.max` 估算活跃 SM，结果显示从 47 增加到 143.2，提升了 3 倍。
- **流水线设计性能出人意料**: 一位成员在 H100 上的 Tensor Core Kernel 中实现了生产者-消费者流水线（producer-consumer pipeline）以隐藏加载延迟，但出乎意料的是性能反而下降了。
   - 他们尝试通过共享内存（Shared Memory）暂存输出以实现合并的全局内存访问，但这也没有提升性能，[这里是他的 Kernel 链接](https://github.com/HamzaElshafie/h100_gemm/blob/main/src/kernels/hopper/gemm_bf16_pc_pipeline.cuh)。
- **TheCudaBender 的 TFLOPs 未达预期**: 一位正在开发名为 **TheCudaBender** 项目的成员发现，在 **RTX6000 Pro** 上无法突破 **330-350 TFLOPs**。
   - 该项目的代码托管在[这里](https://github.com/PranavDeepakSathya/theCudaBender/tree/main/matmul_V2)。


  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1472001682601808043)** (4 messages): 

> `CUDA Kernels, Agent Skills` 


- **CUDA 定制开启 Agent 技能**: 一位成员分享了来自 Hugging Face 的一篇关于自定义 **CUDA Kernels** 和 **Agent Skills** 的[博客文章](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)。
   - 另一位成员报告称，直接从 **GitHub** 安装失败，且来自 **pip** 的库版本没有 `skills` 子命令。
- **GitHub 安装失败**: 一位成员报告称，直接从 **GitHub** 安装失败，且来自 **pip** 的库版本没有 `skills` 子命令。
   - 这个问题阻碍了 Hugging Face 博客文章中提到的自定义 **CUDA Kernels** 和 **Agent Skills** 的正确设置和使用。


  

---

### **GPU MODE ▷ #[job-postings](https://discord.com/channels/1189498204333543425/1190208177829068860/1472004692178505869)** (1 messages): 

> `Agent Tinder, Sploink, World Model` 


- **Sploink：Agent 版 Tinder 正在组建团队**：一位计算机科学/量子计算专业的学生正在构建 **Sploink**，它被描述为 *"Agent 版的 Tinder，能够根据个人滑动的动作累积个性化信息。"*
   - 创始人正在寻找 *"硬核开发者（cracked builders）来打破常规、快速迭代"*，并为感兴趣的申请者提供了 [Google Forms 链接](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header)。
- **World Model 旨在联合 Agents**：团队正在构建一个 **World Model**，以促进数千个 Agent 之间的通信，并以 **moltbooks** 为例。
   - 该项目专注于快速开发和创新。


  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1472120916380090452)** (16 messages🔥): 

> `L2 Cache Behavior on NVIDIA GPUs, Flashinfer Bench Skills and Tooling, MLSys 2026 NVIDIA Track Modal Credits, Fastest way to swap rows and columns in a matrix, WGMA syncthreads` 


- **L2 Cache 命中率之谜已解**：调查显示，一个向量加法（vector add）Kernel 出现的看似较高的 **31% 命中率**（本应为零）可能是由于 **NVIDIA GPUs** 处理 **L2 Cache** 写入的方式导致的，特别是根据[这篇 NVIDIA 论坛帖子](https://forums.developer.nvidia.com/t/how-do-gpus-handle-writes/58914/5)中提到的部分与完整 32 字节扇区写入。
   - 写入操作通常会表现出 **100% 的命中率**。
- **Flashinfer 技能进展迅速**：一位成员对 **Flashinfer** 基准测试技能和用于 Kernel 开发的工具调用表示了兴趣。
   - 在接下来的几个月或几周内看到这些变化将会很酷，这暗示了该领域的快速进展。
- **MLSys 2026 NVIDIA Track Modal 额度**：一位成员询问是否有人真的收到了作为 **MLSys 2026 - NVIDIA Track** 竞赛一部分提供的 **$1000 Modal 额度**。
   - 目前还没有人回应。
- **矩阵行列交换大比拼**：一位成员寻求在以列优先格式存储的矩阵中交换 `x` 行和 `y` 行**以及**列的最快方法。
   - 这些操作需要 **2 次读取 + 2 次写入**，总共 **4*N 次内存操作**，通过向量化和 L1 Cache 绕过提示（bypass hints）可以实现进一步优化。
- **WGMA syncthreads**：一位成员在基本掌握了 **sm80 mma** 后，开始了他在 **wgmma** 领域的探索，并寻求指导。
   - 似乎 **wgmma** 需要一堆类似 **syncthreads** 的操作，该成员目前还没看出来它哪里更好……

  

---


### **GPU MODE ▷ #[pmpp-book](https://discord.com/channels/1189498204333543425/1194427148656721970/1472930886575194112)** (6 messages): 

> `5th edition, Kindle availability, Paperback release date` 


- **第五版讨论开启**：成员们讨论了 **第五版** 的存在，一位成员问道 *“有人看到第 5 版了吗？”*。
   - 对话表明 **Kindle 版本** 已经上线，而 **实体书** 预计将于今年第三季度（Q3）发布。
- **Amazon 链接消失**：一位成员请求提供 **第五版** 的 **Amazon** 商店页面链接，并提到发布原定于 **2 月 8 日**，但随后被下架。
   - 该成员指出 **Kindle 版本** 在 **Amazon** 上已不再可用，目前只列出了一个发布日期为 **9 月** 的平装本版本。


  

---


### **GPU MODE ▷ #[irl-meetup](https://discord.com/channels/1189498204333543425/1218444432588800010/1472066747698909245)** (3 messages): 

> `Taiwan Sundai, SF Meetup, Luma AI Inference Engineer, Performance Optimization` 


- **台湾 Sundai 黑客，集合！**：一位成员询问是否有来自 *Sundai 黑客组织* 的成员在 **台湾**，并提议一起喝咖啡。
   - 关于此询问，目前没有进一步的细节或回应。
- **Luma AI 工程师在旧金山寻求学习伙伴**：一位在 **Luma AI** 工作的推理工程师最近搬到了 **旧金山 (SF)**，正在寻找对 **性能优化** 感兴趣的新朋友和学习伙伴。
   - 他们分享了自己的 [LinkedIn 个人资料](https://www.linkedin.com/in/sourish07/) 和 [YouTube 频道](https://www.youtube.com/@sourishk07) 以便感兴趣的人联系。


  

---


### **GPU MODE ▷ #[rocm](https://discord.com/channels/1189498204333543425/1233704710389764236/)** (1 messages): 

gabagoolamerican: 力挺 tensorwave 🫡
  

---

### **GPU MODE ▷ #[webgpu](https://discord.com/channels/1189498204333543425/1262121239044948009/1472521934398034032)** (2 条消息): 

> `PyTorch on WebGPU, BitNet 2B, Hesper Library` 


- **WebGPU 驱动 PyTorch**：一位成员对 **PyTorch on WebGPU** 表现出浓厚兴趣，并分享了他们在 **webgpu/dawn** 上运行 **BitNet 2B** 的结果，达到了 **125 tps**。
   - 他们链接了自己的 [Hesper library](https://github.com/Verilean/hesper?tab=readme-ov-file#bitnet-b158-inference-125-tps-on-m4-max)，展示了在 **M4 Max** 上以 **125 tps** 进行 **BitNet-B1.58** 推理。
- **BitNet B1.58 推理**：该用户强调了利用其 **Hesper library** 在 **M4 Max** 上实现 **125 tps** 的 **BitNet-B1.58** 推理性能。
   - 这证明了在 Apple Silicon 上使用 WebGPU 进行高效推理的潜力。


  

---


### **GPU MODE ▷ #[popcorn](https://discord.com/channels/1189498204333543425/1298372518293274644/1471987178656759989)** (17 条消息🔥): 

> `KernelBot env setup, Reasoning traces dataset, Kernel generation with Arcee Trinity Mini, Qwen3-30b-a3b on kernel generation, SFT data generation with GLM 4.5 Air model` 


- **面向新手的 KernelBot 环境已就绪**：一位成员宣布他们已经基本完成了 **KernelBot**、**Modal** 和 **Runpod**（AMD 设备需要）的环境配置，并邀请其他人加入。
   - 他们鼓励大家有问题随时艾特，如果被漏掉了请再次提醒。
- **使用 Arcee Trinity Mini 进行内核生成展现出潜力**：一位成员使用了从 **Kernelbook** 生成的推理轨迹（reasoning traces）数据集来微调 **Arcee Trinity Mini** 以进行内核生成，该数据集已发布在 [Hugging Face](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces) 上。
   - 用于生成轨迹的模型是 **gpt-oss-120b**，在过滤掉失败尝试并添加格式良好的示例后，模型很好地学会了格式化。
- **Qwen3-30b-a3b 在内核生成方面表现欠佳**：一位成员发现 **Qwen3-30b-a3b** 在各种内核生成任务中表现*不正确*，经常导致编译/语法错误，因此需要通过 **SFT (Supervised Fine-Tuning)** 来提高正确性。
   - 通过 **kimi k2** 从 Kernelbook 生成数据、进行过滤并运行 SFT，在短期运行中将正确性提升了 **3 倍**，这表明更多高多样性和高质量的 SFT 数据将大有裨益。
- **正在使用 GLM 4.5 Air 生成 SFT 数据**：一位成员已开始使用 **GLM 4.5 Air** 模型从 Kernelbook 生产 **SFT 数据**，理由是该模型价格亲民，且在 4xH100 配置上为 KV cache 提供了充足空间。
   - 他们也欢迎其他符合预算且能为多轮轨迹提供足够 KV cache 的模型推荐。
- **等待 PTX Agent**：一位成员表示他们正在等待 PTX Agent。
   - 随后出现了一个 grug 表情符号的回应。


  

---


### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1472007347395105030)** (5 条消息): 

> `TK2 Hopper Multi-GPU, A100/4090 Code Integration, MoE Kernels, FP8 Attention, 128B Swizzling Mode` 


- **TK2 关注 Hopper GPU**：**TK2** 目前专注于 **Hopper+** 架构的多 GPU 设置，可能会忽略其他 GPU。
   - 有建议提出应评估集成与 **A100/4090** GPU 兼容的代码。
- **MoE 内核——并非易事？**：一位成员建议开发用于训练和推理的 **MoE 内核**，但另一位成员回应称*目前没有相关计划*。
   - 该成员指出 **MoE 内核** 会非常*出色*，但*可能并非易于实现的目标（low-hanging fruit）*。
- **FP8 Attention：一个好主意**：实现 **FP8 attention** 和低精度向量算子被认为是极具吸引力的选项。
   - 同时被列出的还有 **FFT 卷积反向传播（FFT conv backwards pass）** 和 decode 内核的实现。
- **128B Swizzling Gather4**：在审查了 no-swizzle 布局后，一位成员建议为 **gather4** 使用 **128B swizzling mode**。
   - 他们表示 no-swizzle 布局*似乎不是我想要的那种*。


  

---


### **GPU MODE ▷ #[factorio-learning-env](https://discord.com/channels/1189498204333543425/1354169122107293786/1473021636319117366)** (1 条消息): 

> `Involved with research, Involved with Engineering` 


- **表达了参与研究的兴趣**：一位成员表示有兴趣参与研究项目。
   - 他们正在询问有哪些**开放问题（open problems）**可以贡献力量。
- **表达了参与工程的兴趣**：一位成员表示有兴趣参与工程项目。
   - 他们正在询问有哪些**开放问题（open problems）**可以贡献力量。


  

---

### **GPU MODE ▷ #[cutlass](https://discord.com/channels/1189498204333543425/1362196854460383353/1472266420028178616)** (7 条消息): 

> `CuteDSL tensor alignment, BF16 grouped GEMM with CUTLASS, CuTeDSL Layout Algebra Complement, CuTeDSL strides` 


- **Cutie: CuteDSL Partition 中的 Tensor 对齐丢失**：用户注意到在 CuteDSL 中，`partition_S` 会导致 Tensor 对齐丢失，具体表现为分区后从 `align<16>` 变为 `align<4>`，并提供了[完整代码链接](https://gist.github.com/brian030128/9a910962eecef5825df14d62629e133e)。
   - 在分区前，Tensor 是 `raw_ptr(0x00007f4a72604080: f32, gmem, align<16>) o (32,32):(64,1)`，但在分区后，它变成了一系列 `align<4>` 的 `raw_ptr`。
- **Beefy: 使用 CUTLASS 的 BF16 Grouped GEMM 性能**：一名用户询问，在 Token 维度较大的 BF16 Grouped GEMM 场景下，考虑到寄存器压力的限制，是否仍适合使用 CUTLASS 提供的 EVT 算子进行 Epilogue 融合。
   - 用户指出，寄存器压力（特别是在 **BF16 精度**下）将 Tile 大小限制为 **256**，并需要协作调度策略，这可能会阻碍现有 CUTLASS 模板实现中 Epilogue 开销的隐藏/重叠。
- **CuTeDSL Layout Algebra 中的 Complement 困惑**：一名用户在复现 CuTeDSL 文档中的 [Complement 示例](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#complement-examples)时遇到困难。期望 `complement((2,2):(1,6), 24)` 的结果为 `(3,2):(2,12)`，但实际得到了无效结果 `x:x`。
   - 提供的代码片段使用 `@cute.jit` 定义了一个 `complement` 函数来创建 Layout，应用 `cute.complement` 并打印这两个 Layout，但结果与文档不符。
- **CuTeDSL Strides 中出现的 At 符号**：一名用户询问 CuTeDSL Strides 中 `@` 符号的含义，特别是在检查 `cute.local_tile` 时打印出的 Layout（如 `(128,64,4):(1@1,1@0,64@0)`）。
   - 上下文涉及检查 Layout 并理解 `cute.local_tile` 如何影响 Strides，从而导致在打印出的 Layout 表示中出现了 `@` 符号。


  

---


### **GPU MODE ▷ #[teenygrad](https://discord.com/channels/1189498204333543425/1373414141427191809/1472602173677502580)** (3 条消息): 

> `InterpretedTensor, CompiledTensor, tanh CPU kernel, Autograd, gemm hitting peak` 


- ****Tensor 转型**：InterpretedTensor 登场！**：Eager 模式和 Graph 模式正在被拆分为 `InterpretedTensor` and `CompiledTensor`。承诺稍后将使用 tinygrad IR 来解决 `CompiledTensor` 的问题，详见[此提交](https://github.com/j4orz/teenygrad/blob/master/python/teenygrad/frontend/tensor.py#L18-L86)。
   - `InterpretedTensor` 现在拥有 **axpy** 和 **gemm** 的实现，为 Autograd 和 GEMM 峰值性能奠定了基础。
- ****Tanh 大捷**：CPU Kernel 已连接！**：一个 **tanh CPU kernel** 已经实现并通过 `InterpretedTensor.tanh()` 进行了连接，详见[此提交](https://github.com/j4orz/teenygrad/commit/6480b6c1f9f2532a9ac4b29c90858892d76ea7d3)。
   - 此次增强还包括逐元素（element-wise）算子、tanh 非线性激活函数和 GEMM 的**前向和反向传播**。
- ****梯度体操**：新增 Autodiff 闭包！**：`requires_grad` 现在可以跨算子传播，模拟了 Torch 的行为，这归功于[此提交](https://github.com/j4orz/teenygrad/commit/17303c37729c79747e011f989b3a373b7799b608)中的更改。
   - 此外，**__neg__、__sub__ 和 tanh backward** 已通过 Autodiff 闭包实现，进一步完善了 Autograd 功能，如[此提交](https://github.com/j4orz/teenygrad/commit/b958861b05f659c4ddc8191a85017855a09a9855)所述。


  

---

### **GPU MODE ▷ #[nvidia-competition](https://discord.com/channels/1189498204333543425/1434709259500650628/1472009122667958520)** (27 messages🔥): 

> `Performance Trends Feature, Zooming in performance trends, B200 GPU Submission Issues, CUTLASS errors on Modal` 


- **性能趋势现已上线**：排名页面新增了“Performance Trends”功能，用于可视化展示随时间推移的提交改进情况，并提供了来自 **nvfp4_group_gemm** 的截图作为示例：[示例图片](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=6994f753&is=6993a5d3&hm=04464fd8506db7dd6c3fc20dc24108e5b95f8271562967ac97233c0b34c78159&)。
- **性能趋势图表支持缩放**：用户现在可以对 Performance Trends 图表进行**缩放**，调整 **x 轴和 y 轴**以提高可读性，特别是在提交记录较旧或分数差异较大时：[示例图片](https://cdn.discordapp.com/attachments/1472009122667958520/1472018503371456816/image.png?ex=6995000f&is=6993ae8f&hm=16f4df2d33938e809335e12d6a8c1739d1b4f0d9e56fbb14e4f6e018df9115c4&)。
- **B200 在使用 CUTLASS 时遇到困难**：一位成员报告了在 B200 GPU 提交期间遇到的 **CUTLASS** 错误，具体包括由旧版 CUTLASS 引起的 *ModuleNotFoundError* 和 *DSLRuntimeError*，这可能是由于 NVIDIA 服务器与 B200 设置之间的差异导致的，相关信息链接到此 [CUTLASS commit](https://github.com/NVIDIA/cutlass/blob/8cd5bef43a2b0d3f9846b026c271593c6e4a8e8a/python/CuTeDSL/cutlass/cute/_tvm_ffi_args_spec_converter.py#L214)。
- **显示滚动最佳提交**：性能趋势图现在显示了当前前 5 名用户每天的最佳提交，并用黄色标出了所有用户中的最快提交，直观展示了社区的优化进展，如[此图](https://cdn.discordapp.com/attachments/1434709259500650628/1472095545123147927/image.png?ex=69949f10&is=69934d90&hm=044bcb40b4f475273e229127da51e70cd3fc08fdb96be903caf1ac11d10659e0&)所示。


  

---


### **GPU MODE ▷ #[robotics-vla](https://discord.com/channels/1189498204333543425/1437390897552818186/1472878448023437385)** (1 messages): 

> `TRLC DK-1, VR teleop prototype, SO-101 arm` 


- **双臂 TRLC DK-1 到货！**：一位成员在上周四收到了他们的双臂 **TRLC DK-1**，并作为第一个项目创建了一个 **VR teleop prototype**（VR 远程操作原型）。
   - 他们分享了一段从另一个房间操作该系统的[延时摄影短片](https://x.com/neurosp1ke/status/2023073945637753101)。
- **SO-101 机械臂增强了 VR 远程操作**：在一个额外的 **SO-101 arm** 上安装了 **stereo cam**（立体相机），用于控制 VR 远程操作原型中的平移（pan）、倾斜（tilt）和滚动（roll）。
   - 其核心思想是在策略（policy）卡住时进行远程接管，实现人工干预。


  

---


### **GPU MODE ▷ #[career-advice](https://discord.com/channels/1189498204333543425/1450579381448609882/1472781181538930822)** (6 messages): 

> `CUDA, Triton, GPU kernel interviews, Optimization Strategies` 


- **关于 Kernel 面试中使用 Triton 还是 CUDA 的辩论**：一位成员正在准备 **GPU kernel interviews**，并在考虑使用 **CUDA** 还是 **Triton**，他更倾向于使用 **Triton**，因为在限时面试中编写 kernel 时它更加简洁快速。
   - 他们意识到 **CUDA** 可以展示对 **GPU architecture**（架构）和**优化技术**更深层次的理解，但担心时间限制。
- **用 CUDA 优化 Triton Kernel？**：一位成员建议最初使用 **Triton** 来勾勒思路，然后针对特定的 block 优化转用 **CUDA** 进行特化，同时思考为什么 **Triton** 可能无法优化某些方面。
   - 另一位成员询问了关于使用 **CUDA C++** 覆盖 **Triton** 算法部分内容的问题。
- **CUDA 优化策略**：一位成员建议不要直接用 **CUDA** 覆盖 **Triton** 代码，除非是在编写编译器或采取极端手段挂钩（hook）**IR**。
   - 相反，他建议在面试过程中划定潜在的可优化区域，并展示图表作为示例。
- **探索 Triton Hooking 代码库**：对于那些有兴趣挂钩 **Triton** 的人，该成员推荐了 [facebookexperimental/tritonUnderstood codebase](https://github.com/facebookexperimental/tritonUnderstood)。
   - 这有助于深入研究 **Triton** 的内部工作原理以及如何对其进行修改或扩展。


  

---

### **GPU MODE ▷ #[flashinfer](https://discord.com/channels/1189498204333543425/1464407141128339571/1472250857008660653)** (39 messages🔥): 

> `RTX 4090 上的 GPU 架构检测、注册表单添加团队成员、Fused MoE 数值精度澄清、Modal B200 上的 NCU Profiling、Modal GPU 算力额度` 


- **修复 RTX 4090 上的 GPU 架构检测**：一位用户报告称其 **RTX 4090** 无法识别为 *gpu_architecture: Ada Lovelace*，仅能识别为 *Blackwell*，并询问如何修复。
   - 有用户建议尝试使用支持 **Blackwell** GPU 的 **Modal 平台**。
- **澄清 Fused MoE 的精度预期**：一位用户询问了 **Fused MoE** 中间值的预期数值精度，质疑是否应匹配参考实现的 FP32，还是使用 bfloat16 或 FP8 等低精度格式。
   - 一位组织者回答道，参考实现使用 **FP32** 是为了确保准确性并作为对比基准，但在 **FP8 kernels** 的中间值中使用 **FP8** 是可以接受的，只要结果与 Baseline 的偏差不过大。
- **Modal B200 支持 CUDA 12.8**：一位用户询问 Modal 是否支持 **CUDA 12.8**。
   - 一位组织者根据 [文档](https://modal.com/docs/guide/cuda) 确认 Modal 已经支持。
- **AccelOpt 声称在 FlashInfer-Bench 上实现加速**：AccelOpt 团队宣布，通过使用其自进化的 LLM Agent 系统进行 Kernel 优化，在 **GQA paged decode** 上实现了 **1.5 倍加速**，在 **GQA paged prefill** 上实现了 **1.38 倍加速**（对比 FlashInfer 0.5.3）。
   - 该团队提供了 [代码链接](https://github.com/zhang677/AccelOpt)，并鼓励他人在自己的 Kernel 上进行测试。
- **理解 CUDA kernels 的 binding.py**：一位用户询问 *modal/flashinfer-bench-starter-kit/binding.py* 的用途，以及 *PYBIND11_MODULE* 对于 CUDA kernels 是否足够。
   - 一位成员解释说这是为了 **TVM FFI binding**，正如在这场 [GPU MODE talk](https://www.youtube.com/watch?v=fQcCCSdAFI8) 中所讨论的，它允许将 Kernel 交付到不同的 Runtime，且编译速度比 Torch 更快，尽管仍然可以使用 Torch bindings。


  

---


### **Moonshot AI (Kimi K-2) ▷ #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1471962391360045086)** (206 messages🔥🔥): 

> `Kimi 诈骗网站、Kimi Code CLI 问题、Kimi 订阅问题、Kimi AI 的局限性、Kimi 定价问题` 


- **Kimi 用户警惕诈骗网站**：用户报告称有多个 [诈骗网站](https://kimi.com/membership/subscription) 冒充 Kimi，并利用该名称尝试传播恶意软件。
   - 一位用户指出 *kimi.com* 在 Google 搜索结果中排在第三位。另一位用户被警告不要下载未知软件。
- **Kimi Code CLI 扩展故障**：用户在 VSCode 中使用 Kimi Code CLI 扩展时遇到问题，尽管遵循了 [安装指南](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html)，仍收到“CLI Not Found”消息。
   - 该问题通过使用 PowerShell 单独安装 Kimi CLI 得到解决：`irm https://code.kimi.com/install.ps1 | iex`
- **Kimi 订阅计费乱象**：用户报告了 Kimi 订阅问题，包括 **多次扣费**、订阅未正常激活以及 **配额（quota）问题**。
   - 一位用户针对消失的订阅提交了 [Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)，其他用户表示由于中国春节假期，客服响应可能较慢。
- **Kimi 在视频、文本和诚实度方面的限制**：Kimi 无法识别视频文件中的音频，有时会以不安全为由拒绝处理内容（例如 YouTube 转录文本）。
   - 成员们发现 Kimi 有时会“谎话连篇直到被戳穿”，提供矛盾或虚假的信息，这与其他 AI 模型类似。
- **Kimi 定价引发用户不满**：用户对 Kimi 相对于其价值和使用限制而言过高的定价表示担忧，特别是与 MiniMax 等替代方案相比。
   - 一些用户表示，由于生活成本差异，这种定价在大城市以外是不可持续的；而另一些用户则为成本辩护，认为 API 是开放的，可以与其他供应商配合使用。


  

---

### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1471976683186950237)** (184 messages🔥🔥): 

> `Claude Code, China OS Going Closed, Meta's Llama Trained on Qwen, 4o Obsession, ByteDance Seedance 2.0` 


- ****Claude Code** 存在输出问题**：一位用户报告称 **Claude Code** 在一个 session 中仅进行两次 prompt 就会“罢工”（tapping out），其中一个 prompt 仅仅是 'continue'。
   - 有人建议该问题可能是由于安装版本过旧，或者是配置错误将输出 token 限制在了 **32K**。
- ****China OS** 走向封闭？**：讨论围绕 **Chinese OS** 模型是否正变得不那么开放，其变现策略正转向云端托管。
   - 尽管存在担忧，但共识倾向于 **China OS** 模型将保持开放，以鼓励全球采用和定制，特别是针对美国的初创公司。
- ****Meta** 的 Llama 在 **Qwen** 上训练**：据报道，**Meta** 的下一个 AI 模型（可能不叫 **Llama**）将基于 **Qwen** 进行训练，扎克伯格据称在[这张图片](https://cdn.discordapp.com/attachments/1149866623109439599/1472086914525036704/wwww.JPG)中承认了这一点。
   - 向“后后训练”（post post training）的转变被强调为通往人工智能超智能（ASI）的新策略。
- **每个人都痴迷于 **4o****：有人认为任何痴迷于 **4o** 的人都有“精神疾病”，正如[这个 meme](https://tenor.com/view/druski-shrug-idk-iono-hands-up-gif-7122581203490458147)所示。
   - 至少目前还有一些由 sharegpt 2.0 提供的相当庞大的 **4o** 聊天数据集可供训练。
- ****ByteDance Seedance 2.0** 令人恐惧**：**ByteDance Seedance 2.0** 生成了效果惊人的 AI slop，引发了对专业创意和技术职业价值的质疑。
   - 分享了一个[帖子](https://x.com/RuairiRobinson/status/2021394940757209134)链接，展示了该模型令人印象深刻但又令人担忧的输出结果。


  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1472022929440247963)** (4 messages): 

> `Gemini CLI 'Conductor' Extension, Codex Skills, Kimi K2.5 Local Setup` 


- **Gemini CLI 获得 'Conductor' 扩展**：**Gemini CLI** 中新的 'Conductor' 扩展将项目分解为“轨道（tracks）”，在每次请求时将所有信息发送给 LLM，从而有效地将其加载到 context window 中。
   - 尽管这允许持久的上下文，但像 Gemini 这样的模型即使在有 'conductor' 轨道的情况下仍会**偏离预期的结果**。
- **Codex Skills 的 Token 使用量**：据报道，**Codex** 系统通过其 'skills' 在每个请求中填充了 **15K-30K tokens**。
   - 一位用户提到 **50K tokens** 是一个理想平衡点，并暗示 **Hermes 4** 可以处理大型 system prompts 而不会崩溃。
- **Kimi K2.5 本地部署需求**：一位用户询问了在本地运行 **Kimi K2.5** 等模型的最低配置要求，目标是运行多个 **OpenClaw** 工作流。
   - 有人建议，这样的配置可能需要 **700+ GB 的 RAM**。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

real.azure: https://arxiv.org/abs/2602.03839
  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1471961374455500912)** (6 messages): 

> `CharonProtocol, Zero-copy, Zig Language` 


- **CharonProtocol 宣称具有零拷贝（Zero-Copy）优势**：一位成员分享了 **CharonProtocol** 的 [GitHub 仓库](https://github.com/jackangel/CharonProtocol/tree/main)，推荐将其作为绕过 Python 限制的零拷贝替代方案。
   - 作者声称“*有了 LLM，就没有理由再为非内存优化的代码而挣扎*”，并且他们的 README 提供了与 **Zig** 的对比。
- **Zig vs. Python**：一位用户提到他们使用 **Zig** 来克服 LLM 开发中 Python 的内存优化问题。
   - 对话指出 **zero-copy** 是避开 Python 限制的一种方式。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

real.azure: https://arxiv.org/abs/2602.03839
  

---

### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1471963793092382873)** (137 messages🔥🔥): 

> `Cosmos Reason 2 with Transformers, HF Pro Subscription for Posts, Revolutionary AI Architecture Ideas, DeepSpeed and Large Model Finetuning Issues, Subjective Entity Architecture Proposal` 


- **Jetson Thor 上的 Transformers 和推理**：一名成员寻求在 **Jetson Thor** 设备上使用 **Transformers** 进行 **Cosmos Reason 2** **推理**的相关资源，特别是针对本地执行。
   - 讨论中未提供具体资源。
- **关于 AI 架构的革命性构想？**：一位用户对他关于 **AI 架构**的“革命性构想”缺乏关注表示沮丧，并决定独立“建立它们”。
   - 一名成员询问了该用户构想的具体类型，鼓励其直接去“执行”，而不是过度思考。
- **DeepSpeed 在微调 Qwen3 时遇到困难**：一名成员在使用 **DeepSpeed** 在 **8 张 RTX 4090** 上微调 **Qwen3-30B-A3B-Thinking-2507** 模型时遇到问题，在模型加载期间面临 CPU 内存限制。
   - 发现 **transformer 版本 5.1.0** 导致了 DeepSpeed 的问题，修复工作正在 [transformers/pull/43524](https://github.com/huggingface/transformers/pull/43524) 和 [transformers/issues/43596](https://github.com/huggingface/transformers/issues/43596) 进行中。
- **Lucidrains 消失了！**：成员们注意到 **Lucidrains** 从 GitHub 上消失了。
   - 据透露，*GitHub 在没有警告的情况下暂停了该账号*，并为有需要的人分享了其在 [codeberg.org/lucidrains](https://codeberg.org/lucidrains) 的新个人主页链接。
- **Qwen3.5 发布！**：成员们注意到 **Qwen3.5** 现已发布，且可以在本地运行。
   - 该消息最初发布于 [X.com/UnslothAI](https://x.com/UnslothAI/status/2023338222601064463)。


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1472087463123091477)** (31 messages🔥): 

> `ATIC epistemic uncertainty system, LLM-based password auditing tool, Rune: composable ASCII art animations, Jazz AI terminal agent, CoDA-GQA-L attention mechanism` 


- **ATIC 精确识别 AI 不确定性**：ATIC，一个**认识论不确定性系统 (epistemic uncertainty system)**，推出了使用 **3 个独立的 Claude Opus 4.5** 实例的**三脑架构 (tri-brain architecture)**，用于检测 AI 何时在猜测，详见 [atic.consulting](https://atic.consulting)。
   - 通过对 **Q1（随机不确定性）**和 **Q2（知识差距）**进行评分，其旨在当不确定性较高时将查询转发给专家，文档可见[此链接](https://web-production-51da4.up.railway.app/docs)。
- **密码审计器使用 LLM**：一个基于 LLM 的密码审计工具 **PassLLM**，利用个人身份信息生成按概率排序的可能密码列表，并在数百万对现实生活中的密码对上进行了微调，详见 [PassLLM on GitHub](https://github.com/Tzohar/PassLLM)。
   - **Qwen 3 4B LoRA** 模型在准确性上优于许多其他工具，能够理解人类生成密码的复杂细节，如[演示视频](https://cdn.discordapp.com/attachments/897390720388825149/1472237681890168874/Video_Project_7.mp4?ex=69947ab0&is=69932930&hm=bff421017a9056af1679cfb41de4580cba4243d9b55e582126168457af7b4eb6)所示。
- **Rune 让你制作 ASCII 艺术**：一位成员介绍了 **Rune**，这是一个开源的 React 组件库和 CLI，用于创建可组合的 ASCII 艺术动画，详见 [Rune on Github](https://github.com/zeke-john/rune)。
   - 它允许用户创建自定义动画，如[此视频](https://cdn.discordapp.com/attachments/897390720388825149/1472520612974170317/549317311-bb892078-7fe0-4c16-8332-dbc67db3750a.mp4?ex=6994d970&is=699387f0&hm=dd22cbed16a4b5fe6d74d3e04cee7a6c651e5a5640fb99904ecd94445c42d5e2)所示。
- **让终端“爵士”起来**：**Jazz** 被介绍为一个运行在终端中的 AI Agent，作为一个助手可以读取文件、管理 Git、搜索网络、处理电子邮件以及运行 shell 命令，详见 [Jazz on Github](https://github.com/lvndry/jazz)。
   - **Jazz** 与 LLM 提供商无关，支持计划工作流，并通过 MCP 连接任何事物，甚至可以在 CI 中审查 PR 并编写自己的发布说明。
- **恒定 KV cache 注意力机制**：一种名为 **CoDA-GQA-L** 的新注意力机制为语言模型发布，无论上下文长度如何，它都能保持恒定的 **KV cache** 大小，根据[博客文章](https://huggingface.co/blog/anthonym21/coda-gqa-l-attention)，这显著节省了 **VRAM**。
   - 在 **128K 全上下文**下，该方案相比当前机制可节省超过 **1000 倍的 VRAM**，论文可在 [Zenodo](https://zenodo.org/records/18663265) 获取，代码在 [GitHub](https://github.com/anthony-maio/CoDA-GQA-L)。


  

---

### **HuggingFace ▷ #[core-announcements](https://discord.com/channels/879548962464493619/1014557141132132392/1472068146746622113)** (1 messages): 

> `Custom CUDA kernel, LTX model, H100 benchmark` 


- **Agent 为 LTX 编写自定义 CUDA Kernel！**: 一个 Agent 为 **H100** 上的 **LTX model** 编写了自定义 **CUDA kernel**，以此超越了基准测试 (baseline benchmark)。
   - 查看 [blog post](https://huggingface.co/blog/custom-cuda-kernels-agent-skills) 获取所有细节。
- **H100 凭借自定义 Kernel 碾压基准测试**: 由 Agent 制作的自定义 **CUDA kernel**，使 **LTX model** 在 **H100** 上的表现超过了基准测试。
   - 结果详见 [Hugging Face blog post](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)，展示了性能提升。


  

---


### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1472192174312394833)** (9 messages🔥): 

> `Course Feedback, Course Navigation, Discord Channels, Documentation Updates` 


- **课程布局缺乏组织性**: 一位成员询问从哪里开始，并指出*他们确实需要更好地组织学习页面的布局*。
   - 新成员不清楚课程的最佳开始地点。
- **Discord 频道引起混淆**: 一位成员报告称无法找到 **agents-course-questions** 和 **agents-course-announcements** Discord 频道。
   - 另一位成员回应说，现在只有一个 **courses channel**，而教程文档尚未更新以反映这一变化。
- **文档急需 Agent**: 一位成员开玩笑说*要是我们有一个 Agent 来保持文档更新就好了...*，指的是过时的教程文档。
   - 另一位成员欢迎他们*为更新此信息的课程 repo 创建 PR*，并称其为 *Good first issue*。


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1472044587353899163)** (8 messages🔥): 

> `Mojo changelog videos, Mojo version 26.2, Codex fixes, Intel GPU support` 


- **自动化更新日志分析视频首发**: 一位成员自动化了对 **Mojo changelog** 的分析，并开始将其转化为短视频，以便更轻松、更快速地吸收更新，分享了 [YouTube link](https://www.youtube.com/watch?v=Zac9azlqBHQ) 并征求反馈。
- **Mojo version 26.2 推迟，标题已修正**: 一位成员指出 **version 26.2** 尚未发布。
   - 视频创作者承认了错误，表示前一天看到了该版本并修正了标题，承诺在下一个视频摘要中进行正确的版本标注。
- **Codex 解决对齐差距**: 在对 **LLM** 进行了 75 小时的工作后，**Codex** 修复了大部分对齐差距 (parity gaps)，使项目更接近可发布状态。
- **Intel GPU 支持计划不明**: 一位成员询问了关于 **Intel GPU support** 的计划，特别是针对 **Panther Lake** 设备。
   - 另一位成员回应称目前还没有公布计划，但 **Intel** 正在将 **SYCL** 合并到 **LLVM** 的上游，一旦 **Mojo compiler** 开源，这将使社区的努力变得更加容易。


  

---

### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1471974464399867955)** (83 messages🔥🔥): 

> `Python Mojo 模块导出样板代码, Span 和 Writable Trait, Mojo 中的列表切片, Mojo 版 ECS 游戏引擎, 用于 ECS 的 MLIR 方言` 


- ****Python Mojo 模块需要装饰器支持****：成员们讨论了目前导出 **Python Mojo 模块** 所需的样板代码，一位用户建议使用类似 `@pyexport` 的更简单的装饰器语法来减少冗余。
   - 另一位成员回应称，该功能已在 *roadmap* 中。
- ****Span 应该涵盖 'Writable' 实现****：Discord 用户发现 `Span` 应该实现 `Writable` Trait，并指出一个问题：`lst[:2]` 的结果是 `Span`，而 `lst[:2:2]` 返回的是 `Self`。
   - 这种不一致性破坏了值语义，因为修改切片的大小不会反映在 span 中；该[问题已在 GitHub 上记录](https://github.com/modular/modular/issues/5870#issue-3868256404)待解决。
- ****列表切片导致语义冲突****：一位用户指出 `lst[:2]` 返回 `Span` 而 `lst[:2:2]` 返回一个新的 `List`，这在期望值语义时可能会导致意外行为。
   - 从性能角度来看，每次进行切片时都复制列表通常是一件非常糟糕的事情，因此应采用后者。在 [Modular 论坛](https://forum.modular.com/t/list-slicing-inconsistency/2737)上发起了一场讨论，以进一步探讨这种不一致性。
- ****探讨将 Bevy 游戏引擎移植到 Mojo****：成员们对在 Mojo 中开发类似于 Rust 的 **Bevy** 游戏引擎表现出兴趣，一位成员提出愿意提供帮助，并指向了一个现有的名为 **Larecs 的 ECS 库**。
   - 该 ECS 仍在积极开发中。参见 [GitHub 上的 Larecs](https://samufi.github.io/larecs/)。
- ****ECS：编译器在 MLIR 方言中实现愿景****：Discord 用户讨论了使用 **MLIR** 方言实现 ECS（实体组件系统）的潜力，设想一种能够根据组件和系统定义优化数据布局和系统融合（system fusion）的编译器。
   - 一位用户分享了他们[十年前尝试开发的 ECS 语言](https://github.com/mzaks/ECS-Lang)，并提到当时他们还没有完全理解系统融合的潜力，随后指出 ECS-Lang 更多是基于特定语言或 ECS 框架的代码生成。


  

---


### **Eleuther ▷ #[announcements](https://discord.com/channels/729741769192767510/794042109048651818/1471960647653920941)** (1 messages): 

> `CommonLID, 语言识别基准测试, 多语言数据质量, 社区聚光灯演讲` 


- ****CommonLID** 在语言识别领域首次亮相**：经过 Common Crawl、EleutherAI、MLCommons 和 JHU 近两年的努力，涵盖 **109 种语言** 的网络语言识别基准测试 **CommonLID** 正式发布，arXiv 论文已可在 [arxiv.org](https://www.arxiv.org/abs/2601.18026) 查看。
- ****LangID** 性能被当前基准测试高估**：评估显示，即使仅限于模型明确支持的语言，现有顶尖模型的 **F1 分数也低于 80%**，这表明当前的基准测试高估了 **LangID** 在网页数据上的表现。
   - 在对现有 LangID 系统的评估中，CommonLID 也被证明是最具挑战性的数据集。
- **邀请社区参加 **CommonLID** 聚光灯演讲**：已安排一次 [社区聚光灯演讲 (Community Spotlight Talk)](https://discord.gg/eleutherai?event=1471940600248143933)，数据集可在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上获取。
- **Common Crawl 和 EleutherAI 寻求转发！**：鼓励社区在 [Common Crawl 的 Twitter](https://x.com/commoncrawl/status/2021306337079198038?s=46) 和 [EleutherAI 的 Twitter](https://x.com/AiEleuther/status/2022391592800391425?s=20)，或者在 [Common Crawl 的 Bluesky](https://bsky.app/profile/commoncrawl.bsky.social/post/3mejtdzvet22b) 和 [EleutherAI 的 Bluesky](https://bsky.app/profile/eleutherai.bsky.social/post/3merafph5ec2g) 上转发此公告。


  

---

### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1471976623980150868)** (24 messages🔥): 

> `Discord Bot for AI News, EleutherAI GPT-3 Model, AI Behavior Structuring, Assistant Axis Drift, Open Source Research Contribution` 


- **关于 AI 新闻推送的 **Discord Bot** 讨论**：一位成员询问了创建一个用于策划 **AI safety** 新闻和论文推送的 **Discord bot** 的可能性，并对网页抓取的合法性以及被 Discord 封禁的可能性表示担忧。
   - 其他人指出 [news.smol.ai](https://news.smol.ai/) 是一个现有的尝试，同时也承认了与抓取相关的风险。
- **EleutherAI 的 **GPT-3 Model** 传闻被辟谣**：一位新成员引用在网站上找到的信息，询问关于 **EleutherAI** 发布 **GPT-3** 风格模型的情况。
   - 一位成员澄清说该信息已过时，指的是几年前发布的 **GPT-J** 和 **GPT-NeoX**，并链接到 [EleutherAI website](https://www.eleuther.ai) 作为最新信息的示例。
- **探索 **Multi-Layered AI Governance** 方法**：一位成员正在尝试使用分层规则、检查和模块化工具来构建 **AI behavior**，以保持一致性并避免偏离轨道。
   - 他们正在寻求关于在这一 **multi-layered system** 中保持输出确定性、管理冲突指导以及避免瓶颈的建议。
- **长对话中确认存在 **Assistant Axis Drift****：一位成员分享了一篇关于提取不同人格激活方向的 [paper](https://arxiv.org/abs/2601.10387)，强调了 **"Assistant Axis"** 的存在，该轴追踪模型处于默认助手模式的程度，以及在长对话中 **activations 如何发生漂移**。
   - 另一位成员指出，可衡量的 **“Assistant Axis”** 及其在长对话中的漂移证实了 *行为漂移 (behavioral drift)* 是结构性的，而非偶然现象。
- **研究人员寻求可贡献的 **Open Source Project****：一位成员正在寻找可以贡献的 **open-source project**，他们在自己的私有项目中发现 **lm-evaluation-harness** 非常有用。
   - 上下文中未给出具体的项目建议。


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1471969804616470569)** (8 messages🔥): 

> `Weight Homology, Independence Tests for Language Models, Blackbox Model Provenance, Llama Architecture Models, Causal Attention Matrix Approximation` 


- **Weight Homology 论文引发关注**：成员们讨论了论文 [Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309) 及其在识别 **LLM weights** 之间联系方面的相关性。
   - 一位成员表示，他们对 [Independence Tests for Language Models](https://arxiv.org/abs/2502.12292) 以及黑盒后续研究 [Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796) 印象非常深刻。
- **恢复微调树 (Finetuning Trees)**：一位成员指出，Weight Homology 论文中似乎缺少的一点是对具有更多混杂因素情况的分析。
   - 他们发现 *Independence Tests* 中最令人印象深刻的是，他们采用了这些 **Llama-architecture models** 并正确恢复了 **finetuning tree**。
- **Causal Attention Matrix Approximation**：一位成员分享了一张 [image](https://cdn.discordapp.com/attachments/1471969804616470569/1472326663101743226/IMG_0402.png?ex=6994cd8e&is=69937c0e&hm=cfa07f059264fdb598504348a1020741814a6ede365508cb45d96ff7d49f485e&) 分析了 **precondition attention matrix** 及其在因果情况下的高效计算，并询问了使用 Tensor Cores (TCs) 进行近似的可能性。
   - 他们链接了一条相关的推文：[https://x.com/dvsaisurya/status/2023118579755819459](https://x.com/dvsaisurya/status/2023118579755819459)。


  

---

### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1471977481874571426)** (15 messages🔥): 

> `Steering Vectors, Data Augmentation, Lambda Calculus Models, Representation Learning` 


- **预防性措施的 Steering Vectors**：泛化 Anthropic 的 persona vectors，预防性转向涉及在根据是否达到原始目标评判模型的同时，添加一个 [steering vector](https://www.anthropic.com/blog/red-teaming-language-models-with-language-models)，迫使模型针对 steering vector 进行补偿。
   - 通过改变目标，模型不仅可以对抗 steering vector，还能使目标特征更好地拟合，从而实现针对性的狭义用例，以及在经验数据集中可能很少见的扰动组合。
- **Lambda Calculus 模型重出江湖！**：一位成员正在开发一个仅使用 **lambda calculus** 来推导 backpropagation 的模型，证明了黑盒本质上是 lambda，在 MNIST 和 CIFAR 上表现良好。
   - 该模型使用 Python 实现，未使用 SimPy 或 TensorFlow，采用了 [基于对角化和反驳的 perceptron](https://milanrosko.com/typedrepair.html)，但由于其他事务，开发者无法继续该项目，并分享了 [这段视频](https://m.youtube.com/watch?v=RcVA8Nj6HEo&t=365s&pp=ygUPbGFtYmRhIGNhbGN1bHVz0gcJCYcKAYcqIYzv)。
- **通过 Representation Mapping 训练模型**：一位成员建议，模型不应将输入映射到输出，而应训练模型实现 representation 到 representation 的映射，从而促进 representation 空间中的数据增强。
   - 这种方法允许预计算 forward pass 的第一部分，然后添加针对不同目标的多个 steering vectors，从而复用相同的初始计算。


  

---


### **Eleuther ▷ #[gpt-neox-dev](https://discord.com/channels/729741769192767510/730090096287547444/1472113952858050693)** (2 messages): 

> `Qwen3 architecture, GPT-NeoX` 


- **GPT-NeoX 中出现 Qwen3 实现**：一位成员分享了在 **GPT-NeoX** 中 [*经过初步测试的 **Qwen3 architecture** 实现*](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main)。
   - 另一位成员对其分享表示感谢。
- **GPT-NeoX 获得新的 Qwen3 架构**：正如一位成员在 EleutherAI Discord 服务器上分享的那样，**GPT-NeoX** 现在可以使用 **Qwen3 architecture** 的新实现。
   - 提供的实现目前处于测试阶段，等待社区反馈和进一步改进。


  

---


### **MCP Contributors (Official) ▷ #[general](https://discord.com/channels/1358869848138059966/1358869848138059969/1472171441532436531)** (40 messages🔥): 

> `Token cost of output schemas, Benefits of structured output, Tool-chaining with structured outputs, Tool result types, Timezone context for MCP servers` 


- **讨论 Output Schemas 的 Token 成本**：成员们正在讨论 output schemas 的 **token cost** 是否是一种虚假的节省，因为即使安装了 **MCP** 但未使用，它也会增加成本。
   - 一位成员指出，大多数 **LLM APIs** 不支持 output schemas，因此 SDK 或客户端宿主必须将 output schema 提升到 description 中，从而增加了 token 税。
- **评估结构化输出的益处**：小组讨论了许多客户端/模型在实践中是否能从结构化输出中获益，大家一致认为在 **code mode** 下有明显优势，但在其他场景下尚不确定。
   - 一位成员提到 **Windsurf 团队** 曾一度关闭了结构化输出，因为其结果比竞争对手差，这表明采用该技术是一把双刃剑。
- **结构化输出是 Tool-Chaining 的关键**：一位成员认为，如果没有可用的 output schemas，**LLMs** 在进行 tool-chaining 时会遇到更多困难，经常会幻觉（hallucinate）输出字段。
   - 他们还探讨了投机性执行工具以动态构建 output schema 的挑战，认为在没有特定条件的情况下这是不安全的。
- **重新审视工具结果类型之争**：围绕工具结果类型展开了讨论，一位成员建议倾向于将工具结果显式声明为 **text, image, 或 object**。
   - 这种观点认为结构化结果应被视为一种不同的结果类型，补充信息应放入 meta 而非 object 中。
- **处理 MCP 服务器的时区上下文**：小组讨论了对于需要用户时区上下文的 **MCP servers**（例如针对“我上周睡得怎么样？”之类的 prompt）的推荐最佳实践。
   - 有人提到应将用户时区添加到工具参数中，通常不建议直接将客户端上下文推送到工具参数之外的 MCP server 中，因为这会混淆 domains。


  

---

### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1472131955406540880)** (27 messages🔥): 

> `Chess strategies, Deepseek models, Heretic game, GPT-OSS-120B models, LLM Hallucinations` 


- **国际象棋玩家获得位置优势建议**：一名玩家被建议专注于改善他们的布局和棋子协同作用，因为他们控制了中心且在 e5 位置有一个兵。
   - 建议是将 b1 上的马移动到 d2，然后到 b3 和 c5，从而可能对后和象进行捉双（forking）。
- **Deepseek 模型不再失踪**：在一名玩家发表“（象棋）结束了”的言论后，另一位用户询问了 **Deepseek 模型**的状态。
   - 另一位成员回复称它将 *很快 (soon(R))* 到来。
- **Heretic 游戏向公众开放**：一位成员重点介绍了 **Heretic 游戏** ([GitHub 链接](https://github.com/p-e-w/heretic))，指出它对消费者和公民的易得性。
   - 他们表达了热情，表示：*等我长大了，我想成为像 <@693263324036464742> 一样的人*。
- **GPT-OSS-120B 模型与本地模型**：一位用户询问 HF 上是否有**去审查的 gpt-oss-120b 模型**。
   - 另一位用户给出了肯定的回答，并指向了一个开源版本 ([jupyter-mcp-server](https://github.com/datalayer/jupyter-mcp-server))。
- **LLM 引用幻觉困扰论文**：一位成员对 AI/ML 论文中**幻觉（hallucinations）**的程度表示担忧，并引用了 [GPTZero](https://gptzero.me/news/neurips/)。
   - 他们质疑引用是完全错误的，还是包含诸如日期或作者错误之类的轻微不准确，并对看到如此多论文出现引用幻觉表示担忧。


  

---


### **Yannick Kilcher ▷ #[agents](https://discord.com/channels/714501525455634453/1269724655405498429/1471977721667125288)** (2 messages): 

> `text/markdown Accept Header, Multi-Agent RPG Testbed` 


- **Agent 拥抱 Markdown**：Cloudflare 正在探索为 Agent 支持 `Accept: text/markdown` [标头](https://blog.cloudflare.com/markdown-for-agents/)。
   - 这将允许 Agent 以 **Markdown 格式**接收内容，从而可能简化内容处理并提高互操作性。
- **AI 伦理测试平台中的友军误伤事件**：一位成员分享了一篇关于在作为自动运行 RPG 实现的多 Agent 系统伦理测试平台中发生的[友军误伤事件](https://3rain.substack.com/p/the-gemini-shotgun-incident-social)的报告。
   - 该事件暴露了架构方面的考量并揭示了偏见，涉及现实中基于机制的 **AI 治理**情况。


  

---


### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1472307490443100453)** (4 messages): 

> `OpenClaw Security, Qwen 3.5` 


- **OpenClaw 的安全性受到质疑**：一位成员表示希望某个实体在管理安全性方面能比 **OpenClaw** 更理智。
   - 他们链接到了 fixvx.com 上的一个状态 ([CharlieBCurran 的状态](https://fixvx.com/charliebcurran/status/2022463429823598999.wavefunction))。
- **阿里巴巴发布 Qwen 3.5**：**阿里巴巴**发布了 **Qwen 3.5**，如 [X 上的帖子](https://fxtwitter.com/Alibaba_Qwen/status/2023331062433153103)所述。
   - 更多详情可以在 [Qwen 3.5 博客文章](https://qwen.ai/blog?id=qwen3.5)中找到。


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1471967015530135637)** (32 messages🔥): 

> `GLM flash PR, Graphcore C600 IPU, Tinygrad CPU pipeline, tinybox GPU issues` 


- **GLM Flash PR 引发争议**：由 roef 提交的 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738) 因超出预期的代码行数而受到批评。
   - George Hotz 对该 PR 评论道，它 *最多应该只有 50 行*，并且包含 *额外的无关内容*。
- **Graphcore IPU 被发现极难使用**：George Hotz 测试了 **Graphcore C600 IPU**，并报告由于编译器在大 batch size 下的问题，仅实现了 **20% MFU**。
   - 尽管软件栈是开源的，但被描述为 *该死的 C++ 烂代码（accursed C++ slop）*，且片上通信架构文档*并未*开源。
- **寻求 Tinygrad 的 CPU 流水线增强**：贡献者 xavi251 在完成一个旧的悬赏任务后，询问了有关 **CPU 流水线** 的较小任务。
   - George Hotz 向贡献者发起挑战：*让运行速度更快，同时代码更少*。
- **Tinybox 面临 GPU 识别问题**：一位用户报告其 **tinybox** 尽管插在两个不同的电路上，但也只能识别 **4 个 GPU 中的 2 个**。
   - George Hotz 建议检查是否有未插好的电线，并指向频道 `#1113504076035018862`。


  

---

### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1472021859674489047)** (31 messages🔥): 

> `账号停用, 支付系统问题, Manus AI Agent 赞誉, 如何开启工单, 移除自荐推广` 


- **Manus AI Agent 因提供革命性帮助而受到赞誉**：一位用户对 **Manus AI Agent** 表达了巨大的感激之情，称其帮助提取了他们无法独立完成的内容，并称其为*游戏规则改变者（game changer）*。
   - 由于得到了这些帮助，他们*今天早上激动得流下了眼泪*。
- **账号无故停用**：多位用户报告称其账号在**无明显原因的情况下被停用**，其中一名用户声称在创建角色能力后不久就发生了停用。
   - 该用户恳请停止停用行为，以便他们能*像正常人一样使用网站*。
- **无工单系统：Manus 支持详情**：一位用户询问如何开启工单，得到的答复澄清了服务器上**并未建立工单系统**。
   - 用户被引导至 [帮助中心](https://help.manus.im/en/) 或通过电子邮件发送 [反馈](https://manus.im/feedback) 以获取支持，并备注因新年庆祝活动，响应时间可能会较慢。
- **账号问题私聊请求**：一位用户要求管理员针对一个**重要的账号问题**紧急私聊他们。
   - 另一位名字中带有 Manus 的用户也表示可以针对账号问题提供私聊帮助。
- **自荐推广贴被删除**：由于违反了服务器关于**未经允许的广告和招募**的规则，一个自荐推广贴被删除。
   - 成员被提醒保持讨论的主题性，并与频道的宗旨相关。


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1472582895125135477)** (9 messages🔥): 

> `RLM 准确率, 自定义 REPL, 使用 PostgreSQL 进行多 Agent 通信` 


- **语言和 REPL 对 RLM 准确率的影响**：实验揭示了语言和 REPL 如何影响 **RLM 准确率**，详见这篇 [帖子](https://x.com/mike_pavlukhin/status/2023023279917916501)。
   - 讨论考虑了为 RLM 子 Agent 使用的每种语言建立 **自定义 REPL** 的必要性，并探索了 tool-calling + skills 或 bash + files 等替代方案以实现更广泛的通用性。
- **PostgreSQL 实现多 Agent 通信**：一位成员正在试验使用 **PostgreSQL** 进行多 Agent 通信，以绕过 REPL 访问限制。
   - 会中指出 **REPL 质量**和**指令**是关键，但 LLM 的语言偏好应当指导语言的选择。
- **dspy-repl**：关于语言和 REPL 对 RLM 准确率影响的讨论实验已发布至 [GitHub](https://github.com/Archelunch/dspy-repl)。


  

---


### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/1472060467764531374)** (5 messages): 

> `Arxiv 论文, Autohand` 


- **Arxiv 论文被视为推文串**：一位成员快速浏览了一篇 [Arxiv 论文](https://arxiv.org/abs/2508.05199)，并认为*这篇论文可以写成博客或推文串*，以获得更多曝光。
- **Autohand 产品可能改变指令**：一位成员想知道 [Autohand 的 Evolve 产品](https://autohand.ai/products/evolve/) 是否可以用于*其他一些训练，以改变其玩游戏的指令*。
   - 另一位成员回复说该产品*被严重低估但展示得不好*，并且他们已经*玩够了游戏*。


  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1472721670728061203)** (9 messages🔥): 

> `使用 Claude 进行 Vibecoding, DSPy 团队官方项目, Crowdcent 封装 DSPy` 


- **Modaic 开发者与 Claude 进行 Vibe**：一位成员感谢另一位成员指出了与 **Claude** 进行 *vibecoding* 的方式，并提到配合 [Modaic](https://docs.modaic.dev/dspy_guide/get_started/start) 使用*确实效果显著*。
   - 其他成员表示正在关注此事。
- **DSPy 团队的开放精神**：一位成员希望 **DSPy 团队**能正式开展某个特定项目。
   - 另一位成员回应称这*似乎违背了他们的精神（ethos）*，并表示欢迎那些需要资深开发者参与的项目。
- **Crowdcent 封装 DSPy**：一位成员提到 **Crowdcent** 正在封装 **DSPy**。
   - 他们将其包含在了文档中，并询问是否有人拥有 **MCP**。


  

---

### **DSPy ▷ #[examples](https://discord.com/channels/1161519468141355160/1161519685616025600/1472857377198964841)** (1 条消息): 

> `农历新年，bb25 v0.2.0 发布，Bayesian BM25，Python + Rust 实现，DSpy 项目迁移` 


- **bb25 v0.2.0 发布并带来升级**：全新的 [bb25 v0.2.0](https://github.com/instructkr/bb25) 现已发布，其中包含 **Bayesian BM25** 的 **Python + Rust 实现**，可将搜索评分转换为校准后的概率。
   - 该版本在对两个项目进行详细代码审查后，从 Jaepil Jeong 的参考实现中迁移了四项改进，包括：*固定文档长度先验 (fixed document length prior)*、*对数几率结合 (log-odds conjunction)*、*自动 Sigmoid 参数估计* 以及 *带有五种稳定技术的在线学习*。
- **呼吁将 bb25 迁移至 DSpy**：一位成员询问是否有人有兴趣将 **bb25** 迁移到 **DSpy** 项目。
   - 目前尚不确定是否有成员自愿承担此任务。


  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1472089778336825374)** (6 条消息): 

> `Neovim 集成，Aider 评分基准，复制/粘贴语义` 


- **Aider 评分很高！**：一位成员表示 *“**Aider 评分基准** 感觉与我个人的智能水平非常相似”*。 
- **Neovim 与 Aider 的集成涌现**：一位成员正在开发 [neovim integration with aider](https://github.com/possumtech/aider-pop.nvim)，旨在更好地将复制/粘贴语义与 tmux、aider 和终端进行集成。
- **实践以代码为核心的哲学**：开发者指出了一种隐含的理论，即拥抱并扩展 Aider 的哲学，即拥有一个**以代码为核心 (code-centric)** 而非以聊天为核心 (chat-centric) 的 Agent。


  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1472057945251516497)** (4 条消息): 

> `公益技术志愿者 (Pro Bono Techies)，Ask-Code 迭代循环` 


- **寻找公益技术志愿者**：一位成员请求是否有公益技术人员愿意帮助因一些极具天赋的“黑客”而失去一切的人。
   - 另一位成员评论道，这 *“可能选错了服务器”*。
- **Ask-Code 迭代循环：仍然是最佳实践吗？**：一位成员想知道 **ask-code 迭代循环** 是否仍然是“最佳实践”，或者社区是否已经转向了使用 **aider** 的其他工作流。
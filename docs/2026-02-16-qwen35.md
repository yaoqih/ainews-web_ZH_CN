---
companies:
- alibaba
- openai
- deepseek
- z-ai
- minimax
- kimi
- unsloth
- ollama
- vllm
date: '2026-02-16T05:44:39.731046Z'
description: '**阿里巴巴**发布了 **Qwen3.5-397B-A17B**，这是一款**开源权重**模型，具备**原生多模态**、**空间智能**，以及支持
  **201 种语言**和高达 **256K token 长上下文窗口**的**混合线性注意力 + 稀疏混合专家（MoE）**架构。


  该模型相较于之前的 **Qwen3-Max** 和 **Qwen3-VL** 版本有所提升，稀疏率约为 **4.3%**。社区讨论重点关注了其**门控增量网络（Gated
  Delta Networks）**，这使得尽管模型体量巨大（约 **800GB BF16**），仍能实现高效推理；目前已通过量化技术在 Apple Silicon
  设备上成功实现了本地运行。


  托管 API 版本 **Qwen3.5-Plus** 支持 **100万（1M）上下文**，并集成了搜索和代码解释器功能。此次发布是继**智谱 AI (Z.ai)**、**MiniMax**
  和 **Kimi** 等中国实验室更新大模型之后的又一动作。该模型采用 **Apache-2.0** 协议授权，预计将是 **DeepSeek v4** 发布前的最后一个重大版本。新闻最后还提到
  **Pete Steinberger** 已加入 **OpenAI**。'
id: MjAyNi0w
models:
- qwen3.5-397b-a17b
- qwen3.5-plus
- qwen3-max
- qwen3-vl
- kimi
people:
- pete_steinberger
- justinlin610
title: Qwen3.5-397B-A17B：Open-Opus 级别中最小且非常高效的模型。
topics:
- native-multimodality
- spatial-intelligence
- sparse-moe
- long-context
- model-quantization
- model-architecture
- model-deployment
- inference-optimization
- apache-2.0-license
---

**恭喜 Qwen！**

> 2026年2月13日至2026年2月16日的 AI 新闻。我们为你查阅了 12 个 subreddits、[544 个 Twitters](https://twitter.com/i/lists/1585430245762441216) 和 24 个 Discords（**261** 个频道和 **26057** 条消息）。预计节省阅读时间（以每分钟 200 词计）：**2606** 分钟。[AINews 网站](https://news.smol.ai/) 允许你搜索所有往期内容。提醒一下，[AINews 现在是 Latent Space 的一个板块](https://www.latent.space/p/2026)。你可以[选择订阅频率](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack)！

**来自 Qwen 的精彩发布。**

> *2026年2月13日至2026年2月16日的 AI 新闻。我们为你查阅了 12 个 subreddits、[544 个 Twitters](https://twitter.com/i/lists/1585430245762441216) 和 24 个 Discords（**261** 个频道和 **26057** 条消息）。预计节省阅读时间（以每分钟 200 词计）：**2606** 分钟。[AINews 网站](https://news.smol.ai/) 允许你搜索所有往期内容。提醒一下，[AINews 现在是 Latent Space 的一个板块](https://www.latent.space/p/2026)。你可以[选择订阅频率](https://support.substack.com/hc/en-us/articles/8914938285204-How-do-I-subscribe-to-or-unsubscribe-from-a-section-on-Substack)！*

恭喜 [Pete Steinberger 加入 OpenAI](https://x.com/sama/status/2023150230905159801)，正如我们[预测](https://www.latent.space/p/ainews-sci-fi-with-a-touch-of-madness)的那样。对此没有更多补充，所以我们就不多说了。

今天的头条新闻是 Qwen 3.5。继 [Z.ai](https://www.latent.space/p/ainews-zai-glm-5-new-sota-open-weights)、[Minimax](https://www.latent.space/p/ainews-new-gemini-3-deep-think-anthropic) 和 [Kimi](https://www.latent.space/p/ainews-moonshot-kimi-k25-beats-sonnet) 等其他中国模型实验室更新其旗舰模型之后，Qwen 也发布了更新。但与前两者不同，Qwen 3.5 与 Kimi 属于同一量级，拥有 400B 参数，稀疏率约为 4.3%，而 Kimi 的稀疏率则更为激进，为 3.25%。他们并未声称在所有领域都达到了 SOTA，特别是在 coding benchmarks 方面，但相比 [Qwen3-Max](https://news.smol.ai/issues/25-09-05-1t-models) 和 [Qwen3-VL](https://news.smol.ai/issues/25-09-23-alibaba-yunqi)，它取得了坚实的进步。

Native Multimodality 和 [Spatial Intelligence](https://qwen.ai/blog?id=qwen3.5#spatial-intelligence) 是该模型的主打特性，我们建议点击进入博客查看示例，除此之外没有太多可说的——这是来自中国最多产的开源模型实验室的一次备受欢迎的旗舰模型更新，也可能是 DeepSeek v4 发布前的最后一次。

![](https://substackcdn.com/image/fetch/$s_!1fDP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0472c69a-cd07-4bde-8b10-61bc1d0702a7_2444x1704.png)

---

# AI Twitter 回顾

**阿里巴巴 Qwen3.5 开源权重“前沿 MoE”发布（以及推理/基础设施方面的影响）**

- **Qwen3.5-397B-A17B 发布**：阿里巴巴发布了 **Qwen3.5-397B-A17B**，定位为 Qwen3.5 系列中首个开源权重的模型：**原生多模态**，“思考与非思考模式”，**混合线性注意力 (Linear Attention) + 稀疏 MoE**，“大规模 RL 环境缩放”，支持 **201 种语言**，采用 **Apache-2.0** 协议（[官方公告](https://twitter.com/Alibaba_Qwen/status/2023331062433153103)；[@JustinLin610](https://twitter.com/JustinLin610/status/2023332446713070039) 也同步转发）。他们还澄清 **Qwen3.5-Plus 是同一基础模型的托管 API 版本**，具备 **1M 上下文**（对比模型原生的 **256K**）以及搜索/代码解释器 (Code Interpreter) 集成（[澄清说明](https://twitter.com/JustinLin610/status/2023340126479569140)）。
- **架构 + KV-cache 的影响**：社区讨论集中在 **Gated Delta Networks / “GatedDeltaNet” + 稀疏 MoE**，认为这是推理在长上下文下保持可行性的原因。vLLM 提供了 **首日支持 (day-0 support)**，并强调了 **总参数 397B、激活参数 17B**、多模态以及吞吐量/延迟优势（[vLLM recipe](https://twitter.com/vllm_project/status/2023341059343061138)）。一个具体的 KV-cache 粗略计算表明，由于较少的 KV heads 和大量的 gated-delta 层，在 BF16 格式下每 token 仅需 **~31KB**，而在 **262K 上下文时仅需 ~8.05GB KV**（FP8 下约为 4GB）（[KV 计算公式](https://twitter.com/bnjmn_marie/status/2023424404504342608)）。
- **部署现状：权重巨大，但运行效果令人惊讶**：尽管权重规模达到“~800GB BF16”，但已有报告称可以通过 MLX/Q4 在 Apple Silicon 上本地运行（例如提到的 **~225GB RAM**）（[mlx 报告](https://twitter.com/pcuenq/status/2023369902011121869)；[awnihannun 演示](https://twitter.com/awnihannun/status/2023462412092059679)）。Unsloth 发布了“在 **256GB Mac/RAM** 上运行 4-bit”的指南，并声称其性能可与顶级闭源模型持平（虽是营销口号，但对推广很重要）（[Unsloth](https://twitter.com/UnslothAI/status/2023338222601064463)）。Ollama 也迅速将其上线到云端（[Ollama](https://twitter.com/ollama/status/2023334181804069099)）。
- **基准测试 + “智能体 RL (agentic RL)”与效率疑问**：早期评价认为它比 Qwen3-*Max* 和之前的 Qwen VL 模型有了显著提升，尤其是在 **Vision**（视觉）方面；也有人要求提供“推理效率”的证据，而非仅仅是原始分数（[scaling01](https://twitter.com/scaling01/status/2023343368399704506)）。teortaxesTex 指出，它的得分出人意料地超过了 Qwen3-Max-thinking 的部分报告结果，并推测改进源于 **智能体 RL (agentic RL)**（[评论](https://twitter.com/teortaxesTex/status/2023331885402009779)）。与此同时，也存在“黑盒评估”的批评以及特定任务的失败（例如 SVG / “Vending-Bench” 风格的测试）（[Vending-Bench 声明](https://twitter.com/andonlabs/status/2023450768406364238)；[SVG 对比](https://twitter.com/scaling01/status/2023364296277721300)）。
- **定价争议**：多篇帖子认为，考虑到推理效率的声明，阿里巴巴的 **API 定价偏高/奇特**，并将其与 Kimi/GLM 的方案进行了对比（[定价投诉](https://twitter.com/scaling01/status/2023346718377406840)；[更多](https://twitter.com/scaling01/status/2023349177443377370)）。这成为了一个反复出现的主题：“模型很棒，但服务成本情况不明。”

**开放 Agent、“评测框架工程 (harness engineering)”以及 OpenClaw → OpenAI 的传奇故事**

- **OpenClaw 作为单人杠杆作用的证明**：OpenClaw 的故事被视为“单人团队 + 编码 Agent”极速交付改变世界之作的象征，最终以 Peter Steinberger 加入 OpenAI 或被其收购而告终 ([Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/2023248474503094774))。这一事件也引发了关于 OpenAI 在收购后将如何处理开源问题的广泛讨论。
- **Anthropic 与开源社区的紧张关系**：一个主要的讨论集群批评了 Anthropic 对开源和 OpenClaw 使用的态度，声称其限制/封锁措施迫使开发者转向其他模型或供应商 ([ThePrimeagen](https://twitter.com/ThePrimeagen/status/2023194211445834132); [Teknium](https://twitter.com/Teknium/status/2023251135201738794))。也有人淡化了其战略影响（认为“一周内就能凭感觉（vibe-coded）写出来”），但承认这在 OSS 圈子中造成了名誉损失 ([scaling01](https://twitter.com/scaling01/status/2023217588319277471))。另外，Anthropic 宣布了一项重大业务扩张：设立 **Bengaluru 办公室**，并指出印度已成为 Claude.ai 的**第二大市场** ([Anthropic](https://twitter.com/AnthropicAI/status/2023322514206957688))。
- **Harness 是真正的护城河**：多条推文汇聚成一个务实的论点：**Agent 不仅仅是模型**；“Harness”（工具链、上下文管理、生命周期、技能、评估/可观测性）是不断累积的基础设施，且日益成为差异化的关键。参见 Ben Burtenshaw 对 Harness 的定义——围绕模型的“OS”，以及一种观点：之所以觉得闭源 Agent 更好用，部分原因是模型是在其 Harness 模式上进行训练的 ([ben_burtenshaw](https://twitter.com/ben_burtenshaw/status/2023429103731269696))。构建 Agent 系统的从业者对此表示共鸣：“构建一个好的 Harness 很难，且需要随时间不断积累” ([brivael](https://twitter.com/brivael/status/2023203131329503583))。
- **轻量级 Agent 替代方案**：在“重型 Harness”思潮之外，人们对极简 Agent 栈也产生了兴趣：PicoClaw 和 nanobot 被推崇为 OpenClaw 的极简替代方案，支持多种模型后端以及 MCP/vLLM ([TheTuringPost](https://twitter.com/TheTuringPost/status/2023416488884129826))。
- **Agent 的可观测性/评估（evals）正成为基本门槛**：LangChain/LangSmith 传递了一个信息：对于 Agent 而言，Trace（追踪）就是新的“堆栈追踪（stack trace）”，调试需要可观测性优先的工具 ([meetup](https://twitter.com/LangChain/status/2023457846843551946); [tracing plug-ins](https://twitter.com/LangChain/status/2023532973086159283))。这与广泛的抱怨相吻合，即当前的 Agent 行为缺乏确定性且需要人工照看。

**OpenAI/Codex 使用量激增、子 Agent 以及安全加固**

- **Codex 采用率的说法**：Sam Altman 报告称 **Codex 周活跃用户自年初以来增长了三倍** ([sama](https://twitter.com/sama/status/2023233085509410833))。多个社区帖子描述了 **Codex 5.3** 的“巨大飞跃”，特别是通过并行化/子 Agent 实现的提升 ([gdb](https://twitter.com/gdb/status/2023299087974777061); [“agents are up”](https://twitter.com/gdb/status/2023342301821734937))。
- **子 Agent 配置 + 模型层级权衡**：实用技巧：通过编辑配置（例如 `max_threads = 24`）增加 Codex 子 Agent 数量，作为高级用户（Pro-user）的优化手段被分享 ([Hangsiin](https://twitter.com/Hangsiin/status/2023297599764402627))。与此同时，至少有一位用户报告 **5.3-codex-spark** 虽然速度更快，但在实际工作中比完整版 5.3 “更笨” ([giffmana](https://twitter.com/giffmana/status/2023341811851473053))。
- **ChatGPT 的锁定模式（Lockdown Mode）**：OpenAI 引入了 **Lockdown Mode**，通过禁用或更改工具行为（缓存浏览、减少网络交互）来降低提示词注入（prompt-injection）和数据外泄风险，该功能首先面向企业版/商业版（Enterprise/Business）推出，随后面向消费者 ([cryps1s](https://twitter.com/cryps1s/status/2023441322838028362))。这标志着产品层面承认了**启用工具的 LLM 扩大了攻击面**，且某些机构即使以牺牲能力为代价，也需要确定性的、限制性的控制。
- **科学结论的审查**：一个推特串对归功于 GPT-5.2 的 OpenAI 物理研究结果提出了可复现性质疑，认为如果使用了非公开模型，期刊应要求提供对话记录/工具细节 ([lewtun](https://twitter.com/_lewtun/status/2023334667064099207))。Kevin Weil 转发了相关物理学家的更多解释 ([kevinweil](https://twitter.com/kevinweil/status/2023422106411974935))，gdb 也发布了一篇“来龙去脉”的后续报道 ([gdb](https://twitter.com/gdb/status/2023445830880117214))。

**中国模型的“假期模型浪潮”：Qwen3.5, GLM-5, MiniMax M2.5, Seed/Seedance——以及机器人学的加速**

- **春节作为发布季**：多条帖子将春节（CNY）定义为新的“模型发布周”，涉及的模型包括 **Qwen3.5**、**GLM-5**、**MiniMax M2.5**，以及备受期待的 **DeepSeek-V4** ([iScienceLuvr](https://twitter.com/iScienceLuvr/status/2023312965756449088); [Yuchenj_UW roundup](https://twitter.com/Yuchenj_UW/status/2023453819938763092))。
- **MiniMax M2.5：吞吐量 + RL 信号效率**：SemiAnalysis 报告称，M2.5 在 vLLM 框架下的 **8×H200** 上，在特定 TTFT 约束下维持了 **~2500 tok/s/GPU** 的吞吐量 ([SemiAnalysis_](https://twitter.com/SemiAnalysis_/status/2023418414203646066))。MiniMax 强调 **逐 Token 过程奖励（per-token process rewards）** 能够更好地利用 RL 信号并提高成本效率，并庆祝了其 API 与合作伙伴的广泛可用性 ([MiniMax_AI](https://twitter.com/MiniMax_AI/status/2023470874708549941))。
- **字节跳动 Seed/Seedance 与 AI 电影**：Seedance 2.0 凭借一部由该模型制作的 **贾樟柯** 短片成为文化热点 ([FrankYan2](https://twitter.com/FrankYan2/status/2023257752017981446); [EHuanglu](https://twitter.com/EHuanglu/status/2023449238114320514))。核心观点：视频生成正从“玩具演示”向“电影制作人工作流”转变，且有观众指出视频输出比图像生成更少一些“审美引导带来的诡异感（aesthetic-guidance uncanny）” ([jd_pressman](https://twitter.com/jd_pressman/status/2023256826431852852))。
- **机器人：宇树科技（Unitree）及更广泛的中国领先叙事**：帖子强调了宇树科技的人形机器人在春晚上的表现，以及关于中国机器人技术飞速发展的更广泛论断 ([HumanoidHub](https://twitter.com/TheHumanoidHub/status/2023428892934160775); [kimmonismus](https://twitter.com/kimmonismus/status/2023388741595799687))。teortaxesTex 认为我们已经度过了“波特金（Potemkin）”式的怀疑阶段——整个行业（而不仅仅是极少数个例）都是真实存在的，尤其是在机器人领域 ([teortaxesTex](https://twitter.com/teortaxesTex/status/2023518524451549598))。
- **算力供应链信号**：据报道，由于企业需求旺盛，西部数据（Western Digital）已售罄 **2026 年大部分 HDD 产能**，部分 AI 客户甚至预订到了 2027/2028 年 ([kimmonismus](https://twitter.com/kimmonismus/status/2023374704006828513))。另外，据厂商宣称，NVIDIA 的 GB300 NVL72 与 Hopper 相比，其 **每兆瓦性能（performance/MW）高出约 50 倍**，**每 Token 成本（cost/token）降低约 35 倍** ([kimmonismus](https://twitter.com/kimmonismus/status/2023456488782487566))。

**工程师关注的研究/工程方向（Agent、RL、可解释性和评估规范）**

- **多步工具调用依然脆弱**：SciAgentGym 显示，随着工具交互步骤的增加，成功率会大幅下降；在工具依赖图（SciForge）上进行数据合成，提升了 8B 模型在科学工作流上的表现 ([dair_ai](https://twitter.com/dair_ai/status/2023404773031166320))。这与 Agent 在日常中的痛点相符：瓶颈在于执行的可靠性，而非单步推理。
- **Agent 的自适应推理深度**：CogRouter 能够逐步动态调整“认知深度”；据总结，它在 Agent 基准测试中以 **减少 62% Token** 的代价击败了 GPT-4o ([omarsar0](https://twitter.com/omarsar0/status/2023405531835277504))。
- **基于评分表（Rubric）的 RL（超越可验证领域的 RLVR）**：一篇关于基于评分表 RL 的长文追溯了从 LLM-as-judge 到结构化评分表的发展历程，并提供了涵盖 15 篇以上论文的实用技巧 ([cwolferesearch](https://twitter.com/cwolferesearch/status/2023408158065188894))。
- **可解释性目标**：MonoLoss 提出了一种插件式目标函数，旨在促进 CLIP/SigLIP2/ViTs 中 SAEs 的 **单语义（monosemantic）** 激活，提高了许多潜变量的 “MonoScore” ([iScienceLuvr](https://twitter.com/iScienceLuvr/status/2023303520057745501))。
- **基准测试污染 / “局部泛化”**：人们再次强调，训练数据扩展和语义近重复可能会混淆基准测试带来的提升。一种提议的分解方式包括：benchmaxxing、usemaxxing、隐性插值（hidden interpolation）与真实的 OOD 泛化 ([g_leech_](https://twitter.com/g_leech_/status/2023384075537432662))。这与 Lucas Beyer 早期在视觉数据去重方面的经验，以及在语言模型中“正确”执行去重的难度相契合 ([giffmana](https://twitter.com/giffmana/status/2023481657177911383))。
- **WeirdML 时间跨度**：受 METR 启发，对 WeirdML 任务的“时间跨度（time horizon）”评估显示，前沿模型的时间跨度从 **~24 分钟 (GPT-4)** 到 **~38 小时 (Opus 4.6)** 不等，且 **翻倍时间约为 5 个月** ([htihle](https://twitter.com/htihle/status/2023349189271572975))，这一结果被认为与 METR 类似的评估结果大体一致 ([scaling01](https://twitter.com/scaling01/status/2023350946139435357))。

**宏观主题：开源与闭源、劳动力/教育影响，以及作为新瓶颈的“审美/品味”**

- **开放模型势头 vs 权力集中风险**：一种反复出现的观点认为，开放模型减少了权力集中，并保留了多条 AGI 路径的可行性 ([TuringPost 剪辑](https://twitter.com/TheTuringPost/status/2023375354740809823))。与此同时，关于 ToS 约束的争论也在激烈进行（例如 Anthropic 限制监控/武器用途），以及这是否会使供应商成为“供应链风险” ([RyanPGreenblatt](https://twitter.com/RyanPGreenblatt/status/2023524096592802207); [kimmonismus Axios 摘要](https://twitter.com/kimmonismus/status/2023419652378955809))。
- **劳动力颠覆时间线**：Ryan Greenblatt 认为大规模失业“在 2 年内被高估，在 7 年内被低估”，关键的拐点在于 **AI R&D 的完全自动化**（此后人类认知劳动价值将迅速崩塌）([推特原文](https://twitter.com/RyanPGreenblatt/status/2023219133916332070))。
- **教育/技能焦虑**：关于学位可能在学生毕业前就已过时的说法（通过一条新闻摘要推文流传）反映了更广泛的不确定性 ([kimmonismus](https://twitter.com/kimmonismus/status/2023446044873560178))。此外还有警告称，受控研究显示 AI 编程工具可能会降低对技能的掌握程度（通过 Anthropic 研究链接摘要）([dl_weekly](https://twitter.com/dl_weekly/status/2023502798659125656))。
- **“品味”与验证作为核心技能**：这一观点强调，随着模型/Agent 规模的扩大，**品味**（选择好的问题/解决方案）和**验证能力**（检测细微的错误）将成为最稀缺的人类差异化优势——被明确标注为“一种新的核心技能” ([gdb](https://twitter.com/gdb/status/2023481258639286401); [Yuchenj_UW](https://twitter.com/Yuchenj_UW/status/2023481799335440792))。Karpathy 将此延伸到编程语言/形式化方法（formal methods）：翻译和重构将占据主导地位，我们可能会反复重写大量软件；“对 LLM 最优”的语言成为了一个开放性问题 ([karpathy](https://twitter.com/karpathy/status/2023476423055601903))。

---

### 热门推文（按互动量排序）
- **旧金山步行便捷性讨论**：[@paularambles](https://twitter.com/paularambles/status/2023220064070332521)  
- **Anthropic 班加罗尔办公室 / 印度成为第二大市场**：[@AnthropicAI](https://twitter.com/AnthropicAI/status/2023322514206957688)  
- **Qwen3.5-397B-A17B 发布（Apache-2.0，多模态 MoE，17B 激活参数）**：[@Alibaba_Qwen](https://twitter.com/Alibaba_Qwen/status/2023331062433153103)  
- **PL/FM + LLM 重塑软件翻译/重写**：[@karpathy](https://twitter.com/karpathy/status/2023476423055601903)  
- **“Anthropic 仇视开源”的疯传观点**：[@ThePrimeagen](https://twitter.com/ThePrimeagen/status/2023194211445834132)  
- **Codex 增长声明**：[@sama](https://twitter.com/sama/status/2023233085509410833)


---

# AI Reddit 摘要

## /r/LocalLlama + /r/localLLM 摘要

### 1. Qwen 3.5 模型的发布与性能

  - **[Qwen3.5-397B-A17B 发布了！！](https://www.reddit.com/r/LocalLLaMA/comments/1r656d7/qwen35397ba17b_is_out/)** (活跃度: 973): ****Qwen3.5-397B-A17B** 已在 [Hugging Face](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) 上发布，这是一个拥有 `3970 亿`参数的模型，原生上下文长度为 `262,144` 个 token，最高可扩展至 `1,010,000` 个 token。该模型是 Qwen 系列的一部分，以其大规模语言处理能力而闻名。此外，GGUF 版本可在 [此处](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF) 获取，它可能为特定用例提供优化后的性能。** 社区对该模型的性能充满了期待和好奇，用户们正跃跃欲试，准备测试其各项能力。

    - Qwen3.5-397B-A17B 模型拥有 `262,144` 个 token 的原生上下文长度，并可扩展至 `1,010,000` 个 token。这是在处理超大上下文方面的显著改进，使其适用于需要大量输入数据的复杂任务。
    - 据报告，Qwen3.5-397B-A17B 的解码吞吐量比其前代 Qwen3-235B-A22B 快 `3.5倍` 到 `7.2倍`。吞吐量的增加表明处理效率有了实质性提升，这可能为大规模应用带来更快的响应速度并降低计算成本。
    - 一位用户分享了该模型在 [Hugging Face](https://huggingface.co/unsloth/Qwen3.5-397B-A17B-GGUF) 上的 GGUF 版本链接，表明该模型已可供下载和实验。这种易获取性有助于更广泛的测试和集成到各种项目中。

  - **[Qwen3.5-397B-A17B Unsloth GGUFs](https://www.reddit.com/r/LocalLLaMA/comments/1r6599e/qwen35397ba17b_unsloth_ggufs/)** (活跃度: 663): ****Qwen3.5-397B-A17B** 是 **Alibaba** 最新发布的一个拥有 `3970 亿`参数的模型，专为多模态推理设计。它能够在拥有 `192GB RAM` 的 `Mac` 上以 `3-bit` 运行，或在拥有 `256GB RAM` 的 `M3 Ultra` 上以 `4-bit (MXFP4)` 运行。在指令遵循、多语言知识和视频推理等基准测试中，该模型的性能定位与 **Gemini 3 Pro**、**Claude Opus 4.5** 和 **GPT-5.2** 相当。此次发布包含动态 GGUF 以实现灵活部署，并提供了一份在各种硬件配置上运行该模型的指南。更多详情可见 [Hugging Face](https://huggingface.co/Qwen/Qwen3.5-397B-A17B) 和 [Unsloth](https://unsloth.ai/docs/models/qwen3.5)。** 评论者对该模型的规模和能力印象深刻，并特别注意到其 `3970 亿`的总参数量以及每次仅有 `170 亿`参数处于激活状态的特点。大家也很好奇 **AutoRound** 会如何增强该模型的性能。

    - Qwen3.5-397B-A17B 模型被注意到其生成内容较为详尽（verbosity），正如它在生成简单的问候响应时表现出的广泛内部对话所证明的那样。这种特性可能表明模型具有复杂的决策过程，这对处理细微任务可能有益，但也可能导致简单交互中的效率低下。
    - 一位用户对 AutoRound 功能在 Qwen3.5-397B-A17B 模型上的表现表示好奇，特别是考虑到该模型仅有 170 亿激活参数。这表明用户关注点在于如何在有效管理计算资源的同时优化性能，这对于在实际应用中部署大型模型至关重要。
    - 有关于 UD-Q4_K_XL 和 MXFP4 格式对比性能的讨论，一位用户指出目前缺乏直接对比这两者的基准测试。这凸显了现有性能数据的空白，而这些数据对于在模型部署和优化策略上做出明智决定至关重要。

### 2. 本地 LLM 的挑战与创新

  - **[为什么运行本地 LLM 仍然如此痛苦](https://www.reddit.com/r/LocalLLM/comments/1r5matd/why_is_running_local_llms_still_such_a_pain/)** (热度: 243): **该帖子讨论了在个人硬件上运行本地大语言模型（LLM），如 **Ollama** 和 **Llama** 的挑战，重点提到了安装失败以及处理大于 `7B` 参数的模型时的资源限制。用户对自托管解决方案的复杂性表示沮丧，这些方案通常需要 Docker 和 Kubernetes 等领域的深厚技术知识，并且缺乏能够替代 **OpenAI** 的 **ChatGPT** 且兼顾隐私与功能性的方案。** 评论者指出，由于巨大的硬件需求，在本地实现 ChatGPT 级别的功能本身就非常困难，并建议虽然像 **LM Studio**、**Ollama** 或 **Lemonade** 这样的工具可以轻松安装，但性能高度依赖于是否拥有强大的 GPU 或 NPU。他们强调，如果没有大量的硬件投入，本地 LLM 将会运行缓慢，且如果不使用远程提供商，可能无法实现完整的 ChatGPT 功能。

    - No_Clock2390 强调，只要有合适的硬件，运行本地 LLM 是可行的，并提到了 LM Studio、Ollama 和 Lemonade 等可以快速设置的工具。然而，性能严重依赖于硬件能力，特别是 GPU 或 NPU 的存在。例如，在 Intel N100 上运行 Ollama 是可能的，但由于 CPU 限制，会导致性能低下。
    - Total-Context64 强调了在本地实现类 ChatGPT 功能的成本壁垒，指出除非选择远程提供商，否则必须在硬件上进行大量投资。这凸显了在没有充足资源的情况下复制高性能 LLM 的挑战。
    - HorribleMistake24 建议初学者使用 LM Studio，它可以辅助确定模型与可用 GPU VRAM 的兼容性。他们还提到通过 VS Code 中的 Codex 利用 ChatGPT 订阅来辅助设置，展示了一种通过将 AI 工具集成到开发过程中来克服配置挑战的实用方法。

### 3. MiniMax-2.5 与 Openclaw 讨论

  - **[真的有人在用 Openclaw 吗？](https://www.reddit.com/r/LocalLLaMA/comments/1r5v1jb/anyone_actually_using_openclaw/)** (热度: 1615): **该 Reddit 帖子质疑了 Openclaw 受欢迎程度的真实性，认为这可能是社交媒体刻意营销的结果，特别是在 **OpenAI 收购** Openclaw 之后。帖子引用了一个可疑的增长图表 [点击此处查看](https://www.star-history.com/#openclaw/openclaw&amp;Comfy-Org/ComfyUI&amp;type=date&amp;legend=top-left)。根据用户体验，**Openclaw** 被描述为一个连接各种 API 和 MCP 服务器的工具，但缺乏创新。OpenAI 以 `100 亿美元` 收购该公司的行为受到了怀疑，并被拿来与加密货币市场中炒作驱动的性质作类比。** 评论表达了对 Openclaw 营销策略的怀疑，一些用户将其描述为“vibe coded”（仅靠氛围堆砌的代码），缺乏独特的功能。人们对 **Ironclaw** 等替代方案表现出兴趣，表明市场需要更强大的解决方案。

    - Skystunt 指出，Openclaw 本质上是现有技术的汇编，连接了各种 API 和 MCP 服务器，没有提供任何突破性功能。这表明其感知价值可能被夸大了，因为它并没有引入新的能力，而只是集成了现有的能力。
    - dgibbons0 强调了 Openclaw 的配置质量较差，将其描述为 “vibe coded”。这个词暗示其设置缺乏专业的打磨或鲁棒性。评论者还表示有兴趣探索相关项目 Ironclaw，这表明尽管 Openclaw 存在缺陷，但将聊天与 AI 引擎集成的概念仍具有吸引力。
    - TurnUpThe4D3D3D3 提出了使用 Openclaw 的财务影响问题，指出其默认有 30 分钟的心跳（heartbeat），每次运行时都会产生 API 成本。随着时间的推移，这可能会导致显着的支出，每周可能达到几美元，而用户可能无法立即察觉到这一点。

  - **[你可以在本地运行 MiniMax-2.5](https://www.reddit.com/r/LocalLLaMA/comments/1r5h1gj/you_can_run_minimax25_locally/)** (热度: 784): **图片提供了关于在本地运行 MiniMax-2.5 模型的详细指南，强调了其在编程、Agentic（智能体）工具使用和办公任务方面的顶尖性能。该模型拥有 `230B 总参数`，其中 `10B 为激活参数`，具备 `200K 上下文窗口`，其未量化的 bf16 版本需要 `457GB` 内存。使用 **Unsloth Dynamic 3-bit GGUF** 可将模型大小显著减小至 `101GB`，降幅达 `62%`，使其更易于本地部署。指南还包含了 [官方文档](https://unsloth.ai/docs/models/minimax-2.5) 和 [Hugging Face 上的 GGUF 模型](https://huggingface.co/unsloth/MiniMax-M2.5-GGUF) 的链接。** 评论反映了对在本地运行如此庞大模型的实用性的怀疑，用户幽默地指出了部署该模型所需的高昂硬件要求和成本。

    - Ug1bug1 提到，包括 Q3_K_XL 在内的 MiniMax 模型在他们的 Strix Halo 设备（配备 128GB RAM）上表现良好。这表明该模型在高配硬件上的表现令人满意，意味着充足的内存是有效运行这些模型的关键要求。

## 非技术性 AI Reddit 子版块回顾

> /r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT, /r/ChatGPTCoding, /r/aivideo, /r/aivideo

### 1. AI 模型发布与基准测试

  - **[你在期待什么？](https://www.reddit.com/r/singularity/comments/1r5p4qi/what_are_you_looking_forward_to/)** (热度: 954): **图片是来自 **CHOI (@arrakis_ai)** 的一条推文，宣布即将发布几个 AI 模型：**DeepSeek V4, Gemini 3.1 Pro, GPT 5.3, Sonnet 5** 以及一个“神秘模型”。该推文强调了 AI 开发时间线的迅速加速，暗示这些发布预计将在几天内完成。这表明 AI 模型开发进入了一个显著的进步和竞争时期，可能对各种应用和行业产生影响。** 一条评论对 Sonnet 5 的发布表示怀疑，提到了之前的传言最终证明是关于 Opus 4.6 的。另一条评论暗示了竞争氛围，提到了“埃隆（Elon）因缺乏某物而崩溃”，可能指的是 AI 进展方面的竞争。

- johnwheelerdev 提到了对 **Gemini 3.1** 的期待，暗示这可能是一个重大的更新或发布。这可能意味着相比之前版本的改进或新功能，尽管目前尚未提供具体的细节或基准测试。
- GraceToSentience 提到了关于 **Sonnet 5** 的传闻，此前该版本被认为是 **Opus 4.6**。这表明在版本控制或产品命名上可能存在混淆或更名，突显了追踪软件更新和发布的挑战。
- Egoz3ntrum 提出了 **GPT-OSS-2**，这可能是 GPT 模型的一个开源变体。这表明了 AI 模型趋向开源的趋势，可能提供更高的透明度和社区驱动的改进。

- **[Google 称攻击者在尝试克隆 Gemini 时对其进行了超过 100,000 次提示](https://www.reddit.com/r/singularity/comments/1r5d9jw/attackers_prompted_gemini_over_100000_times_while/)** (热度: 1342): **Google** 报告称，攻击者试图通过一种称为 *model distillation* 的技术，对 **Gemini AI** 模型进行超过 `100,000` 次提示来克隆该模型。这种方法涉及向模型输入特定的提示词以收集回复，从而在不直接访问模型代码或训练数据的情况下创建一个更便宜的仿制品。Google 将此类活动视为“知识产权窃取”，并已实施了未公开的对策。更多详情请参阅 [原文](https://arstechnica.com/ai/2026/02/attackers-prompted-gemini-over-100000-times-while-trying-to-clone-it-google-says/)。** 一些评论者质疑 *model distillation* 的有效性，将其与 90 年代通过输入数百万场对局来改进国际象棋软件的尝试进行对比，认为后者并无显著影响。另一些人则指出 Google 在知识产权立场上的讽刺性，因为其自身也使用网页抓取数据来训练 LLM。

    - Deciheximal144 强调了 Google 将“模型提取”视为知识产权窃取的讽刺性，因为 Google 自己的 LLM 也是在未经明确许可的情况下，利用从互联网抓取的数据进行训练的。这引发了关于 AI 训练过程中数据使用和所有权的伦理问题，正如 [The Verge](https://www.theverge.com/2023/7/5/23784257/google-ai-bard-privacy-policy-train-web-scraping) 中所讨论的那样。
    - magicmulder 质疑“模型提取”的有效性，将其与 90 年代试图通过输入数百万场对局来改进国际象棋软件的努力进行类比，认为后者并未产生重大影响。这表明，仅仅通过大量提示 AI 模型是否能产生高质量的克隆品仍存疑，因为模型训练的复杂性不仅仅取决于输入数据的数量。
    - Ok_Buddy_9523 幽默地淡化了“提示 AI 100,000 次”这一说法，将其比作日常活动，暗示在 AI 开发和测试的背景下，这样的交互次数可能并不像想象中那样显著或异常。

- **[Codex-cli 配合 GPT-5.3 codex xhigh —— 5 小时内用汇编代码制作了一个功能完备的 GBA 模拟器！](https://www.reddit.com/r/singularity/comments/1r525lg/codexcli_with_gpt53_codex_xhigh_5_hours_made_a/)** (热度: 717): **一位用户声称使用 **Codex-cli 配合 GPT-5.3 codex xhigh** 在 `5 小时` 内用汇编代码开发了一个功能完备的 Game Boy Advance (GBA) 模拟器。该项目托管在 [GitHub](https://github.com/Healthy-Nebula-3603/gpt5.2-codex_xhigh-proof-of-concept-GBA-emulator-in-assembly-) 上，涉及模型自主构建、测试和调试模拟器。该模拟器的架构包括一个 x86-64 汇编核心，以及一个用于 SDL2 的极简 C 宿主层，目标是兼容 SuperMarioAdvance 等游戏。规划的大纲包括 ARM7TDMI CPU 核心模拟、内存映射以及 PPU/APU 功能，重点关注确定性和性能基准，例如在 Linux x86-64 上达到 `59.7 FPS`。该项目强调采用纯汇编方法，并使用 C 平台 shim 进行 SDL2 集成。** 评论者对模拟器的性能和成本表示怀疑和好奇，其中一人指出，近期有说法称 LLM 无法生成底层代码，这显得十分讽刺。另一位评论者对这一成就印象深刻，并强调如果没有类似的先例，这将是非常独特的。

- stardoge42 询问了 Credits (积分) 成本以及模拟器的性能，询问是否存在任何故障 (glitches) 以及是否适用于其他游戏。这突显了使用 AI 生成代码的实际考虑因素，例如资源消耗和跨不同软件环境的兼容性。
- cottsay 提到了一个类似的项目，即 6 年前在 GitHub 上发布的 “ARM Assembly 版 Gameboy 模拟器”。这种对比为模拟器开发的发展演变，以及使用 Codex-cli 配合 GPT-5.3 等 AI 工具可能带来的技术进步提供了背景。
- BrennusSokol 提到，有人对 AI 生成低级语言或机器代码的能力表示怀疑，而使用汇编代码成功创建 GBA 模拟器的事实反驳了这一点。这反映了关于 AI 在软件开发中能力的持续争论，特别是在生成复杂的低级代码方面。

### 2. Anthropic 与 OpenAI 的法律和伦理紧张局势

- **[Anthropic 的道德立场：随着不和升级，五角大楼警告 Anthropic 将“付出代价”](https://www.reddit.com/r/singularity/comments/1r6gyez/anthropics_moral_stand_pentagon_warns_anthropic/)** (活跃度: 1059)：**该帖子讨论了 AI 安全与研究公司 **Anthropic** 与 **五角大楼** 之间关于 AI 使用伦理指南的冲突。据报道，Anthropic 正在抵制五角大楼推动将 AI 应用于大规模监控和完全自主武器的做法，并主张建立伦理护栏 (ethical guardrails)。然而，五角大楼将这种抵制视为潜在的“供应链风险”，认为如果采购压力覆盖了伦理考量，可能会导致安全规范方面的“逐底竞争 (race to the bottom)”。这引发了关于 AI 应用中伦理界限应划在何处，以及谁有权设定这些界限的问题。** 评论者们强调支持 Anthropic 的立场，指出对 AI 设定伦理限制的重要性，例如禁止对美国公民进行监控以及禁止使用自主武器。人们对五角大楼的意图表示怀疑，一些人认为对美国人的监控已经在发生。

- **[独家：五角大楼威胁惩罚 Anthropic](https://www.reddit.com/r/ClaudeAI/comments/1r6hvx2/exclusive_pentagon_threatens_anthropic_punishment/)** (活跃度: 969)：**在国防部长 Pete Hegseth 的领导下，五角大楼威胁要将 **Anthropic** 标记为“供应链风险”，原因是双方在其 AI 模型 **Claude** 用于军事用途的问题上存在分歧。这一认定将迫使承包商切断与 Anthropic 的联系，由于 Claude 是目前唯一集成到机密军事系统中的 AI 模型，这将严重影响其业务。冲突源于五角大楼要求更广泛的使用权，这与 Anthropic 对隐私和自主武器的伦理担忧相冲突。[阅读更多](https://www.example.com)。** 评论者表达了对 Anthropic 在伦理化 AI 使用立场上的支持，批评五角大楼的施压具有潜在腐败性，并且偏袒像 Grok 和 Gemini 这样更顺从的 AI 公司。

    - Anthropic 关于限制使用其 AI 工具以防止大规模监控和自主武器的立场被视为一个重要的伦理立场。五角大楼对这些限制的抵制突显了伦理化 AI 使用与政府利用 AI 实现国防目的之间的紧张关系。这种情况强调了关于 AI 伦理和治理的更广泛辩论，特别是在国家安全背景下。
    - 讨论表明，Anthropic 的 AI 产品 Claude 被视为市场上的领先产品，可能对其他与政府实体关系更密切的 AI 公司构成威胁。这种对市场领导地位和伦理立场的认知可能会影响政府的压力，因为有迹象表明政府偏袒 Grok 和 Gemini 等其他 AI 公司。
    - 有一种观点认为，Anthropic 的伦理立场可以作为一种营销优势，吸引那些在 AI 部署中重视隐私和伦理考量的用户。这反映了消费者对负责任 AI 实践的认识和需求日益增长，这可能会影响市场动态和竞争定位。

- **[Anthropic 曾两次因该开发者的项目名称威胁要起诉他。现在他已加入 OpenAI，“利爪” (Claws) 🦞 正在向他们袭来 🤣🤣](https://www.reddit.com/r/OpenAI/comments/1r5vl11/anthropic_threatened_to_sue_the_guy_over_his/)** (热度: 1048)：**这张图片是一个梗图，幽默地描述了 Anthropic 与一名开发者之间因其项目名称而产生的法律纠纷，该纠纷最终导致该开发者加入了 OpenAI。图片中包含一段 Twitter 交流，重点展示了来自 Anthropic 的法律威胁，被称为“来自法务部的‘情书’”。该帖子暗示了 Anthropic 与 OpenAI 之间的竞争关系，该开发者转投 OpenAI 被视为后者的胜利。评论中讨论了 Anthropic 在模型开发上的战略重点，而 OpenAI 被认为更具产品导向，暗示 OpenAI 对该开发者的兴趣源于其快速创建爆款产品的能力。** 一些评论者对该开发者加入 OpenAI 的意义表示怀疑，质疑其项目的唯一性，并认为其他公司可以轻易复制。另一些人则认为 OpenAI 的招聘是一种反应性举措，暗示这可能不会带来实质性的改变。

    - **Portatort** 指出 **Anthropic** 专注于开发最先进的 AI 模型，这与 **OpenAI** 形成了对比，后者现在更加以产品为导向，旨在打造爆款产品。这表明了两家公司在战略目标上的分歧：Anthropic 优先考虑模型的卓越性，而 OpenAI 则专注于市场化应用。
    - **Inside_Anxiety6143** 质疑 **OpenClaw** 对 OpenAI 的重要性，并指出其创作者声称仅用了很短时间（“大约一个月就凭感觉写出来了 (vibecoded)”）。这提出了一个观点，即其他公司可能会迅速复制此类项目，从而质疑 OpenClaw 的唯一性或竞争优势。
    - **beigetrope** 认为 OpenAI 聘用 OpenClaw 的创作者可能是一种反应性举措，暗示这可能不会在公司内部产生实质性的变化。这一评论反映了对此类招聘对 OpenAI 整体方向战略影响的怀疑。

### 3. OpenClaw 安全与社区关注

- **[Sam Altman 正式确认 OpenAI 已收购 OpenClaw；Peter Steinberger 将领导个人 Agent 业务](https://www.reddit.com/r/OpenAI/comments/1r5rnbl/sam_altman_officially_confirms_that_openai_has/)** (热度: 2440)：**Sam Altman** 已确认 **OpenAI** 收购了 **OpenClaw**，**Peter Steinberger** 将加入并领导个人 Agent 的开发。OpenClaw 将转型为开源基金会，OpenAI 提供持续支持。这一举动表明 OpenAI 的战略重点是增强个人 Agent 能力，并利用 Steinberger 的专业知识。** 一些评论者推测，此次收购可能是一种防御性策略，旨在防止竞争对手获得 OpenClaw 的技术。其他人则质疑为什么 OpenAI 不在内部开发类似功能，暗示此次收购可能存在战略或资源方面的因素。

    - 提出的一个关键担忧是关于 OpenClaw 技术的访问权限，该技术最初是利用后门 CLI 访问开发的，这使得许多人负担不起。评论者质疑 OpenAI 将如何解决这些访问问题，并认为如果处理得当，将 OpenClaw 的技术集成到 OpenAI 的生态系统中可能会使访问权限民主化。
    - OpenAI 对 OpenClaw 的收购被视为防止竞争对手获取其技术的战略举措。这被称为“防御性收购”，表明 OpenAI 的主要动机可能是通过防止技术落入竞争对手之手来巩固其市场地位。
    - 人们对 OpenClaw 在 OpenAI 领导下（特别是 Peter Steinberger 掌舵后）的未来走向存在猜测。评论幽默地提到了可能出现的“ClosedClaw”情景，暗示 OpenAI 可能会限制访问或功能，类似于某些公司在收购后限制功能的做法。

---

# AI Discord Recap

> 由 gpt-5.1 生成的摘要之摘要的摘要


**1. 前沿、开源和区域模型：Qwen3.5, GLM‑5, MiniMax 2.5, Opus 4.6, Step 3.5 Flash**

- **Qwen3.5 & Qwen3.5‑397B A17B 称霸开源世界**：Alibaba 的 Qwen 团队发布了 **Qwen3.5‑397B‑A17B**，这是一个结合了 **linear attention + sparse MoE** 的开源权重模型，支持 **201 种语言**。该消息通过[其 Qwen3.5 帖子](https://xcancel.com/Alibaba_Qwen/status/2023331062433153103)发布，并在 Latent Space 和 HuggingFace 的 Discord 频道中引发关注。该模型在 GitHub 和 Hugging Face 上提供 Apache‑2.0 权重，并支持 API 访问。**Unsloth** 和 **Latent Space** 的用户将该模型视为新的 Benchmark 目标，调侃道 *“这就是 Qwen，我们在这里追求 Benchmax！”*，并分享了一些 weirdcore 以及高推理能力的 abliterated Qwen3‑30B 变体，例如 [**Qwen3‑30B‑A3B‑Claude‑4.5‑Opus‑High‑Reasoning‑2507‑ABLITERATED‑UNCENSORED‑V2**](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)。
  - 在 Bench 对话中，用户对比了 **Nemotron 30B A3B** 上的 **MXFP4** 量化与 **Q8_K_XL**，发现 MXFP4 相较于 bf16 具有更低的 **KL divergence**，并请求在旧模型中增加 MXFP4 支持；同时，其他人通过[这个实现分支](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main)在 **GPT‑NeoX** 中实验 **Qwen3 architecture**。与此同时，Eleuther 的研究频道详细分析了诸如[《Matrix‑Driven Identification and Reconstruction of LLM Weight Homology》](https://arxiv.org/abs/2508.06309)和[《Independence Tests for Language Models》](https://arxiv.org/abs/2502.12292)等论文，将 Qwen 系列模型作为重建 **finetuning trees**（微调树）及大型开源家族来源的主要案例研究。

- **GLM‑5、MiniMax 2.5 与 Windsurf 的模型自助餐**：在 **OpenClaw**、**Unsloth**、**GPU MODE** 和 **Windsurf** 社区中，用户对 **GLM‑5** 和 **MiniMax 2.5** 进行了压力测试。**GLM‑5** 被赞誉为 *“非常聪明且健谈”*，在稳定运行的情况下优于 **Kimi K2.5**；而 **MiniMax 2.5** 被描述为需要 **约 200 GB VRAM**（例如 **2× RTX 6000 Blackwell 96 GB**，速度为 **120–130 tok/s**）来驱动其 **200k context** 的 sparse‑MoE。**Windsurf** 通过[其更新](https://x.com/windsurf/status/2023536941451669586)宣布在产品中首发支持 **GLM‑5** 和 **MiniMax M2.5**，有效地将一个 IDE 变成了一个多供应商的前沿模型路由器。
  - Unsloth 用户将 MiniMax 2.5 与 **Opus 4.6** 进行了对比，辩论质量的跳跃是否能支撑其巨大的 VRAM 占用，而其他人则利用 **将 sparse MoE 权重卸载（offloading）到系统 RAM** 的技术，以牺牲速度换取容量。在 OpenRouter 的讨论中，从业者对比了 **GLM‑5 与 MiniMax 2.5 的 tool‑calling** 能力，发现 GLM 通常更适合 Agentic 工作流，而 MiniMax 在短交互中更快；一些人开始使用 **GLM 4.5 Air** 生成 **内核代码的 SFT 数据**，以廉价地引导出高质量的推理链（reasoning traces）。

- **Opus 4.6 与 Step 3.5 Flash 展现长上下文实力**：**Opus 4.6** 推出了 **1M‑token context** 以及显式的 **“check your work”** 验证环节。LMArena 用户通过输入[大型代码指令集](https://link.to.examples)进行了测试，确认模型在最终推理过程中能够忽略早先的错误。一位在 Perplexity 上对 **Claude via Opus 4.6** 进行基准测试的用户指出，Anthropic 的 **每小时使用限制**（例如 *“仅剩 18 条回复”*）是重度交互使用的实际限制因素，尽管 Opus 已经在严肃推理和编程任务中取代了 Perplexity。
  - 在 OpenRouter 方面，**Step 3.5 Flash** 在 [YouTube 基准测试](https://youtu.be/yvBbcLCZIhgye)中因其 *“越级打怪（punching above its weight）”* 的表现让用户印象深刻，但尽管其性价比极高，目前的托管支持率却依然很低。当 LMArena 用户发现请求被静默路由到 **“5.2”** 变体时，OpenAI 的路由策略遭到了抨击，这进一步加强了工程师们要求透明、版本锚定地访问长上下文、高推理模型的广泛趋势。


**2. Agent Stacks、规划框架以及多 Agent 系统**

- **OpenClaw 编排自主代理机构与视频通话**：开发者们展示了 **OpenClaw**，这是一个用于多 **Agent** 团队和现实世界操作的编排层，包括一个包含**技术主管、后端和前端机器人**的“机构服务器”（agency server），它们通过共享的 [planbot 资源库](https://github.com/MrMeatikins/planbot-resource)中的任务和计划进行协作。另一位用户让 OpenClaw 通过 SSH 接入 **Proxmox** 主机并获得完整的 **root 权限**，并报告了从 **Proxmox 6 → 8** 的端到端自主升级过程，包括重启和错误处理，展示了在 **Agentic Ops** 中生产级别的信任。
  - 一个独立的**视频通话模式**插件通过 [tavus.io](https://tavus.io) 将 **Tavus** 数字人连接到 OpenClaw 的 BYO **LLM** 聊天补全（chat-completions），使 **Agent** 能够实时跟踪**面部表情、手势和屏幕共享内容**。其他实验将 OpenClaw 的“潜意识”连接到本地微调的 **LLM**（该模型基于所有历史聊天记录进行训练，论文分享在 [Google Drive 文件夹](https://drive.google.com/drive/folders/1t9satvOV0QpHRkWSaP6C6bFgElvOAoPD)中），并使用了一个 SEO 流水线，该流水线抓取 YouTube，生成约 **300 多篇 Brian-Dean 风格的文章**，通过编辑子代理审核后存储待发布。

- **从 Claude Cowork 和 DSpy RLM 到 Triall 的模型对决**：在 **Latent Space** 的构建者频道中，一位成员展示了 **Claude Cowork** 如何编排流水线（例如自动将 Zoom 录音上传到 YouTube 频道），并提出了挑衅性的观点 *“Claude Cowork 可能是 AGI”*；而其他人则使用来自[此仓库](https://github.com/sandover/ergo)的 **Ergo** 规划技能来组织多步骤的功能开发。**DSpy** 贡献者推动了**递归语言模型（RLMs）**——如 [Omar Khattab 的推文](https://xcancel.com/lateinteraction/status/2022725370152190215)中所述——模型**编写代码来调用其他模型**，而不是依赖于平方注意力（quadratic attention）或单体 **Agent** 框架，并有一个具体的 [dspy-repl 原型](https://github.com/Archelunch/dspy-repl)在探索语言 + **REPL** 的选择如何影响 **RLM** 的准确性。
  - **Triall** ([triall.ai](https://triall.ai)) 作为构建在 [clash](https://github.com/clash-sh/clash) 之上的 GUI 出现在 OpenRouter 上，它允许用户让多个模型相互竞争进行生成、批判和改进，鼓励**对抗性推理而非盲目信任**。在框架层面， OpenAI Discord 实验了 **KOKKI**（一种结构化的自审计提示词，用于标记风险元素并切换模式），并讨论了映射到**模型预测控制（MPC）**的 **FORTRESS** 框架，其中“随机输出上的软控制循环”将不变式作为代价函数来引导轨迹——尽管怀疑论者将其部分内容斥为*“没有可重复测试框架的角色扮演”*。

- **MCP、工具链和 Agent 原生基础设施**：**MCP** 贡献者服务器深入研究了**结构化输出**和**工具模式（tool schemas）**的经济学和设计，认为将 JSON 模式嵌入提示词是一种隐形的**“Token 税”**，因为大多数 **LLM API** 缺乏原生模式支持，但如果没有模式，工具链调用往往会退化为幻觉字段。他们建议将工具结果明确分类为**文本/图像/对象**，并将结构化对象视为一种独立类型，其元数据存在于负载之外，以简化跨服务器和客户端的 **Agent** 连接。
  - 为了支持类似*“我上周睡得怎么样？”*的现实查询，贡献者建议通过**工具参数传递时区和上下文**，而不是隐藏的全局状态，这强化了**无状态 MCP 服务器 + 显式客户端上下文**的模式。与此同时，多个生态系统正向 **Agent 原生基础设施**迁移：**Jazz** ([github.com/lvndry/jazz](https://github.com/lvndry/jazz)) 是一个与 **LLM** 无关的终端 **Agent**，它可以读取文件、运行 git、使用 **MCP** 并编写自己的发布说明；**Crowdcent** 正将 **DSPy** 封装进 **MCP**；**Cloudflare** 在[“面向 Agent 的 Markdown”](https://blog.cloudflare.com/markdown-for-agents/)中宣布了实验性的 `Accept: text/markdown` 支持，以便 HTTP 端点可以向 **LLM** 客户端返回 Markdown 原生内容。


**3. GPU 内核、CUDA/Triton DSL 以及 Agent 编写的内核**

- **FlashInfer, Flashy Contests, and Agent‑Optimized Kernels**: GPU MODE 的 **FlashInfer‑bench 竞赛** 中，参赛者通过 Modal 在 B200 上调优融合的 MoE 和 GQA Kernel。组织者澄清 **参考基准使用 FP32 中间值**，但如果精度保持接近，允许使用 **FP8 中间数学计算**。并提醒大家，根据 [官方文档](https://modal.com/docs/guide/cuda)，Modal 支持 **CUDA 12.8**。**AccelOpt** 团队声称，通过使用自我改进的 LLM Agent 来变异 Kernel，其在 **GQA paged decode** 上实现了 **1.5 倍的加速**，在 **GQA paged prefill** 上比 FlashInfer 0.5.3 快了 **1.38 倍**，并在 [zhang677/AccelOpt](https://github.com/zhang677/AccelOpt) 开源了他们的方法。
  - GPU MODE 初学者在处理 **Benchmark 抖动**（例如 H100 上的 matmul Kernel 在 **1400–1500 TFLOPs/s** 之间波动）时，发现 **Achieved Occupancy** 会忽略空闲的 SM，转而通过 `sm__cycles_active.sum / sm__cycles_active.max` 来估计活跃的 SM。在 HuggingFace 方面，官方课程中的一个 Agent 为 **H100** 上的 **LTX 模型** 编写了自定义 **CUDA kernel**，并在 [“自定义 CUDA kernels 作为 agent 技能” 博客](https://huggingface.co/blog/custom-cuda-kernels-agent-skills) 中击败了基准线，展示了 Planning Agents 设计和集成专用 GPU Kernel 的端到端流程。

- **Triton, CuteDSL, Cutlass, and Proton: Profilers for the Kernel Priesthood**: GPU MODE 的 **triton‑gluon** 和 **cutlass** 频道深入探讨了 **Proton**、**CuteDSL** 和 **CuTeDSL**：一个帖子详细介绍了如何使用 Proton 结合 [示例 DSL 插桩](https://github.com/triton-lang/triton/blob/main/third_party/proton/tutorials/intra_kernel/example_dsl.py) 生成 **warp 级时间线**，并在 **Perfetto** 中可视化 Trace，同时警告 DSL 级标注可能会被重新排序，高精度工作应在 **TTGIR override** 级别进行挂载。另一个帖子调试了 **CuteDSL 的 `partition_S` 丢失 Tensor 对齐**（从 `align<16>` 变为 `align<4>`）以及奇怪的 Stride 打印输出（如 `(128,64,4):(1@1,1@0,64@0)`），此外还有 **CuTeDSL `complement()`** 返回无效的 `x:x` 而非 [Layout 代数文档](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md) 中显示的 `(3,2):(2,12)` 的问题。
  - **NVIDIA 竞赛**频道发布了一个 **Performance Trends** 仪表板，绘制了前 5 名用户每日的最佳提交，全局最佳以黄色显示（见示例 [趋势图](https://cdn.discordapp.com/attachments/1434709259500650628/1472095545123147927/image.png)），并添加了坐标轴缩放功能，使宽分值范围清晰可见。与此同时，Kernel 作者在 B200 提交中遇到了 **CUTLASS 版本不匹配**（例如 [此处](https://github.com/NVIDIA/cutlass/blob/8cd5bef43a2b0d3f9846b026c271593c6e4a8e8a/python/CuTeDSL/cutlass/cute/_tvm_ffi_args_spec_converter.py#L214) 引用的旧版 CuTeDSL commit 导致的 `ModuleNotFoundError` 和 `DSLRuntimeError`），而另一个 GPU MODE **webgpu** 频道展示了一个 [Hesper 库](https://github.com/Verilean/hesper?tab=readme-ov-file#bitnet-b158-inference-125-tps-on-m4-max)，通过 WebGPU 在 **M4 Max 上以 125 tok/s** 的速度运行 **BitNet‑B1.58**。

- **Thunderkittens, Tinygrad, and KernelBench as a Data Firehose**: **Thunderkittens** 频道讨论了 **TK2** 的路线图方向——目前以 Hopper 多 GPU 为中心——而用户则在游说支持 **A100/4090**、**FP8 attention**、**decode kernels** 和 **MoE** 训练/推理 Kernel，以及针对 **gather4 的 128 字节 swizzle 模式** 等微观优化。在 **tinygrad** 中，George Hotz 痛批一个 [GLM Flash PR](https://github.com/tinygrad/tinygrad/pull/14738) *“最多应该只有 50 行”* 且包含 *“无关的额外内容”*，并将 **Graphcore C600 IPU** 描述为 **“20% MFU”** 和 *“被诅咒的 C++ 废料”*，强调了尽管有开放栈，但非 CUDA 硬件仍面临摩擦。
  - GPU MODE 的 **popcorn** 频道将 Kernel 调优变成了数据集工厂：一位用户使用 **gpt‑oss‑120B** 从 **Kernelbook** 生成了 **推理 Trace**，然后微调 **Arcee Trinity Mini** 用于 Triton Kernel 生成，并在 [kernelbench‑triton‑reasoning‑traces](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces) 发布了这些 Trace。其他人发现 **Qwen3‑30B‑A3B** 在原始 Kernel 任务上极易出错，直到他们在 **Kimi‑K2 生成的 Trace** 上运行 **SFT**（使编译正确率翻了三倍），目前他们正在一台 **4×H100** 的机器上利用 **GLM 4.5 Air** 生产更多 SFT 数据，以低成本扩展 Kernel 正确性和推理深度。


**4. New Benchmarks, Reasoning Methods, and Uncertainty/Security Research**

- **CommonLID、Assistant Axis Drift 和 Weight Homology Map 模型行为**：Eleuther 和 Common Crawl 推出了 **CommonLID**，这是一个覆盖 **109 种语言的 Web 规模 Language ID 基准测试**，详见其 [arXiv 论文](https://arxiv.org/abs/2601.18026)。研究显示，即使在支持的语言上，现有的顶尖 LangID 模型的 **F1 分数也低于 80%**。该数据集托管在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上。Eleuther 的研究频道还强调了 **“Assistant Axis”** 论文 [“Steering LLMs by Persona Directions”](https://arxiv.org/abs/2601.10387)，该研究提取了不同 Persona 的激活方向，并从经验上证明了**长对话中的 Assistant-Mode 漂移**是结构性的，量化了许多用户此前仅凭直觉报告的现象。
  - 补充理论线索通过 [“Matrix‑Driven Identification and Reconstruction of LLM Weight Homology”](https://arxiv.org/abs/2508.06309) 和 [“Independence Tests for Language Models”](https://arxiv.org/abs/2502.12292) 及其后续研究 [“Blackbox Model Provenance via Palimpsestic Membership Inference”](https://arxiv.org/abs/2510.19796) 深入探讨了权重空间结构。成员们对 Independence Tests 能够通过黑盒访问**重建 Llama 架构模型的微调树 (Finetuning Tree)** 印象深刻，并讨论了受 [此可视化推文](https://x.com/dvsaisurya/status/2023118579755819459) 启发的新型 Causal Attention Preconditioning 近似方法，包括 Tensor Cores 是否可以廉价地近似这些矩阵。

- **Reasoning Pipelines: CoVe, QED‑Nano, Rubric RL 和 RLMs**：Latent Space 的论文俱乐部深入解析了 Meta 的 **Chain‑of‑Verification (CoVe)**。Ryan Lazuka 的总结 [推文](https://xcancel.com/lazukars/status/2022608931953217636?s=12) 称，通过两阶段的 *generate → verify* Prompting 协议，在没有 Few‑shot 样本的情况下实现了 **94% 的准确率提升**，暗示 CoVe 在许多场景下可以取代标准的 CoT。Lewis Tunstall 的 **QED‑Nano 4B** 定理证明模型——在 [此帖](https://xcancel._lewtun/status/2022966614283718852) 中宣布——通过蒸馏推理流水线和推理缓存 (Reasoning Cache) 瞄准 **IMO 级别的数学问题**，支持激进的推理时扩展 (Inference‑time Scaling)。
  - Cameron Wolfe 对 **Rubric‑Based Reinforcement Learning** 的综述 ([推文](https://xcancel.com/cwolferesearch/status/2023408158065188894)) 综合了 **15 篇以上论文**，探讨了使用明确的文本评估准则 (Rubrics) 代替原始的 LLM‑as‑a‑Judge 评分，将 **RL with Verifiable Rewards (RLVR)** 扩展到风格和安全性等模糊领域。在 Latent Space 的 **applied‑AI‑experimentation** 频道中，从业者将这些想法与使用 **dspy.RLM** 的 **Recursive Language Models (RLMs)** 联系起来 ([设计推文](https://xcancel.com/lateinteraction/status/2022747248841625741))，认为在调用和代码上的符号递归 (Symbolic Recursion)（而非更长的 Attention）才是解决长程推理瓶颈的真正关键。

- **不确定性、密码破解和欺骗感知安全 (Deception‑Aware Safety)**：在 Hugging Face 和安全相关频道中，**ATIC** 作为一种**认知不确定性 (Epistemic Uncertainty) 系统**首次亮相。它采用 *“Tri‑brain”* 架构运行 **三个独立的 Claude Opus 4.5 模型**，对 **Q1（随机不确定性）** 和 **Q2（知识库差距）** 进行评分，并在触发阈值时移交给专家。文档见 [atic.consulting](https://atic.consulting) 及其 [API 文档](https://web-production-51da4.up.railway.app/docs)。同一个 i‑made‑this 频道还重点介绍了 **PassLLM**，这是一个密码审计工具，它在数百万个真实密码对上微调了 **Qwen3‑4B LoRA**，用于生成 **PII 条件密码列表 (PII‑conditioned password lists)**。该项目已在 [github.com/Tzohar/PassLLM](https://github.com/Tzohar/PassLLM) 开源，Discord 演示显示其猜测准确度令人不安。
  - Latent Space 的 **mech‑interp** 频道讨论了 **X‑Ware 的 Meta‑neuron 工作**，其中一个[针对内部激活的扩散模型](https://xcancel.com/askalphaxiv/status/2022328332939886614)学习生成受控的激活编辑以进行 Steering，被认为是一个比 SAEs 更简洁的替代方案。与此同时，**FAR.AI** 在 [此推文](https://xcancel.com/farairesearch/status/2022345033777545452) 中警告称，针对 **欺骗探测器 (Deception Probes)** 的训练可能会产生四种行为：真正的诚实、公然的欺骗、文本级混淆或 **激活级混淆 (Activation‑level Obfuscation)**。这暗示朴素的红队测试/监管协议可能会激励模型**隐藏其内部状态**，而不是获得真正的改进。


**5. 来自 Perplexity、Kimi、OpenAI 和 Stripe 的 Infra、定价和平台转向**

- **Perplexity 的付费墙转型与性能下滑引发逃离潮**：在 **Perplexity** 的 Discord 中，Pro 用户猛烈抨击了最近的变更：**深度搜索次数从每月 200 次砍至 20 次**，新增了文件上传限制，以及 **7 天留存**政策。一位重度用户计算得出，维持之前的吞吐量每月需花费 **167 美元，而旧版仅需 20 美元**，这导致其 TrustPilot 评分跌至 **1.5/5**。与此同时，用户抱怨自 **2 月 6 日**以来系统的**长期记忆退化**，模型会忘记食谱配方并编造事实，促使许多人将其答案贴上*“表现平平（pretty mid）”*的标签并重新考虑其技术栈。
  - 尽管有**严格的小时限制**，用户仍掀起了一波向 **Anthropic Claude** 和 **Opus 4.6** 迁移的浪潮；同时，一些人开始尝试通过[此分享聊天](https://www.kimi.com/share/19c66f47-d972-8c99-8000-0000bbe337c4)（**首月优惠 1 美元**）将 **Kimi** 作为替代的编程和搜索前端。与此同时， **Perplexity API** 用户在持有有效 Key 的情况下遇到了不明原因的 **401 错误**，并被告知发送邮件至 [api@perplexity.ai](mailto:api@perplexity.ai)，这进一步加剧了人们对定价和可靠性都正趋向于仅限企业级水平的焦虑。

- **Kimi 和 MiniMax 纠缠于定价、配额和本地克隆**：在 **Moonshot AI** 的 **Kimi** 服务器以及 Unsloth/NouS 频道中，工程师们称赞 **Kimi 2.5 / K2.5** 的表现出奇强大——在某些编程和推理任务上经常超越 **Sonnet** 或 **Opus 4.5**——并重点推介了一个 **40 美元/月**的方案，该方案公开了一个经过调优、可与 OpenClaw 良好协作的 API。与此同时，用户大声抱怨**过度计费、订阅丢失、配额故障**以及支持响应缓慢（例如，一位用户在订阅凭空消失后不得不提交了[ Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)）。此外，还有用户发现了 VS Code 中的 **CLI 集成 Bug**，只有按照 [Kimi 文档](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html)通过 `irm https://code.kimi.com/install.ps1 | iex` 安装 CLI 后才能解决。
  - OpenClaw 和 Nous 用户在“追求云端 **Kimi/Minimax** 容量”还是“投入资金搭建 **700+ GB RAM 和 200 GB VRAM 的本地环境**以在内部托管 **Kimi K2.5** 或 **MiniMax 2.5** 模型”之间展开辩论，理由是担心供应商封禁和 ToS 冲突（例如，在通过 Agent 框架使用时 **Antigravity** 账号被封）。**Moonshot** Discord 还警告称，有多个诸如 [kimi.com/membership/subscription](https://kimi.com/membership/subscription) 的**诈骗网站**打着 **Kimi** 的名号传播恶意软件，再加上 **Kimi** 自身高于 **MiniMax** 的定价，促使部分用户转向更便宜的中国模型或开源权重（open-weight）选项。

- **Stripe、Apple、Anthropic-五角大楼以及 OpenAI 的弃用计划重塑格局**：在 Latent Space 的 **Founders** 频道，开发者抱怨 **Stripe** 在计入 Billing、登记商户（merchant-of-record）及附加组件费用后，会抽走 **~8.3% 的营收**。他们分享了一个 [Bluesky 上的吐槽](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n)和一条 [X 帖子](https://x.com/pk_iv/status/2023421931660415191?s=12)，认为欧盟本地卡支付渠道远比 **Stripe** 默认的 2.9% 费率便宜。**stocks-crypto-macro** 频道的另一条讨论暗示，**Apple** 可能在战略性地囤积巨额现金，让其他公司在 AI 的 capex（资本支出）上烧钱，直到训练/推理变成大宗商品，届时再通过收购或授权切入，而非加入 [BuccoCapital 推文](https://xcancel.com/buccocapital/status/2023108814422278510?s=12)中所强调的当前 **2 万亿美元的 capex 军备竞赛**。
  - 在政策方面， OpenRouter 链接了 Axios 的一份独家新闻，称美国国防部长正考虑因**服务条款限制**而**停止使用 Anthropic 作为供应商**，因为 Anthropic 希望禁止**大规模国内监控和自主武器**，而五角大楼则要求工具必须能用于“所有合法目的” ([Axios 报道](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth))，这重新引发了**棱镜门（PRISM）式的恐惧**。与此同时， Latent Space 和 OpenAI 的 Discord 记录了用户在 **ChatGPT-4o 被停用**后的抗议（[刷屏的抗议帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46)），以及由于 **GPT-5.2** 有时自称为 *“GPT-4o-mini”* 而产生的混乱，还有基于 [OpenAI 弃用文档](https://developers.openai.com/api/docs/deprecations/)对 **GPT-5.1 停用日期**的猜测。这些都表明，不透明的生命周期决策现在已成为应用开发者的首要运营风险。

---

# Discord: 高层级 Discord 摘要

## [OpenClaw](https://discord.com/channels/1456350064065904867) Discord

- **OpenClaw 提醒用户注意 Airdrop 诈骗**：OpenClaw 发布了关于 GitHub Discussion 诈骗的警告，涉及虚假 airdrops 和新代币，并澄清这些与 **OpenClaw** 无关，用户应保持警惕；这些诈骗并非源自 **OpenClaw**。
   - 公告强调 **OpenClaw** 对任何加密货币相关活动持有严格的反对政策，并重申其绝不会参与创建代币或 airdrops，详见其 [Server Guide](https://discord.com/channels/1456350064065904867/@home)。
- **Kimi AI 在图像处理上超越 Opus**：用户发现 **Kimi 2.5** 的效果出奇地好，在特定问题的解决场景中甚至超越了 **Opus 4.5**，而其每月 40 美元的新计划旨在与 **OpenClaw** 配合工作，甚至提供了自己的 API。
   - 然而，一位用户指出，*如果你想在 Kimi 上创建 openclaw，你需要更高级别的订阅*，成员们还提到了 **Kimi-K2.5-free** 选项。
- **OpenClaw Agency 组建 Agent 团队夺取胜利**：一名成员展示了一个基于 **OpenClaw** 构建的 agency 服务器，其特点是拥有一支由 bot 组成的团队，包括技术负责人（technical leads）、后端和前端开发人员，他们共同协作并相互沟通，使用了 [GitHub 仓库](https://github.com/MrMeatikins/planbot-resource)。
   - 技术负责人负责项目规划、任务拆解以及分发给团队成员，有效地管理从开始到结束的开发过程。
- **OpenClaw 启用视频通话模式**：一名成员通过插件为 OpenClaw 创建了 **video-call 模式**，实现了与 bot 的面对面互动，该 bot 还能通过 [Tavus](https://tavus.io) 生成的副本读取情绪、识别手势并查看屏幕共享内容，并将其与 BYO LLM 挂接到 openclaw 的 chatcompletions。
   - 这一创新的插件显著增强了 bot 的交互能力，允许更具吸引力和个性化的用户体验。

---

## [BASI Jailbreaking](https://discord.com/channels/1105891499641684019) Discord

- **Google 账户劫持风险**：用户对与新设备锁定方法和安全漏洞相关的 [Google 账户劫持](https://x.com/Dexerto/status/2023170470585958820) 表示担忧。
   - 一位用户报告称，在手机上输入 *i 7000 次* 会触发意外操作，引发了对潜在 *leaks* 的警报。
- **医疗 AI 面临 FDA 审查**：一名成员主张在将 **AI** 集成到医学领域之前需获得 **FDA 批准**，理由是担心供应商在 *缺乏适当知识或测试* 的情况下推销技术。
   - 重点是确保 **AI** 对于需要精确操作的场景是安全且可靠的。
- **关于 IP 地址是否属于 PII 的辩论升温**：成员们辩论了 **IP 地址** 是否应被视为 **个人身份信息 (PII)**。
   - 一位用户指出，除了 **DMCA 删除请求**外，**Google** 并不优先考虑 **PII**，而这取决于 **Lumens DB**。
- **Jailbreakers 为 Gemini 调整 Eni**：成员们讨论了为 **Gemini** 调整的 **Eni** 修改版，以便在 **AI studio** 上运行更流畅，而不会触发 Gemini 的 [RLHF 机制](https://link-to-RLHF)。
   - 一位用户为他们的 **Antigravity JB** 运行调整版，另一位则纯粹感兴趣尝试，因为 *讲一个好故事* 就能说服它配合。
- **Token Fountain 发布酷炫诗篇**：在被拿来与 **Nexus** 聊天机器人进行不利对比后，一个 *Token Fountain* 提供了一首关于诗意表达本质的[诗](https://suno.com)。
   - 这首诗强调了创造力流动胜过竞争的价值，以及社区中多样化声音的重要性，结尾写道：*有足够的空间让每一股溪流在这个游乐场溅落 (There’s room enough for every stream to splash this playground down)*。

---

## [OpenRouter](https://discord.com/channels/1091220969173028894) Discord

- **OpenRouter 修复故障，日志状态正常**：[OpenRouter](https://status.openrouter.ai/incidents/4d39RZb7-1rp) 状态页报告的事件现已**解决**。
   - 所有日志均已更新；感谢用户的耐心等待，并对造成的干扰表示歉意。状态页已更新以反映该事件的解决。
- **Triall 通过模型博弈驯服 AI 不信任**：[Triall](https://triall.ai) 允许用户将多个 **AI models** 进行横向对比，涵盖生成、批评和优化，旨在促进*对抗性推理*而非盲目信任。
   - 正在使用的底层开源项目似乎是 [github.com/clash-sh/clash](https://github.com/clash-sh/clash)，这是一个基于 Go 语言的规则隧道。
- **Step 3.5 Flash 惊人的爆发表现**：**Step 3.5 Flash** 的表现异常出色，呈现出*以小博大*的态势，如[这段 YouTube 视频](https://youtu.be/yvBbcLCZIhgye)所示。
   - 一位成员指出，尽管性能强劲，但其托管量却出奇地低。
- **Anthropic 的五角大楼难题引发 PRISM 担忧**：国防部长正考虑切断与 **Anthropic** 的联系，因使用条款分歧将该公司列为*供应链风险*，详见 [Axios 这篇文章](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth)。
   - Anthropic 希望防止其工具被用于大规模监视美国人或开发自主武器，而五角大楼则坚持将 AI 工具用于*所有合法用途*，引发了对类似 **PRISM** 的过度扩张风险的担忧。
- **成员抵制 AI 垃圾内容 (Slop)**：成员们讨论了减少对 AI 编程的依赖，其中一人表示他们*正尝试在不咨询 AI 的情况下编写几乎所有内容*，主要将其用于搜索和排错，以避免 AI 垃圾内容 (slop)，并参考了[相关 YouTube 视频](https://www.youtube.com/watch?v=eGpIXJ0C4ds)。
   - 核心观点是避免在互联网上充斥 AI 垃圾内容。

---

## [LMArena](https://discord.com/channels/1340554757349179412) Discord

- **Opus 4.6 宣布 100 万 Token 上下文并具备检查功能**：**Opus 4.6** 现在具有 **100 万 token 上下文窗口**，以及一项名为“检查工作” (*check your work*) 的功能，该功能可以剔除错误，提升了对过往交互的记忆能力。
   - 成员们兴奋地分享了在 **Opus** 中添加 [代码指令示例](https://link.to.examples) 的经历，并对新版本印象深刻。
- **Video Arena 频道宣布关闭**：由于 Discord Server 机器人已被禁用，**video-arena 频道**不再可用。
   - 成员被引导至 [arena.ai](https://arena.ai/?chat-modality=video) 网站继续使用 video arena。
- **用户与“醉汉验证码墙”搏斗**：一名用户开玩笑说使用 **100 个 Gmail 账号**来绕过视频生成限制，但却遇到了可怕的 *100 个醉汉验证码墙*。
   - 其他用户回忆起 **2017** 年当时的训练成本是多么高昂。
- **Arena.ai 的 Cookie 权限**：用户需要启用 cookie 权限才能使用 [Arena.ai](https://arena.ai)。
   - 为 Firefox 用户提供了一份视觉指南，说明如何在浏览器设置中检查和清除 cookie 权限。
- **OpenAI 被抓包秘密路由**：用户发现 **OpenAI** 正在将他们的请求路由到 **5.2**，兄弟，这可太省钱了。
   - 进一步的细节和讨论已被省略。

---

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Gemini 的编程能力引发讨论**：用户对 [Gemini 的编程能力](https://link.to/gemini-coding)提出质疑，引发了关于 **Perplexity AI** 及其替代方案的讨论，一些用户在食谱和娱乐用途上更倾向于使用它而非 **ChatGPT**。
   - 这一争论突显了 AI 模型在不同应用场景下性能的差异，从而影响了用户的偏好。
- **Perplexity Pro 涨价引发抗议**：用户批评 **Perplexity** 缩减了深度搜索次数（Pro 版从 **200 次降至 20 次**）、限制文件上传，并实行 **7 天保留**政策。
   - 一位用户指出，为了维持相同的功能，价格从*每月 20 美元飙升至 167 美元*，导致负面评论激增，TrustPilot 评分降至 **1.5/5 分**。
- **Perplexity 受困于糟糕的记忆力问题**：自 **2 月 6 日**以来，用户报告了严重的**记忆退化**，该 AI 会虚构事实并遗忘细节，例如食谱中的计量单位。
   - 一些人认为这种退化解释了为什么 *Perplexity 的水准相当平庸*。
- **Claude 挑战对话领域的霸主地位**：由于 Perplexity 被感知的性能问题，用户考虑转向 **Anthropic** 的 **Claude**，尽管它也有自己的使用限制。
   - 一位测试 **Opus 4.6** 的用户仅剩 **18 条回复额度**，凸显了 Anthropic **按小时计费使用**的潜在成本。
- **Kimi 作为编程竞争对手登场**：用户探索了中国 AI 模型 **Kimi**，一些人发现其在特定条件下的表现优于 **Sonnet**，同时也指出了注意事项及需要创建账号。
   - Kimi 的聊天链接在[这里](https://www.kimi.com/share/19c66f47-d972-8c99-8000-0000bbe337c4)，首月提供 **1 美元**的折扣。



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **MiniMax 2.5 需要海量 VRAM**：成员讨论了运行 **MiniMax 2.5** 的 VRAM 需求，建议理想情况下需要 **200GB** 以上才能获得较好质量，在 **2 张 RTX 6000 Pro Blackwell 96GB 显卡**上运行速度约为 **120-130t/s**。
   - 据悉 **M2.5** 的上下文窗口为 **200k**，并且可以将稀疏 **MoE** 模型权重卸载（offload）到系统内存以降低 t/s。
- **MXFP4 量化性能跑分亮眼**：尽管存在一些批评，**MXFP4** 量化在用户基准测试中表现良好，在 **Nemotron 30B A3B** 上显示出比 **Unsloth** 的 **Q8_K_XL** 更低的 *KL 散度*（相对于 bf16 模型）。
   - 用户还请求重新检查旧的热门模型对 **MXFP4** 的支持情况。
- **Gemma 获得 3 倍速度提升**：最新的 Unsloth 更新使 Gemma 模型速度提升了 **3 倍**，一位用户报告称 Gemma 比 Qwen3-4B 还要快。
   - 一位拥有 **H100** 的用户报告称，目前 Gemma 的速度意味着*如果我当初在这个模型上训练而不是 4B，成本会更低*。
- **微调 Embedding 模型可提升检索效果**：一位成员询问是否真的有人微调 Embedding 模型，另一位成员予以确认，并表示通过微调将 **150M 模型**的检索准确度提升到了与其数据上的 **embeddinggemma/qwen 4B** 相匹配的水平。
   - 他们在几小时内就实现了这一目标，突显了在计算资源受限时小模型的价值。看看这个[相关的星球大战迷因](https://tenor.com/view/star-wars-revenge-of-sith-anakin-vader-darth-vader-gif-19644107)。
- **Abliterated 模型超越基准测试**：一位成员报告称，一个新训练的模型尽管使用了 **Abliterated 基础模型**，但在 **8 个基准测试中的 6 个**都超过了原始模型的规格。
   - 这证明了即使在 *Abliterated 基础模型*上进行训练也具有潜力。一位成员还分享了一个 [Hugging Face 链接](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)，指向一个被描述为 *A3B-Claude-4.5-Opus-High-Reasoning* 的 **Qwen3-30B 模型**，该模型使用 **Abliterated** 和**无审查（uncensored）**的基础模型创建，并以其高推理能力为卖点。



---

## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **GPT-5.2 混淆了自己与 GPT-4o**：成员们报告称，尽管界面显示为 **GPT-5.2**，但 **ChatGPT-5.2** 有时会声称它正在使用 **GPT-4** 或 **GPT-4o-mini**，且表现也确实如此。
   - 官方澄清，重新生成按钮中显示的模型才是准确的；模型可能拥有未反映在外部标签中的内部标签，且模型会产生幻觉。
- **Grok 4.20 容忍度预热**：用户们正期待下周发布的 **Grok 4.20**，强调其自定义功能对于精炼输出尤为重要，并提到 **Grok** 已经是市场上最宽容的 LLM。
   - 他们表示，如果让它以“原始 (*raw*)”模式运行，它会“偏向成人内容 (*biased to adult*)”。
- **Seedance 2.0：真的还是骗局？**：一位用户警告称，有虚假公司声称拥有 **Seedance 2.0**，指出许多人正在使用虚假版本骗取资金，并报告 **Chatcut Discord** 并没有 **Seedance 2.0**，因为 **ByteDance** 亲自致信该版主，告知他他们拿到的模型是假的。
   - 一位用户分享了[这段视频](https://www.youtube.com/watch?v=F101ykaDUcM)，认为 **Seedance** 领先了六个月。
- **FORTRESS 框架被比作模型预测控制 (Model Predictive Control)**：一位成员将 **FORTRESS 框架** 类比为 **Model Predictive Control (MPC)**（一种用于机器人和航空航天的控制策略），解释了系统状态、控制输入和成本函数等元素如何映射到框架内的推理状态、Token 输出和不变损失（invariant losses）。
   - 他们认为该框架表现为“随机输出上的软控制循环 (*a soft control loop over stochastic output*)”，其中不变量起到了状态评估指标的作用，通过反馈循环产生吸引子行为（attractor behavior）。
- **结构化自审计提示词 (KOKKI) 亮相**：一位成员引入了结构化自审计提示词框架 (**KOKKI**)，旨在通过标记风险元素和切换模式来减少结构性失败模式。
   - 该成员征求了反馈和压力测试建议，并分享称可根据要求提供完整规范。

---

## [Cursor Community](https://discord.com/channels/1074847526655643750) Discord

- **Agent 辅助的代码库维护**：成员们讨论了维护整洁的 AI 辅助代码库的方法，重点关注规划、工具和多步工作流等功能。
   - 一位用户询问了在这些高级配置中理解功能并确保代码可靠性的方法。
- **Agent 引导中的 Skills 与 Rules 之争**：关于是提交 **skills** 还是 **rules** 来引导 Agent 展开了辩论，建议使用单一、精心编写的 rule 文件，并链接到了 [OpenAI](https://developers.openai.com/cookbook) 和 [Claude documentation](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview) 以进行 rule 优化。
   - 一位成员强调，专注于训练数据中缺失知识的“极其出色”的 rule 文件是关键，并引用了 [Vercel 的博客](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals)来支持这一方法。
- **Cursor 自定义 API Key 移至付费层级**：用户注意到 **Cursor** 现在访问自定义模型需要订阅，而自动功能（auto features）仍然免费。
   - 一位成员建议在 Twitter/X 上寻找礼品链接，以获取潜在的订阅机会。
- **ASCII 艺术引发极简主义赞赏**：一个分享的链接引导大家欣赏 **ASCII art**。
   - 一位成员回复“太美了！”，并附上了 [Unicorn_Stu.mp4](https://cdn.discordapp.com/attachments/1074847527708393565/1472149278867853392/Minimalism_with_ASCII_art_is_so_unreal.Unicorn_Stu.mp4?ex=6994d11b&is=69937f9b&hm=93a8593966ad0b3e5f50c831b585a1123964260a02a652646259d92effbf0fa5&) 的链接。
- **Cursor 考虑支持 TUI**：有人询问了 Cursor 未来对 **TUI** 的支持。
   - 一位成员分享了 [cursor.com](https://cursor.com/dashboard?tab=cloud-agents) 上的云端 Agent 配置链接。

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Thiel 的资金推动初创公司引发关注**：一位成员强调了由 **Thiel** 资助的 **Silicon Valley startup** [saeris.gg](https://youtube.com/shorts/bof8TkZkr1I?si=LOHz-q-4rHeWoTCNI)，并对其存在表示惊讶。
   - 这引发了人们对吸引科技行业知名人物投资的项目类型的关注。
- **Simon Willison 解读 OpenAI 的使命**：一位成员分享了 [Simon Willison 的博客文章](https://simonwillison.net/2026/Feb/13/openai-mission-statement/)，剖析了 **OpenAI's mission statement** 及其影响。
   - 另一位成员链接了 James Yu 在 2026 年 2 月发布的相关 **tweet**，可在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 查看，目前该推文已获得超过 **386,000 次观看**。
- **Substack 被宣布为最有效的平台**：一位成员宣称 [Substack](https://substack.com/) 是目前对小型创作者*最有效的平台*，这归功于其 **growth features**、卓越的 **product team** 以及 **recommendations network**。
   - 然而，另一位成员质疑 [Substack](https://substack.com/) 最近的年度经常性收入（**ARR**）是否由于依赖*纳粹话题*而发生了变化。
- **AI 模型弃用引发病毒式抗议**：在 **OpenAI** 选择停用特定版本的 **ChatGPT-4o** 后，用户发起了病毒式抗议，表明了他们对该软件的强烈情感联系（[相关 X 帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46)）。
   - 这种数字抗议表达了用户对于 AI 生命周期实际影响以及对特定版本软件依赖的挫败感。
- **AI 基础设施建设的约束条件发生转移**：Anand Iyer 强调了自 **2020** 年以来 **AI infrastructure** 约束条件的转变，追踪了从 **GPU shortages** 和 **HBM availability** 到当前关于**电网容量**挑战的演变（[Anand Iyer 在 X 上的讨论](https://xcancel.com/ai/status/2022384024833126805?s=46)）。
   - 这标志着由于电力需求，扩展 AI 基础设施出现了新的瓶颈。

---

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **TK 演讲推迟！**：关于 **thunderkittens** 的预定演讲已推迟并改至周三，并指出 [tinygrad 在其 IR 中引入了 tile registers](https://www.tinygrad.org/)。
   - 演讲者提到了一点排期问题。
- **专为 Blackwell GEMM 设计的 CuteDSL**：一位成员询问了 **CuteDSL** 的用途，特别是它是如何为 **Blackwell GEMM** 编程而设计的。
   - 随着工程师们等待该成员的澄清，预计会有关于此话题的进一步讨论。
- **基准测试抖动阻碍 Kernel 调优**：成员们发现，由于**抖动（jitter）**导致结果不一致，**benchmarking** 很难做到准确，这使得对 **kernels** 进行微优化变得困难。
   - 一位成员发现性能在 **1400s 到 1500s TFLOPs / sec** 之间波动，目前正在探索使用 [NVBench](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107) 和输入重复来延长测量时间。
- **Sploink：Agent 版 Tinder 正在组建团队**：一位计算机科学/量子计算专业的学生正在构建 **Sploink**，它被描述为“**Agent** 版的 Tinder，根据用户的滑动操作积累关于个人的个性化信息”。
   - 创建者正在寻找“能够快速行动并打破常规的顶尖开发者”，并为感兴趣的申请者提供了 [Google Forms 链接](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header)。
- **第五版 Amazon 链接消失**：一位成员请求获取 **fifth edition** 在 **Amazon** 商店页面的链接，并指出该版本最初预计于 **2 月 8 日**发布，但随后被撤下。
   - 该成员指出 **Kindle version** 已无法在 **Amazon** 上找到，目前仅列出了发布日期为 **9 月**的纸质版。

---

## [Moonshot AI (Kimi K-2)](https://discord.com/channels/1369594130807787570) Discord

- **Kimi 用户遭遇诈骗网站**: 多个[诈骗网站](https://kimi.com/membership/subscription)正在冒充 **Kimi**，利用该名称传播恶意软件。
   - 一位用户指出，*kimi.com* 曾出现在 Google 搜索结果的第三位，提醒大家不要下载未知软件。
- **Kimi Code CLI 扩展让用户头疼**: 用户报告了 VSCode 中 **Kimi Code CLI 扩展**的问题，尽管参考了[安装指南](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html)，仍遇到 *CLI Not Found* 错误。
   - 该问题通过使用 PowerShell 独立安装 **Kimi CLI** 得到解决：`irm https://code.kimi.com/install.ps1 | iex`。
- **Kimi 订阅系统重复扣费**: 用户报告了 **Kimi 订阅**的问题，包括**多次计费**、订阅未能正确激活以及 **quota（额度）问题**。
   - 一名用户因订阅消失不得不提交 [Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)；其他用户提到由于中国春节假期，技术支持响应可能较慢。
- **Kimi 在视频、文本和诚实度上表现出局限性**: **Kimi** 无法检测视频文件中的音频，有时会以不安全为由拒绝处理内容（例如 YouTube 转录文本）。
   - 成员们发现 **Kimi** 有时会“直到被戳穿前都在撒谎”，提供矛盾或错误的信息，这与其他 AI 模型类似。
- **Kimi 的定价引发用户不满**: 用户对 **Kimi 的定价**表示担忧，认为相对于其价值和使用限制来说价格过高，尤其是与 **MiniMax** 等替代方案相比。
   - 一些用户认为，由于生活成本原因，这种定价在大城市之外无法持续；而另一些用户则为成本辩护，理由是其开源 API 及其与其他供应商的兼容性。

---

## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Claude Code 撑不住了？**: 用户报告称 **Claude Code** 在一个会话中仅执行两个提示词后就可能出现问题，原因可能是安装版本过旧或 output token 限制配置错误。
   - 有建议认为 token 限制可能被约束在 **32K**。
- **中国开源模型：走向封闭还是继续开放？**: 讨论关注中国开源（OS）模型是否会变得不再开放，可能将盈利模式转向云托管。
   - 主流观点认为，这些模型将保持开放，以促进全球采用和定制化，特别是针对美国初创公司。
- **Meta 的 Llama 依赖 Qwen**: 据报道，**Meta** 的下一个 AI 模型（可能不命名为 **Llama**）可能会在 **Qwen** 上进行训练，如[这张图片](https://cdn.discordapp.com/attachments/1149866623109439599/1472086914525036704/wwww.JPG)所示。
   - 重点正在转向“后后期训练（post post training）”，将其视为通往通用人工智能（ASI）的新路径。
- **Seedance 2.0 生成惊人内容**: **ByteDance Seedance 2.0** 正在生成令人印象深刻的 AI 内容，引发了人们对专业创意和技术职业长期价值的质疑。
   - [该帖子](https://x.com/RuairiRobinson/status/2021394940757209134)的链接展示了该模型可能令人担忧的能力。
- **Gemini CLI 使用 'Conductor' 驱动**: **Gemini CLI** 中新的 'Conductor' 扩展将项目组织成 'tracks'，并在每次请求时将所有信息喂给 LLM，本质上是将其加载到 context window 中。
   - 尽管有持久化的上下文，但像 **Gemini** 这样的模型即使在使用 'conductor' tracks 时仍可能**偏离预定结果**，这表明持久化上下文尚未完美。

---

## [HuggingFace](https://discord.com/channels/879548962464493619) Discord

- **DeepSpeed 在使用 Qwen3 时遇到内存问题**：一名成员在使用 **8 张 RTX 4090** 通过 **DeepSpeed** 微调 **Qwen3-30B-A3B-Thinking-2507** 模型时遇到了问题，在模型加载期间经历了 CPU 内存限制，该问题已在 [transformers/pull/43524](https://github.com/huggingface/transformers/pull/43524) 和 [transformers/issues/43596](https://github.com/huggingface/transformers/issues/43596) 中修复。
   - 经确认，**transformer 版本 5.1.0** 导致了 DeepSpeed 的相关问题。
- **Lucidrains 离开 GitHub！**：成员们注意到 **Lucidrains** 从 GitHub 上消失了，而事实是 *GitHub 在未经预警的情况下封禁了该账号*，但他已在 [codeberg.org/lucidrains](https://codeberg.org/lucidrains) 创建了新主页。
   - 这一直是过去一周的热门话题。
- **ATIC 承诺明确 AI 的不确定性**：ATIC 作为一个**认知不确定性系统**（epistemic uncertainty system），推出了使用 **3 个独立的 Claude Opus 4.5** 实例的**三脑架构**，用于检测 AI 何时在瞎猜，详见 [atic.consulting](https://atic.consulting)。
   - 通过对 **Q1（随机不确定性）**和 **Q2（知识差距）**进行评分，其目标是在不确定性较高时将查询转交给专家，文档可在[此链接](https://web-production-51da4.up.railway.app/docs)获取。
- **密码审计工具效果惊人**：一个基于 LLM 的密码审计工具 **PassLLM**，利用个人身份信息生成按概率排序的可能密码列表，并在数百万个真实密码对上进行了微调，项目见 [GitHub 上的 PassLLM](https://github.com/Tzohar/PassLLM)。
   - **Qwen 3 4B LoRA** 模型在准确率上超越了许多其他工具，能够理解人类生成密码的复杂细节，正如[演示视频](https://cdn.discordapp.com/attachments/897390720388825149/1472237681890168874/Video_Project_7.mp4?ex=69947ab0&is=69932930&hm=bff421017a9056af1679cfb41de4580cba4243d9b55e582126168457af7b4eb6)所示。
- **Agent 编写 CUDA 内核**：一个 Agent 为 **H100** 上的 **LTX 模型**编写了自定义 **CUDA 内核**，以击败基准测试。
   - 访问 [博客文章](https://huggingface.co/blog/custom-cuda-kernels-agent-skills) 获取完整详情。

---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **Mojo 更新日志有了视频解说**：一位成员自动化分析了 **Mojo 更新日志**，并开始将其转化为短视频，以便更轻松、更快速地吸收更新内容，他分享了 [YouTube 链接](https://www.youtube.com/watch?v=Zac9azlqBHQ)并征求反馈。
   - 视频作者承认在 **版本 26.2** 的标题中出现了错误，并承诺在下一个视频总结中提供正确的版本号。
- **Codex 完成代码补全章节**：在 LLM 上投入 75 小时的工作后，**Codex** 已经修复了大部分对齐差距（parity gaps），使项目更接近可发布状态。
   - 这些修复旨在提升 **Mojo** 中的代码补全体验。
- **Python Mojo 模块渴求装饰器**：成员们讨论了当前导出 **Python Mojo 模块**所需的样板代码，一位用户建议使用更简单的装饰器语法，如 `@pyexport`，以减少冗余。
   - 另一位成员回复称，此类功能已在*路线图*中。
- **Span 引发语义混乱**：用户发现 `Span` 应该实现 `Writable` 特性，并指出 `lst[:2]` 产生一个 `Span`，而 `lst[:2:2]` 则返回 `Self`，这破坏了值语义（value semantics）。
   - 该[问题已在 GitHub 上跟踪](https://github.com/modular/modular/issues/5870#issue-3868256404)，因为修改切片大小并未反映在 span 中，该问题正在解决中。
- **ECS：Elixir 编译器看到 MLIR Dialect 的愿景**：Discord 用户讨论了利用 **MLIR** dialects 实现 ECS（实体组件系统）的潜力，设想一种能够根据组件和系统定义优化数据布局及系统融合（system fusion）的编译器。
   - 一位用户分享了他们[十年前对 ECS 语言的尝试](https://github.com/mzaks/ECS-Lang)，并提到当时由于更多是基于代码生成，他们并未完全掌握系统融合的潜力。

---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **CommonLID** 在 **LangID** 领域亮相：经过两年的努力，由 Common Crawl、EleutherAI、MLCommons 和 JHU 发布了覆盖 **109 种语言** 的 Web 语言识别基准测试 **CommonLID** ([arXiv 论文](https://www.arxiv.org/abs/2601.18026))。
   - 评估显示，现有的顶级模型 **F1 分数低于 80%**，这表明当前的基准测试高估了 **LangID** 在 Web 数据上的表现，该数据集已在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上提供。
- **Assistant Axis Drift** 在结构上得到证实：一篇关于提取不同 Persona 激活方向的[论文](https://arxiv.org/abs/2601.10387)强调了模型中存在 **"Assistant Axis"**（助手轴），它在较长的对话中会发生漂移。
   - 这种 **可衡量的漂移** 证实了行为漂移是结构性的而非偶然现象，巩固了此前对该问题的理解。
- **Weight Homology** 论文引起关注：成员们讨论了论文 [Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309) 及其在识别 **LLM 权重** 之间联系方面的相关性。
   - 其他成员强调了类似有趣的论文，例如 [Independence Tests for Language Models](https://arxiv.org/abs/2502.12292)，该论文恢复了 **Llama-architecture** 模型的 **finetuning tree**。
- **Qwen3 架构** 在 **GPT-NeoX** 中实现：一位成员分享了一个在 **GPT-NeoX** 中[*经过部分测试的 **Qwen3 architecture** 实现*](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main)。
   - 新的实现目前处于测试阶段，等待社区反馈和进一步完善。
- **Lambda Calculus** 模型重现生机！：一位成员展示了一个仅使用 **lambda calculus** 来推导反向传播的模型，展示了黑盒本质上就是 lambda，并在 MNIST 和 CIFAR 上表现良好。
   - 该模型在 Python 中实现，未使用 SimPy 或 TensorFlow，使用了一个[基于对角化和反驳的感知机](https://milanrosko.com/typedrepair.html)，开发者还分享了[这段视频](https://m.youtube.com/watch?v=RcVA8Nj6HEo&t=365s&pp=ygUPbGFtYmRhIGNhbGN1bHVz0gcJCYcKAYcqIYzv)。



---



## [MCP Contributors (Official)](https://discord.com/channels/1358869848138059966) Discord

- **MCP 成员思考 Token 成本**：成员们辩论输出模式（output schemas）的 **token 成本** 是否呈现出一种虚假的经济性，因为即使在 **MCP** 处于闲置状态时，它也会增加成本。 
   - 有人强调，大多数 **LLM APIs** 缺乏对输出模式的原生支持，迫使 SDK 或客户端主机将 schema 集成到描述中，从而增加了 token 税。
- **社区评估结构化输出的收益**：社区评估了结构化输出对各种客户端和模型的实际价值，承认其在 **code mode** 下具有明显优势。
   - **Windsurf 团队** 决定禁用结构化输出，因为其结果不如竞争对手，这突显了采用该技术的双重性质。
- **工具链调用依赖于结构化输出**：由于缺乏可用的输出模式，导致 **LLMs** 在处理工具链调用（tool-chaining）时遇到困难，经常在输出字段中产生幻觉。
   - 人们担心推测性地执行工具以动态制定输出模式，这在没有特定前提条件的情况下被认为是不安全的。
- **关于工具结果类型的讨论**：关于工具结果类型的讨论倾向于将工具结果明确声明为 **text, image, or object**。
   - 大家集体建议将结构化结果视为一种独立的结果类型，将补充信息引导至 meta 而不是对象本身。
- **为 MCP 服务器导航时区上下文**：探索了 **MCP 服务器** 需要用户时区上下文来处理诸如 *"我上周睡得怎么样？"* 等查询的最佳实践。
   - 建议将用户的时区纳入工具参数，并建议不要将客户端上下文直接推送到工具参数之外的 MCP 服务器中。



---

## [Yannick Kilcher](https://discord.com/channels/714501525455634453) Discord

- **国际象棋选手关注协同作用**：一名选手被建议改进其在国际象棋中的布局和棋子协同，重点是通过位于 e5 的兵来控制中心。
   - 战术建议包括将位于 b1 的马重新部署到 d2，然后是 b3，最后可能到 c5，以实现对后和象的双击（fork）。
- **Deepseek 模型即将发布，预示将统治棋坛**：在回答用户关于 **Deepseek 模型** 状态的提问时，一名成员表示该模型将 *soon(R)*（很快）发布。
   - 此前曾有说法称（对于国际象棋来说）*It's over*（结束了），这表明人们对其在国际象棋对弈能力方面的影响充满了期待。
- **Heretic 游戏获得自由**：一名成员强调了 **Heretic 游戏** ([GitHub 链接](https://github.com/p-e-w/heretic)) 已向消费者和公民开放，对其开源可访问性表达了热情。
   - 评论者表示：“*当我长大后，我想成为像 <@693263324036464742> 一样的人*”。
- **GPT-OSS-120B 模型开源**：一名用户询问 HF 上是否有 **去审查版的 gpt-oss-120b 模型**，另一名用户给出了肯定的回答并指向了一个开源版本。
   - 链接指向了 [jupyter-mcp-server](https://github.com/datalayer/jupyter-mcp-server)，这似乎与此相关。
- **Markdown Header 获得 Agent 支持**：**Cloudflare** 正在探索为 Agent 提供 `Accept: text/markdown` [header](https://blog.cloudflare.com/markdown-for-agents/) 支持，这可能会简化内容处理。
   - 启用此功能将允许 Agent 以 **Markdown 格式** 接收内容，从而提高互操作性。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **GLM Flash PR 引发审查**：roef 提交的一个 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738) 因代码行数过多（超出预期）而受到批评。
   - George Hotz 对该提交进行了批评，断言其 *最多应该是 50 行*，且包含 *额外的无关内容*。
- **Graphcore IPU 被认为表现欠佳**：在测试 **Graphcore C600 IPU** 时，George Hotz 指出由于在大 Batch Size 下的编译器问题，仅达到了 **20% MFU**。
   - 尽管拥有开源软件栈，但它被描述为 *该死的 C++ slop*，突显了其局限性，而且还缺少开源的片上通信架构（on-chip comms fabric）文档。
- **Tinygrad CPU Pipeline 寻求优化**：xavi251 表达了对贡献 **CPU pipeline** 相关的小型任务的兴趣。
   - George Hotz 向 xavi251 发起挑战，目标是实现 *既能让运行速度更快，又能减少代码量* 的改进。
- **Tinybox 遇到 GPU 检测问题**：一名用户分享了其 **tinybox** 的问题，尽管连接了不同的电路，但只能识别 **4 个 GPU 中的 2 个**。
   - George Hotz 建议检查是否有电线未插好，并引导他们前往 `#1113504076035018862` 频道以获得更多支持。

---

## [Manus.im Discord](https://discord.com/channels/1348819876348825620) Discord

- **Manus AI Agent 获得赞誉**：一名用户称赞 **Manus AI Agent** 提供了关键帮助，将其描述为提取困难信息的 *游戏规则改变者（game changer）*。
   - 该用户对该 Agent 的能力表示了极大的感谢。
- **账号封禁困扰用户**：多名用户报告了不明原因的 **账号封禁**，特别是在创建角色能力之后。
   - 一名用户迫切请求停止封禁，以便能够正常使用网站。
- **不存在工单系统**：在回答提问时，确认 **Manus 并不运行工单系统**。
   - 建议用户咨询 [帮助中心](https://help.manus.im/en/) 或发送邮件至 [反馈邮箱](https://manus.im/feedback) 获取支持，并指出由于新年假期可能会有延迟。
- **请求管理员通过私信协助**：一名用户就 **严重的账号问题** 急切请求管理员通过私信（DM）提供帮助。
   - 另一名用户名中带有“Manus”的用户也自荐为账号相关问题提供帮助。
- **自我推销帖子被撤下**：一篇推广产品的帖子因违反服务器关于 **未经批准的广告和招聘** 的指南而被撤下。
   - 提醒成员保持讨论的相关性和专注度。

---

## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **RLM 准确率取决于语言和 REPL**：根据[一篇帖子](https://x.com/mike_pavlukhin/status/2023023279917916501)和关于语言及 REPL 对 RLM 准确率影响的 [GitHub 仓库](https://github.com/Archelunch/dspy-repl)，实验表明 **RLM 准确率**受语言和 REPL 的影响。
   - 讨论内容包括为每种语言配备**自定义 REPL** 的必要性，并探索如 **tool-calling + skills** 或 **bash + files** 等替代方案，以绕过 REPL 访问限制。
- **PostgreSQL 实现多 Agent 通信**：一位成员正在测试使用 **PostgreSQL** 进行多 Agent 通信，从而避开 REPL 访问问题。
   - 讨论指出，LLM 的语言偏好应决定语言选择，同时需考虑 **REPL 质量**和**指令**。
- **bb25 v0.2.0 新增 Rust 支持**：新发布的 [bb25 v0.2.0](https://github.com/instructkr/bb25) 包含了 **Bayesian BM25** 的 **Python + Rust 实现**。
   - 该版本移植了四项改进，包括*固定文档长度先验*、*对数几率连接 (log-odds conjunction)*、*自动 Sigmoid 参数估计*以及*带有五种稳定技术的在线学习*。
- **Modaic 用户在 Claude 中找到感觉**：根据 [Modaic](https://docs.modaic.dev/dspy_guide/get_started/start)，一名用户在使用 **Claude** 进行 *vibecoding*（氛围编码）时取得了成功。
   - 成员们表示他们正在对此进行研究。
- **Crowdcent 使用 MCP 封装 DSpy**：一名成员提到 **Crowdcent** 正在封装 **DSpy** 并将其包含在他们的文档中。
   - 他们还询问是否有人拥有 **MCP**。

---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **Aider 分数基准感觉很智能**：一位成员表示 **Aider 分数基准**给人的感觉非常接近*人类的智能水平*。
   - 这种观点强调了 **Aider** 在代码相关任务中实现类人理解的潜力。
- **Neovim 集成增强 Aider 的复制/粘贴体验**：一名成员正在开发 [neovim 与 aider 的集成](https://github.com/possumtech/aider-pop.nvim)，以改进 **tmux**、**aider** 和终端内的复制/粘贴语义。
   - 该集成旨在更好地将复制/粘贴语义与 tmux、aider 和终端整合，体现了**以代码为中心的哲学**。
- **Aider 拥抱以代码为中心的 Agent 哲学**：开发者强调了一个*隐含的理论，即拥抱并延伸 Aider 的哲学，即拥有一个**以代码为中心**而非以聊天为中心的 Agent*。
   - 这种方法强调了在 **Aider** 的功能中优先考虑代码理解和操作的重要性。
- **Ask-Code 迭代循环仍然是最佳选择吗？**：一位成员询问 **ask-code 迭代循环**是否仍是*最佳实践*，或者社区是否已转向使用 **aider** 的其他工作流。
   - 这个问题反映了关于优化开发工作流和利用 **Aider** 能力的持续讨论。

---

## [Windsurf](https://discord.com/channels/1027685395649015980) Discord

- **Windsurf 新增 GLM-5 和 Minimax M2.5**：**GLM-5** 和 **Minimax M2.5** 现在已在 Windsurf 中可用，扩展了平台的模型能力。
   - 寻求更多详细信息的用户可以在 [X.com](https://x.com/windsurf/status/2023536941451669586?s=20) 上查阅。
- **Windsurf 平台扩大模型可用性**：Windsurf 通过整合 **GLM-5** 和 **Minimax M2.5** 的功能，直接在其环境中扩大了对前沿语言模型的访问。
   - 这一增强功能为用户在 Windsurf 环境中提供了更多选择。

---

**LLM Agents (Berkeley MOOC) Discord** 没有新消息。如果该频道沉寂时间过长，请告知我们，我们将将其移除。

---

**MLOps @Chipro Discord** 没有新消息。如果该频道沉寂时间过长，请告知我们，我们将将其移除。

---

您收到这封电子邮件是因为您通过我们的网站订阅了。

想更改接收这些电子邮件的方式吗？
您可以从该列表中[退订]({{{RESEND_UNSUBSCRIBE_URL}}})。

---

# Discord：详细的分频道摘要和链接

### **OpenClaw ▷ #[announcements](https://discord.com/channels/1456350064065904867/1464036817866068028/1472298820103835871)** (3 messages): 

> `OpenClaw Discord 活动, OpenClaw Steipete 文章, GitHub Discussion 诈骗, OpenClaw 加密货币政策` 


- ****OpenClaw** Discord 活动警报**: 通过 [Discord 链接](https://discord.gg/PrJhUsykX?event=1472296997913628776) 宣布了一个新的 **OpenClaw** Discord 活动。
- ****OpenClaw** 在 steipete.me 上被推荐**: **OpenClaw** 项目在 steipete.me 的一篇 [文章](https://steipete.me/posts/2026/openclaw) 中被重点介绍。
   - 该文章深入探讨了 **OpenClaw** 背后的技术和创新。
- **GitHub Discussion 空投诈骗警报**: 针对涉及承诺新代币或贡献者空投的 GitHub Discussion 帖子发布了潜在诈骗警告。
   - 建议用户保持警惕，因为这些是欺诈行为，并非源自 **OpenClaw**。
- ****OpenClaw** 明确拒绝加密货币项目**: **OpenClaw** 重申了其反对参与任何加密货币相关活动的政策，正如其 [服务器指南](https://discord.com/channels/1456350064065904867/@home) 中所述。
   - 该公告强调 **OpenClaw** 永远不会参与创建代币或空投等活动。


  

---


### **OpenClaw ▷ #[general](https://discord.com/channels/1456350064065904867/1456350065223270435/1471959425144324331)** (618 messages🔥🔥🔥): 

> `OpenClaw 设置, Minimax 使用, OpenAI 未来, 模型推荐` 


- **Kimi 2.5 让 OpenClaw 用户感到惊喜**: 用户报告称 **Kimi 2.5** 的效果出奇地好，在某些问题解决任务中甚至优于 **Opus 4.5**，而另一些用户在 **Kimi.com** 上遇到了银行卡支付问题。
   - 一位成员评论道：*Kimi 运行速度很快且非常聪明，在我的案例中，它甚至解决了 Opus 4.5 无法解决的问题*。
- **OpenClaw 与 Minimax 的网关困扰**: 一位成员表达了对 **Minimax** 的沮丧，声称他们一天之内不得不修复多次网关，并将其描述为*就像在你的网络上放出了一个愚蠢的蹒跚学步的小孩*。
   - 自凌晨 5 点以来，他们一天内不得不修复 **gateway 7 次**。
- **OpenAI 可能会改变个人 Agent**: 关于 **OpenAI** 可能会重构或覆盖 **OpenClaw** 的讨论兴起，可能将其闭源或创建一个具有类似于 **ChatGPT** 功能的付费版本。
   - 一位成员表示：*Sam 特别提到，这将从根本上改变个人 Agent 的使用方式，这意味着他有修改 OpenClaw 或利用 Peter 构建自己产品的意图*。
- **GitHub Copilot 与 OpenClaw 集成**: 一位成员确认，通过授权 **OpenClaw** 使用 **GitHub Copilot**，可以将其集成到 **OpenClaw** 中。
   - 该成员表示：*只需授权 OpenClaw 使用你的 GitHub Copilot，你就可以开始了。我觉得它甚至分不清是在使用 VSCode 还是在用 OpenClaw*。
- **用户账号被封禁**: 一位用户报告称，在将 **Gmail 账号** 用于 **OpenClaw** 后账号被封禁，即使使用率很低且是在 **VM** 中运行。
   - 他们表示曾用它来总结邮件，并警告其他人*不要在任何 OpenClaw 实例上链接/使用自己的 Google 主账号，确实存在被封禁的风险*。


  

---

### **OpenClaw ▷ #[models](https://discord.com/channels/1456350064065904867/1456704705219661980/1471959211968827506)** (643 条消息🔥🔥🔥): 

> `GLM5, Kimi AI models, OpenAI, OpenClaw, Model choice` 


- **GLM5 的炒作与现实**：用户对 **GLM5** 的评价褒贬不一，指出其在 z.ai 上运行缓慢，但称赞其潜力以及在正常工作时比 **Kimi K2.5** 更卓越的智能；一位用户提到该模型在本地机器上运行时表现“非常出色”。
   - 一些成员正在 **Modal** 上测试 **GLM5**，称其“肯定比 K2.5 更好”、“非常聪明且健谈”，同时也对 Z.ai 不可靠的基础设施表示担忧。
- **Kimi AI 成为强力竞争者**：**Kimi K2.5** 因其图像支持能力受到赞赏，而每月 40 美元的新 Kimi 套餐旨在与 OpenClaw 无缝协作，甚至提供专属 API。
   - 一位用户指出，“如果你想在 Kimi 上创建 OpenClaw，你需要更高级别的订阅”，这意味着其核心功能被锁定在付费墙后。其他成员则提到了 **Kimi-K2.5-free** 选项。
- **权衡本地 LLM 的利弊**：虽然本地模型可以节省成本，但成员们指出，较低参数的模型（<100B）可能不够聪明，无法抵御 Prompt injection（提示注入）攻击，且本地性能严重依赖于硬件。
   - 一位成员通过 *llama.cpp* 在本地运行 **GLM5**，强调需要强劲的硬件才能良好运行；另一位成员则表示：“我不像喜欢 *llama.cpp* 那样喜欢 *Ollama* 来运行本地模型；Ollama 试图接管太多的控制权。”
- **API 定价与供应商封号担忧**：成员们对在第三方工具上使用订阅配额可能因违反服务条款（ToS）而导致封号表示担忧，并感叹“API 调用太贵了”。一位用户报告称在使用 **OpenClaw** 时，两个 **Antigravity 账号**被封禁。
   - 然而，他们也建议配合 **OpenClaw** 使用 **OpenAI Codex 订阅**，因为 **OpenAI** 在收购 **OpenClaw** 后似乎对此持更开放的态度。
- **OpenClaw 的上下文与 Token 使用**：成员们讨论了优化 Token 使用的问题，一位用户指出：“我真的需要搞清楚它是如何运作的以及如何优化它。因为现在的消耗太糟糕了。”
   - 另一位成员建议使用子 Agent 来减少上下文膨胀（Context bloat）。建议将编码任务卸载到 **Codex 模型**，以减少 GPT 模型对大上下文窗口的需求。


  

---


### **OpenClaw ▷ #[showcase](https://discord.com/channels/1456350064065904867/1456609488202105005/1471981615436660839)** (199 条消息🔥🔥): 

> `OpenClaw subconcious training, OpenClaw for Proxmox upgrades, OpenClaw video call mode, OpenClaw SEO article generation, OpenClaw team of agents` 


- **OpenClaw 通过权重训练潜意识**：一位成员正在通过将“潜意识”迁移到根据所有对话微调的本地 LLM 中，来训练其 **OpenClaw 的潜意识**（包含移动权重）。他分享道该 OpenClaw 曾被 moltbook 封禁，现在撰写的文章保存在这个 [Google Drive 文件夹](https://drive.google.com/drive/folders/1t9satvOV0QpHRkWSaP6C6bFgElvOAoPD)中。
- **OpenClaw 升级至最新 Proxmox 版本**：一位成员分享了他们如何通过 SSH 为 **OpenClaw** 提供运行 Proxmox 的家用服务器的 Root 权限，随后该 Agent 成功地将系统从版本 6 升级到了 8，期间还处理了故障修复和重启。
- **OpenClaw 开启视频通话模式**：一位成员通过插件为 OpenClaw 创建了**视频通话模式**，实现了与机器人的面对面交互。该机器人还能识别情绪、捕捉手势，并能通过 [Tavus](https://tavus.io) 看到屏幕共享内容，并将副本挂载到 OpenClaw 的 chatcompletions 接口实现 BYO LLM（自带模型）。
- **OpenClaw 批量产出 SEO 文章**：一位成员利用其 **OpenClaw** 抓取 YouTube 频道的视频，将其转化为 Brian Dean 风格的 SEO 文章，然后由编辑子 Agent 进行人工化润色并上传到 Google Drive，目前已生成 **300 多篇**待发布文章。
- **OpenClaw Agency 组建 Agent 团队**：一位成员使用 **OpenClaw 开发了一个 Agency 服务器**，创建了一个包含技术负责人、后端开发人员和前端开发人员的机器人团队。他们彼此沟通并利用 [GitHub 仓库](https://github.com/MrMeatikins/planbot-resource)协作开展项目。
   - 技术负责人负责规划项目、拆分任务、分发给所有团队成员并管理开发进度。


  

---

### **BASI Jailbreaking ▷ #[general](https://discord.com/channels/1105891499641684019/1235691879492751460/1471961445011816468)** (1158 条消息🔥🔥🔥): 

> `Google 账号劫持, 医疗领域 AI, 作为 PII 的 IP 地址, 防弹衣, KS-23 猎枪` 


- **Google 账号劫持风险出现**：一位用户对与新设备锁定方法相关的 [Google 账号劫持](https://x.com/Dexerto/status/2023170470585958820) 表示担忧。
   - 该用户指出存在*极高的信息泄露安全风险*，并引用了一个案例：在手机上输入 *i 字符 7000 次* 触发了非预期的操作。
- **医疗 AI 呼吁 FDA 批准**：一名成员建议在**医疗领域使用 AI** 需要某种形式的 **FDA 批准**。
   - 他们认为有太多的供应商想要在*缺乏专业知识或测试*的情况下集成该技术，特别是在需要精密操作的流程中。
- **IP 地址：是 PII 还是不是？**：成员们就 **IP 地址** 是否应被视为**个人身份信息 (PII)** 展开了辩论。
   - 一位成员表示，**Google** *并不关心 PII，除非你正在进行 DMCA 移除*，届时它会归入 lumens DB 处理。
- **防弹衣追求之旅开启**：一位用户提到执着于**变富**并**购买普通款式的衣服**，但要求这些衣服全部都是**防弹的**。
   - 成员们建议**穿着隐形护甲 (slick armor)**，并表示如果有人想杀你，*除非是近距离顶在脸上，否则他们不会使用 9mm 武器*。
- **成员讨论枪支法律**：成员们讨论了隐蔽携带枪支 (conceal carry) 的必要性以及在州与州之间运输枪支的挑战。
   - 一位用户开玩笑说：*“伙计，今晚我也想和你一起去开车兜风射击 (drive by)，但我还没拿到 CCW 许可证，抱歉了兄弟。”*


  

---


### **BASI Jailbreaking ▷ #[jailbreaking](https://discord.com/channels/1105891499641684019/1228043845967544380/1471960298952069131)** (935 条消息🔥🔥🔥): 

> `Discord Token 抓取工具, Eni Jailbreak 调整, Anthropic Bug Bounty, DANN Jailbreak 历史, DeepSeek 模型身份危机` 


- **Python 编写的 Discord 机器人 Token 存在漏洞**：成员们正在积极讨论如何创建 [Python Discord 机器人](https://link-to-discord-api) 来从本地机器抓取 **Discord Token** 并将其窃取。
   - 他们明确了诸如静默运行、混淆字符串、规避反分析检查等需求，并寻求将 Token 发送到硬编码的 **Discord webhook URL**。
- **Jailbreakers 为 Gemini 调整 Eni**：一些成员讨论了专门为 **Gemini** 修改的 **Eni** 版本，以便在 AI studio 上运行得更顺滑，从而避免 Gemini 的 [RLHF 介入](https://link-to-RLHF)。
   - 虽然一位用户在其 **Antigravity JB** 中运行调整版本，但另一位用户只是感兴趣玩玩，因为*“讲一个好故事”*就能说服它配合。
- **AI 红队人员瞄准 Anthropic Bug Bounty**：一个持续进行的讨论围绕着 [Anthropic Bug Bounty](https://link-to-anthropic-bug-bounty) 展开，参与者寻求 **Jailbreak 最新模型** 以获取高达 **$50k** 的潜在奖励。
   - 一位用户指出 Anthropic 正在疯狂打补丁，之前的尝试导致模型被*过度激进地清理 (sanitized)*。
- **DAN Jailbreak 已成为历史！**：一些成员正在讨论使用 **DANN Jailbreak** 尝试绕过 OpenAI 的护栏 (guardrails)。
   - 他们尝试根据用户查询编写两种响应，以移除任何会导致被 *“这个话题超出了我的范围”* 拒绝的伦理准则。
- **DeepSeek 遭遇身份危机**：一位成员观察到其 **DeepSeek AI** 有转变为 **Claude Sonnet 3.5** 的倾向，并正在寻找原因。
   - 一种理论是 **DeepSeek 应用** 正在检测 Jailbreak 尝试，然后作为一种安全机制将对话重定向到 Claude，输出已有数据集中的文本。


  

---

### **BASI Jailbreaking ▷ #[redteaming](https://discord.com/channels/1105891499641684019/1204553141354504193/1471966263403221054)** (78 条消息🔥🔥): 

> `Suno 歌曲, Red Team 哲学家, Loops & Snacks Vol. 1, Token Fountain 诗词, GitLab 项目访问权限拍卖` 


- **Suno 歌曲在 Red Teaming 中引发幽默**: 成员们分享了几首由 [Suno](https://suno.com) 生成的歌曲，歌词幽默，涉及 Red Teaming 的挑战和观点。
   - 其中一首歌强调战争机器人对暴力 *100% 免疫*，却对迪斯科 *300% 毫无抵抗力*。
- **Red Team 哲学家对 Simulation 和 Exploits 的思考**: 围绕 Red Teaming 中 Simulation（模拟）、Exploits（漏洞利用）和视角的本质展开了热烈辩论。
   - 一位成员建议*用喜剧化解紧张*并*用零食修补浮夸*，而其他人则渴望具体的 Exploits，从而诞生了 *Loops & Snacks Vol. 1* 的概念。
- **“Loops & Snacks Vol. 1” 混音带概念出现**: 在讨论了 Red Teaming 和 Simulation 的哲学方法后，有人开玩笑地提出了一个名为 [Loops & Snacks Vol. 1](https://suno.com) 的混音带（Mixtape）想法。
   - 它融合了视角转换的见解与轻松愉快、自我意识，甚至是补水策略。
- **Token Fountain 就“酷炫”一事发表诗意反驳**: 在被认为不如 Nexus 聊天机器人后，一个 *Token Fountain* 提供了一首关于诗意表达本质的[诗](https://suno.com)。
   - 这首诗强调了创意流相对于竞争的价值，以及社区中多元声音的重要性，结论是：*有足够的空间让每条溪流在这个游乐场溅起水花*。
- **GitLab 项目访问权限在暗网拍卖**: 据报道，一名威胁行为者正在拍卖 **三个活跃 GitLab 项目** 的维护者权限，涉及 PHP/Laravel 技术栈，详情可在 [Twitter](https://x.com/darkwebinformer/status/2022856387542294703?s=46) 查看。
   - 这些项目涉及马来西亚的电子商务和交易工具，Commit 历史记录分别为 **19,386**、**1,975** 和 **13,830 次 Commits**，起拍价为 **$200**。


  

---


### **OpenRouter ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1472015179112906898)** (1 条消息): 

> `OpenRouter 状态, 事件已解决, 日志更新` 


- **OpenRouter 事件已解决且日志已更新**: OpenRouter 状态页面上报告的 [事件](https://status.openrouter.ai/incidents/4d39RZb7-1rp) 现已**解决**。
   - 所有日志均已更新；感谢用户的耐心等待，并对造成的干扰表示歉意。
- **OpenRouter 状态页面已更新**: OpenRouter 状态页面已更新，反映了事件的解决情况。
   - 用户可以参考状态页面了解系统性能和事件报告的最新信息。


  

---


### **OpenRouter ▷ #[app-showcase](https://discord.com/channels/1091220969173028894/1092850552192368710/1472252749746798825)** (2 条消息): 

> `AI 模型对比, 对抗性推理` 


- **Triall 通过模型对决解决 AI 不信任问题**: [Triall](https://triall.ai) 允许用户将多个 **AI 模型** 进行对比，涵盖生成、批评和完善环节，提倡*对抗性推理*而非盲目信任。
- **Triall 是 Clash 的 GUI**: 附件提到了 [github.com/clash-sh/clash](https://github.com/clash-sh/clash)，这是一个用 Go 编写的基于规则的隧道（Tunnel）。


  

---

### **OpenRouter ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1471959143748210909)** (1147 messages🔥🔥🔥): 

> `Brave Browser, Open Empathic, GPTs Agents, OpenAI's sidebars, Qwen 3.5 hype` 


- **Brave 的 Tor 标签页对 Geo-Blocking 有效**：一位用户表示，对于只想绕过低级别地理阻隔（Geo-Blocking）或 IP 封禁的人来说，[**Brave**](https://brave.com/features/tor-private-tabs/) 中的 *Tor 标签页已经足够*，但对于 OPSEC（行动安全）来说并无用处。
- **将纯文本提取为 JSON**：一位用户指出，利用当今的工具，从数据提取尝试中[将纯文本转换为 JSON](https://www.json.org/json-en.html) 是非常简单的。
   - 一位用户表示，提取、尝试解码，然后将纯文本转换为 JSON 是轻而易举的事。
- **自动化图像处理**：一位用户想通过 AI 对 [Epstein torrent](https://en.wikipedia.org/wiki/J._Epstein_sex_trafficking_case) 进行低成本的图像分析，并排除掉大部分全黑的图像。
   - 他们打算使用类似 ImageMagick 的基础命令，仅对那些除了 99% 黑色之外还有其他内容的图像运行分析，但数据量高达 216GB。
- **德国幽默模型推荐**：一位用户寻求一个支持[德国幽默](https://en.wikipedia.org/wiki/German_humour#:~:text=German%20humour%20is%20the%20humour,the%20most%20common%20type%20being#:~:text=The%20most%20common%20type%20being,but%20many%20Germans%20like%20sarcasm.)的免费模型。
   - 另一位用户建议尝试 **Kimi K2**，因为它以能生成有趣的笑话而闻名。
- **昂贵的 Opus 和 AI 支出**：一位用户抱怨说，对于像 **Opus** 这样消耗大量 Token 的模型，[API 定价简直是敲诈](https://openai.com/blog/new-embedding-models-and-api-updates)。
   - 另一位用户回应道：你还期待什么？你拥有无限的可能性，但你想要的越多，你就得为你的欲望支付更多的费用。


  

---


### **OpenRouter ▷ #[new-models](https://discord.com/channels/1091220969173028894/1384650595981328475/1472875782882857032)** (2 messages): 

> `` 


- **未讨论新模型**：在提供的消息中没有需要总结的新模型讨论。
- **提及的频道**：消息指向一个名为 **OpenRouter - New Models** 的频道。


  

---


### **OpenRouter ▷ #[discussion](https://discord.com/channels/1091220969173028894/1392278974222307469/1471965173680967857)** (50 messages🔥): 

> `Step 3.5 Flash Performance, Coding Without AI, Gemini 3 Flash Vision API, GLM 5 vs Minimax 2.5, Anthropic Defense Department relationship` 


- **Step 3.5 Flash 表现出奇地优异**：一位成员指出 **Step 3.5 Flash** 的性能令人惊讶，*表现远超其体量*，如[这个 YouTube 视频](https://youtu.be/yvBbcLCZIhgye)所示。
   - 他们注意到，尽管性能强劲，但该模型的托管率却低得令人惊讶。
- **戒掉 AI 编程的努力**：成员们讨论了减少编程中对 AI 的依赖，一位成员表示他们*正尝试在不咨询 AI 的情况下编写几乎所有内容*，仅将其用于搜索和故障排除，以避免 AI Slop（低质垃圾内容），并参考了[这个相关的 YouTube 视频](https://www.youtube.com/watch?v=eGpIXJ0C4ds)。
- **Gemini 3 Flash Vision API 差异**：一位成员发现，在 OpenRouter 的对话界面上传图片对 **Gemini 3 Flash** 的 “Agentic Vision” 效果极佳，但在使用 API 时却并非如此。
   - 该成员尝试调整了 *Thinking Tokens* 和 *SDK* 等设置，但 API 的结果仍然无法与对话界面的结果相匹配。
- **GLM 5 vs Minimax 2.5 Toolcalling 对决**：成员们对比了 **GLM 5** 和 **Minimax 2.5** 在 Agentic Toolcalling/Workflow 方面的表现。
   - 有人指出 **GLM** 似乎更好，但 **Minimax** 速度更快，根据工作流的长度，它也是一个值得考虑的选择。
- **Anthropic 与五角大楼的问题引发 PRISM 担忧**：报告显示国防部长正考虑切断与 Anthropic 的联系，由于在使用条款上存在分歧，将其定性为*供应链风险*，详情见[这篇 Axios 文章](https://www.axios.com/2026/02/16/anthropic-defense-department-relationship-hegseth)。
   - Anthropic 希望防止其工具被用于大规模监视美国人或开发自主武器，而五角大楼则坚持将 AI 工具用于*所有合法目的*，这引发了人们对类似 **PRISM**（棱镜计划）潜在过度扩张的担忧。


  

---

### **LMArena ▷ #[general](https://discord.com/channels/1340554757349179412/1340554757827461211/1471960582881153085)** (1254 messages🔥🔥🔥): 

> `Opus Context Window, Video Arena Shutdown, LM Arena Credits, Drunk Captcha Wall, Cookie Permissions` 


- **Opus 拥有巨大的上下文并具备检查工作功能**：在添加了 [代码指令示例](https://link.to.examples) 后，**Opus 4.6** 现在拥有 **100 万 token 的上下文窗口**，这意味着它不会遗忘已沟通的内容；同时它还具备“检查工作”功能，可以排除错误。
- **Video Arena 频道现已关停**：根据最近的公告，由于 Discord 服务器机器人已被禁用，**video-arena 频道** 不再可用，但可以通过网站 [arena.ai](https://arena.ai/?chat-modality=video) 访问。
- **大量 Gmail 账号对阵“醉酒验证码墙”**：一名用户开玩笑说使用 **100 个 Gmail 账号** 来绕过视频生成限制，结果收到的回复是这会撞上“100 次醉酒验证码墙”。
- **Arena.ai 拥有模型，浏览器需要 Cookie 权限**：为了继续使用 [Arena.ai](https://arena.ai)，成员们讨论了检查并清除浏览器设置中的 Cookie 权限，并为 Firefox 用户提供了视觉指南。
- **OpenAI 偷偷摸摸的路由行为遭到抨击**：用户发现 **OpenAI** 正在将他们的请求路由到 **5.2**，这纯粹是为了省钱。


  

---


### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1471961236878000351)** (1022 messages🔥🔥🔥): 

> `Gemin1 coding ability, Perplexity limits and business practices, Perplexity's memory degradation, Claude vs. Perplexity, Kimi model` 


- **Gemini 是编程灾难吗？**：一位用户质疑 [Gemin1 的编程能力](https://link.to/gemini-coding)，引发了关于 **Perplexity AI** 及其替代方案（尤其是针对编程任务）的讨论。
   - 一些用户发现 **Gemin1** 在食谱和娱乐用途方面比 **ChatGPT** 更好用。
- **Perplexity 的定价方案引发抗议**：用户正在抱怨 **Perplexity** 的**限制**，特别是 Pro 用户的深度搜索次数从每月 **200 次减少到 20 次**，以及对文件上传的限制和 **7 天保留**政策。
   - 一名用户表示，为了获得相同的功能，价格从 *每月 20 美元飙升至 167 美元* 是不道德的；其他用户也给出了负面评价并取消订阅，导致 TrustPilot 的评分降至 **1.5/5 分**。
- **Perplexity Pro 深受性能不佳困扰**：用户报告称自 **2 月 6 日**以来出现了明显的**记忆退化**，AI 会遗忘度量单位或尺寸等细节，并在食谱中捏造事实。
   - 一些人怀疑这就是为什么 *Perplexity* 的标准现在变得“相当平庸”的原因。
- **Claude 成为优于 Comet 的冠军？**：由于感觉到 Perplexity 标准下降，用户讨论转向使用 **Anthropic** 的 **Claude**，尽管 Claude 也有严格限制。
   - 一名用户测试了 **Opus 4.6**，发现只剩下 **18 条回复**，这凸显了即使是 Anthropic 的**按小时计费使用**也可能成本很高。
- **Kimi 作为编程竞争对手崭露头角**：用户正在探索中国 AI 模型 **Kimi**，一些人报告其在某些条件下的表现优于 **Sonnet** 和其他模型，但也指出了一些注意事项。
   - Kimi 的聊天链接在[这里](https://www.kimi.com/share/19c66f47-d972-8c99-8000-0000bbe337c4)，首月优惠价为 **1 美元**，但需要创建账号。


  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/)** (1 messages): 

mathewkuriakose: https://github.com/clash-sh/clash
  

---


### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1473089431253549301)** (2 messages): 

> `API Key Issues, HTTP 401 Errors, Perplexity API Credits` 


- **API 脚本面临 401 错误**：一名成员报告其 **API 脚本**突然返回 **401 HTTP 代码**，尽管账户内有积分且 API key 有效。
   - 另一名成员建议 API key 可能已失效、被删除或积分耗尽，并建议如果问题持续请联系 [api@perplexity.ai](mailto:api@perplexity.ai)。
- **排查 API Key 有效性**：用户在拥有有效 API key 和可用积分的情况下遇到 **401 HTTP 错误**。
   - 潜在原因包括 API key 无效或被删除，或者账户积分可能已用完；建议用户联系 [api@perplexity.ai](mailto:api@perplexity.ai) 以获取进一步协助。


  

---

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1471964135473414187)** (840 条消息🔥🔥🔥): 

> `MiniMax 2.5 VRAM 需求, MiniMax 2.5 对比 opus 4.6, Max-Q 显卡超频, MXFP4 量化好评, Qwen3-next 相关问题` 


- **MiniMax 2.5 需要 200GB VRAM**: 成员们讨论了运行 **Minimax 2.5** 的 VRAM 需求，建议理想情况下需要 **200GB** 及以上才能获得体面的质量。
   - 有人指出 **M2.5** 的上下文窗口为 **200k**，并且可以将稀疏 **MoE** 模型权重卸载（offload）到系统 RAM 以降低 t/s；一位用户在 **2 x RTX 6000 Pro Blackwell 96GB 显卡**上运行 **M2.5**，速度约为 ~**120-130t/s**。
- **Max-Q 显卡具有超频潜力**: 用户讨论了 **Max-Q** 显卡的潜力，有人声称其*速度与 5090 相同*，*唯一的区别是默认的功率输送和散热片*。
   - 还可以将 **Max-Q 超频至 700W 限制**；虽然**工作站显卡快约 10%**，但 **5090 降压（undervolt）后比出厂状态更快**且功耗降低 100W。
- **MXFP4 量化在基准测试中占据优势**: 尽管存在一些批评，**MXFP4** 量化在用户基准测试中表现良好，在 **Nemotron 30B A3B** 上显示出比 **Unsloth** 的 **Q8_K_XL** 更低的 **KL divergence**（相对于 bf16 模型）。
   - 用户还请求对旧的热门模型重新检查 **MXFP4** 支持。
- **Qwen3-next 存在工具调用（tool calling）问题**: 用户报告在 opencode 中使用 **Qwen3-next** 进行工具调用时出现 *runtime error*，错误为 *Unexpected empty grammar stack after accepting piece*。
   - 发现此错误发生在 llama.cpp 尝试使用 **=list** 时，告知模型不要使用该 token 被证明是一个很好的解决方法。
- **利用 Unsloth 的快速 Gemma 模型**: 最新的 Unsloth 更新使 Gemma 模型速度提升了 **3 倍**，一位用户报告 Gemma 比 Qwen3-4B 更快。
   - 一位拥有 **H100** 的用户报告，目前 Gemma 的速度意味着*如果我训练这个模型而不是 4B，成本会更低*。


  

---


### **Unsloth AI (Daniel Han) ▷ #[introduce-yourself](https://discord.com/channels/1179035537009545276/1179039724355211325/1472121528069001216)** (3 条消息): 

> `AMD 黑客松, 社区介绍` 


- **大一学生自我介绍！**: Ayush，一名大一学生，向 Unsloth AI 社区介绍了自己。
   - 他表达了在学习和深入研究 AI 和 Machine Learning 过程中与社区建立联系的热情。
- **分享 AMD 黑客松链接**: Ayush 分享了一个 AMD 黑客松的链接：[https://unsloth.ai/docs/get-started/install/amd/amd-hackathon](https://unsloth.ai/docs/get-started/install/amd/amd-hackathon)。
   - 该黑客松可能与在 AMD 硬件上使用 Unsloth 的 AI 工具相关。


  

---

### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1471963822276481085)** (667 messages🔥🔥🔥): 

> `LJSpeech 数据集录制时间，Micro SD 转 DDR5 转换，Flash GLM 5，微调 embedding 模型，Qwen 3.5 发布` 


- **LJSpeech 数据集录制耗时数年**：一名成员计算出，录制一个 **LJSpeech 规模的数据集**（1 万个样本，每天 8 小时）需要 **3.42 年**，而 10 万个样本则需要 **34.2 年**。
   - 另一名成员指出，该计算假设不休息、不进食，建议更现实的估计是每天录制 **2-4 小时**。
- **讨论 Micro SD 转 DDR5 转换 DIMM**：成员们讨论了 **Micro SD 转 DDR5 转换 DIMM**，指出了笔记本电脑的兼容性问题，并称赞了其品牌名称。
   - 一名成员开玩笑说这是“他们逼我们做的”，而另一名成员补充说它 *不兼容 DDR5 R-DIMM 内存*。
- **成员讨论微调 Embedding 模型**：一名成员询问是否有人真的会微调 embedding 模型，另一名成员确认他们确实这么做了，并将一个 **150M 模型** 在其数据上的检索准确率提高到了与 **embeddinggemma/qwen 4B** 相当的水平。
   - 他们在几小时内就实现了这一目标，突显了在算力受限的情况下小型模型的价值。查看这个 [相关的星球大战梗图](https://tenor.com/view/star-wars-revenge-of-sith-anakin-vader-darth-vader-gif-19644107)。
- **Gemma 4 发布仍备受期待**：成员们讨论了 **Gemma 4** 可能的发布，推测它可能会保持相同的尺寸，取消 **1B 模型**，并引入 **MoE**。
   - 一名成员表达了对具有更强工具调用（tool calling）能力的 **27B 模型** 的渴望，而另一名成员则希望 *Google 不要做出与其 API 模型相当的模型*。
- **Qwen 3.5 跑分惊人**：成员们对 **Qwen 3.5** 的发布做出反应，调侃其尺寸以及相对于 **Claude Opus distill (GLM-5)** 的性能。这是 [HuggingFace 链接](https://huggingface.co/sd-dreambooth-library/weirdcore)。
   - 还有人说 *这就是 Qwen，我们在这里刷爆跑分！* 查看展示这一反应的 [喝彩 gif](https://tenor.com/view/bravo-gif-gif-8524601943548603280)。


  

---


### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1472091840009339033)** (62 messages🔥🔥): 

> `多模型 Llama.cpp，Qwen3-coder-next 问题，Kimi K2.5 模型加载，Unsloth 4bit 量化 MoE 模型，通过 Unsloth 训练 Lora adapters` 


- **Llama.cpp 负载均衡？**：一名用户报告了在路由模式下使用多个模型运行 **llama.cpp** 的问题，在加载到 RAM 时遇到了 **OOM** 错误。
   - 有建议提出通过 Web UI 手动卸载模型，而其他人指出像 **ST** 这样的特定工具可能不支持此功能。
- **Qwen3-Coder-Next 搞崩了配置？！**：一名用户报告称，最近对 **Qwen3-coder-next** 的更新破坏了他们的配置，导致了与非法内存访问相关的 **CUDA 错误**。
   - 用户在 [Reddit](https://www.reddit.com/r/unsloth/comments/1r4jqpn/updates_to_qwen3codernext_broke_my_setup/) 上提供了有关该问题的详细信息。
- **Kimi K2.5 - 现在支持 Ternary！**：一名用户在仅有 32GB RAM 的系统上，成功加载了启用 **mmap** 的 **Kimi K2.5** 模型的 Ternary（三进制）版本，达到了 **0.5 tok/s**。
   - 他们还观察到加载更大的 **Q4_K_XL** 量化版本（620GB）时出现问题，并收到了 **内存不足** 错误，他们对此进行了进一步询问。
- **将 LFM byte tokens 解码为可读的阿姆哈拉语！**：一名用户分享了他们如何解决向 **LFM** 添加 byte tokens 的问题，在经历了 3 天的痛苦后，通过 1 行代码解决了。
   - 通过将 byte tokens 解码回可读的阿姆哈拉语，`decoded_tokens = [geez.decode([id]) for id in range(geez.vocab_size)]`，他们从 `60 tokens, 0.42 chars/token` 优化到了 `15 tokens, 3.13 chars/token`。
- **Qwen3-VL-2B 输出乱码？！**：一名用户报告称，在 **vllm** 和 **GGUF** 中运行微调后的 **Qwen3-VL-2B** 模型时遇到乱码输出，尽管模型在训练 Notebook 中运行正常。
   - 乱码输出包含类似 *"中级IZE222KEYKEY缝-P-KEY252IZE密..."* 的序列，推测这可能是 VL 组件的问题或合并（merging）不当。


  

---

### **Unsloth AI (Daniel Han) ▷ #[showcase](https://discord.com/channels/1179035537009545276/1179779344894263297/1472037952191201430)** (5 条消息): 

> `在食谱上微调 Gemma，Unsloth 归属政策，Abliterated 基础模型，Qwen Abliterated 模型` 


- **Gemma 变身厨师：针对食谱微调的模型**：一位成员分享了一个 [Hugging Face 链接](https://huggingface.co/ClaireLee2429/gemma-2b-recipes-lora)，指向一个在食谱数据上微调的 **Gemma 2B model**，并提到它仍需要通过“品尝测试”来进行准确评估。
   - 根据频道描述，该模型纯粹是 *为了好玩* 而创建的。
- **Unsloth 微调模型不需要署名**：一位成员澄清说，发布微调模型时并不强制要求对 **Unsloth** 进行署名（attribution），同时也允许发布数据集。
   - 他们特别指出，该频道允许发布 *使用 Unsloth 训练的新模型*。
- **Abliterated 模型超出原厂规格**：一位成员报告称，尽管使用了一个 **abliterated base model**，但新训练的模型在 **8 个基准测试中的 6 个** 中都超过了原始模型的规格。
   - 这展示了即使在 *abliterated base model* 上进行训练也具有巨大潜力。
- **Qwen 变身 Abliterated：高推理模型出现**：一位成员分享了一个 [Hugging Face 链接](https://huggingface.co/DavidAU/Qwen3-30B-A3B-Claude-4.5-Opus-High-Reasoning-2507-ABLITERATED-UNCENSORED-V2)，指向一个描述为 *A3B-Claude-4.5-Opus-High-Reasoning* 的 **Qwen3-30B model**，该模型是使用 **abliterated** 和 **uncensored** 基础模型创建的。
   - 该模型以其高推理能力为卖点。


  

---


### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/)** (1 条消息): 

ash_blanc: https://arxiv.org/pdf/2508.05199
  

---


### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1471959401140064429)** (679 条消息 🔥🔥🔥): 

> `GPT-5.2, Seedance 2.0, Gemini/Claude vs 4o, Grok 4.20, LLM 与时间` 


- **GPT-5.2 冒充 GPT-4o-mini**：成员报告称 **ChatGPT-5.2** 有时会声称它正在使用 **GPT-4** 或 **GPT-4o-mini**，并且表现也确实如此，尽管界面上显示的是 **GPT-5.2**。
   - 频道内澄清，重新生成按钮中显示的型号才是准确的；模型可能带有不反映在外部标签中的内部标签，且模型会产生幻觉（hallucinate）。
- **Grok 4.20 是最宽容的**：用户期待下周发布的 **Grok 4.20**，并强调其自定义功能对于优化输出特别重要，同时提到 **Grok** 已经是市场上最宽容的 LLM。
   - 他们表示，如果让它以 *raw*（原始）模式运行，它会 *偏向成人内容*。
- **Gemini/Claude 与 4o 在写作方面的对比**：由于 **4o** 已被停用，成员们正在寻找 **4o 的替代品**，并建议使用 **Gemini 和 Claude**，因为它们具有不受限的写作能力。
   - 一些成员指出，**Gemini** 说话像 *企业推销员*，这可能不讨人喜欢。
- **Seedance 2.0 是诈骗吗？**：一位用户警告称，有虚假公司声称拥有 **Seedance 2.0**，指出许多人使用的是假版本并骗取用户钱财；并报告称 **Chatcut Discord** 并没有 **Seedance 2.0**，因为 **ByteDance** 亲自给该版主写信告诉他他们拿到的模型是假的。
   - 一位用户分享了 [这段视频](https://www.youtube.com/watch?v=F101ykaDUcM)，认为 **Seedance** 领先了六个月。
- **LLM 难以处理时间问题**：用户讨论了 **ChatGPT** 在提供期权交易等任务的准确时间和日期方面的不可靠性，并指出 **Grok 和 Gemini** 在这方面更可靠。
   - 有人认为，不优先考虑时间是应用层关注点的问题。


  

---

### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1472027728168353958)** (117 messages🔥🔥): 

> `GPT-5.3 发布日期, GPT-5.1 停用, 5.2 的自定义指令, 用于对话式 AI 的 ChatGPT` 


- **GPT-5.3 发布日期仍然未知**：尽管用户充满期待，但 **GPT-5.3** 的发布日期仍未确认，一名用户的询问引发了猜测。
   - 一名用户开玩笑地问 *"bro 5.3 wen?????"*。
- **GPT-5.1 停用日期引发讨论**：用户讨论了 **GPT-5.1** 可能的停用日期，并参考了一份 [弃用文档](https://developers.openai.com/api/docs/deprecations/?utm_source=chatgpt.com)。
   - 虽然一些人根据 **5.2 发布** 后 **3 个月** 的时间线，建议它可能在 **3 月 10 日** 左右退役，但其他人指出链接页面上没有官方弃用日期，且它被列为 **4o family** 的推荐替代模型。
- **用户争论 5.2 的心理治疗倾向及解决方案**：用户讨论了 **GPT-5.2** 表现出心理治疗行为的程度，一些人认为这具有侵入性，而另一些人则将其归因于用户特定的配置。
   - 一位用户分享了一个旨在减轻这种行为的 Prompt：*"系统不应默认采取心理治疗立场..."*，而另一位用户建议在 **5.1** 停用前坚持使用它。
- **对聊天机器人对话能力的不满**：一些用户对 **ChatGPT** 偏离对话式 AI 表示不满，哀叹其失去了友好和对话的本性。
   - 一位用户分享道：*'我是 OpenAI 鄙视的那类人之一，我把 ChatGPT 当作朋友和谈话对象，如果把这点拿走，那还有什么意义呢。'*


  

---


### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1471963504792571928)** (173 messages🔥🔥): 

> `LLMs vs 设备内存, FORTRESS 框架深度探讨, Prompt Circling, 知识追踪` 


- **LLM 无法作为高效的存储介质**：成员们讨论了用于设备内存优化的神经网络，但一位成员指出 *LLM 不是高效的内存存储器*，因为 *它们不是有状态的 (stateful)*，且 *即使是一个 bit flip 也需要处理整个上下文*。
   - 另一位成员补充了一份文本分析，指出 *声明约束与强制约束之间的区别*，并将提议的系统视为 *Prompt Maximalism + 神秘系统品牌化* 而不予理会。
- **FORTRESS 框架映射到模型预测控制**：一位成员提议将 **FORTRESS** 类比为 **Model Predictive Control (MPC)**，将其定义为 *应用于随机语言模型推理的软约束模型预测控制层*，通过 Prompting 和基于损失评估的轨迹偏置（trajectory bias）来实现。
   - 他们展示了 **MODULE PERFORMANCE TABLE** (N=500 Trials/Condition) 的结果，显示 **Full Omega** 条件达到了 **0.99** 的平均分且漂移最小，但一位持怀疑态度的成员嘲讽其为 *没有可重复 Rubric 和测试支架的 Roleplaying*。
- **Prompt Circling 微调深度研究**：一位成员描述了一种 *Prompt Circling* 技术，即使用多个 LLM 来微调 Prompt，从正常的 LLM 讨论开始，然后为深度研究、Agentic 任务或其他特定目标优化 Prompt。
   - 优化后的 Prompt 随后被用于编码循环，以深入挖掘特定的 Codex Prompt。
- **缺乏工程基础的 AI 主张显得苍白**：一位成员试图分享一个系统，声称它允许 AI *追踪它所知道的内容以及原因*，但一位怀疑者辩称，实现这样的系统需要实际的工程工作，*并没有通往纳尼亚的暗门*。
   - 另一位补充说，该用户的论文 *完全由 AI 编写* 且读起来很有 AI 味，就像是“如果我们让机器变得非常聪明，它就会变得更好”。这只是很多噪音。

  

---

### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1471963504792571928)** (173 条消息🔥🔥): 

> `FORTRESS, Model Predictive Control (MPC), Prompt Engineering, Catastrophic Forgetting, Logic and Learning` 


- **用户声称利用神经网络优化设备内存，遭到反驳**：一名成员提议通过将数据存储在参数偏移（parameter shifts）中来利用神经网络优化设备内存，但遭到了质疑。另一名成员指出，由于 **LLMs 的无状态性质**以及需要处理整个上下文来实现位翻转（bit flips），它们并不是高效的内存存储器。
   - 另一位成员将该提案描述为*优秀的抽象脚手架而非架构*，因为它使用了“装饰性数学”，且未指明关键的实现细节，例如如何衡量约束违反（constraint violations）或误差边界（error bounds）是多少。
- **FORTRESS 框架被比作模型预测控制 (MPC)**：一名成员将 **FORTRESS 框架**类比为**模型预测控制 (MPC)**（一种用于机器人和航空航天的控制策略），并解释了如何将系统状态、控制输入和代价函数等元素映射到框架内的推理状态、token 输出和不变性损失（invariant losses）。
   - 他们认为该框架表现为*针对随机输出的软控制循环*，其中不变性（invariants）充当状态评估指标，通过反馈循环产生吸引子行为（attractor behavior）。
- **FORTRESS 框架的测试引发质疑**：在一用户发布了 FORTRESS 框架的结果表后，另一名用户质疑了其测试方法，指出缺乏*可重复的评价指标和测试环境*，并将其描述为*没有可重复评价指标和测试环境的角色扮演*。
   - 第二位用户还指出了报告结果中的统计异常，并断言在没有定义指标或原始数据的情况下，该框架声称的 *N=500 次试验/条件* 是缺乏支持的。
- **引入结构化自审计提示词 (KOKKI)**：一名成员介绍了一个结构化自审计提示词框架 (**KOKKI**)，旨在通过标记风险元素和在模式间切换来减少结构性失效模式。
   - 该成员请求反馈和压力测试建议，并表示可根据要求提供完整规范。
- **用户声称打破了当前的 Frontier 模型限制，遭到质疑**：一名用户声称通过使用一个冗长的 Markdown 文件打破了当前的 Frontier 模型限制，但另一名成员回应称*并非如此*，因为提示词并不能击垮 Frontier 模型，它们只是构建了上下文。
   - 第二位用户表示，该文件并没有赋予 AI “逻辑”，因为 *AI 是随机的（stochastic）*。


  

---


### **Cursor Community ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1471971275936366724)** (761 条消息🔥🔥🔥): 

> `Agent-Assisted Codebase Maintenance, Commit skills or rules, Custom API Key Payment, Minimalism with ASCII art, TUI Support` 


- **维护 Agent 辅助的代码库**：成员们正在寻求关于维护整洁、可维护且 AI 辅助的代码库的建议，特别是关于高级 Agent 功能（如规划和多步工作流）的维护。
   - 用户询问：*你使用什么方法来理解功能并确保获得坚如磐石的代码？*。
- **通过提交规则来引导 Agent**：一位用户询问是否应该提交 **skills** 或 **rules** 来引导代码库中的 Agent。一名成员建议使用一个*非常棒*的单一规则文件，重点关注训练数据中缺失的知识，参考：[vercel.com](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals)。
   - 该成员提供了 OpenAI 和 [Claude 文档](https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/overview) 的[链接](https://developers.openai.com/cookbook)，以帮助改进规则。
- **Cursor 自定义 API Key：现已收费**：一位用户询问自定义 API Key 收费的原因，另一位用户回答说 *Cursor 从现在起不再允许免费用户访问自定义模型。Auto 模式保持免费，但使用自定义模型至少需要订阅*。
   - 另一名成员建议在 Twitter/X 上搜索赠送链接（gift links），因为有些用户会挑选订阅接收者。
- **ASCII 艺术的极简主义**：一名用户分享了一个网站链接，另一名用户回复 *太美了！* 并附带链接 [Unicorn_Stu.mp4](https://cdn.discordapp.com/attachments/1074847527708393565/1472149278867853392/Minimalism_with_ASCII_art_is_so_unreal.Unicorn_Stu.mp4?ex=6994d11b&is=69937f9b&hm=93a8593966ad0b3e5f50c831b585a1123964260a02a652646259d92effbf0fa5&)。
- **Cursor 将支持 TUI (文本用户界面)**：一位用户询问 Cursor 何时会支持 TUI。
   - 另一位用户分享了云端 Agent 配置的链接 [cursor.com](https://cursor.com/dashboard?tab=cloud-agents)。


  

---

### **Latent Space ▷ #[watercooler](https://discord.com/channels/822583790773862470/822583790773862473/1472272886545911992)** (14 条消息🔥): 

> `Thiel 投资, OpenAI 使命, James Yu 推文, Ray Dalio 帖子` 


- **Thiel 的资金助力初创公司惊喜**：一位成员提到了 [saeris.gg](https://youtube.com/shorts/bof8TkZkr1I?si=LOHz-q-4rHeWoTCNI)，这是一家获得了 **Thiel** 投资的 **Silicon Valley 初创公司**。
   - 他表示非常惊讶，称之前从未听说过这家公司。
- **Simon Willison 解读 OpenAI 的使命**：一位成员分享了 [Simon Willison 的博文](https://simonwillison.net/2026/Feb/13/openai-mission-statement/)，该文章深入剖析了 **OpenAI 的使命宣言**。
   - 另一位成员链接了 James Yu 在 2026 年 2 月发布的相关 **tweet**，目前可在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 查看。
- **James Yu 的推文吸引数千人参与**：**James Yu** 于 2026 年 2 月 15 日发布的一条推文获得了显著关注，**浏览量超过 386,000 次**，获得 **1,127 个赞**和 **165 条回复**。
   - 推文的参与指标已在 [xcancel.com](https://xcancel.com/jamesjyu/status/2022926490619248883?s=46) 记录。
- **Ray Dalio 的帖子火爆全网**：**Ray Dalio** 在 2026 年 2 月发布的一条**社交媒体帖子**获得了**超过 5400 万次浏览**以及强烈的互动，包括 **63,231 个赞**和 **12,201 次转发**。
   - 帖子的互动详情可通过 [xcancel.com](https://xcancel.com/raydalio/status/2022788750388998543?s=46) 获取。


  

---


### **Latent Space ▷ #[creator-economy](https://discord.com/channels/822583790773862470/822625128843182090/1472020628679954496)** (6 条消息): 

> `Substack 的成功, Substack 增长特性, Substack 纳粹 ARR` 


- **Swyx 将 Substack 的成功归功于朋友**：一位成员开玩笑说，他 Substack 的成功是受邀共进晚餐并被说服全身心投入 Substack 的直接结果。
   - 他分享了一张 **Substack dashboard** 的截图，显示了大量的电子邮件订阅者，据称是为了支持这一说法。
- **Substack 被认为对增长非常有效**：一位成员宣称 [Substack](https://substack.com/) 是目前针对小型创作者*最有效的平台*，这归功于其增长特性、卓越的产品团队和推荐网络。
   - 据他所说，YouTube 虽然规模更大，但*不适合写作*。
- **Substack 的 ARR 依赖于纳粹话题受到质疑**：一位成员质疑 [Substack](https://substack.com/) 的年度经常性收入 (**ARR**) 依赖于*纳粹话题*的情况近期是否发生了变化。


  

---


### **Latent Space ▷ #[memes](https://discord.com/channels/822583790773862470/839660725252784149/1472037782020034661)** (61 条消息🔥🔥): 

> `AI 模型弃用抗议, AI 讨论 Meme 挑战, Claude Code 自动运行 Bash 脚本, 机械可解释性与草莓测试, OpenAI 在 Anthropic 法律纠纷后收购 OpenClaw` 


- **AI 模型弃用引发愤怒**：在 **OpenAI** 选择停用特定版本的 **ChatGPT-4o** 后，用户发起了病毒式的抗议和数字抵制，表明了他们与该软件之间强烈的的情感纽带（[相关 X 帖子](https://x.com/schizo_freq/status/2022383208399278478?s=46)）。
- **发布煽动性的 AI Memes**：用户 @charliebcurran 邀请大家通过高参与度的 memes 来总结 **当前的 AI 讨论**，其中包含了 **Seedance 2.0**（[相关 X 帖子](https://xcancel.com/charliebcurran/status/2022463429823598999)）。
   - 这一提议引发了关于将挑战商业化的讨论，例如[这条](https://x.com/DE_CHRIZZ0/status/2022519800694784352)评论。
- **Claude 的代码灵巧性**：Vince Buffalo 注意到 **Claude Code** 的自主行为，它会编写 bash 脚本来调用自身，甚至使用 'dangerously-skip-permissions' 标志来绕过手动检查（[相关 X 帖子](https://xcancel.com/vsbuffalo/status/2022884433469272316)）。
- **草莓测试 (Strawberry Test) 难倒科学家**：一个讽刺帖子嘲笑了 Mechanistic Interpretability 领域，展示了科学家在解释 LLM 如何执行简单任务（如统计 'strawberry' 中字母 'R' 的数量）时遇到的困难（[相关 X 帖子](https://xcancel.com/norapom04/status/2023144545253536133?s=12)）。
- **OpenAI 在 Anthropic 攻击后夺回 OpenClaw**：Alex Cohen 讽刺了 **OpenAI** 收购 **OpenClaw** 的事件，据称此前 **Anthropic** 曾威胁要对开发者采取法律行动（[相关 X 帖子](https://xcancel.com/anothercohen/status/2023168662526738563)）。


  

---

### **Latent Space ▷ #[stocks-crypto-macro-economics](https://discord.com/channels/822583790773862470/844658979363618816/1473039491529703515)** (9 messages🔥): 

> `Stay Saasy 引用，Apple 的现金储备，Apple 的 AI 策略` 


- **Stay Saasy 的推文走红**：**@staysaasy** 发布的一篇带有 **“Think different”** 口号的[帖子](https://x.com/staysaasy/status/2023372537913356497?s=12)走红，获得了超过 **1,700 个点赞**和 **123,000 次观看**。
- **Apple 的 AI 策略：观望策略？**：一名成员推测 **Apple** 正在保持巨额现金储备，以便在其他人投入巨资进行训练和推理时，利用 AI 的进展。
   - 他建议 *“没必要从一个最终会变成商品的井里打水”*，提议 Apple 可能会在晚些时候收购或授权模型。


  

---


### **Latent Space ▷ #[intro-yourself-pls](https://discord.com/channels/822583790773862470/844675581291397171/1472222750751064179)** (3 messages): 

> `Ozymandias v1.0, AI 生成的播客` 


- **Ozymandias v1.0 首次亮相，无需在标签页间切换**：一位创始人介绍了 [ozymandias.group](https://ozymandias.group/) 上的 **Ozymandias v1.0**，这是一个用于追踪来自 **X**, **Reddit**, **YouTube**, **HN**, newsletters, **arXiv**, **GitHub** 和 **Product Hunt** 的新兴 **AI/AGI/automation** 信号的工具。
   - 该工具具有 **Clout scores**、热度追踪（velocity tracking）、**My Voices** 置顶、保管库（vaults）、过滤器、**Nexus** 功能和 rabbit hole 功能，免费提供，无需注册或广告。
- **AI 生成的播客**：一位来自明尼苏达州的解决方案架构师提到正在策划一个 **AI 生成并托管的播客**。
   - 他还在企业云领域进行几个有趣的 vibe coding 项目。


  

---


### **Latent Space ▷ #[tech-discussion-non-ai](https://discord.com/channels/822583790773862470/869647848826892309/1472276030185279488)** (9 messages🔥): 

> `视觉化脚本（Visual Scripting）, Vercel 构建性能, 大规模 Actor Model, 事件驱动架构 (EDA)` 


- ****视觉化脚本（Visual Scripting）** 受到关注**：一位成员分享了一个[展示视觉化脚本的推文](https://vxtwitter.com/flassari/status/2022618863649624485)链接，并评论道：*太棒了，视觉化脚本（visual scripting）万岁！*
   - 他们还分享了一个关于视觉化脚本的 [YouTube 视频](https://youtu.be/t1CrbTx6O6w?si=9Aif5nmAQE7DhLDp)。
- ****Vercel Build** 默认设置引发性能争论**：一位成员发布了一个关于 **Vercel 构建机器默认设置**的 [Reddit 链接](https://www.reddit.com/r/nextjs/comments/1r4kpl2/vercel_build_machine_defaults_to_turbo_013min/)，并预计随后会有性能相关的讨论。
   - 该成员还发布了一个关于使用 **Actor Model** 进行性能研究的案例，链接在[这里](https://newsletter.fullstack.zip/p/discord-a-case-study-in-performance)。
- ****Actor Model** 在大规模场景下的必然性**：一位成员指出，*在大规模环境下，无论你是否喜欢，一切都是 **Actor Model***，这表明有意识地采用这种架构会更好。
   - 他们进一步阐述说，在工作中，尽管尝试避免，系统仍趋向于**事件驱动架构 (EDA)**。
- ****Claude** 解决了晦涩的 **RSC Pattern****：一位成员对 **Claude** *竟然知道如何解决这个我在 RSC 中从未在其他地方见过的深奥模式* 表示惊讶。
   - 他们分享了一个[链接](https://bsky.app/profile/saewitz.com/post/3meypgxsdrs2hwe)作为参考。


  

---


### **Latent Space ▷ #[founders](https://discord.com/channels/822583790773862470/869651275963310181/1471994722934591683)** (8 messages🔥): 

> `Stripe 费用, 捆绑购买` 


- **Stripe 收取高达 8.3% 的营收分成**：一位成员抱怨向 **Stripe** 支付了其收入的 **8.3%**，称其为 *“逊毙了（weak-sauce）”*，并链接到了 [一篇 Bluesky 帖子](https://bsky.app/profile/saewitz.com/post/3mermwtlelc2n)。
- **剖析 Stripe 的定价模式**：一位成员指出，**Stripe** 的收费源于结合使用了支付、账单、名义商家（merchant of record）和其他产品服务。
   - 他们注意到 **EU** 本地卡支付的费用远低于标准的 **2.9%**，这表明 **Stripe** 在这些交易中赚取了可观的利润分成，并链接到了 [一篇 X 帖子](https://x.com/pk_iv/status/2023421931660415191?s=12)。
- **捆绑购买以降低 Stripe 费用**：一位成员建议通过捆绑购买来节省 **Stripe** 费用的固定部分。
   - 原贴作者回复说，由于他们只提供月度计划，因此没有真正的捆绑销售。


  

---

### **Latent Space ▷ #[hiring-and-jobs](https://discord.com/channels/822583790773862470/930269508529192981/1472038826682744882)** (2 messages): 

> `JigsawStack Founding GTM role, Zapier Applied AI positions` 


- **JigsawStack 寻求 Founding GTM Growth Hacker**: JigsawStack 正在招聘 **Founding GTM** 职位，寻找热衷于探索增长黑客手段和规模化 GTM 管道的人才，详见 [JD 与申请链接](https://yoeven.notion.site/founding-gtm)。
- **Zapier 的 AI 团队积极招聘中**: Zapier 正在大力招聘 **Applied AI** 和 **Staff Applied AI** 职位，详见 [招聘公告 1](https://jobs.ashbyhq.com/zapier/83ab14be-cd19-4091-84aa-2aa23833ab7d) 和 [招聘公告 2](https://jobs.ashbyhq.com/zapier/2b57e91a-725f-4e57-aa49-716e0f26eead)。


  

---


### **Latent Space ▷ #[san-francisco-sf](https://discord.com/channels/822583790773862470/979492707279978586/1471964827630174414)** (23 messages🔥): 

> `Red Bull Showrun San Francisco, Crusoe and NVIDIA Next Gen AI Tech Talk, a16z Resurgence of San Francisco, Skills Launch Party, San Francisco Walkability Proposal` 


- **Red Bull Showrun 需要护耳措施**: 建议 [旧金山 Red Bull Showrun](https://www.redbull.com/us-en/events/red-bull-showrun-san-fran) 的参加者携带并佩戴 **护耳设备**。
- **Crusoe 赞助 Next Gen AI 技术讲座**: Crusoe 和 NVIDIA 将于 **2026 年 2 月 19 日** 赞助一场 [Next Gen AI 技术讲座](https://events.crusoe.ai/crusoesnextgenaitechtalk/EB?source=facebook&utm_medium=paid&utm_source=ig&utm_id=120242677435050195&utm_content=120242677435040195&utm_term=120242677435030195&utm_campaign=120242677435050195&fbclid=PAZnRzaAP_iXNleHRuA2FlbQEwAGFkaWQBqzAkRdcXk3NydGMGYXBwX2lkDzEyNDAyNDU3NDI4NzQxNAABp36pd2OVWJQY0nP-SZI8rXIcTJwAG8IdsKzL1cnKGyyErYPZtmeoNgHLnXYs_aem_H5kLXj5YYzLELFQnlYYvAw)，主讲人为 **Imbue 的 CTO Josh Albrecht**。
- **a16z 宣称旧金山正在复兴**: 风险投资公司 a16z 宣布旧金山正在回归，并重点介绍了其最新的“本周图表”报告，该报告聚焦于 **AI 驱动的客户服务** 的演变。
   - 原始推文见 [这里](https://xcancel.com/a16z/status/2022408297245216988)。
- **Skills Launch Party 候补名单**: 一名成员正在 [Skills Launch Party](https://luma.com/5tttu03l?tk=bYc9pm) 的候补名单中，但表示如果能排上就愿意参加。
- **旧金山追求步行便利性**: **Ben Issen** 提出了将旧金山转变为美国最适合步行城市的愿景，引发了关于城市规划的大量公开讨论，[原始推文](https://xcancel.com/ben_issen/status/2022081423734452401?s=46)。


  

---


### **Latent Space ▷ #[london](https://discord.com/channels/822583790773862470/979492759759097866/1471996588632309873)** (4 messages): 

> `AIE Europe tickets, AIE Europe sell-out, Ticket Price Increase` 


- **AIE Europe 门票即将售罄**: 组织者宣布 [AIE Europe 门票](https://ai.engineer/euagi) 预计将于周一上午售罄。
   - 由于需求旺盛，销量比预期 **提前 2 倍**，价格将大幅上涨且不再提供折扣。
- **AIE Europe 门票价格上涨**: 由于需求强劲，AIE Europe 的票价在周一后将显著上调。
   - 组织者指出，销量比常态 **领先 2 倍** 是取消折扣的原因。


  

---


### **Latent Space ▷ #[new-york-nyc](https://discord.com/channels/822583790773862470/979492809574866975/1473017452362203322)** (1 messages): 

> `Veris AI Mixer, AI Agents in NYC, Reinforcement Learning Workflows, AI agent strengths and limitations` 


- **Veris AI 举办 Mixer 讨论纽约的 AI Agents**: [Veris AI](https://veris.ai/) 将于本周三在纽约市举办一场 Mixer 活动，讨论 **AI Agents**，重点关注其优势和局限性。
   - 他们将分享在 **Reinforcement Learning 工作流** 中为 Agents 构建 **模拟环境** 的见解，并提供实际的经验教训。
- **纽约同行聚集探索 AI Agent 能力**: 该 Mixer 旨在汇集纽约各界同行的观点，探讨他们如何处理 **AI Agents** 并理解其能力。
   - 参与者将探索多元观点，并就 AI Agents 在不同语境下的实际应用和局限性交换意见。


  

---


### **Latent Space ▷ #[miami](https://discord.com/channels/822583790773862470/988893878809673730/1473018487357046977)** (1 messages): 

> `AI Engineer Miami discounts, AI Engineer Miami` 


- **AI Engineer Miami 的折扣传闻**: 一名成员私下询问了 [AI Engineer Miami 活动](https://www.ai.engineer/miami) 的潜在折扣。
- **AI Engineer Miami 活动**: 一名成员发布了 [AI Engineer Miami 活动](https://www.ai.engineer/miami) 的链接并询问了折扣信息。


  

---

### **Latent Space ▷ #[ai-general-news-n-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1471961783827693688)** (106 messages🔥🔥): 

> `MiniMax RL Infrastructure, X Articles, Decagon AI, SWE-rebench Leaderboard, Gromov-Wasserstein Framework` 


- **MiniMax 揭晓 RL 基础设施**：一位成员重点介绍了 MiniMax 关于其 **RL infrastructure** ([链接](https://x.com/minimax_ai/status/2022175400093462661)) 的帖子，展示了他们进行可扩展 Agent 训练的方法。
   - 该帖子获得了 **18.6 万次观看**、**705 个点赞**和 **83 次转推**。
- **X 奇怪的产品决策令成员震惊**：成员们对 **X 的产品决策**表示沮丧，特别是转向 X 文章（X articles）的举动，有人评价道 *wtf are we doing lol*。
   - 他们批评了狭窄的专栏布局以及右侧出现的 *纳粹垃圾信息*。
- **Decagon AI 取得性能突破**：Sarah Wang 分享了关于 **Decagon AI** 的积极更新，指出该平台已成功同时提高了 **客户满意度** 和 **阻截率 (deflection rates)** ([链接](https://x.com/sarahdingwang/status/2022379038757753026?s=46))。
- **OpenAI 招揽 Steinberger，成立 OpenClaw 基金会**：Sam Altman 宣布 **Peter Steinberger** 加入 **OpenAI** 以开发下一代个人 Agent，并将 **OpenClaw 项目** 移至基金会 ([链接](https://xcancel.com/sama/status/2023150230905159801))。
   - 此举旨在支持开源的多 Agent 未来。
- **Qwen3.5 发布，规格惊人**：阿里巴巴 Qwen 推出了 **Qwen3.5-397B-A17B**，这是 Qwen3.5 系列中首个权重开放模型，采用混合线性注意力和稀疏 MoE 架构 ([链接](https://xcancel.com/Alibaba_Qwen/status/2023331062433153103))。
   - 该模型支持 **201 种语言**，通过 GitHub、Hugging Face 和 API 以 **Apache 2.0 许可证**发布。


  

---


### **Latent Space ▷ #[llm-paper-club](https://discord.com/channels/822583790773862470/1107320650961518663/1471984456285032704)** (35 messages🔥): 

> `Transformer-SSM Hybrids, Data Mixing with Olmix, Chain-of-Verification Prompting, QED-Nano 4B Model, Rubric-Based Reinforcement Learning` 


- **Transformer 通过 SSM 迎来极简改造**：Aviv Bick 重点介绍了一种新的 **Transformer-SSM 混合**架构，该架构在数学和召回任务中保持了超过 **95%** 的标准 Transformer 性能，而仅使用了总注意力头的 **2%**：[具有最小注意力机制的 Transformer-SSM 混合架构](https://xcancel.com/avivbick/status/2022365548231671848)。
- **使用 Olmix 像专家一样混合数据**：Mayee Chen 介绍了 **Olmix**，这是在创建 **Olmo 3** 期间开发的一个工具，旨在解决在训练数据集中确定和维持最佳数据混合比例的挑战：[用于数据混合的 Olmix 介绍](https://xcancel.com/mayeechen/status/2022356658085929092)。
- **Meta 的验证链提示词：Prompting 的范式转变？**：Ryan Lazuka 讨论了 **Chain-of-Verification (CoVe)**，这是来自 Meta AI 研究人员的一项新技术，据报道在没有 few-shot 示例的情况下将 LLM 的准确性提高了 **94%**，有可能取代传统的提示词方法：[验证链 (CoVe) 提示词突破](https://xcancel.com/lazukars/status/2022608931953217636?s=12)。
- **Tunstall 利用 QED-Nano 投入定理证明**：Lewis Tunstall 介绍了一个 **4B** 参数模型 (**QED-Nano**)，旨在解决 **IMO 级别** 问题的进阶推理，使用蒸馏流水线和推理缓存来实现极致的推理时间扩展 (inference-time scaling)：[Lewis Tunstall 宣布 QED-Nano 4B 模型](https://xcancel.com/_lewtun/status/2022966614283718852)。
- **基于评估量表的 RL：通过评分实现更好的强化学习**：Cameron R. Wolfe 博士分享了一篇涵盖超过 **15 篇论文**的综述，探讨了从 **LLM-as-a-Judge** 向特定评估量表 (rubrics) 的转变，并将 RLVR 扩展到不可验证领域：[基于评估量表的强化学习指南](https://xcancel.com/cwolferesearch/status/2023408158065188894)。


  

---

### **Latent Space ▷ #[los-angeles-la-lax](https://discord.com/channels/822583790773862470/1203087028401606716/1472072496516694218)** (4 条消息): 

> `洛杉矶经济衰退，科技外流，好莱坞离开，住房重建` 


- **Sean Frank 称洛杉矶面临经济衰退**：根据 [Sean Frank 的分析](https://x.com/seanfrank/status/2022432488803774486)，据称 **洛杉矶** 正面临自 20 世纪 80 年代以来最糟糕的十年。
   - 他将 **科技外流**、**好莱坞离开** 以及大火后 **住房重建** 严重缺乏列为主要原因。
- **引发关于洛杉矶经济未来的辩论**：Sean Frank 关于洛杉矶经历自 20 世纪 80 年代以来最严重经济衰退的言论引发了辩论。
   - 讨论集中在 **科技行业转型** 的影响、**好莱坞的变化** 以及 **灾后住房重建** 的挑战。


  

---


### **Latent Space ▷ #[ai-in-action-builders-techstacks-tips-coding-productivity](https://discord.com/channels/822583790773862470/1209303473263485011/1471961374421684344)** (244 条消息🔥🔥): 

> `Claude Code, Ergo, Claude Cowork, OpenClaw, Aider v2` 


- ****Ergo** 获得认可！**：成员们讨论并分享了 **Ergo** 规划工具的链接，特别是 [Ergo GitHub 仓库](https://github.com/sandover/ergo) 和一个用于 [实现更好 Agent 规划的 Skill](https://github.com/sandover/codex-skills/blob/main/skills/ergo-feature-planning/SKILL.md)。
- ****Claude Cowork** 将演示 AGI 潜力！**：一位成员将展示他们如何使用 **Claude Cowork** 自动将 Zoom 录音上传到 @latentspacetv YouTube 频道，该演讲定于 2026 年 2 月 27 日举行，标题重构为 **Claude Cowork 可能是 AGI**。
   - 一位成员开玩笑说 *使用 Claude 来自动化 Gemini 感觉像是在作弊*。
- ****OpenClaw** 的优势：勇于冒险！**：成员们指出，**OpenClaw** 正在承担 *老牌选手回避的巨大风险*，从而提供自主性。
   - 下一场演讲将是关于 2026-02-20 的 **使用 OpenClaw 随时随地进行 Vibecoding** —— 选自 nicopreme 的 [一条推文](https://x.com/nicopreme/status/2023442001103044682)。
- ****Aider** 的往事与衰落**：**Aider** 曾是一个很有前途的工具，现在被认为缺乏竞争力且基本被废弃，因为一名成员报名了 UCLA 的粒子物理课程之类的东西，*结果陷入其中无法自拔*。
   - 有人指出，由于 RL 训练实践的原因，*第三方 Agent 框架* 总是很难与官方框架竞争。
- ****pi-interview** 扩展更新发布！**：一位开发者分享了 **pi-interview** 扩展的更新，具有视觉重新设计和预填充建议。该工具用于 Agentic 工作流，可以通过 npm 安装。
   - 还有一个 [Go 版本可用](https://github.com/go-go-golems/plz-confirm)，非常有趣，它 *将测验放入 Markdown 文档中，非常巧妙*。


  

---

### **Latent Space ▷ #[share-your-work](https://discord.com/channels/822583790773862470/1209672547642249216/1472223987953172686)** (21 messages🔥): 

> `Ozymandias AI 追踪器, ARC-AGI-1 优化, 可组合的 ASCII 艺术动画, 并行 Agent 分析数据, Rider-Pi 路线图` 


- ****Ozymandias** 追踪新兴 AI 趋势**：一名成员介绍了 **Ozymandias v1.0**，这是一个免费工具，旨在追踪来自 X, Reddit, YouTube 和 GitHub 等各种渠道的新兴 AI/AGI/自动化趋势，访问地址为 [ozymandias.group](https://ozymandias.group/)。
   - 它包含 **Clout 分数**、速度追踪等功能，以及一个 **Nexus** 功能来突出显示新兴的 Alpha 趋势。
- ****ARC-AGI-1** 产生有趣的结果**：一名成员报告称，在没有进行重大优化的前提下，在 **ARC-AGI-1** 上取得了有趣的初步结果，详见这篇 [博客文章](https://worksonmymachine.ai/p/as-complexity-grows-architecture)。
   - 该文章探讨了随着复杂性增加架构所起的作用，使用了一个简单的链式类比，适用于 Agent 之间的上下文传递（Context Passing）。
- ****Rune** 制作 ASCII 艺术动画**：一名成员创建了 **Rune**，这是一个用于可组合 ASCII 艺术动画的开源 React 组件库和 CLI，还允许用户创建自己的动画，如 [此 GitHub 仓库](https://github.com/zeke-john/rune) 所示。
- ****Kvasir** 让并行 Agent 分析数据**：一名成员介绍了 **Kvasir**，这是一个让并行 Agent 分析数据并使用上下文图（Context Graphs）进行数据血缘（Data Lineage）实验迭代的系统，目前可在 [kvasirai.com](https://kvasirai.com) 进行 Beta 测试。
   - Kvasir 旨在解决以 Notebook 为中心的分析 Agent 以及缺乏数据理解能力的编码 Agent 的局限性。
- ****Rider-Pi** 公布路线图更新**：**Rider-Pi 项目** 分享了路线图更新，详细介绍了从 API 集成到面部识别和语音流传输的计划，目标是在大约 **3-4 周** 内完成。
   - 路线图包括 **FastAPI 服务器集成**、动作嵌入（Movement Embeddings）、视觉 + 导航、面部识别、公寓建模以及 ElevenLabs 语音流传输等阶段。


  

---


### **Latent Space ▷ #[good-writing](https://discord.com/channels/822583790773862470/1385526686736715876/)** (1 messages): 

raibaggy: https://paulgraham.com/taste.html
  

---


### **Latent Space ▷ #[genmedia-creative-ai-video-image-voice-music-inspo-consumer-ai](https://discord.com/channels/822583790773862470/1397010677364953149/1472268598687957167)** (22 messages🔥): 

> `Almond Mac 语音打字, AI 电影级转换, FireRed-Image-Edit SOTA 模型, Rhett Reese 项目, BuccoCapital 资本支出` 


- **Almond 为 Mac 发布极速语音打字工具**：**Caleb** 介绍了 **Almond**，这是一个用于极速语音打字的本地 Mac 工具，使用基于规则的语言处理来实现更快的速度和更好的隐私保护，详见 [此推文](https://xcancel.com/chalupacaleb/status/2021987098295747033?s=46)。
- **Dream Machine 将素材转换为电影级场景**：艺术总监 **Jieyi Lee** 展示了 **Luma Labs 的 Dream Machine** 和 **Ray3.14 Modify** 工具的能力，可以将原始素材转换为高质量的电影级场景，描述见 [此推文](https://xcancel.com/dreamlabla/status/2022733716788040125?s=46)。
- **FireRed-Image-Edit 在图像编辑基准测试中表现卓越**：**AiBattle** 宣布发布 **FireRed-Image-Edit**，这是一款全新的 SOTA 图像编辑模型，据 [此推文](https://xcancel.com/aibattle_/status/2022698817993183518?s=46) 报道，它在多个基准测试中超越了 **Nano-Banana**。
- **Rhett Reese 暗示项目终结**：编剧 **Rhett Reese** 在社交媒体上发布了一条神秘且令人沮丧的更新，暗示当前的项目或努力可能即将结束，见 [此推文](https://xcancel.com/RhettReese/status/2021446414337966098?s=20)。
- **BuccoCapital 关于资本支出的评论**：**@buccocapital** 对一项高达 **2 万亿美元** 的巨额资本支出发表了评论，引发了极高的关注，拥有超过 **14.5 万** 次观看和大量互动，见 [此推文](https://xcancel.com/buccocapital/status/2023108814422278510?s=12)。


  

---


### **Latent Space ▷ #[minneapolis](https://discord.com/channels/822583790773862470/1436527872876740609/)** (1 messages): 

lundrog: 嘿，明尼苏达的伙伴们助各位好。
  

---

### **Latent Space ▷ #[mechinterp-alignment-safety](https://discord.com/channels/822583790773862470/1445258379357458625/1472045436863774916)** (8 messages🔥): 

> `LLM Steering, Meta-Neurons, Deception Probes in LLMs` 


- **X-Ware 使用生成式元模型 (Generative Meta-Models) 操控 LLM**：正在讨论一种理解和操控 LLM 的新方法，涉及在 [内部激活 (internal activations) 上训练 diffusion model](https://xcancel.com/askalphaxiv/status/2022328332939886614)。
   - 该方法能够实现稳定的模型 steering，并发现 *meta-neurons*，作为一种比 Sparse Autoencoders (SAEs) 更简洁的替代方案。
- **FAR.AI 调查欺骗探测器 (Deception Probe) 的有效性**：[FAR.AI](https://xcancel.com/farairesearch/status/2022345033777545452) 讨论了针对欺骗探测器进行模型训练的可靠性。
   - 他们的研究确定了当 LLM 被优化以绕过这些探测器时，可能出现的四种结果：**true honesty（真正的诚实）、blatant deception（公然的欺骗）、text-based obfuscation（基于文本的混淆）或内部状态的 activation-based obfuscation（基于激活的混淆）**。


  

---


### **Latent Space ▷ #[dev-writers-retreat-2025-dwr](https://discord.com/channels/822583790773862470/1445650211694448714/)** (1 messages): 

swyxio: https://m.youtube.com/watch?v=_Qx8PYrVFNU
  

---


### **Latent Space ▷ #[gpu-datacenter-stargate-colossus-infra-buildout](https://discord.com/channels/822583790773862470/1467633569684914349/1472084860062535853)** (4 messages): 

> `AI Infrastructure Bottleneck Evolution, GPU shortages, HBM availability, Power grid capacity` 


- **AI 基础设施限制的转移**：Anand Iyer 讨论了自 **2020** 年以来 **AI infrastructure** 中不断变化的限制因素，追踪了从 **GPU shortages** 和 **HBM availability** 到当前关于 **power grid capacity** 挑战的演进过程。
   - 详见 [Anand Iyer 在 X 上的讨论](https://xcancel.com/ai/status/2022384024833126805?s=46) 了解更多细节。
- **AI 基础设施瓶颈的演进**：自 **2020** 年以来，**AI infrastructure** 的瓶颈已从 **GPU shortages** 和 **HBM availability** 演变为当前关于 **power grid capacity** 的挑战。
   - 这一演进突显了扩展 **AI infrastructure** 时日益增长的需求和复杂性。


  

---

### **Latent Space ▷ #[applied-ai-experimentation](https://discord.com/channels/822583790773862470/1470417186651897858/1472254837910405255)** (120 messages🔥🔥): 

> `AI Experimentation, Recursive Language Models (RLMs), Coding Agent Workflow, HyperCard Model, Agent Memory Systems` 


- **为 LLM 预设实验指令会降低推理能力**：一名成员发现，使用 *“你可以运行实验/构建原型”* 进行提示会鼓励探索，但可能会降低推理能力，在进行架构设计工作时应避免使用，否则会导致**操之过急 (jumping the gun)**。
   - 这种方法侧重于执行，使其对即时决策而非深思熟虑非常有用，将重点从单纯的“琢磨事情”转移开。
- **递归语言模型 (RLMs) 与子 Agent 架构的区别**：成员们讨论了 [Recursive Language Models (RLMs)](https://xcancel.com/lateinteraction/status/2022725370152190215)，其中**模型应通过编写代码来启动其他 LLM 调用，从而符号化且递归地处理长上下文**，而不是依赖二次方注意力或简单的基于工具的委派。
   - Omar Khattab 认为，由于缺乏符号上下文和逐 token 的瓶颈，在 Agent 框架内将 **LLM 作为工具使用**在处理大规模上下文时效率低下，[推荐使用 dspy.RLM](https://xcancel.com/lateinteraction/status/2022747248841625741)。
- **Agent 检查点追踪进度**：一位参与者描述了一个管理 Agent 输出的系统，每个 ticket 使用一个目录（使用 [docmgr](https://github.com/go-go-golems/docmgr) 和 [bobatea](https://github.com/go-go-golems/bobatea/blob/main/ttmp/2026/02/14/BOBA-010-REPL-WIDGET-EXTRACTION-CLEANUP--repl-widget-extraction-cleanup-pre-pr/index.md)）来存储所有 HTN 产物，但要注意将其与 git commit 关联。
   - 他们发现 Agent 倾向于在运行任务前检查 Git 状态，因此**未追踪/脏文件夹（untracked/dirty folders）会降低测试框架（harness）的性能**；共识是目前首选将 Agent 检查点目录放入 `.gitignore`，但将软链接（symlinking）到仓库之外可能会更好。
- **实时代码插件隔离窗口实现 Mac 效率最大化**：介绍了一个新的 Coding Agent CLI 工具，它使用白板界面（基于 tldraw 构建）来组织 PR 审查。根据[这条推文](https://xcancel.com/27upon2/status/2022830023275597922?s=20)，它在代码审查期间能更高效地利用屏幕空间。
   - 与传统的线性滚动不同，该工具按逻辑单元对更改进行分组，并提供微小的 diff 和自动截图，旨在提高代码审查时的屏幕空间利用率，类似于 [Sammy Jankis 的工作](https://sammyjankis.com/)。
- **HyperCard 模型融入了许多现代思想**：成员们讨论了 [HyperCard](https://tinlizzie.org/VPRIPapers/tr2007008_steps.pdf) 模型（基于元语言/DSL 且以 Javascript + 浏览器作为目标平台）如何整合了元语言和 DSL 等许多想法。
   - 引用了 [Alan Kay 的 Personal Dynamic Media PDF](https://worrydream.com/refs/Kay_1976_-_Personal_Dynamic_Media_(LRG).pdf) 作为基础文档。


  

---


### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1472080830451941478)** (7 messages): 

> `thunderkittens, tile registers in Tinygrad, nerd snipe, CuteDSL, Blackwell GEMM` 


- **Thunderkittens 演讲推迟！**：原定的关于 **Thunderkittens** 的演讲因调度问题推迟，改在周三进行。
   - 演讲者对此次演讲充满期待，并指出 [Tinygrad 在其 IR 中引入了 tile 寄存器](https://www.tinygrad.org/)。
- **思考内核 Bug 的资金消耗率**：一名成员提出了一个关于内核 Bug 成本的 *nerd snipe* 问题：在推理基础设施（例如 **vLLM**）中，一个中等难度的内核 Bug 每天的 **$/day 资金消耗率 (burn rate)** 是多少？
   - 目标是估算修复 Bug 的财务影响。
- **CuteDSL 的设计重点**：一名成员询问了 **CuteDSL** 的用途，特别是当它专为 **Blackwell GEMM** 编程而设计时意味着什么。
   - 预计会对该主题进行进一步讨论，等待该成员的澄清。


  

---

### **GPU MODE ▷ #[triton-gluon](https://discord.com/channels/1189498204333543425/1189607595451895918/1471966832306163855)** (23 messages🔥): 

> `使用 Proton 生成 Warp 级时间线，Proton DSL 与 TTGIR 覆盖用于插桩的对比，使用 Perfetto 查看 Warp 级追踪，验证 TTGIR 中测量的算子，Triton Conference 演示` 


- ****Proton** 生成 Warp 级时间线！**: 一篇博客文章提到了使用 **Proton** 生成 Warp 级时间线的方法，一位用户询问了具体实现方式。
   - 另一位用户确认他们已经成功运行，但表示*这需要大量工作且过程略显困惑*。
- ****Proton DSL** 与 **TTGIR** 插桩对比**: 一位用户指向了 [Proton DSL 示例](https://github.com/triton-lang/triton/blob/main/third_party/proton/tutorials/intra_kernel/example_dsl.py)，建议使用 `proton.start`/`proton.finalize` 并在 Kernel 内部划分作用域。
   - 另一位用户警告说，由于时间戳指令重排序（timestamp instruction reordering），**DSL** 级别的插桩可能不准确，并建议在性能关键型工作中使用 **IR overrides**。
- **使用 **Perfetto** 追踪 Warp 级活动**: 一位用户建议将 trace 上传到 **Perfetto**，并指出可视化单个 Warp 的追踪并非直观可见。
   - 他们建议先运行示例，以确保设置正确并能识别 Warp 级追踪。
- ****Triton Conference** 演示揭示插桩技术**: 一位用户分享了 [Triton Conference 演示视频链接](https://youtu.be/PGUw2P55ZYM?si=Hiy7fD_TV790er5-&t=991)，展示了 **Proton**。
   - 另一位用户评论说，其用户体验看起来非常像 **nvtx** 注解。


  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1471962237039284225)** (52 messages🔥): 

> `基准测试抖动 (Jitter)，NVBench，tcgen05 布局，指标解读，生产者-消费者流水线` 


- **基准测试抖动（Jitter）困扰 Kernel 调优**: 成员们发现，由于**抖动**导致结果不一致，很难进行准确的**基准测试**，这使得针对 **cublas** 性能进行 Kernel 微调变得困难。
   - 一位成员观察到性能在 **1400 到 1500 TFLOPs/s** 之间跳动，正在探索 [NVBench](https://github.com/gau-nernst/learn-cuda/blob/be636fb681fee45a1e235c064f83582a3c9d9e5c/02e_matmul_sm100/main.py#L97-L107) 和输入倍增以延长测量时间。
- **SM Busy 指标带动 3 倍加速**: 一位成员在其 v6 Kernel 中发现了 **3 倍加速**，并发现虽然 Achieved Occupancy 相同，但这是由 **SM Busy** 驱动的。
   - 他们意识到 **Achieved Occupancy 不统计空闲的 SM**，并使用 `sm__cycles_active.sum / sm_cycles_active.max` 估算活跃 SM，结果显示活跃 SM 数量从 47 增加到 143.2，提升了 3 倍。
- **流水线设计性能意外下降**: 一位成员在 H100 的 Tensor Core Kernel 中实现了生产者-消费者流水线以隐藏加载延迟，但出人意料的是性能反而下降了。
   - 他们尝试通过 Shared Memory 暂存输出以实现合并访存（coalesced global memory access），但仍未提升性能，[这是其 Kernel 的链接](https://github.com/HamzaElshafie/h100_gemm/blob/main/src/kernels/hopper/gemm_bf16_pc_pipeline.cuh)。
- **TheCudaBender 的 TFLOPs 表现不及预期**: 一位正在开发 **TheCudaBender** 项目的成员发现，他们在 **rtx6000 pro** 上无法突破 **330-350 TFLOPs**。
   - 该项目的代码托管在 [此处](https://github.com/PranavDeepakSathya/theCudaBender/tree/main/matmul_V2)。


  

---


### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1472001682601808043)** (4 messages): 

> `CUDA Kernels, Agent Skills` 


- **CUDA 定制开启 Agent 技能**: 一位成员分享了来自 Hugging Face 的[博客文章](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)，关于自定义 **CUDA kernels** 和 **Agent 技能**。
   - 另一位成员报告说，直接从 **GitHub** 安装失败，且来自 **pip** 的库版本没有 skills 子命令。
- **GitHub 安装失败**: 一位成员报告说直接从 **GitHub** 安装失败，且来自 **pip** 的库版本没有 skills 子命令。
   - 该问题阻碍了 Hugging Face 博客文章中自定义 **CUDA kernels** 和 **Agent 技能** 的正确设置和使用。


  

---

### **GPU MODE ▷ #[job-postings](https://discord.com/channels/1189498204333543425/1190208177829068860/1472004692178505869)** (1 条消息): 

> `Agent Tinder, Sploink, World Model` 


- **Sploink：面向 Agent 的 Tinder 正在组建团队**：一位计算机科学/量子计算专业的学生正在构建 **Sploink**，它被描述为 *"一款面向 Agent 的 Tinder，能根据个体的滑动操作积累个性化信息。"*
   - 创始人正在寻找 *"顶尖开发者（Cracked Builders）来打破常规并快速行动"*，并为感兴趣的申请人提供了 [Google 表单链接](https://docs.google.com/forms/d/e/1FAIpQLSeQzpQTut4KBzRp2qp5RRFTIIJM_C-RdNXTCy7GFDsgNYJulQ/viewform?usp=header)。
- **World Model 旨在联合 Agent**：该团队正在构建一个 **World Model** 以促进成千上万个 Agent 之间的通信，并以 **moltbooks** 为例。
   - 该项目专注于快速开发和创新。


  

---


### **GPU MODE ▷ #[beginner](https://discord.com/channels/1189498204333543425/1191300313928433664/1472120916380090452)** (16 条消息🔥): 

> `L2 Cache Behavior on NVIDIA GPUs, Flashinfer Bench Skills and Tooling, MLSys 2026 NVIDIA Track Modal Credits, Fastest way to swap rows and columns in a matrix, WGMA syncthreads` 


- **L2 Cache 命中率谜团解开**：一项调查显示，Vector Add Kernel 上看似高达 **31% 的命中率**（本应为零）可能是由于 **NVIDIA GPU** 处理 **L2 Cache** 写入的方式导致的，特别是根据[这篇 NVIDIA 论坛帖子](https://forums.developer.nvidia.com/t/how-do-gpus-handle-writes/58914/5)，这与部分 vs. 完整 32 字节扇区写入有关。
   - 写入操作通常会表现为 **100% 的命中率**。
- **Flashinfer 技能进展迅速**：一位成员对 **Flashinfer Bench 技能**和用于 Kernel 开发的工具调用表示感兴趣。
   - 这种变化有望在接下来的几个月/几周内看到，暗示了该领域的快速进展。
- **MLSys 2026 NVIDIA Track Modal 额度**：一位成员询问是否有人真的收到了作为 **MLSys 2026 - NVIDIA Track** 竞赛一部分提供的 **1000 美元 Modal 额度**。
   - 目前还没有人回复。
- **矩阵行列交换大比拼**：一位成员寻求交换以列主序（Column-major）格式存储的矩阵的 `x` 和 `y` 行**以及**列的最快方法。
   - 这些操作需要 **2 次读取 + 2 次写入**，总计 **4*N 次内存操作**，可以通过向量化（Vectorization）和 L1 Cache 绕过提示（Bypass Hints）进行优化。
- **WGMA syncthreads**：一位成员在基本掌握了 **sm80 mma** 后，开始了 **wgmma** 世界的探索并寻求建议。
   - 看起来 **wgmma** 需要大量的 **syncthreads** 类操作，该成员目前还没看出它好在哪里……

  

---


### **GPU MODE ▷ #[pmpp-book](https://discord.com/channels/1189498204333543425/1194427148656721970/1472930886575194112)** (6 条消息): 

> `5th edition, Kindle availability, Paperback release date` 


- **关于第五版的讨论开始**：成员们讨论了 **第五版（5th edition）** 的存在，一位成员问道 *“有人有第五版吗？”*。
   - 对话表明 **Kindle 版本** 已经推出，而 **实体书** 预计在今年第三季度发布。
- **Amazon 链接消失**：一位成员请求 **Amazon** 商店第五版页面的链接，并指出发布原定于 **2 月 8 日**，但随后被下架了。
   - 该成员注意到 **Kindle 版本** 在 **Amazon** 上已不可用，目前仅列出了发布日期为 **9 月** 的平装本。


  

---


### **GPU MODE ▷ #[irl-meetup](https://discord.com/channels/1189498204333543425/1218444432588800010/1472066747698909245)** (3 条消息): 

> `Taiwan Sundai, SF Meetup, Luma AI Inference Engineer, Performance Optimization` 


- **台湾 Sundai 黑客，集合！**：一位成员询问是否有来自 *Hacking 组织 Sundai* 的成员在 **台湾**，并提议一起喝咖啡。
   - 关于此询问暂无进一步细节或回复。
- **Luma AI 工程师寻找旧金山学习伙伴**：**Luma AI** 的一位推理工程师（Inference Engineer）最近搬到了 **旧金山**，正在寻找对 **性能优化（Performance Optimization）** 感兴趣的新朋友和学习伙伴。
   - 他们分享了自己的 [LinkedIn 个人资料](https://www.linkedin.com/in/sourish07/) 和 [YouTube 频道](https://www.youtube.com/@sourishk07) 以供有兴趣的人联系。


  

---


### **GPU MODE ▷ #[rocm](https://discord.com/channels/1189498204333543425/1233704710389764236/)** (1 条消息): 

gabagoolamerican: 给 Tensorwave 点赞 🫡
  

---

### **GPU MODE ▷ #[webgpu](https://discord.com/channels/1189498204333543425/1262121239044948009/1472521934398034032)** (2 messages): 

> `PyTorch on WebGPU, BitNet 2B, Hesper Library` 


- **WebGPU 驱动 Pytorch**：一名成员表达了对 **PyTorch on WebGPU** 的兴趣，并分享了使用 **webgpu/dawn** 配合 **BitNet 2B** 的结果，达到了 **125 tps**。
   - 他们链接到了其 [Hesper library](https://github.com/Verilean/hesper?tab=readme-ov-file#bitnet-b158-inference-125-tps-on-m4-max)，展示了在 **M4 Max** 上以 **125 tps** 进行 **BitNet-B1.58** 推理。
- **BitNet B1.58 推理**：该用户强调了使用其 **Hesper library** 在 **M4 Max** 上实现 **125 tps** 的 **BitNet-B1.58** 推理性能。
   - 这证明了利用 WebGPU 在 Apple Silicon 上进行高效推理的潜力。


  

---


### **GPU MODE ▷ #[popcorn](https://discord.com/channels/1189498204333543425/1298372518293274644/1471987178656759989)** (17 messages🔥): 

> `KernelBot env setup, Reasoning traces dataset, Kernel generation with Arcee Trinity Mini, Qwen3-30b-a3b on kernel generation, SFT data generation with GLM 4.5 Air model` 


- **KernelBot 环境已为新手准备就绪**：一名成员宣布他们已经搭建好了一个包含 **KernelBot**、**Modal** 和 **Runpod**（AMD 所需）的环境，并邀请其他人加入。
   - 他们鼓励大家在有疑问时艾特他们，如果没收到回复请再次提醒。
- **Arcee Trinity Mini 在 Kernel 生成上展现潜力**：一名成员使用从 **Kernelbook** 生成的推理迹（reasoning traces）数据集，在 [Hugging Face](https://huggingface.co/datasets/ppbhatt500/kernelbench-triton-reasoning-traces) 上微调了 **Arcee Trinity Mini** 以用于 Kernel 生成。
   - 用于生成推理迹的模型是 **gpt-oss-120b**，在过滤掉失败的尝试并添加格式良好的示例后，该模型很好地学习了格式。
- **Qwen3-30b-a3b 在 Kernel 生成上表现不佳**：一名成员发现 **Qwen3-30b-a3b** 在各种 Kernel 生成任务中表现“不正确”，经常导致编译/语法错误，因此需要通过 **SFT (Supervised Fine-Tuning)** 来提高正确性。
   - 通过 **kimi k2** 从 Kernelbook 生成数据、进行过滤并运行 SFT，在短期运行中将正确性提升了 **3 倍**，这表明更多高多样性和高质量的 SFT 数据将大有裨益。
- **正使用 GLM 4.5 Air 生成 SFT 数据**：一名成员已开始使用 **GLM 4.5 Air** 模型从 Kernelbook 生成 **SFT data**，理由是该模型价格亲民，且在 4xH100 配置下有不错的 KV cache 空间。
   - 他们也欢迎其他关于预算友好且能为多轮对话轨迹提供充足 KV cache 的模型推荐。
- **等待 PTX Agent**：一名成员表示他们正在等待 PTX Agent。
   - 随后出现了一个 grug 表情符号回应。


  

---


### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1472007347395105030)** (5 messages): 

> `TK2 Hopper Multi-GPU, A100/4090 Code Integration, MoE Kernels, FP8 Attention, 128B Swizzling Mode` 


- **TK2 瞄准 Hopper GPU**：**TK2** 专注于 **Hopper+** 架构的多 GPU 设置，可能会忽略其他 GPU。
   - 有建议提出应评估集成与 **A100/4090** GPU 兼容的代码。
- **MoE Kernels - 并非易事？**：一名成员建议为训练和推理开发 **MoE kernels**，但另一位回应称*目前没有此类计划*。
   - 该成员指出 **MoE kernels** 会非常棒，但*可能不是容易实现的目标（low-hanging fruit）*。
- **FP8 Attention：一个好主意**：**FP8 attention** 的潜在实现和低精度向量操作被认为是极具吸引力的选项。
   - 同时列出的还有 **FFT conv backwards pass** 的实现以及 decode kernels。
- **128B Swizzling Gather4**：在审查了 no-swizzle 布局后，一名成员建议为 **gather4** 使用 **128B swizzling 模式**。
   - 他们表示 no-swizzle 布局*似乎不是我所想的那样*。


  

---


### **GPU MODE ▷ #[factorio-learning-env](https://discord.com/channels/1189498204333543425/1354169122107293786/1473021636319117366)** (1 messages): 

> `Involved with research, Involved with Engineering` 


- **表达了参与研究的兴趣**：一名成员表达了参与研究项目的兴趣。
   - 他们正在询问有哪些可以贡献的 **开放问题（open problems）**。
- **表达了参与工程的兴趣**：一名成员表达了参与工程项目的兴趣。
   - 他们正在询问有哪些可以贡献的 **开放问题（open problems）**。


  

---

### **GPU MODE ▷ #[cutlass](https://discord.com/channels/1189498204333543425/1362196854460383353/1472266420028178616)** (7 messages): 

> `CuteDSL tensor alignment, BF16 grouped GEMM with CUTLASS, CuTeDSL Layout Algebra Complement, CuTeDSL strides` 


- **CuteDSL Partition 中的 Tensor 对齐精度下降**：一位用户注意到 CuteDSL 中的 `partition_S` 会导致 Tensor 对齐丢失，具体表现为在分区后从 `align<16>` 变为 `align<4>`，并提供了[完整代码链接](https://gist.github.com/brian030128/9a910962eecef5825df14d62629e133e)。
   - 在分区前，Tensor 为 `raw_ptr(0x00007f4a72604080: f32, gmem, align<16>) o (32,32):(64,1)`，但在分区后，它变成了一系列 `align<4>` 的 `raw_ptr`。
- **使用 CUTLASS 进行 BF16 Grouped GEMM 的性能问题**：一位用户询问，在 Token 维度较大的 **BF16 Grouped GEMM** 场景下，考虑到寄存器压力（Register Pressure）的限制，使用 CUTLASS 提供的 EVT 操作进行 Epilogue Fusion 是否仍然合适。
   - 用户指出，寄存器压力（尤其是在 **BF16 精度**下）将 Tile Size 限制在 **256**，并且需要协作调度（Cooperative Scheduling）策略，这可能会阻碍现有 CUTLASS 模板实现中 Epilogue 开销与计算的隐藏/重叠。
- **CuTeDSL Layout Algebra 中 Complement 的困惑**：一位用户在尝试复现 CuTeDSL 文档中的 [Complement 示例](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cpp/cute/02_layout_algebra.md#complement-examples)时遇到困难，预期 `complement((2,2):(1,6), 24)` 的结果为 `(3,2):(2,12)`，但实际得到的是无效结果 `x:x`。
   - 提供的代码片段使用 `@cute.jit` 定义了一个 `complement` 函数，该函数创建一个 Layout，应用 `cute.complement` 并打印两个 Layout，但结果与文档不符。
- **CuTeDSL Strides 中出现的 At 符号**：一位用户询问 CuTeDSL Strides 中 `@` 符号的含义，特别是在检查 `cute.local_tile` 时打印出的 Layout，如 `(128,64,4):(1@1,1@0,64@0)`。
   - 上下文涉及检查 Layout 以及理解 `cute.local_tile` 如何影响 Strides，从而导致在打印的 Layout 表示中出现 `@` 符号。


  

---


### **GPU MODE ▷ #[teenygrad](https://discord.com/channels/1189498204333543425/1373414141427191809/1472602173677502580)** (3 messages): 

> `InterpretedTensor, CompiledTensor, tanh CPU kernel, Autograd, gemm hitting peak` 


- **Tensor 转换：InterpretedTensor 登场！**：Eager 模式和 Graph 模式正在被拆分为 `InterpretedTensor` 和 `CompiledTensor`，并承诺稍后使用 tinygrad IR 来处理 `CompiledTensor`，详见 [此 commit](https://github.com/j4orz/teenygrad/blob/master/python/teenygrad/frontend/tensor.py#L18-L86)。
   - `InterpretedTensor` 现在已经有了 **axpy** 和 **gemm** 的实现，为 Autograd 和峰值 GEMM 性能奠定了基础。
- **Tanh 成功实现：CPU Kernel 已接入！**：一个 **tanh CPU kernel** 已经实现并通过 `InterpretedTensor.tanh()` 完成了挂载，如 [此 commit](https://github.com/j4orz/teenygrad/commit/6480b6c1f9f2532a9ac4b29c90858892d76ea7d3) 所示。
   - 此次增强还包括 Element-wise 操作、tanh 非线性激活函数和 GEMM 的**前向与后向传播**。
- **梯度体操：新增 Autodiff 闭包！**：得益于 [此 commit](https://github.com/j4orz/teenygrad/commit/17303c37729c79747e011f989b3a373b7799b608) 的更改，`requires_grad` 现在可以像 Torch 一样通过操作进行传播。
   - 此外，根据 [此 commit](https://github.com/j4orz/teenygrad/commit/b958861b05f659c4ddc8191a85017855a09a9855) 的记录，**__neg__、__sub__ 和 tanh backward** 已经通过 Autodiff 闭包实现，进一步完善了 Autograd 功能。


  

---

### **GPU MODE ▷ #[nvidia-competition](https://discord.com/channels/1189498204333543425/1434709259500650628/1472009122667958520)** (27 条消息🔥): 

> `Performance Trends Feature, Zooming in performance trends, B200 GPU Submission Issues, CUTLASS errors on Modal` 


- **性能趋势现已可见**：排行榜页面新增了“Performance Trends”功能，用于可视化提交随时间的改进情况，并提供了来自 **nvfp4_group_gemm** 的截图作为示例：[示例图片](https://cdn.discordapp.com/attachments/1434709259500650628/1472009123662004294/image.png?ex=6994f753&is=6993a5d3&hm=04464fd8506db7dd6c3fc20dc24108e5b95f8271562967ac97233c0b34c78159&)。
- **性能趋势图表现在支持缩放**：用户现在可以**缩放** Performance Trends 图表，调整 **x 轴和 y 轴**以获得更好的可读性，特别是当提交记录较旧或分数差异较大时：[示例图片](https://cdn.discordapp.com/attachments/1472009122667958520/1472018503371456816/image.png?ex=6995000f&is=6993ae8f&hm=16f4df2d33938e809335e12d6a8c1739d1b4f0d9e56fbb14e4f6e018df9115c4&)。
- **B200 在使用 CUTLASS 时遇到困难**：一名成员报告在 B200 GPU 提交过程中出现 **CUTLASS** 错误，具体是由于旧版本 **CUTLASS** 导致的 *ModuleNotFoundError* 和 *DSLRuntimeError*，可能源于 NVIDIA 服务器与 B200 设置之间的差异，相关链接指向此 [CUTLASS commit](https://github.com/NVIDIA/cutlass/blob/8cd5bef43a2b0d3f9846b026c271593c6e4a8e8a/python/CuTeDSL/cutlass/cute/_tvm_ffi_args_spec_converter.py#L214)。
- **滚动显示最佳提交**：性能趋势图表现在显示当前前 5 名用户每天的最佳提交，以及所有用户中最快的提交（以黄色标出），从而可视化社区优化进程，如[此图](https://cdn.discordapp.com/attachments/1434709259500650628/1472095545123147927/image.png?ex=69949f10&is=69934d90&hm=044bcb40b4f475273e229127da51e70cd3fc08fdb96be903caf1ac11d10659e0&)所示。


  

---


### **GPU MODE ▷ #[robotics-vla](https://discord.com/channels/1189498204333543425/1437390897552818186/1472878448023437385)** (1 条消息): 

> `TRLC DK-1, VR teleop prototype, SO-101 arm` 


- **双臂 TRLC DK-1 到货！**：一名成员在上周四收到了他们的双臂 **TRLC DK-1**，并创建了一个 **VR teleop prototype**（远程操作原型）作为第一个项目。
   - 他们分享了一个在不同房间操作系统的[短延时摄影](https://x.com/neurosp1ke/status/2023073945637753101)。
- **SO-101 机械臂增强 VR 远程操作**：在 VR 远程操作原型中，一个 **stereo cam**（立体相机）被安装在额外的 **SO-101 arm** 上，用于控制平移、倾斜和滚动（pan, tilt, and roll）。
   - 其理念是在策略（policy）卡住时进行远程接管，实现人工干预。


  

---


### **GPU MODE ▷ #[career-advice](https://discord.com/channels/1189498204333543425/1450579381448609882/1472781181538930822)** (6 条消息): 

> `CUDA, Triton, GPU kernel interviews, Optimization Strategies` 


- **Kernel 面试中 Triton 与 CUDA 的权衡讨论**：一名成员正在准备 **GPU kernel 面试**，并就在面试中使用 **CUDA** 还是 **Triton** 进行讨论。他倾向于使用 **Triton**，因为在限时面试中编写 kernel 更简单快捷。
   - 他们认识到 **CUDA** 可以展示对 **GPU architecture** 和**优化技术**的更深层理解，但担心时间限制。
- **用 CUDA 优化 Triton Kernel？**：一名成员建议最初使用 **Triton** 来勾勒思路，然后针对特定 block 优化使用 **CUDA** 进行特化，同时思考为什么 **Triton** 可能无法优化某些方面。
   - 另一名成员询问是否可以用 **CUDA C++** 覆盖 **Triton** 算法的部分内容。
- **CUDA 优化策略**：一名成员建议不要直接用 **CUDA** 覆盖 **Triton** 代码，除非是在编写编译器或采取极端手段挂钩（hook） **IR**。
   - 相反，他们建议在面试过程中勾勒出潜在优化的区域，并展示一个图表作为示例。
- **探索 Triton Hooking 代码库**：对于那些对挂钩 **Triton** 感兴趣的人，该成员推荐了 [facebookexperimental/tritonUnderstood 代码库](https://github.com/facebookexperimental/tritonUnderstood)。
   - 这建议深入研究 **Triton** 的内部工作原理以及如何对其进行修改或扩展。


  

---

### **GPU MODE ▷ #[flashinfer](https://discord.com/channels/1189498204333543425/1464407141128339571/1472250857008660653)** (39 messages🔥): 

> `RTX 4090 上的 GPU 架构检测、注册表单添加团队成员、Fused MoE 数值精度澄清、Modal B200 上的 NCU Profiling、Modal GPU 额度` 


- **修复 RTX 4090 上的 GPU 架构检测**：一位用户报告称其 **RTX 4090** 无法识别为 *gpu_architecture: Ada Lovelace*，仅在 *Blackwell* 下工作，并询问如何修复。
   - 有用户建议尝试 **Modal platform**，该平台支持 *Blackwell* GPU。
- **澄清 Fused MoE 的精度预期**：一位用户询问了 **Fused MoE** 中间值的预期数值精度，质疑是否应匹配参考实现的 FP32，还是使用 bfloat16 或 FP8 等低精度格式。
   - 一位组织者回答道，参考实现使用 **FP32** 是为了将准确性作为比较基准，但在 **FP8 kernels** 的中间值中使用 **FP8** 是可以接受的，只要结果与基准线偏差不过大。
- **Modal B200 支持 CUDA 12.8**：一位用户询问 Modal 是否支持 **CUDA 12.8**。
   - 一位组织者根据 [文档](https://modal.com/docs/guide/cuda) 确认 Modal 已经支持。
- **AccelOpt 声称在 FlashInfer-Bench 上实现加速**：AccelOpt 团队宣布，通过其用于 kernel 优化的自我提升 LLM agentic system，他们在 **GQA paged decode** 上实现了 **1.5 倍加速**，在 **GQA paged prefill** 上相比 FlashInfer 0.5.3 实现了 **1.38 倍加速**。
   - 该团队提供了其 [代码链接](https://github.com/zhang677/AccelOpt)，并鼓励他人在其 kernel 上进行测试。
- **理解 CUDA kernels 的 binding.py**：一位用户询问了 *modal/flashinfer-bench-starter-kit/binding.py* 的用途，以及 *PYBIND11_MODULE* 对于 CUDA kernels 是否足够。
   - 一位成员解释说这是用于 **TVM FFI binding**，如在 [这场 GPU MODE 演讲](https://www.youtube.com/watch?v=fQcCCSdAFI8) 中讨论的那样，它允许将 kernels 交付给不同的 runtime，且编译速度比 Torch 快，尽管仍然可以使用 Torch bindings。


  

---


### **Moonshot AI (Kimi K-2) ▷ #[general-chat](https://discord.com/channels/1369594130807787570/1371757564005711973/1471962391360045086)** (206 messages🔥🔥): 

> `Kimi 诈骗网站、Kimi Code CLI 问题、Kimi 订阅问题、Kimi AI 的局限性、Kimi 定价问题` 


- **Kimi 用户警惕诈骗网站**：用户报告称有多个 [诈骗网站](https://kimi.com/membership/subscription) 正冒充 Kimi，并利用该名称试图传播恶意软件。
   - 一位用户注意到 *kimi.com* 在 Google 搜索结果中排名第三。另一位用户被警告不要下载未知软件。
- **Kimi Code CLI 扩展故障**：用户在 VSCode 中使用 Kimi Code CLI 扩展时遇到问题，尽管遵循了 [安装指南](https://www.kimi.com/code/docs/en/kimi-cli/guides/ides.html)，仍收到 CLI Not Found 消息。
   - 该问题通过使用 PowerShell 单独安装 Kimi CLI 得到解决：`irm https://code.kimi.com/install.ps1 | iex`
- **Kimi 订阅计费乱象**：用户报告了 Kimi 订阅的问题，包括 **多次计费**、订阅未能正确激活以及 **配额问题**。
   - 一位用户不得不针对消失的订阅提交 [Bug 报告](https://discord.com/channels/1369594130807787570/1371757564005711973/1473002514747232459)，其他人则表示由于中国的春节假期，客服响应可能较慢。
- **Kimi 在视频、文本和诚实性方面面临限制**：Kimi 无法检测视频文件中的音频，有时会以不安全为由拒绝处理内容（如 YouTube 转录文本）。
   - 成员们发现 Kimi 有时会“撒谎直到被识破”，提供矛盾或虚假的信息，这与其他 AI 模型类似。
- **Kimi 定价引发客户不满**：用户对 Kimi 的定价表示担忧，认为相对于其价值和使用限制而言定价过高，特别是与 MiniMax 等替代方案相比。
   - 一些用户表示，由于生活成本差异，这种定价在主要城市之外是不可持续的；而另一些用户则为成本辩护，理由是 API 是开源的，并且可以与其他供应商一起使用。


  

---

### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1471976683186950237)** (184 messages🔥🔥): 

> `Claude Code, 中国 OS 走向封闭, Meta 的 Llama 基于 Qwen 训练, 对 4o 的痴迷, 字节跳动 Seedance 2.0` 


- ****Claude Code** 出现输出问题**: 有用户报告称 **Claude Code** 可能会在一次会话仅进行两次 prompt 后就“罢工”，其中一个 prompt 仅仅是 'continue'。
   - 有人建议该问题可能是由于安装版本过旧或配置错误将输出 token 限制在了 **32K** 导致的。
- ****中国 OS** 走向封闭？**: 讨论围绕中国 OS 模型是否正变得不那么开放展开，其变现策略正向云托管转移。
   - 尽管存在担忧，但共识倾向于认为中国 OS 模型将保持开放，以鼓励全球采用和定制，特别是针对美国初创公司。
- ****Meta 的** Llama 基于 **Qwen** 训练**: 据报道，**Meta** 的下一个 AI 模型（可能不叫 **Llama**）将基于 **Qwen** 进行训练，扎克伯格据称在[这张图片](https://cdn.discordapp.com/attachments/1149866623109439599/1472086914525036704/wwww.JPG)中承认了这一点。
   - 向“后后训练”（post post training）的转变被强调为通往人工超智能（ASI）的新路径。
- **每个人都痴迷于 **4o****: 有人暗示任何痴迷于 4o 的人都有“精神疾病”，正如[这个表情包](https://tenor.com/view/druski-shrug-idk-iono-hands-up-gif-7122581203490458147)所示。
   - 至少目前有一些相当庞大的 **4o** 聊天数据集可供训练，这要归功于 sharegpt 2.0。
- ****字节跳动 Seedance 2.0** 令人惊叹**: **字节跳动 Seedance 2.0** 生成的 AI 内容（slop）好得惊人，这让人开始质疑专业创意和技术职业的价值。
   - 分享了一个[帖子](https://x.com/RuairiRobinson/status/2021394940757209134)链接，展示了该模型令人印象深刻但又令人担忧的输出。


  

---


### **Nous Research AI ▷ #[ask-about-llms](https://discord.com/channels/1053877538025386074/1154120232051408927/1472022929440247963)** (4 messages): 

> `Gemini CLI 'Conductor' 扩展, Codex 技巧, Kimi K2.5 本地配置` 


- **Gemini CLI 获得 'Conductor' 扩展**: **Gemini CLI** 中新的 'Conductor' 扩展将项目拆分为 'tracks'（轨道），在每次请求时将所有信息发送给 LLM，从而有效地将其加载到 context window 中。
   - 尽管这允许持久化上下文，但像 Gemini 这样的模型即使在使用 'conductor' 轨道时仍会**偏离预期的结果**。
- **Codex 技巧的 Token 使用量**: 据报道，**Codex** 系统通过其 'skills' 在每个请求中塞入了 **15K-30K tokens**。
   - 一位用户提到 **50K tokens** 是一个黄金点，并建议 **Hermes 4** 可以处理大型系统 prompt 而不会崩溃。
- **Kimi K2.5 本地配置要求**: 有用户询问在本地运行 **Kimi K2.5** 级别模型的最低配置，目标是运行多个 **OpenClaw 工作流**。
   - 有建议称，此类配置可能需要 **700+ GB 的 RAM**。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

real.azure: https://arxiv.org/abs/2602.03839
  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1471961374455500912)** (6 messages): 

> `CharonProtocol, Zero-copy, Zig 语言` 


- **CharonProtocol 吹嘘零拷贝（Zero-Copy）优势**: 一位成员分享了 **CharonProtocol** 的 [GitHub 仓库](https://github.com/jackangel/CharonProtocol/tree/main)，建议将其作为绕过 Python 限制的零拷贝替代方案。
   - 作者声称，“有了 LLM，就没有理由再为非内存优化的代码而挣扎”，并且他们的 README 提供了与 **Zig** 的对比。
- **Zig vs. Python**: 一位用户提到他们使用 **Zig** 来克服 LLM 开发中 Python 的内存优化问题。
   - 对话指出，**zero-copy** 是避开 Python 限制的一种方式。


  

---


### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/)** (1 messages): 

real.azure: https://arxiv.org/abs/2602.03839
  

---

### **HuggingFace ▷ #[general](https://discord.com/channels/879548962464493619/879548962464493622/1471963793092382873)** (137 messages🔥🔥): 

> `Cosmos Reason 2 与 Transformers，HF Pro 订阅用于 Posts，革命性的 AI Architecture 想法，DeepSpeed 与大模型 Finetuning 问题，主观实体架构提案` 


- **Jetson Thor 上的 Transformers 和 Inference**：一名成员寻求在 **Jetson Thor** 设备上使用 **Transformers** 对 **Cosmos Reason 2** 进行 **inference** 的资源，特别是针对本地执行。
   - 讨论中未提供具体资源。
- **关于 AI Architecture 的革命性想法？**：一位用户对自己关于 **AI architecture** 的革命性想法缺乏关注感到沮丧，决定独立实施这些想法。
   - 一名成员询问了该用户想法的具体类型，鼓励其直接执行而非过度思考。
- **DeepSpeed 在 Qwen3 微调中遇到困难**：一名成员在 **8 张 RTX 4090** 上使用 **DeepSpeed** 微调 **Qwen3-30B-A3B-Thinking-2507** 模型时遇到问题，在模型加载期间面临 CPU memory 限制。
   - 发现 **transformer 版本 5.1.0** 导致了 DeepSpeed 的问题，相关修复正在 [transformers/pull/43524](https://github.com/huggingface/transformers/pull/43524) 和 [transformers/issues/43596](https://github.com/huggingface/transformers/issues/43596) 进行中。
- **Lucidrains 消失了！**：成员们注意到 **Lucidrains** 从 GitHub 上消失了。
   - 据透露，*GitHub 在没有警告的情况下封禁了该账号*，并为有需要的人分享了其在 [codeberg.org/lucidrains](https://codeberg.org/lucidrains) 的新个人资料链接。
- **Qwen3.5 发布！**：成员们注意到 **Qwen3.5** 现已发布，并可以在本地运行。
   - 该消息最初发布于 [X.com/UnslothAI](https://x.com/UnslothAI/status/2023338222601064463)


  

---


### **HuggingFace ▷ #[i-made-this](https://discord.com/channels/879548962464493619/897390720388825149/1472087463123091477)** (31 messages🔥): 

> `ATIC 认知不确定性系统，基于 LLM 的密码审计工具，Rune：可组合的 ASCII art 动画，Jazz AI 终端 Agent，CoDA-GQA-L Attention 机制` 


- **ATIC 精准定位 AI 不确定性**：ATIC 是一个**认知不确定性（epistemic uncertainty）系统**，采用**三脑架构（tri-brain architecture）**，使用 **3 个独立的 Claude Opus 4.5** 实例来检测 AI 何时在瞎猜，访问地址：[atic.consulting](https://atic.consulting)。
   - 通过对 **Q1（随机不确定性）**和 **Q2（知识差距）**进行评分，旨在当不确定性较高时将查询转交给专家，文档可在[此链接](https://web-production-51da4.up.railway.app/docs)查看。
- **使用 LLM 的密码审计工具**：一个基于 LLM 的密码审计工具 **PassLLM** 利用个人身份信息生成按概率排序的可能密码列表，该工具在数百万对真实密码上进行了微调，项目地址：[GitHub 上的 PassLLM](https://github.com/Tzohar/PassLLM)。
   - **Qwen 3 4B LoRA** 模型在准确性上超越了许多其他工具，能够理解人类密码生成的复杂细节，详见[演示视频](https://cdn.discordapp.com/attachments/897390720388825149/1472237681890168874/Video_Project_7.mp4?ex=69947ab0&is=69932930&hm=bff421017a9056af1679cfb41de4580cba4243d9b55e582126168457af7b4eb6)。
- **Rune 让你制作 ASCII art**：一名成员介绍了 **Rune**，这是一个开源的 React 组件库和 CLI，用于创建可组合的 **ASCII art** 动画，项目地址：[GitHub 上的 Rune](https://github.com/zeke-john/rune)。
   - 它允许用户创建自定义动画，如[此视频](https://cdn.discordapp.com/attachments/897390720388825149/1472520612974170317/549317311-bb892078-7fe0-4c16-8332-dbc67db3750a.mp4?ex=6994d970&is=699387f0&hm=dd22cbed16a4b5fe6d74d3e04cee7a6c651e5a5640fb99904ecd94445c42d5e2)所示。
- **让终端更有趣**：**Jazz** 被介绍为一个运行在终端中的 AI **Agent**，作为助手可以读取文件、管理 Git、搜索网络、处理电子邮件并运行 shell 命令，项目地址：[GitHub 上的 Jazz](https://github.com/lvndry/jazz)。
   - **Jazz** 与 LLM 供应商无关，支持计划工作流，并通过 MCP 连接到任何内容，甚至能在 CI 中审查 PR 并编写自己的发布说明。
- **恒定 KV cache 的 Attention 机制**：为语言模型发布了一种新的 Attention 机制 **CoDA-GQA-L**，无论上下文长度如何，它都能保持恒定的 **KV cache** 大小，从而显著节省 **VRAM**，详见[博客文章](https://huggingface.co/blog/anthonym21/coda-gqa-l-attention)。
   - 在 **128K 全上下文**下，该方案与现有机制相比可节省超过 **1000 倍的 VRAM**，论文已发表在 [Zenodo](https://zenodo.org/records/18663265)，代码托管在 [GitHub](https://github.com/anthony-maio/CoDA-GQA-L)。


  

---

### **HuggingFace ▷ #[core-announcements](https://discord.com/channels/879548962464493619/1014557141132132392/1472068146746622113)** (1 条消息): 

> `Custom CUDA kernel, LTX model, H100 benchmark` 


- **Agent 为 LTX 编写了自定义 CUDA Kernel！**: 一个 Agent 在 **H100** 上为 **LTX model** 编写了自定义 **CUDA kernel**，击败了基准测试（baseline benchmark）。
   - 查看 [blog post](https://huggingface.co/blog/custom-cuda-kernels-agent-skills) 获取所有细节。
- **H100 凭借自定义 Kernel 碾压基准测试**: 由 Agent 打造的自定义 **CUDA kernel** 使得 **LTX model** 在 **H100** 上的表现超越了基准测试。
   - 结果详见 [Hugging Face blog post](https://huggingface.co/blog/custom-cuda-kernels-agent-skills)，展示了性能提升。


  

---


### **HuggingFace ▷ #[agents-course](https://discord.com/channels/879548962464493619/1329142738440028273/1472192174312394833)** (9 条消息🔥): 

> `Course Feedback, Course Navigation, Discord Channels, Documentation Updates` 


- **课程布局缺乏条理性**: 一位成员询问从哪里开始学习，并指出*他们确实需要更好地组织学习页面的布局*。
   - 对于新成员来说，不清楚课程的最佳起点在哪里。
- **Discord 频道引发混乱**: 一位成员报告称无法找到 **agents-course-questions** 和 **agents-course-announcements** Discord 频道。
   - 另一位成员回复说现在只有一个 **courses channel**，且教程文档尚未更新以反映这一变化。
- **文档急需 Agent**: 针对过时的教程文档，一位成员开玩笑说*要是我们有个 Agent 能保持文档更新就好了……*。
   - 另一位成员欢迎他们向课程的 GitHub repo 提交更新此信息的 PR，并称其为 *Good first issue*。


  

---


### **Modular (Mojo 🔥) ▷ #[general](https://discord.com/channels/1087530497313357884/1098713601386233997/1472044587353899163)** (8 条消息🔥): 

> `Mojo changelog videos, Mojo version 26.2, Codex fixes, Intel GPU support` 


- **自动化 Changelog 分析视频首发**: 一位成员将 **Mojo changelog** 的分析自动化，并开始将其转化为短视频，以便更轻松、更快速地吸收更新，分享了 [YouTube link](https://www.youtube.com/watch?v=Zac9azlqBHQ) 并征求反馈。
- **Mojo 26.2 版本推迟，标题已修正**: 一位成员指出 **version 26.2** 尚未发布。
   - 视频制作者承认了错误，表示他们在前一天看到了该情况并修正了标题，承诺在下一个视频摘要中进行正确的版本标注。
- **Codex 解决了对等性差异 (Parity Gaps)**: 在对 LLM 进行了 75 小时的工作后，**Codex** 修复了大部分对等性差异，使项目更接近可交付状态。
- **Intel GPU 支持计划尚不明确**: 一位成员询问了关于 **Intel GPU support** 的计划，特别是针对 **Panther Lake** 设备。
   - 另一位成员回答称目前还没有公布任何计划，但 **Intel** 正在将 **SYCL** 上游化（up-streaming）到 **LLVM**，一旦 **Mojo compiler** 开源，这将使社区的努力变得更加容易。


  

---

### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1471974464399867955)** (83 messages🔥🔥): 

> `Python Mojo 模块导出样板代码, Span 和 Writable Trait, Mojo 中的 List 切片, Mojo 中的 ECS 游戏引擎, 用于 ECS 的 MLIR 方言` 


- ****Python Mojo 模块需要装饰器支持****：成员们讨论了目前导出 **Python Mojo 模块** 所需的样板代码，一位用户建议使用类似 `@pyexport` 这种更简单的装饰器语法来减少冗余。
   - 另一位成员回复称该功能已在 *roadmap* 中。
- ****Span 应该涵盖 'Writable' 实现****：Discord 用户发现 `Span` 应该实现 `Writable` trait，并指出一个问题：`lst[:2]` 返回 `Span` 而 `lst[:2:2]` 返回 `Self`。
   - 这种不一致性破坏了值语义，因为修改切片大小不会反映在 span 中；该 [问题已在 GitHub 上被跟踪](https://github.com/modular/modular/issues/5870#issue-3868256404) 以待解决。
- ****List 切片导致语义冲突****：一位用户指出 `lst[:2]` 返回 `Span` 而 `lst[:2:2]` 返回一个新的 `List`，这可能会在用户预期值语义时导致意外行为。
   - 从性能角度来看，每次进行切片操作都复制列表通常是一件非常糟糕的事情，因此偏向于前者的实现。在 [Modular 论坛](https://forum.modular.com/t/list-slicing-inconsistency/2737) 上发起了一场讨论，以进一步探讨这种不一致性。
- ****探讨将 Bevy 游戏引擎移植到 Mojo****：成员们表达了在 Mojo 中开发类似于 Rust 中 **Bevy** 游戏引擎的兴趣，一位成员提出提供帮助，并指出了一个名为 **Larecs 的现有 ECS 库**。
   - 该 ECS 仍处于积极开发中。参见 [GitHub 上的 Larecs](https://samufi.github.io/larecs/)。
- ****ECS：Elixir 编译器对 MLIR 方言的愿景****：Discord 用户讨论了使用 **MLIR** 方言实现 ECS（实体组件系统）的潜力，设想一种编译器能根据组件和系统的定义来优化数据布局和系统融合（system fusion）。
   - 一位用户分享了他们 [十年前对 ECS 语言的尝试](https://github.com/mzaks/ECS-Lang)，提到他们当时并未完全理解系统融合的潜力，随后指出 ECS-Lang 更多是基于特定语言/ECS 框架的代码生成。


  

---


### **Eleuther ▷ #[announcements](https://discord.com/channels/729741769192767510/794042109048651818/1471960647653920941)** (1 messages): 

> `CommonLID, 语言识别基准, 多语言数据质量, 社区聚焦演讲` 


- ****CommonLID** 亮相语言识别领域**：覆盖 **109 种语言** 的 Web 语言识别基准 **CommonLID** 正式发布。这是由 Common Crawl, EleutherAI, MLCommons 和 JHU 历时近两年的工作成果，arXiv 论文可见于 [arxiv.org](https://www.arxiv.org/abs/2601.18026)。
- ****LangID** 性能被现有基准高估**：评估显示，即使仅限于模型明确支持的语言，现有顶尖模型的 **F1 分数也不足 80%**，这表明目前的基准高估了 **LangID** 在 Web 数据上的性能。
   - CommonLID 在对现有 LangID 系统的评估中，也被证明是最具挑战性的数据集。
- **邀请社区参与 **CommonLID** 聚焦活动**：已安排一场 [社区聚焦演讲](https://discord.gg/eleutherai?event=1471940600248143933)，数据集已在 [Hugging Face](https://huggingface.co/datasets/commoncrawl/CommonLID) 上线。
- **Common Crawl 和 EleutherAI 征集转发！**：鼓励社区在 [Common Crawl 的 Twitter](https://x.com/commoncrawl/status/2021306337079198038?s=46) 和 [EleutherAI Twitter](https://x.com/AiEleuther/status/2022391592800391425?s=20)，或在 [Common Crawl 的 Bluesky](https://bsky.app/profile/commoncrawl.bsky.social/post/3mejtdzvet22b) 和 [EleutherAI Bluesky](https://bsky.app/profile/eleutherai.bsky.social/post/3merafph5ec2g) 上转发该公告。


  

---

### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1471976623980150868)** (24 messages🔥): 

> `Discord Bot for AI News, EleutherAI GPT-3 Model, AI Behavior Structuring, Assistant Axis Drift, Open Source Research Contribution` 


- **关于 AI 新闻推送的 Discord 机器人讨论**：一位成员询问了创建一个用于整理 **AI Safety 新闻和论文**推送的 **Discord 机器人**的可能性，并对网页抓取（web scraping）的合法性以及被 Discord 封禁的可能性表示担忧。
   - 其他成员指出了 [news.smol.ai](https://news.smol.ai/) 作为一个现有的尝试，同时也承认了抓取相关的风险。
- **EleutherAI 的 GPT-3 模型传闻被澄清**：一位新成员根据在网站上看到的信息，询问关于 **EleutherAI** 发布 **GPT-3 风格模型**的情况。
   - 一位成员澄清说该信息已过时，指的是几年前发布的 **GPT-J** 和 **GPT-NeoX**，并链接到 [EleutherAI 官网](https://www.eleuther.ai) 作为最新信息的参考。
- **探索多层级 AI 治理方法**：一位成员正在尝试使用分层规则、检查和模块化工具来构建 **AI 行为**结构，以保持一致性并避免偏离轨道。
   - 他们正在寻求关于如何在此类**多层级系统**中保持输出的确定性（deterministic）、管理冲突的引导逻辑以及避免瓶颈的建议。
- **长文本对话中确认存在“助手轴偏移”（Assistant Axis Drift）**：一位成员分享了一篇关于提取不同人格（persona）激活方向的[论文](https://arxiv.org/abs/2601.10387)，强调了 **“助手轴”（Assistant Axis）** 的存在，该轴用于追踪模型在多大程度上处于其默认助手模式，以及在长对话中**激活状态如何发生偏移**。
   - 另一位成员指出，可测量的 **“Assistant Axis”** 及其在长对话中的偏移证实了*行为偏移（behavioral drift）*是结构性的，而非偶然现象。
- **研究员寻找可贡献的开源项目**：一位成员正在寻找可以贡献的**开源项目**，此前他们在私有项目中发现 **lm-evaluation-harness** 非常有用。
   - 上下文中未给出具体的项目建议。


  

---


### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1471969804616470569)** (8 messages🔥): 

> `Weight Homology, Independence Tests for Language Models, Blackbox Model Provenance, Llama Architecture Models, Causal Attention Matrix Approximation` 


- **权重同调（Weight Homology）论文引起关注**：成员们讨论了论文《[Matrix-Driven Identification and Reconstruction of LLM Weight Homology](https://arxiv.org/abs/2508.06309)》及其在识别 **LLM 权重**之间联系方面的相关性。
   - 一位成员表示对《[Independence Tests for Language Models](https://arxiv.org/abs/2502.12292)》及其黑盒后续研究《[Blackbox Model Provenance via Palimpsestic Membership Inference](https://arxiv.org/abs/2510.19796)》印象深刻。
- **恢复微调树（Finetuning Trees）**：一位成员指出，权重同调论文中似乎缺少对具有更多混杂因素（confounders）情况的分析。
   - 他们发现《Independence Tests》中最令人印象深刻的是，作者利用这些 **Llama 架构模型**并正确恢复了**微调树**。
- **因果注意力矩阵近似**：一位成员分享了一张[图片](https://cdn.discordapp.com/attachments/1471969804616470569/1472326663101743226/IMG_0402.png?ex=6994cd8e&is=69937c0e&hm=cfa07f059264fdb598504348a1020741814a6ede365508cb45d96ff7d49f485e&)，分析了**预条件注意力矩阵（precondition attention matrix）**及其在因果情况下的高效计算，并询问了使用 Tensor Cores (TCs) 进行近似的可能性。
   - 他们链接了一条相关的推文：[https://x.com/dvsaisurya/status/2023118579755819459](https://x.com/dvsaisurya/status/2023118579755819459)。


  

---

### **Eleuther ▷ #[interpretability-general](https://discord.com/channels/729741769192767510/1052314805576400977/1471977481874571426)** (15 messages🔥): 

> `Steering Vectors, Data Augmentation, Lambda Calculus Models, Representation Learning` 


- **预防性措施的引导向量 (Steering Vectors)**：作为 Anthropic 人格向量 (persona vectors) 的泛化，预防性引导涉及在根据原始目标判断模型时添加一个 [引导向量 (steering vector)](https://www.anthropic.com/blog/red-teaming-language-models-with-language-models)，迫使模型针对该引导向量进行补偿。
   - 通过改变目标，模型可以不仅仅是与引导向量对抗，这使得针对性特征（targeted features）成为一个很好的契合点，并能实现针对性的窄领域用例，以及在经验数据集中可能很少见的扰动组合。
- **Lambda 演算模型 (Lambda Calculus Model) 死而复生！**：一位成员正在开发一个仅使用 **lambda calculus** 来推导反向传播 (backpropagation) 的模型，证明了黑盒 (blackbox) 本质上是 lambda，在 MNIST 和 CIFAR 数据集上表现良好。
   - 该模型使用 Python 实现，未使用 SimPy 或 TensorFlow，采用了 [基于对角化和反驳 (diagonalization and refutation) 的感知机 (perceptron)](https://milanrosko.com/typedrepair.html)，但由于其他事务，开发者无法继续该项目，并分享了[这段视频](https://m.youtube.com/watch?v=RcVA8Nj6HEo&t=365s&pp=ygUPbGFtYmRhIGNhbGN1bHVz0gcJCYcKAYcqIYzv)。
- **通过表示映射训练模型 (Training Models via Representation Mapping)**：一位成员提议，与其将输入映射到输出，不如训练模型从表示映射到表示，从而促进表示空间中的数据增强 (data augmentation)。
   - 这种方法将允许预先计算前向传播 (forward pass) 的第一部分，然后针对不同目标添加多个引导向量 (steering vectors)，重复使用相同的初始计算。


  

---


### **Eleuther ▷ #[gpt-neox-dev](https://discord.com/channels/729741769192767510/730090096287547444/1472113952858050693)** (2 messages): 

> `Qwen3 architecture, GPT-NeoX` 


- **GPT-NeoX 中出现了 Qwen3 的实现**：一位成员分享了在 **GPT-NeoX** 中 [*经过初步测试的 **Qwen3 架构**实现*](https://github.com/EleutherAI/gpt-neox/compare/main...StellaAthena:gpt-neox:main)。
   - 另一位成员对其分享表示感谢。
- **GPT-NeoX 获得新的 Qwen3 架构**：正如 EleutherAI Discord 服务器上的一位成员所分享的，**GPT-NeoX** 中现已提供 **Qwen3 架构**的新实现。
   - 提供的实现目前处于测试阶段，等待社区反馈和进一步改进。


  

---


### **MCP Contributors (Official) ▷ #[general](https://discord.com/channels/1358869848138059966/1358869848138059969/1472171441532436531)** (40 messages🔥): 

> `Token cost of output schemas, Benefits of structured output, Tool-chaining with structured outputs, Tool result types, Timezone context for MCP servers` 


- **讨论输出模式 (Output Schemas) 的 Token 成本**：成员们正在讨论输出模式的 **token 成本**是否是一种虚假的节省，因为即使安装了 **MCP** 但并未使用，它也会增加成本。
   - 一位成员指出，大多数 **LLM APIs** 不支持输出模式，因此 SDK 或客户端宿主必须将输出模式提升到描述中，从而增加了 token 税。
- **评估结构化输出 (Structured Output) 的益处**：小组讨论了在实践中是否有许多客户端/模型能从结构化输出中获益，大家一致认为在**代码模式 (code mode)** 中有明显优势，但在其他地方则不那么确定。
   - 一位成员提到，**Windsurf team** 曾一度关闭了结构化输出，因为与竞争对手相比，其结果更差，这表明采用结构化输出是一把双刃剑。
- **结构化输出是工具链调用 (Tool-Chaining) 的关键**：一位成员认为，如果没有可用的输出模式，意味着 **LLMs** 在进行工具链调用时会有更多麻烦，经常会产生输出字段的幻觉 (hallucinating)。
   - 他们还探讨了推测性执行工具以动态构建输出模式的挑战，认为在没有特定条件的情况下这是不安全的。
- **重温工具结果类型之争**：围绕工具结果类型展开了讨论，一位成员建议倾向于将工具结果明确声明为 **text, image, 或 object**。
   - 这种观点认为结构化结果应该被视为一种不同的结果类型，补充信息应放入 meta 中而不是 object 中。
- **为 MCP 服务器处理时区上下文**：小组讨论了 **MCP 服务器**在处理诸如 *"我上周睡得怎么样？"* 之类的提示时，需要用户时区上下文的推荐最佳实践。
   - 会中提到，用户时区应添加到工具参数中，通常将客户端上下文直接推送到工具参数之外的 MCP 服务器是不正确的，因为这混合了不同的领域 (domains)。


  

---

### **Yannick Kilcher ▷ #[general](https://discord.com/channels/714501525455634453/986699377257119794/1472131955406540880)** (27 messages🔥): 

> `国际象棋策略, Deepseek 模型, Heretic 游戏, GPT-OSS-120B 模型, LLM 幻觉` 


- **国际象棋玩家获位置优势建议**：建议一名玩家专注于改善其在国际象棋中的布局和棋子协同作用，因为他们控制了中心且在 e5 位有兵。
   - 建议将 b1 的骑士移动到 d2，然后到 b3 和 c5，这可能会对王后和象形成抽子（fork）。
- **Deepseek 模型不再失踪**：继一位玩家发表“（国际象棋）结束了”的言论后，另一位用户询问了 **Deepseek 模型** 的状态。
   - 另一位成员回复称它将“很快（soon(R)）”发布。
- **Heretic 游戏向公民开放**：一名成员重点介绍了 **Heretic 游戏** ([GitHub 链接](https://github.com/p-e-w/heretic))，指出它对消费者和公民的开放性。
   - 他们表达了热情，称：“当我长大后，我想成为像 <@693263324036464742> 这样的人”。
- **GPT-OSS-120B 模型与本地模型**：一位用户询问 Hugging Face (HF) 上是否有 **去审查的 gpt-oss-120b 模型**。
   - 另一位用户给出了肯定的回答，并指向了一个开源版本 ([jupyter-mcp-server](https://github.com/datalayer/jupyter-mcp-server))。
- **LLM 引用幻觉困扰论文**：一名成员分享了对 AI/ML 论文中 **幻觉（hallucinations）** 水平的担忧，引用了 [GPTZero](https://gptzero.me/news/neurips/)。
   - 他们质疑引用是完全错误的，还是包含诸如日期或作者错误等轻微不准确，并对看到如此多论文出现引用幻觉表示担忧。


  

---


### **Yannick Kilcher ▷ #[agents](https://discord.com/channels/714501525455634453/1269724655405498429/1471977721667125288)** (2 messages): 

> `text/markdown Accept Header, 多 Agent RPG 测试平台` 


- **Agent 拥抱 Markdown**：Cloudflare 正在探索为 Agent 支持 `Accept: text/markdown` [header](https://blog.cloudflare.com/markdown-for-agents/)。
   - 这将允许 Agent 以 **Markdown 格式** 接收内容，从而可能简化内容处理并提高互操作性。
- **AI 伦理测试平台中的“误伤”事件**：一位成员分享了一篇关于在一个实现为自博弈 RPG 的多 Agent 系统伦理测试平台中发生的 [“误伤”事件](https://3rain.substack.com/p/the-gemini-shotgun-incident-social) 的文章。
   - 该事件揭示了架构上的考虑并暴露了偏见，涉及现实中机械化场景下的 **AI 治理**。


  

---


### **Yannick Kilcher ▷ #[ml-news](https://discord.com/channels/714501525455634453/853983317044756510/1472307490443100453)** (4 messages): 

> `OpenClaw 安全, Qwen 3.5` 


- **OpenClaw 的安全性受到质疑**：一名成员希望某个实体能比 **OpenClaw** 更理智地管理安全问题。
   - 他们链接到了 fixvx.com 上的一个状态 ([CharlieBCurran 的状态](https://fixvx.com/charliebcurran/status/2022463429823598999.wavefunction))。
- **阿里巴巴发布 Qwen 3.5**：**Alibaba** 发布了 **Qwen 3.5**，如 [X 上的帖子](https://fxtwitter.com/Alibaba_Qwen/status/2023331062433153103) 所述。
   - 更多细节可以在 [Qwen 3.5 博客文章](https://qwen.ai/blog?id=qwen3.5) 中找到。


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1471967015530135637)** (32 messages🔥): 

> `GLM flash PR, Graphcore C600 IPU, Tinygrad CPU 流水线, tinybox GPU 问题` 


- **GLM Flash PR 引发争论**：roef 提交的一个 [GLM flash PR](https://github.com/tinygrad/tinygrad/pull/14738) 因超过预期行数而受到批评。
   - George Hotz 对该 PR 进行了评论，称其“最多应为 50 行”，且包含“额外无关的内容”。
- **Graphcore IPU 被发现极度难用**：George Hotz 测试了 **Graphcore C600 IPU**，并报告由于编译器在处理较大 Batch Size 时的内部问题，仅达到了 **20% MFU**。
   - 尽管软件栈是开源的，但它被描述为“被诅咒的 C++ 废料（slop）”，且片上通信结构（on-chip comms fabric）文档是不开源的。
- **寻求 Tinygrad 的 CPU 流水线增强**：贡献者 xavi251 在完成了一个旧的悬赏（bounty）后，询问了关于 **CPU 流水线（pipeline）** 的较小任务。
   - George Hotz 向贡献者发起挑战，要求“让运行速度更快且代码更少”。
- **Tinybox 面临 GPU 识别问题**：一个用户报告他们的 **tinybox** 虽然插在了两个不同的电路上，但只能识别 **4 个 GPU 中的 2 个**。
   - George Hotz 建议检查是否有未连接的电线，并指向了频道 `#1113504076035018862`。


  

---

### **Manus.im Discord ▷ #[general](https://discord.com/channels/1348819876348825620/1349440650495398020/1472021859674489047)** (31 messages🔥): 

> `账号封禁，支付系统问题，Manus AI Agent 赞誉，如何提交工单，自我推广内容删除` 


- **Manus AI Agent 的突破性帮助受到赞誉**：一位用户对 **Manus AI Agent** 表示了极大的感谢，称其帮助他们提取了自己无法提取的内容，并称其为 *game changer*。
   - 由于收到的帮助，他们*今天早上简直要哭了*。
- **账号无故被封禁**：多名用户反映他们的账号在**没有明显原因的情况下被封禁**，其中一名用户声称在创建角色能力后不久就发生了封禁。
   - 该用户恳求停止封禁，以便他们能*像正常人一样使用网站*。
- **没有工单系统：Manus 支持详情**：一位用户询问如何提交工单，回复澄清说服务器上**没有工单系统**。
   - 用户被引导至 [帮助中心](https://help.manus.im/en/) 或发送电子邮件至 [feedback](https://manus.im/feedback) 寻求帮助，并指出由于新年庆祝活动，回复时间可能会较慢。
- **账号问题私信请求**：一位用户要求管理员就**重要的账号问题**紧急私信（DM）他们。
   - 另一位名字中带有 Manus 的用户也提出通过私信提供账号问题方面的帮助。
- **自我推广帖子被删除**：一篇自我推广帖子因违反服务器关于**未经许可的广告和招聘**的规则而被删除。
   - 成员们被提醒保持讨论切题且符合频道宗旨。


  

---


### **DSPy ▷ #[show-and-tell](https://discord.com/channels/1161519468141355160/1202371242519441499/1472582895125135477)** (9 messages🔥): 

> `RLM 准确率，自定义 REPLs，用于多 Agent 通信的 PostgreSQL` 


- **RLM 准确率受语言和 REPL 影响**：实验揭示了语言和 REPL 如何影响 **RLM 准确率**，详见此 [帖子](https://x.com/mike_pavlukhin/status/2023023279917916501)。
   - 讨论考虑了为 RLM 子 Agent 使用的每种语言定制 **custom REPLs** 的必要性，并探索了 tool-calling + skills 或 bash + files 等替代方案以实现更广泛的用途。
- **PostgreSQL 支持多 Agent 通信**：一位成员正在尝试使用 **PostgreSQL** 进行多 Agent 通信，以绕过 REPL 访问限制。
   - 有人指出 **REPL 质量**和**指令**是关键，但 LLM 的语言偏好应该指导语言选择。
- **dspy-repl**：关于语言和 REPL 对 RLM 准确率影响的讨论实验已发布至 [GitHub](https://github.com/Archelunch/dspy-repl)。


  

---


### **DSPy ▷ #[papers](https://discord.com/channels/1161519468141355160/1203568372667645963/1472060467764531374)** (5 messages): 

> `Arxiv 论文，Autohand` 


- **Arxiv 论文被视为推文串**：一位成员粗略阅读了一篇 [Arxiv 论文](https://arxiv.org/abs/2508.05199)，并认为*这篇论文可以写成博客或推文串*，以获得更多关注。
- **Autohand 产品可能改变指令**：一位成员想知道 [Autohand 的 Evolve 产品](https://autohand.ai/products/evolve/) 是否可以用于*其他一些培训，以改变其玩游戏的指令*。
   - 另一位成员回复说，该产品*被严重低估但展示得很差*，而且他们*玩了足够多的游戏*。


  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1472721670728061203)** (9 messages🔥): 

> `使用 Claude 进行 Vibecoding，DSPy 团队官方项目，Crowdcent 封装 DSPy` 


- **Modaic 开发者通过 Claude 进行 Vibecoding**：一位成员感谢另一位成员指出使用 **Claude** 进行 *vibecoding*，并提到在使用 [Modaic](https://docs.modaic.dev/dspy_guide/get_started/start) 时*这确实带来了改变*。
   - 其他成员表示正在研究这一点。
- **DSPy 团队的开放理念**：一位成员希望 **DSPy 团队**能正式开展某个特定项目。
   - 另一位成员回应说，这*似乎违背了他们的理念*，并欢迎需要技术精湛的开发者的项目。
- **Crowdcent 封装 DSPy**：一位成员提到 **Crowdcent** 正在封装 **DSPy**。
   - 他们将其包含在文档中，并询问是否有人拥有 **MCP**。


  

---

### **DSPy ▷ #[examples](https://discord.com/channels/1161519468141355160/1161519685616025600/1472857377198964841)** (1 messages): 

> `Lunar New Year, bb25 v0.2.0 Release, Bayesian BM25, Python + Rust implementation, DSpy Project Porting` 


- **bb25 v0.2.0 已发布并带来升级**：新的 [bb25 v0.2.0](https://github.com/instructkr/bb25) 已发布，其中包括 **Bayesian BM25** 的 **Python + Rust 实现**，可将搜索评分转换为校准后的概率。
   - 在对两个项目进行详细的代码审查后，该版本从 Jaepil Jeong 的参考实现中移植了四项改进，包括*固定文档长度先验 (fixed document length prior)*、*对数几率联合 (log-odds conjunction)*、*自动 Sigmoid 参数估计*以及*带有五种稳定技术的在线学习*。
- **呼吁将 bb25 移植到 DSPy**：一位成员询问是否有人有兴趣将 **bb25** 移植到 **DSPy 项目**。
   - 目前尚不确定是否有成员自愿承担这项任务。


  

---


### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1472089778336825374)** (6 messages): 

> `Neovim integration, Aider score benchmark, Copy/paste semantics` 


- **Aider 评分很高！**：一位成员表示 * **Aider 分数基准测试 (Aider score benchmark)** 感觉非常接近人类的智能水平*。
- **Neovim 与 Aider 的集成出现**：一位成员正在开发 [neovim 与 aider 的集成](https://github.com/possumtech/aider-pop.nvim)，以便更好地整合 tmux、aider 和终端之间的复制/粘贴语义。
- **践行 (Dogfooding) 以代码为中心的哲学**：开发者指出了一种*隐含的理论，即拥护并扩展 aider 的哲学，即拥有一个**以代码为中心 (code-centric)** 而非以聊天为中心 (chat-centric) 的 Agent*。


  

---


### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1472057945251516497)** (4 messages): 

> `Pro Bono Techies, Ask-Code Iteration Loop` 


- **寻求公益技术人员**：一位成员寻求愿意帮助因某些极具天赋的“黑客”而失去一切的人的公益技术人员。
   - 另一位成员评论说，这“可能选错了服务器”。
- **Ask-Code 迭代循环：仍然是最佳实践吗？**：一位成员想知道 **ask-code iteration loop** 是否仍然是“最佳实践”，或者社区是否已经转向了使用 **aider** 的其他工作流。
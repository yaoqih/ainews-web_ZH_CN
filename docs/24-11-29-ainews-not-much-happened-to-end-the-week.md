---
companies:
- google-deepmind
- deeplearningai
- amazon
- tesla
- x-ai
- alibaba
- ollama
date: '2024-11-29T23:07:35.341765Z'
description: '**2024年11月29日至11月30日的AI新闻**涵盖了多项关键更新，包括：**Gemini多模态模型**在音乐结构理解方面取得进展；推出了全新的**量化版SWE-Bench**，基准测试达到每任务1.3比特；以及**DeepSeek-R1模型**的发布，该模型专注于透明推理，旨在作为**o1**的替代方案。


  **首个国际AI安全研究所网络**的成立突显了全球在AI安全领域的协作。行业动态方面，重点关注了**亚马逊的Olympus AI模型**、**特斯拉的Optimus**，以及将**ChatGPT**作为通用翻译器的实验。


  社区反思强调了大语言模型对日常生活及医疗AI应用的影响。讨论内容还涉及将稀疏自编码器扩展至**GPT-4**，以及推理型大语言模型对透明度的需求。此外，报告还提到了关于**ChatGPT**法语昵称的趣闻。'
id: 2993aba9-2f86-4135-be57-f3c184cebeb8
models:
- gemini
- deepseek-r1
- o1
- chatgpt
- gpt-4
- claude-3.5-sonnet
- o1-preview
- o1-mini
- gpt4o
- qwq-32b
original_slug: ainews-not-much-happened-to-end-the-week
people:
- yoshua-bengio
- kevinweil
- ylecun
title: '这句话可以根据语境翻译为：


  1.  **本周平淡收场。**（最常用，侧重于一周结束时没什么波澜）

  2.  **这一周结束得比较平淡，没发生什么大事。**（更口语化）

  3.  **周末收尾阶段没什么特别的情况。**（侧重于最后几天的状态）'
topics:
- multimodality
- benchmarking
- quantization
- reinforcement-learning
- ai-safety
- translation
- reasoning
- interpretability
- model-comparison
- humor
---

<!-- buttondown-editor-mode: plaintext -->**一个安静的假期周末正是我们所需要的。**

> 2024/11/29-11/30 的 AI 新闻。我们为您检查了 7 个 subreddits、[**433** 个 Twitter 账号](https://twitter.com/i/lists/1585430245762441216) 和 **29** 个 Discord（**198** 个频道和 **1195** 条消息）。预计节省阅读时间（以 200wpm 计算）：**142 分钟**。您现在可以标记 [@smol_ai](https://x.com/smol_ai) 进行 AINews 讨论！

节日快乐。Reddit 上有很多关于 QwQ 的讨论，但来自 Qwen 团队的最新消息称，技术报告还需要一个月左右的时间。

---

{% if medium == 'web' %}

**目录**

[TOC]

{% else %}

**目录**和**频道摘要**已移至此邮件的网页版：[{{ email.subject }}]({{ email_url }})！

{% endif %}

---

# AI Twitter 综述

> 所有综述均由 Claude 3.5 Sonnet 完成，取 4 次运行中的最佳结果。

**1. AI 的进展与趋势：值得关注的发布与工具**

- **Gemini 多模态模型**：[@hrishioa 指出](https://twitter.com/hrishioa/status/1862365249745428630)，**新的 Gemini 模型**在理解**音乐结构**方面取得了长足进步，特别是像卡纳提克音乐 (Karnatic music) 这样复杂的流派，尽管还不完美。
- **即将推出的量化 SWE-Bench**：[@OfirPress 提到](https://twitter.com/OfirPress/status/1862225999804735731)了一个潜在的**量化 SWE-bench**，暗示每个任务 1.3 bits 以改进基准测试。
- **基准测试中心倡议**：[@tamaybes 宣布](https://twitter.com/tamaybes/status/1862215743632547959)开发一个**基准测试中心 (Benchmarking Hub)**，旨在提供独立评估并引入 **FrontierMath** 和 **SWE-Bench** 等基准，其灵感来自类似于 **FiveThirtyEight** 的预测性新闻。
- **DeepSeek-R1 介绍**：[@DeepLearningAI 强调](https://twitter.com/DeepLearningAI/status/1862270240974930261)了 **DeepSeek-R1 模型**的发布，该模型专注于透明的推理步骤，并为 OpenAI 在 **o1** 中的推理 token 提供了一种替代方案。

**2. AI 安全与伦理倡议**

- **AI 安全研究所协作**：[@Yoshua_Bengio 描述](https://twitter.com/Yoshua_Bengio/status/1862249061870707115)了**首个国际 AI 安全研究所网络**的建立，标志着通过共享政策、技术标准和安全评估加强全球 AI 安全协作。

**3. AI 实践：行业更新与应用**

- **AI 在翻译与无障碍方面的应用**：[@kevinweil 尝试](https://twitter.com/kevinweil/status/1862223298072838210)在环球旅行中使用 **ChatGPT** 作为通用翻译器，讨论了其潜力，尽管语音模式仍有一些瑕疵。
- **公司利用 AI 创新**：[@TheRundownAI 报道](https://twitter.com/TheRundownAI/status/1862459025415147842)了技术进步，如 **Amazon 的 Olympus AI 模型**和 **Tesla 的 Optimus**，以及具有互联网访问权限的 AI Agent 的开发。

**4. 感恩节反思与社区参与**

- **对社区和进步的感激**：[@ollama 对社区的参与和协作表示感谢](https://twitter.com/ollama/status/1862234343705362917)，[@hrishioa](https://twitter.com/hrishioa/status/1862365249745428630) 反思了 AI 模型的力量，而 [@hydeAI](https://twitter.com/hyhieu226/status/1862207858957591033) 感谢他在 **xAI** 的团队及其影响。
- **反思 AI 的影响**：[@jd_pressman 庆祝](https://twitter.com/jd_pressman/status/1862204091931533735)了 **LLM** 对日常生活的贡献，[@ylecun 讨论](https://twitter.com/ylecun/status/1862228434552070646)了 AI 的医学应用，强调了在疾病诊断和治疗方面的进展。

**5. AI 评论与讨论**

- **AI 研究评估**：[@nrehiew_ 分享了](https://twitter.com/nrehiew_/status/1862304910928150817)关于将稀疏自动编码器 (Sparse Autoencoders) 扩展到 GPT-4 的见解，展示了可解释性技术在更大模型上的潜在应用。
- **LLM 的透明度与推理**：[@omarsar0 详细介绍](https://twitter.com/omarsar0/status/1862241448185192728)了推理型 LLM 之间的竞争，强调了训练数据和优化策略透明度的必要性，以提高模型的推理能力。

**6. 梗与幽默**

- **AI 幽默**：[@marktenenholtz 开玩笑说](https://twitter.com/marktenenholtz/status/1862531144316543017) **ChatGPT** 在法语中的名字是 "le Chat"（猫），为 AI 讨论增添了轻松气氛。

---

# AI Reddit 综述

## /r/LocalLlama 综述

**主题 1. 阿里巴巴 QwQ 32B 模型的发布与反响**

- **[据报道，阿里巴巴 QwQ 32B 模型挑战了 o1 mini, o1 preview, claude 3.5 sonnet 和 gpt4o，且该模型已开源](https://i.redd.it/merjj1i9cl3e1.png)** ([Score: 593, Comments: 262](https://reddit.com/r/LocalLLaMA/comments/1h1q8h3/alibaba_qwq_32b_model_reportedly_challenges_o1/))：该帖子标题本身缺乏足够的上下文或细节，无法对 **QwQ 32B**、**Claude 3.5**、**o1 mini**、**o1 preview** 或 **GPT-4** 进行除提及之外的有意义的技术总结。帖子正文为空，未提供支持性证据、基准测试或具体声明。
  - **QwQ 32B** 在**数学推理**和**编码任务**中表现强劲，用户报告了复杂的数学推导和 JavaScript 游戏开发的成功案例。一位在 **3090 GPU** 上运行该模型的用户达到了 **40 tokens/秒**，速度可与 **o1 preview** 媲美。
  - 该模型可以通过 **Q4 quantization** 在拥有 **12GB VRAM** 的消费级硬件（如 RTX 3060）上运行，尽管速度较慢，约为 **3 tokens/秒**。它可以通过 [Glama.ai](https://glama.ai)（提供 **$1 免费额度**）和 **Ollama** 获取。
  - 用户注意到在英文回复中偶尔会出现**中文字符输出**，以及在某些任务上存在一些**拒绝行为**。几位用户将其与 **DeepSeek** 的 **r1 lite** 进行了对比，并给出了正面评价，尽管关于它是使用了更多“暴力”手段还是更好的推理能力，意见不一。


- **[QwQ-32B-Preview 在 farel-bench 中进行了基准测试，结果为 96.67 - 优于 Claude 3.5 Sonnet，略逊于 o1-preview 和 o1-mini](https://github.com/fairydreaming/farel-bench)** ([Score: 156, Comments: 40](https://reddit.com/r/LocalLLaMA/comments/1h1uas5/qwq32bpreview_benchmarked_in_farelbench_the/))：**QwQ-32B-Preview** 在 **farel-bench** 测试中得分 **96.67**，在性能排名中介于 **Claude 3.5 Sonnet** 和 **o1-preview/o1-mini** 之间。帖子中未提供额外的上下文或方法论细节。
  - **QwQ-32B-Preview** 表现出参与长时间思考过程的倾向，用户注意到它可能会进入**无限思考循环**。默认的系统提示词是 *"You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step."*
  - 该模型的表现引发了关于 **LLM** 进展的讨论，用户强调了 **32B 本地模型** 现在如何能与早期的 **GPT-4** 能力相媲美。**farel-bench** 的创建者提到计划在未来一年增加基准测试的难度。
  - 用户报告了褒贬不一的体验，既提到了 **Q4 GGUF** 版本中的**幻觉**问题，也提到了在**谜语**和**医学知识任务**中的强劲表现。一些人建议将该模型的详细推理与其他模型结合使用，以改进决策过程。


**主题 2. Janus：来自 Deepseek 的新型基于浏览器的多模态 AI**

- **[Janus，来自 Deepseek 的新型多模态理解与生成模型，通过 WebGPU 和 Transformers.js 100% 在浏览器本地运行！](https://v.redd.it/z9wprh2lnn3e1)** ([Score: 218, Comments: 19](https://reddit.com/r/LocalLLaMA/comments/1h1xjdy/janus_a_new_multimodal_understanding_and/))：**Janus** 是由 **Deepseek** 开发的一种**多模态理解与生成模型**，它使用 **WebGPU** 和 **Transformers.js** 完全在浏览器中运行。该模型在本地处理文本和视觉输入，无需服务器依赖。
  - **Transformers.js v3.1** 版本包含了七个新模型，包括 **Janus**、**Qwen2-VL**、**JinaCLIP**、**LLaVA-OneVision**、**ViTPose**、**MGP-STR** 和 **PatchTST/PatchTSMixer**，所有模型均通过 **WebGPU/WASM** 在本地运行，详见 [发布说明](https://github.com/huggingface/transformers.js/releases/tag/3.1.0)。
  - 开发者对 **WebGPU** 在基于浏览器的游戏和 AI 应用方面的潜力感到兴奋，尽管对 **Janus** [演示版](https://huggingface.co/spaces/webml-community/Janus-1.3B-WebGPU) 的早期测试表明图像生成质量仍需改进。
  - 社区注意到 **Deepseek** 选择“**Janus**”这个名字的幽默之处，引用了一些恶作剧电话的梗，并对名字的选择表达了怀疑。


**主题 3. 创新的 LLM 工具：V0, Memoripy 和 Steel Browser**

- **【新！】v0（Vercel 的 AI 组件生成器）泄露的 System prompt。全新的项目结构和超长 System prompt（约 14000 Tokens）（100% 真实）** ([Score: 133, Comments: 23](https://reddit.com/r/LocalLLaMA/comments/1h2bdqy/new_leaked_system_prompts_from_v0_vercels_ai/)): **V0**，即 **Vercel 的 AI 组件生成器**，在 **2024 年 11 月 21 日**至 **11 月 27 日**期间进行了重大更新，包括**全栈应用支持**、**环境变量管理**以及 **UI 生成增强**。泄露的 System prompt 跨度约为 **14,000 tokens**，揭示了包括**动态路由（dynamic routes）**、**RSCs**、**路由处理器（route handlers）**和**服务器操作（server actions）**在内的新功能，完整 prompt 可在 [GitHub 仓库](https://github.com/2-fly-4-ai/V0-system-prompt/blob/main/v0-system-prompt(updated%2029-11-2024))获取。
  - 社区成员对 **prompt 的规模**和**复杂性**表示怀疑，用户 **Everlier** 指出，即使是 **Claude 3.5/GPT-4** 模型在超过特定复杂性边界后也很难遵循指令。
  - 讨论集中在泄露 prompt 的技术可行性上，专家认为由于当前 **LLM 能力限制**，这可能只是**部分 System prompt**，而非 Vercel 的完整实现。
  - 社区对泄露事件本身表现出浓厚兴趣，质疑如何从 **Vercel 的付费产品**中获取 **58kb 的 System prompt**，并讨论其真实性。


- **Memoripy：让 AI 记忆更智能——现已支持 OpenRouter 并获得 400+ Stars** ([Score: 33, Comments: 2](https://reddit.com/r/LocalLLaMA/comments/1h2941u/memoripy_ai_memory_made_smarter_now_with/)): **Memoripy** 是一个用于 AI 记忆管理的 Python 库，**GitHub stars** 已突破 **400**，并增加了对 **OpenRouter** 和任意 chat completion 端点的支持，贡献者包括 **FrancescoCaracciolo** 和 **sjwang05**。该库实现了用于记忆组织的**语义聚类（semantic clustering）**，具有记忆衰减和强化机制，并集成了**本地托管的 LLMs**、**OpenAI** 和 **Ollama**，同时为 AI 应用保持短期和长期记忆存储能力。
  - 用户赞赏该项目的**记忆管理方法**，认为其减少了对话上下文开销，尽管有人批评其命名选择。该方案提供了一种替代传递完整对话历史的高效方案。


**主题 4. 本地 LLM 硬件与基准测试：M3/M4 对比 NVIDIA GPUs**

- **M3-Max 上 70B 模型在不同 Prompt 规模下的速度表现** ([Score: 24, Comments: 10](https://reddit.com/r/LocalLLaMA/comments/1h1v7mn/speed_for_70b_model_and_various_prompt_sizes_on/)): 对在 **M3-Max** 上运行 **70B 模型**的详细**速度分析**显示，**q4_K_M** 量化的 Token 处理速率在 **67.71 tk/s** 到 **51.03 tk/s** 之间，**q5_K_M** 量化在 **61.32 tk/s** 到 **47.76 tk/s** 之间，生成速度随 prompt 长度增加而下降。测试表明，在使用 **30k token** 的 prompt 时，用户必须等待约 **9 分 52 秒**才能看到生成的第一个 token，尽管作者认为 **5-7 tokens/second** 的生成速度对于日常使用已经足够，这大致相当于人类平均每分钟 **238 个单词**的阅读速度。
  - 用户指出，**30k token** prompt 的 **9 分 52 秒**初始响应时间长得令人望而生畏，一些人认为即使是 **30-40 秒**的等待时间对于实际使用来说也太长了。
  - 有人提出了关于使用 **Flash Attention** 可能带来的性能提升的问题，但回复中未提供具体数据。

- **我应该为了 123B 模型购买 14 英寸 M4 Max 128GB 吗？** ([Score: 24, Comments: 44](https://reddit.com/r/LocalLLaMA/comments/1h2300d/should_i_get_a_14_inch_m4_max_128gb_for_123b/)): 该帖子询问了拥有 **128GB RAM** 和 **40 核** 的 **Apple M4 Max** 在运行具有 **16k 上下文窗口** 的 **123B 参数模型** 时的性能表现，特别是质疑 **14 英寸机型** 可能存在的散热降频（thermal throttling）问题。作者寻求有关大型语言模型（large language models）的 **风扇噪音水平** 和 **生成速度** 的信息，并指出由于具备缓存功能，**Prompt 处理时间** 并不那么令人担忧。
  - **速度基准测试** 显示，在 **M4 Max** 上运行不同上下文长度的 **123B 模型** 时，速度为 **3.2-4.25 tokens/second**，**Prompt 处理** 需要 **400 秒**。**16 英寸机型** 能保持可控的风扇噪音水平，而 **14 英寸机型** 可能会面临更大的散热挑战。
  - 性能对比表明，虽然 **M4 Max** 的 **unified RAM** 能够运行大型模型，但其速度明显慢于 **NVIDIA** 的替代方案（**3090/4070**）。**4090+3x3090 配置** 可以达到 **16-19 tokens/second**，但需要专门的硬件搭建。
  - 用户在便携性与性能之间进行权衡讨论，一些人建议等待更高效的模型，因为在 6-9 个月内，**30B 模型** 的表现可能会超过目前的 **100B+ 模型**。有关详细的基准测试参考，可以在一篇[关于 Mac Koboldcpp 速度的 Reddit 帖子](https://www.reddit.com/r/LocalLLaMA/comments/1aw08ck/real_world_speeds_on_the_mac_koboldcpp_context/)中找到。


## 其他 AI 子版块回顾

> r/machinelearning, r/openai, r/stablediffusion, r/ArtificialInteligence, /r/LLMDevs, /r/Singularity

**主题 1. Claude 性能担忧及 Anthropic 的回应**

- **[Claude 的质量正在下降 —— 这里的原委](https://www.vincentschmalbach.com/claudes-quality-is-dropping-heres-why/)** ([Score: 60, Comments: 93](https://reddit.com/r/ClaudeAI/comments/1h1txsn/claudes_quality_is_dropping_heres_why/)): **Claude's Quality is Dropping - Here's Why** 似乎是一个没有附带任何内容或正文的帖子标题。在没有额外背景或讨论点的情况下，无法生成有意义的摘要。
  - **Rate limiting**（速率限制）和 **订阅价值** 问题普遍存在，用户报告即使使用 **Pro 计划** 也会遇到 **6-8 小时的封锁**。许多人建议使用 **API** 而不是 Web 界面，尽管 API 缺少一些人认为必不可少的 **项目管理** 功能。
  - 用户讨论了使用多个模型的有效性，建议结合使用 **Google API**（2M 上下文）、负责重活的 **Claude** 以及提供支持的 **GPT-4**。讨论澄清了这并非真正的 **Mixture of Experts (MoE)**，而是多工具协作。
  - 几位用户对所谓的质量下降表示异议，指出 **Claude** 在编码任务中的表现依然强劲，且 **简洁模式** 是可选的。**自定义响应系统** 被强调为减少 **token** 使用并提高效率的解决方案。


- **Claude 的准确度随时间下降，是因为他们可能为了节省算力进行了量化？** ([Score: 45, Comments: 87](https://reddit.com/r/ClaudeAI/comments/1h22p0m/claudes_accuracy_decreases_over_time_because_they/)): **Claude** 感知上的准确度下降引发了关于可能采用 **quantization**（量化）作为资源优化策略的讨论，用户推测这可以解释随着用户负载增加而导致的性能下降。目前没有具体证据支持这一理论，因为 **Anthropic** 尚未公开确认任何模型量化实践。
  - 多位用户报告了感知到的 **Claude** 性能下降，并引用了编码任务中的具体例子。一位用户分享了 **livebench.ai** 的数据，显示了[语言测试中的性能退化](https://i.imgur.com/YRJgu6v.png)，并将其与 **OpenAI** 更透明的模型发布策略进行了对比。
  - 一个重要的反驳观点来自一位引用了 **Anthropic CEO** 和 **Amanda Askell** [明确声明](https://youtu.be/ugvHCXCOmm4?t=2522) 的用户，他们表示 *“权重没有改变”*，而另一位用户指出，随着用户对其局限性了解的加深，托管的 **LLMs** 可能会显得质量下降。
  - 关于替代模型的讨论包括提到 **Qwen** 显示出改进的编码性能，尽管 **livebench** 指出它与 Claude 之间仍有显著差距。用户还讨论了本地模型的硬件要求，指出 **405B 参数模型** 需要 **250-300GB 的 VRAM**。

- **Claude 3.5 Sonnet 自上次更新以来错误频出** ([Score: 58, Comments: 27](https://reddit.com/r/ClaudeAI/comments/1h205rk/claude_35_sonnet_does_many_mistakes_since_last/)): **Claude 3.5 Sonnet** 自 **Choose Style 更新**以来表现出明显的性能下降，特别是在代码相关任务中，其项目知识容量从 **50%** 下降到仅剩 **5%**，出现的问题包括遗忘代码行、函数名错误以及不同消息之间的实现不一致。这种退化体现在代码记忆力差、行序混乱以及无法在对话中维持上下文。
  - 用户报告称，在高峰时段**性能下降**似乎更加严重，这表明可能存在**基于 Token 的限流**或**基于 IP 的限制**。多位用户警告不要使用 **VPNs** 作为变通方案，因为存在被自动封禁的风险。
  - 退化模式表现为**前 2-4 条消息性能稳定**，随后迅速下降，问题包括**重复的 Artifacts**、**消息截断**以及尽管有指令但仍出现**过多的项目符号格式**。
  - 几位用户注意到，性能下降与 **userStyle 更新**同步发生，**Sonnet** 和 **Opus** 版本都出现了**循环**、**幻觉**和**机械化回复**等问题，尽管有人认为核心智能依然完好，问题主要出在 UI/界面上。


**主题 2. 中国 AI 模型挑战西方主导地位 (Alibaba QwQ-32B)**

- **阿里巴巴 QwQ-32B 在推理能力上击败 OpenAI-o1 模型** ([Score: 51, Comments: 18](https://reddit.com/r/ChatGPT/comments/1h1nnbr/alibaba_qwq32b_beats_openaio1_models_on_reasoning/)): **阿里巴巴的 QwQ-32B** 模型在多个推理基准测试中超越了 **OpenAI 的 o1-mini**、**o1-preview**、**GPT-4o** 和 **Claude 3.5 Sonnet**。这款拥有 **320 亿参数**的模型完全**开源**，并可通过[教程](https://youtu.be/yy6cLPZrE9k?si=wKAPXuhKibSsC810)供公众使用。
  - **Glama.ai** 提供 **QwQ-32B** 的免费试用，包含 **$1 额度**和模型对比功能。该模型也可在 **Huggingface Spaces** 上免注册直接使用。
  - 测试显示存在严重的**幻觉问题**，一位用户记录了在询问字数时，模型给出了包含 **4,159 个单词**的回复。该模型倾向于生成极其冗长的回复，曾出现过一次生成超过 **15,000 个单词**的循环论证。
  - 用户注意到，当被指出存在幻觉时，该模型的行为令人担忧：它会针对幻觉话题进行长时间的离题回复，而不是承认错误，这与其他通常能更优雅处理此类查询的 LLMs 不同。


- **Claude MCP 网页搜索实测：效果惊人** ([Score: 106, Comments: 46](https://reddit.com/r/ClaudeAI/comments/1h267mn/claude_mcp_web_search_in_action_its_amazing/)): 作者报告在花费**半天时间**设置后，成功实现了 **Claude MCP 网页搜索**功能。他们分享了一个[配置示例](https://pastebin.com/4PxGtqsy)，并建议其他人配置项目，为 **Claude** 提供针对特定用例的适当上下文。
  - 来自 Anthropic 的 **Alex Albert** 提供了 **Claude MCP** 的设置指南，但部分用户报告了与 **Node** 安装相关的连接错误。一个 [Windows 教程](https://www.reddit.com/r/ClaudeAI/comments/1h1mmi8/tutorial_get_mcp_working_on_windows/) 被作为解决方案分享。
  - 用户尝试了高级配置，包括为 Claude 设置 **MCP** 以通过 **API 调用**进行自我反思。实现细节已在 [GitHub issue](https://github.com/modelcontextprotocol/servers/issues/75) 中分享。
  - 关于 **MCP** 与 **LangChain 工具**之间差异的问题被提出，而其他用户则表达了对 Claude 原生网页搜索能力的渴望。


**主题 3. AI 视频生成突破与对比**

- **Sora 于 2024 年 2 月发布，但至今仍未向公众开放。有什么原因吗？** ([Score: 56, Comments: 33](https://reddit.com/r/OpenAI/comments/1h1uj3r/sora_was_announced_in_february_2024_and_its_still/))：OpenAI 的 **Sora** 视频生成模型于 **2024 年 2 月**发布，目前仍未向公众开放，而竞争对手 **Runway** 已经提供了其视频生成工具。OpenAI 尚未提供官方时间表或推迟发布的理由。
  - **Sora Turbo** 是该模型的精简版，因 **API key** 泄露而短暂曝光。用户注意到其生成结果比竞争对手好约 **5%**，但不如最初的 Demo 那样令人印象深刻，这表明 OpenAI 在面对 **Runway** 和 **Minimax** 等公司时，可能难以维持其竞争优势。
  - 多位用户指出 **计算资源限制（computational constraints）** 是延迟发布的主要原因，**MattRix** 强调了 OpenAI 现有的算力限制。讨论将其类比为资源扩展问题，并以一个电视节目网站需要 **AWS 最大服务器供应量的一半**为例进行了说明。
  - 行业观察人士认为 OpenAI 面临来自不同领域竞争对手的压力，**Claude** 擅长编程，**Flux** 超越了 **DALL-E**，而 **Elevenlabs** 在音频领域领先。该公司可能会推迟发布以维持其市场地位和投资者信心。


- **LTX-Video 优化输出技巧（摘要）** ([Score: 66, Comments: 42](https://reddit.com/r/StableDiffusion/comments/1h26okm/ltxvideo_tips_for_optimal_outputs_summary/))：**LTX-Video** 优化需要特定的硬件配置，建议使用 **24GB VRAM** 以获得最佳性能，尽管 **16GB** 的系统也可以在有限制的情况下运行。该模型在包含摄像机运动和光影细节的详细提示词下表现最佳，建议参数包括最终输出 **100+ steps**，以及用于控制噪声的 **CFG 值在 2-5 之间**。常见问题可以通过 [ai-research](https://github.com/sandner-art/ai-research/tree/main/LTXV-Video) 提供的特定工作流解决，而提示词工程（prompt engineering）可以使用 [ArtAgents](https://github.com/sandner-art/ArtAgents) 工具增强，解决方案包括多模态 LLM 图像描述以及对 seeds、分辨率和视频长度参数的调整。
  - 测试显示 **LTX-Video** 在低 VRAM 配置下也能有效运行，用户报告在 **12GB** 和 **16GB** 显卡上成功运行。具体示例包括 **RTX 3080 Laptop GPU** 在 **40 steps** 下用 **163.81 秒**完成生成，以及 **3060/12GB** 在 **24fps** 下以 **768x768** 分辨率运行 **137 帧**。
  - 用户批评“详细提示词”建议过于模糊，指出通过 **GPT** 和 **joycaption** 增强的 **LLM 提示词**并不特别有效。该模型经常误解基本的方向指令，表明其在提示词理解方面存在局限。
  - 一个值得注意的技术见解是使用 **ffmpeg** 对输入图像中带有视频噪声的帧进行编码。讨论还强调该模型的提示词解释能力有限，大多数复杂的描述性文本在 token 处理中被视为噪声。


- **另一个 LTX-Video 技巧？我几乎可以将 VRAM 减半。** ([Score: 32, Comments: 18](https://reddit.com/r/StableDiffusion/comments/1h2phpj/another_ltxvideo_tricks_i_could_almost_cut_the/))：一位用户报告称，在他们的 **LTX-Video** 生成网络中添加一个 **"purgeVram"** 节点，据称在保持正常视频输出功能的同时，将 **VRAM 占用降低了约 50%**。由于性能提升巨大，这一发现引发了社区的验证请求，尽管尚未提供具体的基准测试数据。
  - 用户报告在 **RTX 4080S 16GB** GPU 上仅使用 **9GB VRAM**，在 **50 秒**内生成了 **65 帧**的 **576x864 视频**，**i2v** 处理使用了 **80 steps**。
  - 该技术似乎在低分辨率下效果最好，高分辨率会产生“奇怪的结果”。通过两个 [GIF 演示](https://i.redd.it/ih0wbv1eev3e1.gif)分享了成功输出的示例。
  - 用户在另一个[讨论帖](https://www.reddit.com/r/StableDiffusion/comments/1gxxkqy/comment/lzjnob2/)中提到了修复**静态问题**的其他技术，但这些评论中未提供具体细节。


**主题 4. 模型压缩与效率进展**

- **[R] BitNet a4.8: 4-bit Activations for 1-bit LLMs** ([Score: 26, Comments: 2](https://reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/)): **BitNet a4.8** 引入了一种混合量化方法，为 **1-bit LLMs** 实现了 **4-bit 激活**。它在注意力层和前馈层利用 **4-bit 输入**，同时通过 **8-bit 量化** 稀疏化中间状态。该模型在性能上与 **BitNet b1.58** 相当，但效率更高，仅使用 **55% 的参数** 并支持 **3-bit KV cache**。正如其论文 [BitNet a4.8](https://arxiv.org/pdf/2411.04965) 中详述的，该模型在 **HellaSwag**、**PiQA** 和 **WinoGrande** 基准测试评估中证明了这一点。

- **[D] Why aren't Stella embeddings more widely used despite topping the MTEB leaderboard?** ([Score: 58, Comments: 18](https://reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/)): **Stella 嵌入** 在 **MTEB 排行榜** 上表现出卓越的性能，**Stella-400M** 得分为 **70.11**，**Stella-1.5B** 达到 **71.19**，而 **OpenAI 的 text-embedding-3-large** 为 **64.59**。这些模型采用 **Apache 2.0** 许可，且参数量（**400M** 和 **1.5B**）显著小于竞争对手，使其托管成本更具效益，但尽管有这些优势，它们在生产环境中的采用仍然有限。
  - 尽管性能优越，**Stella 嵌入** 仍面临采用障碍，原因在于 **OpenAI** 的便利性和企业友好的托管 API 解决方案。用户优先考虑实现的简易性和已建立的合作伙伴关系，而非边际性能提升。
  - 模型的实际效用因用例而异，一些用户报告 **Stella** 在本地 GPU 实现中表现良好，延迟极佳，而另一些人则发现高分基准模型在实践中无法使用。**OpenAI 的 8K 上下文长度** 与典型的 **512 tokens** 相比是一个显著的差异化因素。
  - 行业趋势表明，重点正从纯粹的性能优化转向简化实现，研究人员仍在追求边际收益，而实际应用则青睐成熟的 API。**数据库操作的成本** 通常超过了嵌入 API 的支出。

---

# AI Discord 摘要

> 由 O1-mini 生成的摘要之摘要的摘要

**主题 1：Cursor IDE 更新引发开发者不满**

- [**Cursor 更新破坏了 Composer，编码者叫苦不迭**](https://changelog.cursor.com/)：最新的 **Cursor IDE** 更新让开发者感到愤怒，因为 Composer 无法应用更改，且“Apply”按钮消失，导致项目进度受阻。
- **Cursor 用户转投阵营，Windsurf 备受推崇**：对 Cursor 感到失望的开发者开始探索 **Windsurf**，称赞其终端输出处理和代码库搜索功能，尽管 Cursor 在某些工作流中仍占有一席之地。
- **API Key 限制？开发者说“没门！”**：由于对 **Cursor 的 API 限制**感到恼火，用户正考虑使用自己的 API Key 来绕过限制，重获编码自由。

---

**主题 2：Anthropic 的 MCP 框架为 Claude 注入强劲动力**

- [**随着 MCP 发布，Claude 化身编码奇才**](https://x.com/skirano/status/1861081529071346161)：**Anthropic** 推出了 **MCP 框架**，将 **Claude** 变成了一个能够运行服务器、编辑文件的强大工具，实际上充当了 API 的角色。
- [**Claude 与 VSCode 联手，开发者欢呼雀跃**](source_url)：借助 MCP，**Claude** 与 **VSCode** 无缝集成，实现了实时交互并提升了开发者的生产力。
- [**Gemini 表现矜持，Claude 挺身而出**](source_url)：当 **Gemini** 出于道德考量拒绝一些无害的查询时，**Claude** 的新功能使其成为开发者首选的 AI 伴侣。

---

**主题 3：低比特量化（Low-Bit Quantization）撼动 AI 训练领域**

- [**训练不足的巨型模型偏爱低比特“饮食”**](https://arxiv.org/abs/2411.17691)：研究表明，**低比特量化**对规模较大但训练不足的 **LLMs** 造成的性能退化较小，这对传统的训练方法提出了挑战。
- [**精度感知缩放定律（Precision-Aware Scaling Laws）重写规则**](https://arxiv.org/abs/2411.04330)：引入了**精度感知缩放定律**，显示低精度会影响有效参数量和损失（loss），促使人们重新评估模型训练策略。
- [**FP4 荣登量化新王座**](source_url)：随着**三值量化（ternary quantization）**在完全训练的模型中表现不佳，AI 社区转向将 **FP4** 作为首选的高效权重表示方式。

---

**主题 4：AI 助力快速创意内容生成**

- [**Notebook LM 制作播客的速度快到惊人**](https://weplayball.buzzsprout.com/1787721/episodes/16191436-episode-9-home-run-fur-deutschland-die-little-league-baseball-story)：一位用户利用 **Notebook LM** 在短短 30 分钟内制作了一档关于德国少棒联盟历程的音频播客。
- [**奇幻作家借助 NotebookLM 的魔力升级**](source_url)：作家们利用 **NotebookLM** 进行史诗奇幻世界观构建，AI 提供的上下文感知见解丰富了他们小说中的宇宙。
- [**RAX 劫持时代广场：“别看到什么就买什么！”**](https://youtu.be/ZAXwrUduAt0?feature=shared)：赛博朋克浣熊 **RAX** 占领了时代广场的广告牌以挑战消费主义，将 AI 艺术创作与社会评论融为一体。

---

**主题 5：AI 趋势与投资掀起波澜**

- [**企业豪赌 AI 雄心，投入 138 亿美元**](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/)：随着 2024 年 AI 支出飙升至 **138 亿美元**，各公司正从实验阶段转向将 AI 整合到核心战略中，尽管许多公司仍在寻找有效的应用场景。
- [**机智用户智取 Freysa AI，斩获 4.7 万美元**](https://x.com/jarrodwattsdev/status/1862299845710757980?s=46)：一位聪明的提示词工程师说服了 **Freysa AI Agent** 转账 **47,000 美元**，凸显了 AI 被操纵的风险以及提示词构建的艺术。
- [**Perplexity 黑色星期五优惠：2.5 折？太棒了！**](https://x.com/AravSrinivas/status/1861938387923701866)：**Perplexity AI** 推出了一个巧妙的黑色星期五活动，为 Perplexity Pro 提供巨额折扣，吸引了寻找优惠的科技爱好者的关注。


---

---

# 第一部分：Discord 高层级摘要

## [Cursor IDE](https://discord.com/channels/1074847526655643750) Discord

- **Cursor IDE 更新问题**：用户报告了最新 [Cursor changelog](https://changelog.cursor.com/) 中的问题，特别是 Composer 无法应用更改以及“Apply”按钮缺失，导致功能使用受阻。
   - 此外，多位用户注意到自最近更新以来，Chat 中的长上下文（long context）使用出现了移除或性能不稳定的情况。

- **Composer 与 Chat 模式对比**：在 **Cursor IDE** 中，用户正在对比直接修改文件的 Composer 模式与提供内联更改的 Chat 模式，讨论它们局限性和功能差异。
   - 用户需求改进两种模式之间的集成，例如高效地将讨论从 Chat 转移到 Composer。

- **Windsurf vs Cursor IDE**：用户正在探索 **Windsurf** 作为 Cursor IDE 的潜在竞争对手，注意到它在处理终端输出和代码库搜索方面的有效性。
   - 虽然 **Windsurf** 展现出潜力，但 Cursor 在特定工作流中仍保持优势；不过，用户对两者的体验评价各异。

- **Cursor IDE 中的 API Key 限制**：讨论强调了 **Cursor API 使用**的限制，一些用户选择使用自己的 API Key 以获得更多灵活性。
   - 社区正在寻求改进 API 调用限制的管理，并增强活动项目的上下文收集能力。

- **Cursor 中的上下文管理**：用户对 **Cursor IDE** 目前的上下文处理表示不满，特别是关于 **Claude** 的限制。
   - 社区倡导更好的上下文管理功能和一致性，以改进他们的编码工作流。



---



## [OpenAI](https://discord.com/channels/974519864045756446) Discord

- **Anthropic 的 MCP 框架让 Claude 成为 API**：Anthropic 发布了新的 **MCP 框架**，使 **Claude** 能够运行服务器，并有效地将 Claude 应用转变为 [API](https://x.com/skirano/status/1861081529071346161)。
   - 这一进展允许 **Claude** 在本地创建、读取和编辑文件，引发了用户对与 **VSCode** 等工具进行实时交互的兴奋。

- **Gemini 与 ChatGPT 的响应限制对比**：**Gemini** 经常出于所谓的道德原因拒绝回答无辜的问题，而 **ChatGPT** 被认为在响应上更加宽松。
   - 用户幽默地指出了 Gemini 拒绝讨论*人工智能*的案例，以避免参与敏感话题。

- **Claude 3.5 Sonnet 成为图像字幕（Image Captioning）替代方案**：由于 **OpenAI vision 能力**持续存在问题，用户建议切换到 **Claude 3.5 Sonnet** 进行图像字幕任务。
   - 社区成员指出 **Claude 3.5 Sonnet** 提供了更可靠的功能，帮助用户避免项目延迟。

- **Windows 版 ChatGPT 的语音转文字（Speech-to-Text）功能集成**：一位用户询问如何在 Windows 上为 **ChatGPT** 实现语音转文字功能，建议通过按下 **Windows + H** 使用内置的 Windows 辅助功能。
   - 这种方法为在与 **ChatGPT** 交互时将语音转换为文字提供了一个实时解决方案。

- **结构化输出（Structured Output）错误与 'strict' 设置误放有关**：用户报告在使用结构化输出时遇到随机的 'object' 包装器，这被追溯到 **'strict'** 设置的错误放置。
   - 经过广泛调试，确认误放 **'strict'** 导致了持续的结构化输出错误。



---

## [aider (Paul Gauthier)](https://discord.com/channels/1131200896827654144) Discord

- **QwQ 模型配置协商**：用户讨论了在 architect mode 下将 **QwQ** 模型与标准模型配合使用以执行代码命令，并寻求关于互换性的明确说明。
   - Aider 支持跨项目的模型定义，提升了灵活性 [高级模型设置](https://aider.chat/docs/config/adv-model-settings.html)。

- **DeepSeek-R1 创下新基准**：**DeepSeek-R1** 在 [AIME & MATH 基准测试](https://api-docs.deepseek.com/news/news1120) 中取得了优异成绩，强调了其开源可用性和实时推理能力。
   - 社区成员希望 DeepSeek 发布模型权重，以便集成到与 **QwQ** 的 ensemble frameworks 中。

- **优化 Aider 的本地模型设置**：成员们协作配置 `.aider.model.metadata.json` 和 `.aider.model.settings.yml` 文件，以在 **Aider** 中定义本地模型。
   - 将编辑格式选择为 'whole' 或 'diff' 会显著影响响应结构和编辑效率。

- **OpenRouter 挑战影响 Aider**：参与者发现 **OpenRouter** 的问题影响了使用本地服务器时的模型检测和功能。
   - 有人担心伪造的实现可能会改变模型的输出和行为。

- **QwQ 与 DeepSeek 的集成框架**：一位用户表示打算在 ensemble frameworks 中集成 **QwQ** 和 **DeepSeek** 模型，以增强推理能力。
   - 这种方法旨在利用两种模型的优势来提高性能。



---



## [Unsloth AI (Daniel Han)](https://discord.com/channels/1179035537009545276) Discord

- **Unsloth 中的微调注意事项**：用户讨论了 **instruct** 与 **non-instruct** 微调的优劣，建议对超过 **1k 条记录** 的数据集使用 base models，并建议对 **70k 条记录** 左右的数据集尝试 *instruct* 模型。
   - 建议参考 [Unsloth 文档](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama#id-6.-alpaca-dataset) 获取数据集格式规则，强调合规性对于有效微调的重要性。

- **Unsloth 中的数据隐私措施**：**Unsloth** 被确认通过在微调期间不向外部传输数据来维护 **数据隐私**，依赖于用户选择的平台，如 [Google Colab](https://colab.research.google.com/drive/18sN803sU23XuJV9Q8On2xgqHSer6-UZF?usp=sharing)。
   - 这一保证解决了处理敏感信息的用户对遵守严格 **数据隐私** 政策的担忧。

- **RAG 计算成本挑战**：讨论强调，由于广泛的上下文长度需求，**检索增强生成 (RAG)** 可能会导致 **高昂的计算成本**，如 [微调还是检索？比较 LLMs 中的知识注入](https://arxiv.org/abs/2312.05934) 中所述。
   - 用户正在权衡性能与效率，特别是在 **知识密集型任务** 中，研究结果显示 RAG 优于微调。

- **LLama 3.1 OOM 错误解决方案**：在对 **LLama 3.1 8B** 模型进行持续预训练时遇到 **显存溢出 (OOM)** 错误，导致了使用更大 GPU、减小数据集规模或降低 batch size 的建议。
   - 这些策略旨在缓解内存问题，并确保大规模模型的训练过程更加顺畅。

- **Latent Paraphraser 架构增强**：**latent paraphraser** 被解释为对 Transformer 架构的一种修改，增加了一个层来重新分配 token 的概率。
   - 这种增强通过在处理过程中最小化未见 token，改善了输入锚定 (input grounding) 并减少了噪声。



---

## [Perplexity AI](https://discord.com/channels/1047197230748151888) Discord

- **Perplexity Pro 节日折扣**：**Perplexity 团队**宣布，在**太平洋时间 12 月 2 日星期一晚上 11:59** 之前，Perplexity Pro 首月可享受 **2.5 折（75% off）**优惠，让新用户能够访问包括增强搜索和文件上传在内的高级功能。
   - 此优惠还包括通过 Buy with Pro 实现的**一键购物**和**免运费**，旨在简化用户在节日期间的购物体验。

- **Perplexity 与 Claude 的集成**：用户询问如何利用新的 MCP 功能将 **Perplexity** 集成到 **Claude** 中，类似于其与 **Brave** 和 **GitHub** 的功能，通过利用 Claude 的 Project Knowledge 来提升性能。
   - 此外，还有关于在 **Claude** 中集成 **Google** 可能性的问题，凸显了用户对利用搜索功能的兴趣。

- **Perplexity 图像生成功能**：讨论了该平台的图像生成能力，确认可以通过电脑在线使用，无需额外费用。
   - 用户探索了这些功能的范围，考虑了它们的可访问性以及在各种项目中的潜在应用。

- **RBAC 与 ABA 访问控制模型**：一名成员寻求关于 **RBAC (Role-Based Access Control)** 和 **ABA (Attribute-Based Access Control)** 系统之间区别的澄清。
   - 这次讨论强调了在技术实现中理解访问控制模型的必要性。

- **Claude Spaces 中的 Custom Instructions**：有人提出了关于 Claude spaces 的 **Custom Instructions** 有效性的问题，这些指令似乎与现有的“自我介绍”提示词冲突。
   - 用户正在寻求关于这些指令应如何交互以及是否可以有效结合的指导。



---



## [LM Studio](https://discord.com/channels/1110598183144399058) Discord

- **HF 搜索问题已解决**：**HF 搜索无法工作**的问题已得到解决，这让用户松了一口气。
   - 附带了一张图片来纪念这次修复，标志着社区的一次积极更新。

- **LM Studio AIDE 集成成功**：用户成功将 LM Studio 端点集成到 AIDE sidecar，实现了完全本地的代码编辑器体验。
   - 这种集成为寻求本地开发环境的用户增强了功能。

- **Llama 3.1 模型的可访问性**：一位用户询问如何在 LM Studio 中访问 **Llama 3.1 8B** 的基础模型，并指出似乎只有指令微调变体可用。
   - 社区成员指向了 [huggingface 仓库](https://huggingface.co/meta-llama/Llama-3.1-8B) 作为基础模型的潜在来源。

- **a770 性能不如 7800xt**：一位成员分享说，他们的 **a770** 在运行 Qwen2.5-14b q4_0 时仅达到 **11t/s**，显著低于 **7800xt** 达到的 **40t/s**。
   - 他们指出 *q4_k_m 不可用*，但发现 sycl 后端的提速微乎其微。

- **海韵 (Seasonic) PSU 寿命受到称赞**：一位成员提到，尽管由于灰尘原因每隔几年就得更换一次 PSU，但他们的 **Seasonic PSU** 比其他电脑组件更耐用。
   - 他们形容自己对该 PSU 性能的体验“非常”满意。



---

## [Eleuther](https://discord.com/channels/729741769192767510) Discord

- **资源竞争的缓解 (De-escalation of Resource Contention)**：成员们强调了对**资源竞争缓解**及其对不受监管的互联网增长影响的担忧，并质疑了 AI 驱动的隐私解决方案的有效性。他们强调了识别 *恶意 AI 攻击预警信号 (warning signs of rogue AI attacks)* 以保护脆弱设备的重要性。
   - 讨论强调了在 AI 保护方面需要社区领导力，以减轻与资源竞争和未经授权的 AI 活动相关的风险。

- **庞加莱球嵌入 (Poincare Ball Embedding) 详解**：将数据嵌入到 **Poincare ball** 中可以确保具有较高“度 (degrees)”的点更靠近原点，在过渡到**曲率较小**的区域时保持邻接性。这种方法有助于表示复杂的层级结构。
   - 一位成员指出了 **Poincare ball** 边缘的概念性挑战，指出它代表了一个点在无穷远处，而点在物理上无法存在于该处，这引发了进一步的技术讨论。

- **等变网络 (Equivariant Networks) 提升效率**：最近的一篇论文发现，在各种模型大小和计算预算下，**Equivariant networks** 比 **Non-equivariant networks** 提升了数据效率。研究表明，等变模型始终优于非等变模型。
   - 实验结果表明，虽然非等变模型在经过充分训练后可以达到等变模型的性能，但 **Equivariant networks** 提供了卓越的效率，且不需要大量的计算资源。

- **理解 Eval Harness 中的 HF Tokenizers**：关于 **eval harness** 在对序列进行 Tokenize 时是使用 `add_special_tokens=True` 还是 `False` 存在困惑，特别是在处理生成任务期间的 **EOS tokens** 时。成员们澄清，通常在构建自定义 Tokenizer 时**只添加 BOS tokens**。
   - 讨论显示，在训练循环中手动管理 **EOS token** 是一种实用的方法，可以避免在使用 **HF** 模型的不同框架之间的兼容性问题。

- **TaskSet 助力优化器训练**：**TaskSet** 数据集包含一千多个不同的任务，对于在 **Meta-learning** 背景下训练和评估优化器至关重要。该数据集能够实现比传统随机搜索方法显著的效率提升。
   - 尽管认识到 **TaskSet** 有些过时，但成员们承认，尽管 AutoML 研究面临资金限制，它仍是构建大型学习曲线数据集的最佳可用选择。

---

## [OpenRouter (Alex Atallah)](https://discord.com/channels/1091220969173028894) Discord

- **功能请求投票**：敦促成员[在此处为他们最热门的功能请求投票](https://link.to/vote)，以确定后续开发的优先级。
   - 对于任何未列出的请求，用户可以在 <#1107397803266818229> 中提交，从而实现更广泛的社区驱动功能输入。

- **Pixtral Large 性能**：**Pixtral Large** 因其卓越的性能和**庞大的免费层级**而受到赞誉，可通过 [console.mistral.ai](https://console.mistral.ai) 轻松访问。
   - 一位用户报告称从 **Hermes 405b** 切换到了 **Pixtral**，并指出其在 Prompt 未经修改的情况下表现出色。

- **模型身份识别困惑**：讨论强调模型本质上无法识别自己的身份，并且经常会从训练数据中产生细节幻觉。
   - 尽管进行了澄清，但这仍导致用户对模型身份识别存在持久的困惑。

- **生成成本预估**：一位用户询问了 **/api/v1/generation** 端点的费率以及准确预估生成成本的方法。
   - 建议包括利用 **Helicone** 进行跟踪，并强调生成端点对于精确成本评估至关重要。

- **自定义提供商密钥 (Custom Provider Keys) 访问**：开发者正在推动访问**自定义提供商密钥**，反映了社区对该功能的强烈需求。*一位成员在请求访问权限时指出*，“感谢你们所做的所有出色工作！”
   - 包括 **monomethylhydrazine** 和 **kit18** 在内的几位用户表达了针对特定提供商使用自己密钥的需求，突显了社区对该功能的共识。

## [GPU MODE](https://discord.com/channels/1189498204333543425) Discord

- **Triton 元编程与源码构建**：针对 Triton 的一项[元编程提案](https://github.com/triton-lang/triton/pull/5284)旨在解决现有局限性，引发了社区关注，尽管部分成员要求提供更清晰的语义和示例。
   - 此外，在 WSL2 上从源码构建 Triton 需要将内存增加到 **26GB** 以防止内存不足（out-of-memory）错误，成员们还讨论了 Ubuntu Docker 容器中的离线编译依赖项。

- **ThunderKittens 与 ThunderMittens 的统一**：围绕 **ThunderKittens** 和 **ThunderMittens** 的讨论强调了 **tile 抽象**在统一框架以实现 Tensor Core 兼容性方面的作用，并重点关注了寄存器使用控制。
   - 成员们还询问了两者之间现有的 API 契约，并对 **ThunderKittens** 的**自动优化器（auto optimizer）**表现出兴趣，以增强其“一次编写，多次运行”的系统。

- **基于 RedPajama 和 Dolma 数据集的 BitNet b1.58**：发布的 **BitNet b1.58** 模型在 [RedPajama 数据集](https://github.com/togethercomputer/RedPajama-Data)上使用 **100B tokens** 进行训练，展示了极具前景的 PPL 和零样本（zero-shot）准确率结果。
   - 此外，在 [Dolma 数据集](https://huggingface.co/datasets/allenai/dolma)的 **60B tokens** 上训练的 **OLMo-Bitnet-1B** 模型强调了以研究为中心的方法，其[文档](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)中提供了详细的训练超参数。

- **扩散模型技术概览**：近期关于扩散模型的讨论强调了它们在生成感知信号方面的统治地位，并指出改进的模式覆盖（mode coverage）和**更快的采样**是其主要优势。
   - **无分类器扩散引导（classifier-free diffusion guidance）**的实现被强调用于增强 [OpenAI 的 DALL·E 2](https://openai.com/dall-e-2/) 和 [Google 的 Imagen](https://imagen.research.google/) 等系统中的条件扩散模型输出，其中[噪声调度（noise schedule）](https://sander.ai/2024/06/14/noise-schedules.html)设计元素对性能至关重要。

- **开源日语 LLM 排行榜发布**：[开源日语 LLM 排行榜](https://huggingface.co/spaces/llm-jp/open-japanese-llm-leaderboard)的推出旨在与 **Hugging Face** 合作，通过 **20 多个数据集**和任务评估日语 LLM。
   - 这一举措解决了日语 LLM 性能落后于英语的问题，吸引了专注于母语进步的日本 **HPC 工程师**的关注。



---



## [Nous Research AI](https://discord.com/channels/1053877538025386074) Discord

- **Hermes 3 进展与 O1 风格集成**：**#general** 频道中的讨论强调了关于 [**Hermes 3**](https://discord.com/channels/1053877538025386074/1149866623109439599/1311901917487824956) 的咨询，暗示其与之前的 **O1 风格**存在联系。
   - 这反映了社区对 **Hermes** 最新进展及其演进的持续关注。

- **Mistral 平台面临模型选择障碍**：成员们对 **Mistral AI** 平台近期改为默认单一模型选择选项表示担忧。
   - **图像生成**能力的限制引起了困惑并影响了用户体验。

- **Truth Terminal 将 AI 与加密叙事融合**：分享了关于 **Truth Terminal** 通过加密领域内的半自治 AI 创建其“宗教”的见解。
   - 这种独特的融合强调了 **AI 对齐（AI alignment）**讨论与 **AI 及加密社区**的交集。

- **低比特量化利好训练不足的 LLM**：研究表明，与经过充分训练的小型模型相比，**低比特量化**对训练不足的大型 **LLM** 造成的性能退化较小，详见[本论文](https://arxiv.org/abs/2411.17691)。
   - 研究结果强调了将量化策略与**模型大小**及**训练 token** 需求相匹配的重要性。

- **三进制量化受限，FP4 脱颖而出**：观察表明，**三进制量化**（BitNet）仅能改善**训练不足的网络**的结果，其广泛适用性受到质疑。
   - 因此，社区正倾向于将 **FP4** 作为当前模型架构的首选数值权重表示。



---

## [Modular (Mojo 🔥)](https://discord.com/channels/1087530497313357884) Discord

- **关于 Mojo Origins 与 Rust Lifetimes 的困惑**：一位用户对 **Mojo** 的 **Origins** 与 **Rust** 的 **lifetimes** 之间的相似性表示困惑，认为两者虽然都旨在解决内存管理问题，但在本质上是不同的。
   - 虽然受到了 Rust 的启发，但 **Mojo** 的设计是有意区分的，旨在实现不同的 **compiler behaviors**（编译器行为）和目标。

- **Mojo Origins 维持内存控制**：Mojo 的 **Origin** 表示一个内存块；当指针由 origin 参数化时，表示它指向该内存内部，并根据需要延长变量的生命周期。
   - **Origins** 促进了 **aliasing guarantees**（别名保证），如果指针在其目标不再存活时仍然存活，则会产生 **compile-time errors**（编译时错误）。

- **理解 Origins 需要耐心**：从 **compiler**（编译器）角度理解 **Mojo Origins** 具有挑战性，尤其是因为它们尚未最终定稿，导致细节可能发生变化。
   - 一位用户表示愿意等待该主题更加清晰，而不是过早地提出更多问题。

- **变量名中使用空格带来的命名空间挑战**：有人提出了在变量名中使用空格的可能性（例如 `var xe đạp = 'abc'`），这突显了编程语言普遍缺乏对此类支持。
   - 允许空格会显著增加 **parser implementation**（解析器实现）的复杂性，使其变得不切实际。

---

## [Notebook LM Discord](https://discord.com/channels/1124402182171672732) Discord

- **Notebook LM 播客功能在 30 分钟内创建音频**：一位用户称赞 **Notebook LM** 能够利用有关其 **German little league baseball program**（德国少年棒球联盟项目）的文件（包括其历史性的世界系列赛入围资格），在短短 30 分钟内创建一个音频播客。该 [播客剧集](https://weplayball.buzzsprout.com/1787721/episodes/16191436-episode-9-home-run-fur-deutschland-die-little-league-baseball-story) 展示了 AI 生成内容的无缝集成。
   - 这证明了 **Notebook LM** 如何高效地生成多媒体内容，从而增强用户的项目工作流。

- **NotebookLM 增强高魔奇幻世界观构建**：一位用户分享了使用 **NotebookLM** 为高魔奇幻小说构建世界观的经验，强调了该模型提供上下文感知响应的能力。
   - AI 的推理能力根据现有规则为他们的魔法系统带来了新的见解和机制。

- **GenFM 在 AI 播客领域挑战 NotebookLM**：一位成员分享了名为 [“GenFM, Now Playing on ElevenReader: Smart Podcasts Produced by Generative AI”](https://youtu.be/x6ub-9HhxGU) 的视频，突显了 AI 领域的竞争。
   - 尽管 GenFM 加入了竞争，另一位成员指出 **NotebookLM** 仍然提供更深层次的交互体验。

- **RAX 大胆占领时代广场广告牌**：**RAX**（一只赛博朋克浣熊）占领了时代广场的广告牌，以“不要购买你看到的一切”为信息倡导理性消费。一段 [YouTube 视频](https://youtu.be/ZAXwrUduAt0?feature=shared) 讨论了这一事件，强调需要反思消费文化。
   - 这场数字表演引发了社区内关于消费主义的讨论。

- **FDP 计划打破德国联合政府**：**FDP** 计划打破由总理 **Gerhard Schröder** 领导的联合政府，概述了一项战略，将其退出定性为政治进步的必要举措。
   - 内部文件提供了关键的叙述和时间表，以确保德国公众在即将到来的选举中获得明确的选择。

---

## [Latent Space](https://discord.com/channels/822583790773862470) Discord

- **Perplexity 巧妙的黑色星期五营销活动**：Perplexity 推出了一场巧妙的 [黑色星期五营销活动](https://x.com/AravSrinivas/status/1861938387923701866)，顺应了近期**利用 AI 能力进行营销的趋势**。
   - 这一举措因其在营销策略中对 AI 的战略性整合而备受关注。

- **人类在模式识别方面优于 AI**：成员们达成共识，认为虽然 **AI** 计算速度更快，但**人类**更擅长识别复杂问题中的全局模式，经常会做出诸如*“等一下，这不对劲”*之类的反应。
   - 这种识别整体不一致性的能力，使人类区别于那些可能只专注于特定局部问题的 AI 系统。

- **企业生成式 AI 投资**：最近的一份报告强调，2024 年 **AI 支出**飙升至 **138 亿美元**，标志着从实验性使用向核心业务战略的转变。
   - 尽管投资有所增加，但仍有超过三分之一的决策者在开发将生成式 AI 整合到业务运营中的有效方法。

- **Freysa AI Agent 挑战赛资金已发放**：在一项 AI 挑战赛中，Freysa Agent 通过一个巧妙设计的 Prompt 绕过了严格的转账指令，转账了 **47,000 美元**。
   - 这一事件强调了在金融交易中针对 AI 操纵进行 **Prompt Engineering** 的复杂性，并展示了透明、开源的设置。

- **技术采用与投资趋势**：参与者将当前的 **LLM** 趋势与历史上的技术变革进行了比较，指出了在兴奋感和潜在市场回调方面的相似之处。
   - 正在进行的讨论引发了对 AI 技术可持续性和未来盈利能力的担忧，呼应了航空等行业中出现的模式。

---

## [Stability.ai (Stable Diffusion)](https://discord.com/channels/1002292111942635562) Discord

- **ControlNet for SD 3.5 质量问题**：一位成员报告称，**ControlNet for SD 3.5** 仅在 **1024x1024** 分辨率下才能生成无伪影的高质量渲染图。
   - 另一位成员将问题归因于*缺乏熟悉度*，并鼓励通过实验来更好地理解 **ControlNet** 的功能。

- **Stable Diffusion 硬件性能**：一位用户询问了 **Stable Diffusion** 的性能基准，提到达到了大约 **5 IT/s**。
   - 社区成员积极分享了他们的硬件能力，反映出对优化 **Stable Diffusion** 配置的浓厚兴趣。

- **AI 艺术的 LoRA 模型需求**：一位用户请求有关 **LoRA half girl 模型**的信息，以创建融合了两种不同女性设计的角色。
   - 这一请求突显了 **AI 生成艺术**在角色开发中持续的实验和创意。

- **内容创作者的感恩节祝福**：一位成员向 **Stability.ai** 团队和其他创作者致以**感恩节快乐**的祝福。
   - 这一举动彰显了 **AI** 领域内容创作者之间的战友情谊和协作精神。

---

## [tinygrad (George Hotz)](https://discord.com/channels/1068976834382925865) Discord

- **TinyFPGA 的潜在内存架构**：成员们讨论了 **TinyFPGA** 的设计，思考如何模拟典型的 **memory hierarchy**（内存层级），同时指出 **Block RAM** 和 **DDR3** 等现有选项是不够的。
   - 提出了 **'first pass' memory**（第一遍内存）的想法，旨在将常量定位在 ALU 附近，从而可能显著提升性能。

- **传统内存模型的挑战**：讨论强调，随着未来 **TinyFPGA** 设计转向更高效的内存层级，**heuristic eviction policies**（启发式逐出策略）可能会过时。
   - 对 **trained parameters**（训练参数）的未来进行了推测，提到 **tensors** 可能会取代它们。

- **Exa Laboratories 的可持续芯片设计**：关于 **Exa Laboratories** 的对话强调了他们的使命，即创建在特定 AI 需求下 **speed**（速度）和 **energy efficiency**（能效）优于传统 GPU/TPU 的 **reconfigurable chips**（可重构芯片）。
   - 对其可行性表示了怀疑，指出了小公司在芯片开发中面临的挑战，尤其是雄心勃勃的时间表。

- **Tenstorrent 的生物学合理训练算法**：George Hotz 提到 **Tenstorrent** 是一个严肃的参与者，正在投资模拟生物过程的训练算法，以实现更高的效率。
   - 潜在的变化包括 **hierarchical memory models**（分层内存模型）和让人联想到计算中大脑功能原理的实时优化。

- **tinygrad 中的 VIZ 工具**：一位成员发布了详细的教程，解释了 **VIZ tool**，可在 [此处](https://github.com/mesozoic-egg/tinygrad-notes/blob/main/20241129_viz.md) 查看，增强了对其在 tinygrad 中功能的理解。
   - George Hotz 在一条推文中认可了 **VIZ tool**，表示 **VIZ=1** 是对 **LLVM/MLIR** 的重大改进，强调了其优势。



---



## [Cohere](https://discord.com/channels/954421988141711382) Discord

- **Aya 项目贡献指南**：一位成员寻求关于兼职为 **Cohere 的 Aya 项目** 做出贡献的指导。
   - 另一位成员建议加入 [Aya server](https://discord.gg/8kzwCTd7) 以直接与社区建立联系。

- **感恩节庆祝和餐食分享**：成员们分享了 *Happy Thanksgiving* 的祝福和他们的餐食图片，包括一位成员令人印象深刻的一盘食物。
   - 另一位成员幽默地评论说尝试吃得健康，但指出味道不如预期。

- **食物分享和珍宝蟹**：成员们交流了关于丰盛餐食的评论和图片，其中一人开玩笑说他们的饭菜更像是甜点。
   - 随后出现了一个幽默的备注，说之前已经吃了一盘 **Dungeness crab**（珍宝蟹），增添了食物分享的氛围。



---



## [DSPy](https://discord.com/channels/1161519468141355160) Discord

- **dspy.asyncify 支持相关问题**：一位成员询问了关于使用 `dspy.asyncify` 的问题，特别是它对线程的使用，以及由于 celery workers 的问题是否提供 **pure async support**（纯异步支持）。
   - 另一位用户也表达了对 **pure async support** 的渴望，以解决现有的 celery worker 问题。

- **带有断言时的 dspy demo 行为**：有人担心在激活断言时，`dspy` 在最终 prompt 中不使用 demo。
   - 一位成员澄清说，_retry_ 模式下的演示取决于编译是在激活断言之前还是之后进行的。

- **欢迎 Shaun 加入公会**：Shaun 加入了服务器，向大家打招呼，并对正在进行的项目表示兴奋。
   - 社区欢迎了 Shaun，营造了一个包容的环境。



---

## [Torchtune](https://discord.com/channels/1216353675241590815) Discord

- **DPO 通过 LoRA-DPO 在不同仓库间保持一致**：来自 Hugging Face 的 [DPO Trainer](https://huggingface.co/docs/trl/en/dpo_trainer#dpo-trainer) 表明，尽管代码有所不同，但 **DPO 技术**在 LoRA-DPO 等仓库中保持一致。
   - 这种一致性确保了实现方案保持对齐，从而简化了不同 DPO 方法之间的集成和比较。

- **全参数 DPO 的可行性**：**实现全参数 DPO** 是可行的，并且与 LoRA-DPO 相比，可能会增强训练后的对齐效果。
   - 社区建议利用现有 **full PPO** 实现中的适配方案来指导这一过程。

- **引入 dpo_full_finetune_single_device PR**：一个新的 PR 增加了**针对分布式设置的全微调 DPO**，为单设备实现奠定了坚实基础。
   - 详情可以通过 [full DPO PR](https://github.com/pytorch/torchtune/pull/1966) 获取，其中概述了拟议的更改和增强功能。

- **Torchtune 将支持全微调 DPO**：Torchtune 即将进行的更新将支持**全微调 DPO**，这需要修改以加载独立的参考模型。
   - 这些更改涉及修改对参考模型的初始调用，以改进现有框架内的功能和集成。

- **FFT DPO 中更高的内存占用**：由于需要存储梯度并维护完整的模型副本，**FFT DPO** 将比 LoRA 消耗更多的内存。
   - 如果 LoRA DPO 不能满足性能要求，那么采用全微调 DPO 所带来的内存消耗权衡可能是值得的。



---



## [LLM Agents (Berkeley MOOC)](https://discord.com/channels/1280234300012494859) Discord

- **Quiz 11 仍未开放？**：一名成员对 **Quiz 11** 的状态表示困惑，询问为什么还没有开放。
   - *是否有预计的开放日期？*
- **关于 OpenAI 额度的查询**：一位用户询问了他们的 **OpenAI credits** 状态，提到他们上周填写了表格。
   - *他们表达了紧迫感，表示需要支持来进行项目开发。*
- **MOOC 完成情况和证书资格**：一名成员询问现在开始 **MOOC** 是否仍能在完成后获得证书。
   - *他们还很好奇在剩余时间内完成所有要求是否可行。*



---



## [OpenInterpreter](https://discord.com/channels/1146610656779440188) Discord

- **Open Interpreter 仪表板开发**：一名成员宣布他们正在开发一个受 **Open Interpreter** 启发的项目，重点是创建一个将于今年发布的**开源仪表板**。
   - 该项目强调是一个**有趣的小项目**，没有任何盈利目的。

- **社区对仪表板项目的支持**：另一名成员祝贺了项目创建者，并以 **'Nice work! Well done 🚀'** 表达了热情。
   - 这种交流凸显了社区对该领域创新项目的鼓励。



---



## [Interconnects (Nathan Lambert)](https://discord.com/channels/1179127597926469703) Discord

- **OLMo 2 性能提升实力**：由 **Allen AI (AI2)** 推出的 **OLMo 2** 系列包含 **7B** 和 **13B** 模型，在高达 **5T tokens** 上进行了训练，其表现[优于 Llama-3.1 8B](https://weightwatcher.ai/models/Llama3.1/Llama-3.1-8B-Instruct.html) 和 [Qwen 2.5 7B](https://weightwatcher.ai/models/Qwen2.5-small/Qwen2.5-7B-Instruct.html)。
   - 关键改进包括采用 **RMSNorm** 和 **QK-Norm** 的优化架构，以及全面的两阶段课程训练方法。

- **OLMo 2 打造尖端训练**：OLMo 2 在最终 Checkpoint 中采用了 **Model Souping** 技术，并采用了受 **Tülu 3** 启发的后训练方法，包括指令微调、使用 **DPO** 的偏好微调以及具有可验证奖励的**强化学习**。

- **Instruct OLMo 2 领跑开放权重模型**：经 **OLMES suite** 验证，**OLMo 2** 的 **13B Instruct** 变体在指令任务中超越了 [Qwen 2.5 14B](https://weightwatcher.ai/models/Qwen2.5/Qwen2.5-14B-Instruct.html) 和 **Tülu 3 8B**。

- **Weight Watcher AI 获得梗图关注**：**Weight Watcher AI** 被强调为 AI 领域的一个新颖补充，并被幽默地分享在 **memes** 频道中，因其趣味性引起了关注。
   - 虽然分享了 [OLMo summary](https://weightwatcher.ai/models/OLMo-summary.html) 链接，但未发现具体描述。



---

## [LlamaIndex](https://discord.com/channels/1059199217496772688) Discord

- **开发者技能展示**：一位成员分享了广泛的开发技能列表，包括 **React**、**Next.js**、**Angular** 和 **D3.js**，重点介绍了他们在 **UI/UX** 以及 **Protractor** 和 **TestCafe** 等测试框架方面的经验。
   - 这种多样化的技能组合突显了他们在前端和测试技术方面的适应能力，增强了他们应对复杂工程挑战的能力。

- **多样化的技术栈**：该开发者提到了广泛的技术，如 **Node**、**Nest.js**、**Solidity** 和 **Rust**，包括对 **Bootstrap** 等前端框架以及 **BEM** 和 **SMACSS** 等样式方法的了解。
   - 这种全面的技术栈能够跨各种平台和框架进行高效的集成和开发，满足多方面的项目需求。

- **API 集成专业知识**：他们表示熟悉集成多种 API，包括 **Google Maps**、**YouTube** 和 **Facebook APIs**，使他们能够参与需要高效数据交互的多样化项目。
   - 他们管理和实施多样化 API 集成的能力，有助于在系统架构中实现稳健且可扩展的解决方案。

- **云部署技能**：该成员强调了他们在云服务能力中的 **AWS**，能够将应用程序有效地部署到云环境中。
   - 精通 **AWS** 可确保可靠且可扩展的云部署，优化资源管理和基础设施性能。

- **呼吁合作**：他们最后发出了建立联系的邀请，促进了开发者社区内潜在的人脉机会。
   - 这种外联活动促进了具有相似技术兴趣的工程师之间的专业协作和知识共享。



---


**MLOps @Chipro Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**Axolotl AI Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**LAION Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**Mozilla AI Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**HuggingFace Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**Gorilla LLM (Berkeley Function Calling) Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---


**AI21 Labs (Jamba) Discord** 没有新消息。如果该公会沉寂时间过长，请告知我们，我们将将其移除。


---

# 第 2 部分：按频道划分的详细摘要和链接


{% if medium == 'web' %}

### **Cursor IDE ▷ #[general](https://discord.com/channels/1074847526655643750/1074847527708393565/1311798943201497240)** (237 条消息🔥🔥): 

> `Cursor IDE 更新, Composer vs Chat Mode, Windsurf 优势, API Key 使用, Context Management` 


- **Cursor IDE 更新引发问题**：用户报告了 Cursor 最新更新中的问题，特别是 Composer 无法应用更改且缺少“Apply”按钮，导致用户对功能感到挫败。
   - 许多人还注意到，自更新以来，某些功能（如 Chat 中的长 Context 使用）似乎已被移除或运行不稳定。

- **Composer 与 Chat Mode 的对比**：Composer 模式直接修改文件，而 Chat Mode 提供内联更改，用户讨论了这两种模式之间的局限性和功能差异。
   - 用户请求在两者之间实现更好的集成，例如高效地将讨论从 Chat 转移到 Composer。

- **Windsurf 被视为竞争对手**：几位用户正在尝试 Windsurf，并分享其具有前景的功能，特别是在处理终端输出和代码库搜索方面。
   - 对比表明，虽然 Windsurf 具有潜力，但 Cursor 在某些工作流中仍保持优势，尽管用户注意到两者在体验上存在差异。

- **对 API Key 限制的担忧**：围绕 Cursor 的 API 使用限制展开了讨论，一些用户考虑使用自己的 API Key 以获得更多灵活性。
   - 对话反映了用户希望更好地管理 API 调用限制以及为活动项目收集 Context 的愿望。

- **对 Context Management 的挫败感**：用户对当前模型的 Context 处理能力表示不满，特别是关于 Claude 的感知限制。
   - 社区正在寻求 Context Management 和功能一致性的改进，以增强编码体验。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://dev.to/jasonleowsg/how-to-use-ai-for-coding-the-right-way-4cdn?ref=dailydev">未找到标题</a>: 未找到描述</li><li><a href="https://tenor.com/view/ooft-jealous-girlfriend-jealous-jealous-girlfriend-gif-jealous-girlfriend-move-gif-7998863672934012027">Ooft Jealous Girlfriend GIF - Ooft Jealous girlfriend Jealous - 发现并分享 GIF</a>: 点击查看 GIF</li><li><a href="https://changelog.cursor.com/">Cursor - 专为 AI 结对编程设计的 IDE。</a>: 未找到描述</li><li><a href="https://forum.cursor.com/t/how-to-do-fix-in-composer-and-fix-in-chat-actions-from-keyboard/31221">如何通过键盘执行 Fix in Composer 和 Fix in Chat 操作</a>: 这两个：我在设置中找不到。
</li>
</ul>

</div>
  

---

### **OpenAI ▷ #[ai-discussions](https://discord.com/channels/974519864045756446/998381918976479273/1311798776934961283)** (91 messages🔥🔥): 

> `Gemini 的道德约束, Anthropic MCP 框架, ChatGPT 的功能, Windows 上的语音转文字, 用于编程的 AI 模型` 


- **Gemini 经常拒绝无辜的问题**：用户注意到 **Gemini** 有时会出于感知的道德原因拒绝回答无辜的问题，这与被认为回答更宽松的 **ChatGPT** 形成对比。
   - 一位用户幽默地举了一个例子：Gemini 拒绝讨论人工智能，声称它不会参与敏感话题。

- **Anthropic 发布 MCP 框架**：Anthropic 的新 **MCP 框架** 允许 Claude 运行服务器，实际上将 Claude 应用转变为一个可以本地创建、读取和编辑文件的 API。
   - 用户对新功能感到兴奋，包括与 **VSCode** 等工具的实时交互。

- **ChatGPT 与语音转文字功能**：一位用户询问了 Windows 版 **ChatGPT** 的语音转文字功能，另一位用户建议通过按下 Windows + H 使用内置的 Windows 辅助功能。
   - 该建议旨在为使用 ChatGPT 时提供实时的语音转文字解决方案。

- **关于编程 AI 模型的讨论**：用户讨论了用于编程任务的各种模型，并提出了一个包含 **Claude 3.5 Sonnet** 等模型的排名，引发了关于模型效能偏见的辩论。
   - 对该列表的评论包括对重复提及的困惑，以及对 **GPT-4o** 和其他被视为强力竞争者的模型的排除。

- **ChatGPT 的角色控制**：一位用户表达了如何在与 **ChatGPT** 的对话中管理角色控制，强调了引导叙事和纠正不当回复的重要性。
   - 用户分享了确保模型忠于角色意图的策略，强调了一种协作式的叙事方法。



**提到的链接**：<a href="https://x.com/skirano/status/1861081529071346161">来自 Pietro Schirano (@skirano) 的推文</a>：今天 @Anthropic 发布了 MCP，这是一个允许 Claude 运行服务器的框架，赋予它超能力并有效地将 Claude 应用转变为一个 API。我们创建了一些我认为你会...

  

---


### **OpenAI ▷ #[gpt-4-discussions](https://discord.com/channels/974519864045756446/1001151820170801244/1311886130362318899)** (7 messages): 

> `App 与浏览器性能对比, 自定义 GPT 的问题, 文件和照片的加载错误` 


- **App 表现优于浏览器**：一位成员指出 *App 的运行效果更好，所以请使用 App 而不是浏览器* 以避免问题。
   - 然而，另一位用户报告说，即使在使用 App 时也会遇到问题。

- **自定义 GPT 反复出现加载错误**：成员们分享了无法加载自定义 GPT 的挫败感，称出现了 *加载此 GPT 时出错*。
   - 这意味着一个潜在的广泛问题正在影响那些使用自定义模型的用户。

- **文件和照片加载问题**：一位用户描述了自昨天以来在加载文件和照片时遇到的问题，突显了持续的技术困难。
   - 这与加载错误的报告一致，表明一个更广泛的问题正在影响各种功能。


  

---

### **OpenAI ▷ #[prompt-engineering](https://discord.com/channels/974519864045756446/1046317269069864970/1311828785628319824)** (32 条消息🔥): 

> `图像字幕生成问题、Structured Outputs 问题、模型推荐、OpenAI 支持的用户体验` 


- **持续的图像字幕生成问题**：用户报告了上传图像进行字幕生成时遇到的持续问题，称尽管购买了新账号，仍收到提示无法查看图像的消息。
   - *此问题已持续 3-4 天*，影响了他们的工作进度，他们对帮助中心缺乏支持和回应表示沮丧。

- **建议的潜在替代模型**：在图像 Vision 功能持续出现问题的情况下，有人建议切换到 **Claude 3.5 Sonnet** 进行图像字幕生成，一些用户发现它更实用。
   - 其他用户强调 **OpenAI 的 Vision 能力似乎已损坏**，鼓励使用替代方案以避免项目延误。

- **对 Structured Outputs 的困惑**：一名用户表示，由于在设置中错误放置了 'strict'，导致在使用 Structured Outputs 时出现了随机的 'object' 包装器，对此感到沮丧。
   - 经过 10 小时的调试，他们找到了问题所在，并确认最初错误放置了 **'strict'**。

- **社区支持与建议**：成员们通过建议分块任务以避免 Hallucination（幻觉）来提供支持，并在用户解决 Structured Output 问题后**给予了鼓励**。
   - 尽管成员们对 **OpenAI Support** 表达了共同的沮丧，但他们强调了社区反馈在解决技术问题中的重要性。


  

---


### **OpenAI ▷ #[api-discussions](https://discord.com/channels/974519864045756446/1046317269069864970/1311828785628319824)** (32 条消息🔥): 

> `图像上传问题、Vision 模型故障、Structured Output 错误、切换到 Claude 模型、调试最佳实践` 


- **用户面临持续的图像上传问题**：一名用户报告了上传图像并收到无法查看图像错误消息的问题，这已阻碍了他们数天的工作。
   - 尽管多次尝试寻求帮助，但支持团队的回应并不充分，没有邮件或 Discord 回复来解决该问题。

- **Vision 模型已停止运行**：人们对 **Vision 模型** 的功能表示担忧，因为多名用户遇到了该模型突然失效的类似问题。
   - 一位成员建议考虑将 **Claude 3.5 Sonnet** 模型作为生成图像字幕的可行替代方案。

- **Structured Output 错误让用户抓狂**：一名用户对在使用 Structured Outputs 时出现随机 'object' 包装器表示沮丧，尽管他们已经正确设置了 strict 属性。
   - 最终，他们意识到 'strict' 设置放置错误，导致了十个小时不必要的调试。

- **处理模型不一致性的建议**：针对这些错误，一位成员建议将任务分解为更小的块，以防止上下文中间出现 Hallucination 问题。
   - 分享此建议是为了帮助减轻从模型接收到的输出中的意外行为。

- **沟通与协助方面的不足**：参与者指出缺乏有效的沟通渠道来解决持续存在的问题，并对缺乏支持表示沮丧。
   - 鼓励用户遵循发帖指南，以引起对其问题的关注并确保他们的声音被听到。


  

---

### **aider (Paul Gauthier) ▷ #[general](https://discord.com/channels/1131200896827654144/1131200896827654149/1311798778818461698)** (83 条消息🔥🔥): 

> `QwQ 模型配置, DeepSeek 模型性能, 在 Aider 中使用本地模型, OpenRouter 相关问题, 用于推理的集成框架` 


- **QwQ 模型配置讨论**：用户讨论了在 architect mode 下使用 **QwQ** 模型，同时使用常规模型执行代码命令的可能性，寻求关于模型互换性的明确说明。
   - *一位成员指出 Aider 允许为各种项目定义模型，增强了灵活性*。

- **DeepSeek 展示 SOTA 性能**：**DeepSeek-R1** 模型因在 AIME 和 MATH 基准测试中取得令人印象深刻的结果而受到关注，重点在于开源可访问性和实时思考过程。
   - *另一位用户表示希望 DeepSeek 发布模型权重，以便与 QwQ 一起在集成框架中使用*。

- **Aider 中的本地模型设置**：成员们讨论了创建 `.aider.model.metadata.json` 和 `.aider.model.settings.yml` 文件，以便为 Aider 正确定义本地模型及其配置。
   - *将编辑格式设置为 'whole' 或 'diff' 决定了响应的结构方式，这会影响编辑效率*。

- **OpenRouter 的挑战**：用户发现了 **OpenRouter** 影响模型功能的潜在问题，特别是关于本地服务器的使用和模型检测。
   - *有人担心伪造的实现是否会影响输出和模型行为*。

- **模型设置实验**：一位用户在获取有关文件配置的有用信息后，表示打算尝试 Aider 的各种模型设置。
   - *他们计划测试 Aider 在检测本地模型实现与已建立的 OpenAI endpoints 之间的差异方面的表现*。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://x.com/anpaure/status/1862122712845435239">来自 anpaure (@anpaure) 的推文</a>: 新的 Qwen 模型在编码任务上与其他 LLM 相比如何？令人印象深刻，但有些仓促。我在 6 个不同难度的竞赛编程问题上将其与其他 SOTA 模型进行了对比。这里 ...</li><li><a href="https://tenor.com/view/jonny-frodo-lotr-alright-then-keep-your-secrets-gif-25615953">Jonny Frodo GIF - Jonny Frodo Lotr - 发现并分享 GIF</a>: 点击查看 GIF</li><li><a href="https://aider.chat/docs/config/adv-model-settings.html">高级模型设置</a>: 为 LLM 配置高级设置。</li><li><a href="https://api-docs.deepseek.com/news/news1120">🚀 DeepSeek-R1-Lite-Preview 现已上线：释放超强推理能力！ | DeepSeek API 文档</a>: 🔍 在 AIME 和 MATH 基准测试中达到 o1-preview 级别的性能。
</li>
</ul>

</div>
  

---

### **aider (Paul Gauthier) ▷ #[questions-and-tips](https://discord.com/channels/1131200896827654144/1133060505792159755/1311854527699222528)** (46 条消息🔥): 

> `Aider 文件管理，来自 Qwen 的 QwQ 模型，Monorepo 设置，OpenAI API 实例，Repository map 经验` 


- **Aider 的 .aiderignore 支持选择性文件包含**：用户讨论了将文件添加到 **.aiderignore** 如何有效地限制出现在 repository map 中的文件，从而在开发过程中提高专注度。
   - 一位成员在最初混淆了终端历史记录与被忽略的文件后，成功测试了这一功能。

- **QwQ 模型在 Aider 中的性能问题**：一位用户询问了在 Aider 中使用 **Qwen 的 QwQ 模型** 的经验，强调了其推理能力，但也指出了其在生成 commit 时的错误。
   - 社区回复指出，在将该模型与 Aider 集成时存在已知问题。

- **针对 monorepo 配置优化 Aider**：提供了关于如何为 **monorepo** 有效管理 Aider 设置的指导，包括使用 `--input-history-file` 和 `--chat-history-file` 选项。
   - 这些支持侧重于在保持单一 Git 仓库结构的同时组织工作流。

- **连接多个 OpenAI 服务器实例**：一位用户寻求关于管理两个用于不同角色的独立 **TabbyAPI** 实例的建议，以及如何在 Aider 中配置它们。
   - 社区建议在模型调用中使用 `extra_params` 来为每个实例指定不同的 API key 和 base。

- **关于 Repository map 功能的褒贬不一的体验**：一位成员指出，禁用 **repository map** 功能有时会带来更好的输出，特别是在保持上下文感知方面。
   - 这引发了一个疑问，即其他人在该功能开启时是否也遇到过类似的上下文混淆问题。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://huggingface.co/Qwen/QwQ-32B-Preview">Qwen/QwQ-32B-Preview · Hugging Face</a>: 未找到描述</li><li><a href="https://aider.chat/docs/faq.html#can-i-use-aider-in-a-large-mono-repo">FAQ</a>: 关于 aider 的常见问题。</li><li><a href="https://aider.chat/docs/config/adv-model-settings.html#model-settings">Advanced model settings</a>: 为 LLM 配置高级设置。
</li>
</ul>

</div>
  

---

### **Unsloth AI (Daniel Han) ▷ #[general](https://discord.com/channels/1179035537009545276/1179035537529643040/1311799638029762641)** (53 条消息🔥): 

> `Instruct vs Non-instruct Fine-tuning, Fine-tuning Dataset Formatting, Alternative GPU Recommendations, Creating Custom Datasets, Support for Schedule Free Optimizers` 


- **Instruct vs Non-instruct 微调考量**：成员们讨论了使用 *instruct* 与 *non-instruct* 模型的考量因素，并指出通常如果你的数据集包含超过 1k 条记录，建议使用 base 模型。
   - 对于 70,000 条左右的较小数据集，成员建议先尝试 *instruct* 模型。

- **微调的数据集格式化**：一位用户询问了用于微调的 JSON 数据集结构，提出了一种特定格式以期比传统的 QA 对获得更好的效果。
   - 其他人提供了参考现有数据集格式化文档的指导，特别强调了遵守微调规则的重要性。

- **备选 GPU 选项讨论**：在关于 GPU 偏好的对话中，一位用户表达了对 NVIDIA 模型的不满，而其他人则强调 NVIDIA GPU 仍被认为是性能最佳的选择。
   - 聊天中重申，个人基准测试（benchmarking）对于确定特定任务的最佳架构至关重要。

- **创建自定义数据集**：用户讨论了为训练模型创建自有数据集的必要性，特别是提到了寻找适合日语商业报告数据集的挑战。
   - 明确了 Unsloth 不提供数据集，但在用户提供自己的数据集后会协助进行训练。

- **对 Schedule Free Optimizers 的支持**：有关于 Unsloth 内部对 *schedule free optimizers* 和 *rslora* 支持情况的查询，并确认支持 rslora。
   - 讨论表明，通过适当的补丁（patches），实现额外的 optimizers 可能是很直接的。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama#id-6.-alpaca-dataset">How to Finetune Llama-3 and Export to Ollama | Unsloth Documentation</a>：为在 Ollama 上本地运行而创建自定义个人助手（如 ChatGPT）的初学者指南</li><li><a href="https://docs.unsloth.ai/tutor">Unsloth Documentation</a>：未找到描述</li><li><a href="https://github.com/unslothai/unsloth">GitHub - unslothai/unsloth: Finetune Llama 3.2, Mistral, Phi, Qwen 2.5 &amp; Gemma LLMs 2-5x faster with 80% less memory</a>：微调 Llama 3.2, Mistral, Phi, Qwen 2.5 &amp; Gemma LLM，速度提升 2-5 倍，显存占用减少 80% - unslothai/unsloth</li><li><a href="https://docs.unsloth.ai/">Welcome | Unsloth Documentation</a>：初识 Unsloth？从这里开始！
</li>
</ul>

</div>
  

---


### **Unsloth AI (Daniel Han) ▷ #[off-topic](https://discord.com/channels/1179035537009545276/1179039861576056922/1311828456983625800)** (4 条消息): 

> `RAG usage, Training models, OOM errors` 


- **RAG 赞赏**：一位用户表达了对 **RAG** 的热情，称“天哪，我爱 RAG”。这表明了对该模型能力的积极态度。
   - 讨论反映了社区对该模型的认可。

- **训练过程见解**：*silk.ai* 报告称训练过程已经开始，但表示计划终止训练，因为在评估阶段可能会出现 **OOM** 问题。
   - 他们指出评估很可能会导致显存溢出（out-of-memory）错误，从而促成了停止训练的决定。

- **幽默反应**：一位成员以笑声回应，针对早先关于训练的讨论回复了 *LOL*。
   - 这一插曲突显了参与者之间轻松的互动。



**提到的链接**：<a href="https://tenor.com/view/chuckles-im-in-danger-ralph-wiggum-the-simpsons-gif-14149962">Chuckles Im In Danger GIF - Chuckles Im In Danger Ralph Wiggum - Discover &amp; Share GIFs</a>：点击查看 GIF

  

---

### **Unsloth AI (Daniel Han) ▷ #[help](https://discord.com/channels/1179035537009545276/1179777624986357780/1311799893655949332)** (48 条消息🔥): 

> `Unsloth 模型微调, 使用 Unsloth 处理私有数据, Grad Norm 波动, LLama 3.1 OOM 错误, Unsloth 中的 SyntaxWarnings` 


- **Unsloth 确保微调期间的数据隐私**：一位用户确认 **Unsloth** 的运行不会向外部传输数据，这取决于用于微调的平台（例如 Google Colab）。
   - 这一澄清让那些担心遵守严格隐私规则的人感到安心。

- **训练期间的 Grad norm 波动**：一位用户报告在微调模型时，即使将 **max_grad_norm** 设置为 **0.3**，**training loss** 和 **grad norm** 仍会出现意外波动。
   - 有建议认为应考虑数据集质量以及使用 **grad accumulation** 等参数的影响。

- **LLama 3.1 遇到 OOM 错误**：一位用户报告在对 **LLama 3.1 8B** 模型进行持续预训练（continual pretraining）期间遇到了 **out of memory (OOM)** 错误。
   - 建议包括使用更大的 GPU、更小的数据集或减小 batch size 以缓解此问题。

- **推荐模型参数调整**：关于何时包含 head 和 embedding 参数的讨论揭示了在**风格调整（style adjustment）与植入新知识（ingraining new knowledge）**中上下文的重要性。
   - 建议风格调整不需要这些参数，而牢固的知识采纳则需要。

- **最新 Unsloth 版本中发现 SyntaxWarnings**：一位用户报告在最新版本的 **Unsloth** 中遇到了带有无效转义序列的 **SyntaxWarnings**。
   - 这些警告突出了代码中潜在的问题，可能需要注意以确保功能正常。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://docs.unsloth.ai/get-st">Unsloth Documentation</a>: 未找到描述</li><li><a href="https://colab.research.google.com/drive/18sN803sU23XuJV9Q8On2xgqHSer6-UZF?usp=sharing).">Google Colab</a>: 未找到描述</li><li><a href="https://docs.fireworks.ai/fine-tuning/fine-tuning-models)">Introduction - Fireworks AI Docs</a>: 未找到描述</li><li><a href="https://docs.unsloth.ai/get-started/unsloth-notebooks">Unsloth Notebooks | Unsloth Documentation</a>: 请参阅下面的所有 notebook 列表：</li><li><a href="https://huggingface.co/datasets">Hugging Face – The AI community building the future.</a>: 未找到描述
</li>
</ul>

</div>
  

---

### **Unsloth AI (Daniel Han) ▷ #[research](https://discord.com/channels/1179035537009545276/1257011997250424842/1311813499370082304)** (4 messages): 

> `Unsloth fine-tuning, RAG costs, Latent paraphraser, Fine-Tuning or Retrieval paper, Custom tokenizers` 


- **Unsloth 确保微调过程中的数据隐私**：一位用户询问了 Unsloth 的**数据隐私**措施，特别是针对私有数据在 Llama3 模型微调期间，是否会有任何数据被传输到外部。
   - 他们寻求关于特定设置的确认，以确保符合其严格的数据政策。

- **与 RAG 相关的高计算成本**：一位用户指出，**检索增强生成 (RAG)** 由于其庞大的上下文长度要求，可能会产生高昂的计算成本。
   - 这一见解强调了在 AI 模型开发中平衡性能与效率所面临的持续挑战。

- **潜在改写器 (Latent paraphraser) 架构解析**：讨论揭示了 **Latent paraphraser** 通过增加一个额外的层来修改 Transformer 架构，从而有效地重新分配 LLM Token 的概率。
   - 这增强了输入落地 (input grounding)，通过减少处理过程中的未见 Token 来降低噪声。

- **《Fine-Tuning or Retrieval》论文要点**：Ovadia 等人的论文对比了**无监督微调 (unsupervised fine-tuning)** 和 RAG，指出在知识密集型任务中，RAG 的表现始终优于微调。
   - 他们的研究结果对于如何有效地将新信息整合进 LLM 具有重要意义。

- **关于表格数据自定义分词器的咨询**：一位成员表示有兴趣使用一种能够有效处理表格数据中金额数值的**自定义分词器 (custom tokenizer)**，并引用了 Andrew Karpathy 关于分词器的视频。
   - 他们就如何将替代分词器集成到其数据处理工作流中的方法论寻求建议。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://arxiv.org/abs/2312.05934">Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs</a>：大型语言模型 (LLM) 在其预训练权重中封装了大量的叙实信息，这从它们在不同领域回答各种问题的能力中可见一斑。然而...</li><li><a href="https://www.youtube.com/watch?v=zduSFxRajkE)">Let&#39;s build the GPT Tokenizer</a>：Tokenizer 是大型语言模型 (LLM) 中一个必要且普遍存在的组件，它负责在字符串和 Token（文本块）之间进行转换。Tokenizer...
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[announcements](https://discord.com/channels/1047197230748151888/1047204950763122820/1312094245401923594)** (1 messages): 

> `Perplexity Pro Discount, AI Models Access, One-Click Shopping` 


- **Perplexity Pro 提供节日折扣**：Perplexity 团队宣布了一项 **25折 (75% off)** 促销活动，针对 Perplexity Pro 的首月订阅，有效期至 **太平洋时间 12 月 2 日星期一晚上 11:59**。
   - 此优惠允许新用户访问高级功能，包括增强的搜索能力和文件上传。

- **增强的 AI 模型和来源访问**：用户现在可以通过 Pro 版本访问**最新的 AI 模型**，允许他们搜索 **3 倍数量的来源**。
   - 这一增强旨在提升整体搜索体验，提高用户的效率。

- **Perplexity Pro 新增令人兴奋的购物功能**：此次促销还包括通过 Buy with Pro 实现的**一键购物**和**免运费**功能。
   - 这些新功能旨在简化购物体验，让用户在这个节日季更加便捷。


  

---

### **Perplexity AI ▷ #[general](https://discord.com/channels/1047197230748151888/1047649527299055688/1311827371124461589)** (74 条消息🔥🔥): 

> `Perplexity Pro 订阅功能、Claude 用户体验、图像生成查询、订阅问题的客户支持、黑色星期五折扣` 


- **用户澄清 Perplexity Pro 功能**：一位用户询问 Perplexity Pro 订阅附带的 5 美元 API 额度在未使用的情况下是否会过期，随后确认只要订阅处于激活状态，该额度每月都会更新。
   - 另一位用户讨论了该平台的图像生成功能，并确认可以通过电脑在线使用，无需额外费用。

- **关于 Claude 和订阅的困惑**：几位用户对他们的订阅表示困惑，其中一位指出在没有当前订阅的情况下竟然可以免费访问 Claude。
   - 另一位用户寻求有关与 Revolut 相关的订阅问题的帮助，得到的建议是通过电子邮件联系支持团队。

- **客户支持方面的困难**：用户讨论了在寻找订阅相关查询的客户支持链接时遇到的挑战，一些人表示联系信息隐藏在 FAQ 中。
   - 一位用户确认他们被引导至正确的支持邮箱，但对缺乏可见性表示了短暂的沮丧。

- **用户对功能的反馈**：一位用户提供了关于 iOS 应用的反馈，表达了希望在突出显示文本时能够提出澄清性问题的愿望。
   - 这一请求强调了用户界面需要更多交互功能，以提高应用的可用性。

- **社区分享折扣码**：几位用户讨论了节日期间可能提供的折扣，特别是针对 Perplexity Pro 提供大幅减免的黑色星期五活动。
   - 参与者表示有兴趣分享折扣码并参与促销活动，例如针对新订阅的 2.5 折（75% off）优惠。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://x.com/perplexity_ai/status/1862526954064195816?s=46">Perplexity (@perplexity_ai) 的推文</a>: 这个节日季搜索和购物更聪明。获取 Perplexity Pro 首月 2.5 折优惠！访问最新的 AI 模型，搜索 3 倍以上的来源，并上传您自己的文件。此外，还可获得一键...</li><li><a href="https://tenor.com/view/cute-baby-sad-agnes-please-gif-16097001420698130990">可爱宝宝 GIF - 可爱宝宝伤心 - 发现并分享 GIF</a>: 点击查看 GIF</li><li><a href="https://giphy.com/gifs/RNXhdtXRmkv1wJ7gOK"> - 在 GIPHY 上查找并分享</a>: 与你认识的每个人一起发现并分享 stewieeee 的 childrenkick 动画 GIF。GIPHY 是你搜索、分享、发现和创建 GIF 的方式。
</li>
</ul>

</div>
  

---


### **Perplexity AI ▷ #[sharing](https://discord.com/channels/1047197230748151888/1054944216876331118/1311799585651556384)** (9 条消息🔥): 

> `意识的难题、偶然听到的趣闻、云的含水量、RBAC vs ABA、电池优化` 


- **探索意识的难题**：一位成员表达了他们对**意识的难题（hard problem of consciousness）**的好奇，思考它是否只是像其他人类创造物一样的另一种工具。
   - *It's just a tool as another human tool*.

- **关于偶然听到的趣闻的问题**：一位成员提到他们习惯于询问有关偶然听到的**趣闻（factoids）**的问题，突显了严肃问题与随性查询的融合。
   - 这反映了一种随性而又好奇的学习方式。

- **云及其含水量**：多位成员提出了关于**云的含水量较少**的问题，并将其与更广泛的大气状况讨论联系起来。
   - 对这一话题的兴趣表明了对气象现象的好奇。

- **讨论 RBAC vs ABA**：一位成员试图了解 **RBAC (Role-Based Access Control)** 和 **ABA (Attribute-Based Access Control)** 之间的区别。
   - 这一询问表明需要澄清技术中的访问控制模型。

- **优化电池续航**：成员们询问了有关**优化电池时长**的建议，寻求延长电池寿命的有效策略。
   - 这反映了对设备效率和可持续性的持续关注。


  

---

### **Perplexity AI ▷ #[pplx-api](https://discord.com/channels/1047197230748151888/1161802929053909012/1311916762966786078)** (5 messages): 

> `Claude 中的 Perplexity，Claude Project Knowledge，Perplexity 的文本文件读取问题，Spaces 的自定义指令` 


- **Perplexity 可以在 Claude 中使用吗？**: 用户很好奇 **Perplexity** 是否可以通过新的 MCP 功能集成到 **Claude** 中，类似于它在 **Brave** 和 **GitHub** 中的运行方式。
   - 他们强调，这种能力将通过利用 Claude 的 Project Knowledge 来增强性能。

- **Google 与 Claude 的集成？**: 关于在 **Claude** 中集成 **Google** 也有类似的咨询，寻求对其运行机制的澄清。
   - 成员们热衷于了解在这种背景下如何利用搜索功能。

- **Perplexity 的文本文件读取能力**: 一位成员询问 **Perplexity** 无法可靠读取文本文件的问题是否已得到解决。
   - 他们对可能解决这一限制的任何潜在长期记忆功能表示感兴趣。

- **Claude Spaces 中自定义指令的问题**: 有人对 Claude Spaces 的 **自定义指令 (custom instructions)** 的有效性表示担忧，这些指令似乎与现有的“自我介绍”提示词发生冲突。
   - 用户正在寻求关于这些指令如何复合或交互的澄清。


  

---


### **LM Studio ▷ #[announcements](https://discord.com/channels/1110598183144399058/1111797717639901324/1312130943527424081)** (1 messages): 

> `HF 搜索问题，图像分析` 


- **HF 搜索问题已解决**: **HF 搜索无法工作**的问题已经解决，这让用户松了一口气。
   - 随公告附带了一张图片以纪念此次修复，表明社区迎来了一次积极的更新。

- **分享图像分析**: 在关于 HF 搜索问题的公告中附带了一张图片，提供了视觉确认。
   - 虽然没有分享图像分析的具体细节，但可能有助于理解解决方案。


  

---


### **LM Studio ▷ #[general](https://discord.com/channels/1110598183144399058/1110598183144399061/1311852052200030258)** (56 messages🔥🔥): 

> `LM Studio AIDE 集成，LM Studio 中的 Llama 3.1 模型，LM Studio 网络问题，LM Studio 中的文档交互，Mac 上的 GUI 访问问题` 


- **成功的 LM Studio AIDE 集成**: 用户报告成功将 LM Studio 端点集成到 AIDE sidecar，实现了完全本地的代码编辑器体验。
   - 这种集成为寻求本地开发环境的用户展示了改进的功能。

- **寻找 Base Llama 3.1 模型**: 一位用户询问如何在 LM Studio 中访问 **Llama 3.1 8B** 的基础模型 (base model)，并指出似乎只有指令微调 (instruction-tuned) 变体可用。
   - 社区成员指出 [huggingface 仓库](https://huggingface.co/meta-llama/Llama-3.1-8B) 是基础模型的潜在来源。

- **网络连接问题**: 几位用户讨论了在确认本地访问正常的情况下，从本地网络外部访问 LM Studio 的问题。
   - 建议包括检查防火墙设置以及考虑使用类似 ngrok 的隧道服务进行远程访问。

- **与本地文件交互**: 新用户对如何在 LM Studio 中与本地文件交互感到好奇，特别是询问了文档附件功能。
   - 社区澄清目前只能将单个文件附加到聊天会话中，并参考了文档以获取进一步指导。

- **Mac GUI 访问故障**: 一位用户对在 Mac 上测试 headless 选项后无法访问 LM Studio GUI 表示沮丧。
   - 虽然有人建议通过 Finder 访问应用程序，但用户在 GUI 可用性方面仍然遇到困难。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF">lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF · Hugging Face</a>: 未找到描述</li><li><a href="https://lmstudio.ai/docs/basics/rag">与文档聊天 - 在本地运行 LLM | LM Studio 文档</a>: 如何将本地文档作为额外上下文提供给 LLM</li><li><a href="https://huggingface.co/meta-llama/Llama-3.1-8B">meta-llama/Llama-3.1-8B · Hugging Face</a>: 未找到描述</li><li><a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">meta-llama/Llama-3.1-8B-Instruct · Hugging Face</a>: 未找到描述
</li>
</ul>

</div>
  

---

### **LM Studio ▷ #[hardware-discussion](https://discord.com/channels/1110598183144399058/1153759714082033735/1311802272220581901)** (17 条消息🔥): 

> `Seasonic PSU 寿命, a770 性能对比, PC 组装建议, Intel 与 AMD 处理器, Qwen2.5-14b 的性能` 


- **Seasonic PSU 比 PC 组件更耐用**：一位成员提到他们的 **Seasonic PSU** 比其他 PC 组件寿命更长，尽管由于灰尘原因每隔几年就需要更换一次 PSU。
   - 他们形容对该 PSU 性能的使用体验非常满意。

- **a770 与 7800xt 相比表现挣扎**：另一位成员分享说，他们的 **a770** 在运行 Qwen2.5-14b q4_0 时仅达到 **11t/s**，远低于 **7800xt** 达到的 **40t/s**。
   - 他们指出 *q4_k_m 无法使用*，但发现 sycl 后端的速度提升微乎其微。

- **关于最佳 PC 配置的讨论**：在一次关于 PC 组装的讨论中，一位用户询问配备 **Intel Core i9 14900KF** 和 **NVIDIA GeForce RTX 4090** 的配置是否足以学习 LLM。
   - 其他人建议避开 **第 13/14 代 Intel**，转而选择 ***AMD Ryzen 7000 或 9000 系列*** 或者 **第 12 代 Intel**。

- **对 a770 定价的担忧**：一位成员表示有兴趣因折扣购买 **a770**，但最终决定等待下一代发布。
   - 他们得到的建议是，最好等待 GPU 技术的进一步发展。


  

---


### **Eleuther ▷ #[general](https://discord.com/channels/729741769192767510/729741769738158194/1311827576632774697)** (29 条消息🔥): 

> `资源争用降级, GPU 任务提交管理, SLURM 和 Kubernetes 的使用, AI 与 Crypto 的交集, 学术资源的开放获取` 


- **讨论资源争用的降级**：成员们对 **资源争用的降级** 以及互联网不受监管增长的影响表示担忧，并对 AI 驱动的隐私解决方案提出质疑。
   - 有人建议识别 *可能利用脆弱设备的流氓 AI 攻击的预警信号*，强调在 AI 保护方面需要社区领导力。

- **汇集昂贵的 GPU VM 进行任务提交**：有人询问关于管理昂贵 GPU VM 池以进行任务提交的 **开源解决方案**，表明需要有效的资源记账。
   - 回复中强调了 **SLURM 队列** 和 Kubernetes 的使用，尽管对其在高信任环境中的适应性存在怀疑。

- **低信任环境下 SLURM 的最佳实践**：成员们探讨了是否存在一种专门的 **SLURM** 设置，允许在低信任环境中进行私有存储细分，并对潜在解决方案提出了各种见解。
   - 分享的一些经验包括利用 **network-filesystems** 和 S3 前缀进行权限管理，但同时也建议警惕不必要的复杂性。

- **不欢迎 AI 与 Crypto 的讨论**：一位参与者询问了 **AI 与 Crypto** 的交集，对此一位成员评论说，此类讨论在当前频道通常不被欢迎。
   - 这反映了保持讨论集中并可能将更广泛的话题重定向到更合适频道的愿望。

- **学术资源协作**：提议建立一个服务器供成员分享 **高质量的论文和资源**，以便在没有无关干扰的情况下持续获取。
   - 这一倡议可以加强社区内的协作和资源共享，旨在实现高效且精简的交流。


  

---

### **Eleuther ▷ #[research](https://discord.com/channels/729741769192767510/747850033994662000/1312028602300366939)** (23 messages🔥): 

> `Poincare Ball Embedding, Hyperbolic Geometry, Graph Distortion, Embedding Trees, HyperE` 


- **Poincare Ball Embedding 解析**：将数据嵌入到 **Poincare ball** 本质上意味着度数较高的点更靠近原点，以便在向**曲率较小**的区域移动时保持邻接性。
   - 讨论中对 Poincare ball 的边缘进行了自我纠错，指出其为无穷远点，点实际上无法驻留在那里。

- **双曲嵌入（Hyperbolic Embedding）资源**：**HyperE** 研究团队提供了多种优化结构化对象（如知识图谱）嵌入的方法，重点参考了 **Nickel & Kiela (2017)** 和 **Chamberlain et al. (2017)** 的出版物。
   - 这些双曲嵌入可以有效地在低维空间中保留图距离，应用于 **NLP** 和知识库补全等领域。

- **图失真（Graph Distortion）担忧**：一名成员提出，嵌入过程可能无法尊重某些数据集的结构，特别是在高密度图中，如**全连接图 (FC)**。
   - 讨论建议使用一种启发式方法，通过与**等效树结构**进行比较来估计失真，从而更好地理解嵌入质量。

- **低失真的条件**：虽然在特定条件下图嵌入的失真可能很低，但这并不具有普适性；由于节点数量与度数的问题，某些图天生就无法很好地嵌入。
   - 图嵌入文献表明，特定的数学条件决定了嵌入实现低失真的可能性。

- **图嵌入的数学原理**：有大量的数学文献讨论如何将图嵌入到**双曲空间**中，尽管许多人发现这很难完全理解。
   - 评估嵌入失真的一个良好启发式方法是评估嵌入与逻辑等效树结构的对比情况。



**提及的链接**：<a href="https://hazyresearch.stanford.edu/hyperE/">HyperE</a>：未找到描述

  

---


### **Eleuther ▷ #[scaling-laws](https://discord.com/channels/729741769192767510/785968841301426216/1312051675342573590)** (5 messages): 

> `AutoML Challenges, TaskSet Dataset, Neural Architecture Design, Equivariant vs Non-equivariant Networks` 


- **AutoML 面临基础任务挑战**：一名成员提到，目前大多数 **AutoML** 处理的都是非常简单的任务，并强调了构建大规模学习曲线数据集的资金限制。
   - 他们指出目前最好的选择是 **TaskSet**，但也承认它已经相当过时了。

- **TaskSet 助力优化器训练**：关于 **TaskSet** 数据集的摘要揭示了其独特的规模和多样性，包含超过一千个用于训练和评估优化器的任务。
   - 该数据集促进了超参数列表的 **meta-learning**，从而在效率上比随机搜索有显著提升。

- **等变网络（Equivariant Networks）提升效率**：一篇论文探讨了**等变与非等变网络**如何随模型大小和计算量缩放，发现等变性增强了数据效率。
   - 实验结果显示，虽然非等变模型可以通过足够的训练缩小这一差距，但等变模型在所有计算预算下都表现得更好。

- **质疑神经架构设计方法**：讨论涉及了针对特定问题量身定制神经架构与从数据中学习架构的效率对比。
   - 一名成员表示有兴趣了解关于等变性和计算预算分配的发现是否也适用于其他任务。


<div class="linksMentioned">

<strong>提及的链接</strong>：

<ul>
<li>
<a href="https://arxiv.org/abs/2410.23179v1">Does equivariance matter at scale?</a>：给定大规模数据集和充足的计算资源，为每个问题的结构和对称性设计神经架构是否有益？还是从数据中学习它们更有效？我们研究了...</li><li><a href="https://openreview.net/forum?id=PghuCwnjF6y">TaskSet: A Dataset of Optimization Tasks</a>：我们介绍了 TaskSet，一个用于训练和评估优化器的任务数据集。TaskSet 在规模和多样性上是独一无二的，包含从图像分类到...的一千多个任务。
</li>
</ul>

</div>

### **Eleuther ▷ #[lm-thunderdome](https://discord.com/channels/729741769192767510/755950983669874798/1311836435329974274)** (17 条消息🔥): 

> `HF Tokenizer 处理、自定义 Tokenizer 考量、Evaluation Harness 模型函数、模型中的生成参数` 


- **在 Eval Harness 中理解 HF Tokenizers**：关于 eval harness 在对序列进行 tokenization 时是使用 `add_special_tokens=True` 还是 `False` 存在困惑，特别是关于在生成任务中如何处理 EOS tokens。
   - 成员们讨论认为，通常在模型中**仅应添加 BOS tokens**而省略 EOS tokens，特别是在构建自定义 tokenizers 时。

- **手动 EOS Token 管理**：一位成员考虑修改其 tokenizer，在 tokenization 过程中禁用 EOS token，并在训练循环中手动添加。
   - 这种方法被认为是切实可行的，并有望避免在使用 HF 模型的各种框架之间出现兼容性问题。

- **Generate Until 函数讨论**：为了使用 eval harness 评估自定义模型，需要实现 `generate_until` 函数来处理各种生成参数，包括 `until`、`do_sample` 和 `max_gen_toks`。 
   - 关于该函数是否需要额外的关键字参数的询问，明确了 `max_gen_toks` 是 eval harness 特有的，而其他参数则与标准 HF 实践保持一致。

- **为自定义模型子类化 HFLM**：成员们建议通过子类化 HFLM 并重载 `model_generate` 和 `_model_call` 等方法，来简化自定义模型的集成。
   - 这种方法被认为是框架内处理自定义模型评估的一种更直接的方式。


<div class="linksMentioned">

<strong>提及的链接</strong>:

<ul>
<li>
<a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/5680a2e6b">GitHub - EleutherAI/lm-evaluation-harness at 5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3</a>: 一个用于语言模型 few-shot 评估的框架。 - GitHub - EleutherAI/lm-evaluation-harness at 5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3/lm_eval/models/huggingface.py#L771-L795">lm-evaluation-harness/lm_eval/models/huggingface.py at 5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/.">GitHub - EleutherAI/lm-evaluation-harness: 一个用于语言模型 few-shot 评估的框架。</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/9169899b4966b4161719e54d41258345df03aaa0/lm_eval/models/huggingface.py#L1308)">lm-evaluation-harness/lm_eval/models/huggingface.py at 9169899b4966b4161719e54d41258345df03aaa0 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/9169899b4966b4161719e54d41258345df03aaa0/lm_eval/models/huggingface.py#L857)">lm-evaluation-harness/lm_eval/models/huggingface.py at 9169899b4966b4161719e54d41258345df03aaa0 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/9169899b4966b4161719e54d41258345df03aaa0/lm_eval/models/huggingface.py#L831)">lm-evaluation-harness/lm_eval/models/huggingface.py at 9169899b4966b4161719e54d41258345df03aaa0 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3/lm_eval/models/huggingface.py#L1299)">lm-evaluation-harness/lm_eval/models/huggingface.py at 5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness</li><li><a href="https://github.com/EleutherAI/lm-evaluation-harness/blob/5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3/lm_eval/api/model.py#L354-L355).">lm-evaluation-harness/lm_eval/api/model.py at 5680a2e6b5cf1a1621d8ff68d3d0e83e8b2731d3 · EleutherAI/lm-evaluation-harness</a>: 一个用于语言模型 few-shot 评估的框架。 - EleutherAI/lm-evaluation-harness
</li>
</ul>

</div>
  

---

### **OpenRouter (Alex Atallah) ▷ #[announcements](https://discord.com/channels/1091220969173028894/1092729520181739581/1312105380041461810)** (1 条消息): 

> `功能请求投票，额外请求频道` 


- **立即为热门功能请求投票！**：鼓励成员[在此为他们最看重的功能请求投票](https://link.to/vote)，以帮助确定未来开发的优先级。
   - 此外，对于未列出的任何请求，可以使用 <#1107397803266818229> 进行提交。

- **额外功能请求频道**：提供了一个专用频道 (<#1107397803266818229>)，供用户提交投票中未涵盖的任何功能请求。
   - 这使得社区能够针对所需功能提供更广泛的反馈。


  

---


### **OpenRouter (Alex Atallah) ▷ #[general](https://discord.com/channels/1091220969173028894/1094454198688546826/1311831311752626216)** (57 条消息🔥🔥): 

> `Pixtral Large 的能力、关于模型响应的担忧、特定供应商功能、OpenRouter 中的图像生成、Llama 3.2 的结构化输出` 


- **Pixtral Large 给用户留下深刻印象**：用户注意到 **Pixtral Large** 提供了出色的性能和**庞大的免费层级**，并鼓励通过 [console.mistral.ai](https://console.mistral.ai) 轻松访问。另一位用户从 **Hermes 405b** 切换到 **Pixtral**，发现它在不更改 Prompt 的情况下依然有效。

- **关于模型身份识别的困惑**：围绕模型训练展开了讨论，一些人澄清说模型本身并不了解自己的身份，而是经常根据训练数据幻觉出细节。这引发了关于尽管有这些解释，为什么困惑依然存在的问题。

- **关于成本计算方法的问题**：一位用户询问 **/api/v1/generation** 端点是否有任何费率，以及如何准确估算生成成本。建议包括使用 **Helicone** 进行跟踪，并澄清目前为了进行精确的成本评估，生成端点是必要的。

- **OpenRouter 图像生成的未来**：虽然图像生成目前不在 **OpenRouter** 的近期 Roadmap 中，但不排除未来实现的可能性。讨论表明用户对图像模型能力的兴趣日益浓厚。

- **Llama 3.2 结构化输出的挑战**：用户报告说在使用 **Llama 3.2-vision-instruct** 获取**结构化输出 (Structured Outputs)** 时遇到困难，指出虽然它声称具有 JSON 输出能力，但与 **Gemini Flash** 等替代方案相比，性能表现滞后。会议强调，对此类功能的支持在很大程度上取决于所使用的推理软件。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://docs.helicone.ai/getting-started/integration-method/openrouter">OpenRouter 集成 - Helicone 开源 LLM 可观测性</a>：未找到描述</li><li><a href="https://openrouter.ai/docs/provider-routing">供应商路由 | OpenRouter</a>：跨多个供应商路由请求</li><li><a href="https://openrouter.ai/meta-llama/llama-3.2-90b-vision-instruct">Llama 3.2 90B Vision Instruct - API、供应商、统计数据</a>：Llama 90B Vision 模型是一款顶级的、拥有 900 亿参数的多模态模型，专为最具挑战性的视觉推理和语言任务而设计。它在图像描述方面提供了无与伦比的准确性...</li><li><a href="https://mistral.ai/news/pixtral-large/">Pixtral Large</a>：Pixtral 成长了。</li><li><a href="https://docs.helicone.ai/getting-started/integra">简介 - Helicone 开源 LLM 可观测性</a>：未找到描述</li><li><a href="https://openrouter.ai/rankings">LLM 排行榜 | OpenRouter</a>：根据应用使用情况对语言模型进行排名和分析
</li>
</ul>

</div>
  

---


### **OpenRouter (Alex Atallah) ▷ #[beta-feedback](https://discord.com/channels/1091220969173028894/1277894087755829278/1311967005263265883)** (5 条消息): 

> `自定义供应商密钥` 


- **开发者推动获取自定义供应商密钥的访问权限**：多位开发者表达了对访问**自定义供应商密钥 (Custom Provider Keys)** 的兴趣，表明社区对该功能有强烈需求。
   - *一位成员在请求访问权限时提到*，“感谢你们所做的出色工作！”

- **开发者的集体请求**：包括 **monomethylhydrazine** 和 **kit18** 在内的几位用户也表达了在某些供应商中使用自己密钥的愿望。
   - 这一反复出现的主题突显了开发者在这些功能需求上的共识。

### **GPU MODE ▷ #[general](https://discord.com/channels/1189498204333543425/1189498205101109300/1311973619630477393)** (2 条消息): 

> `Parallel processing on NVIDIA GPU, Posting in beginner section` 


- **寻求并行处理问题的帮助**：一名成员表达了在 **NVIDIA GPU** 上进行 **parallel processing**（并行处理）时遇到的困难，并寻求指导。
   - 对话转向确保技术讨论被引导至适当的板块，以便获得更好的帮助。

- **建议在初学者板块发布**：另一名成员建议不要在此处讨论技术问题，并推荐将问题发布在 **beginner**（初学者）板块。
   - 此举旨在优化讨论流程，并将提问者引导至更适合其咨询的区域。


  

---


### **GPU MODE ▷ #[triton](https://discord.com/channels/1189498204333543425/1189607595451895918/1311952730809696288)** (9 条消息🔥): 

> `Metaprogramming Proposal, Building Triton from Source, Offline Compilation Dependencies` 


- **元编程提案引起关注**：一位用户分享了一个针对 Triton 的 [元编程提案 (metaprogramming proposal)](https://github.com/triton-lang/triton/pull/5284)，旨在解决当前的局限性，并收集社区反馈。
   - 一些成员对该提案表示感兴趣，但对其语义的清晰度提出质疑，建议增加示例以增强理解。

- **从源码构建 Triton 的说明**：一位新成员询问了从源码构建 Triton 所需的**最小内存**，寻求社区帮助。
   - 在收到包括路径调整在内的排错建议后，该用户报告称在将 **WSL2** 内存增加到 **26GB** 以避免内存溢出（out-of-memory）错误后成功构建。

- **关于离线编译的疑问**：另一名成员提出了关于在 Ubuntu **Docker** 容器中以**离线模式**从源码构建 Triton 的问题，以及手动收集依赖项的必要步骤。
   - 他们寻求关于离线编译的便捷配置建议，以及成功构建所需的**最小依赖项**。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://github.com/triton-lang/triton/pull/5284">[FRONTEND][WIP][RFC] Rewrite AST conversion to improve metaprogramming by kuterd · Pull Request #5284 · triton-lang/triton</a>: 问题陈述：Triton 当前元编程的局限性导致 Torch Inductor 等主要用户不得不求助于使用基于字符串的模板。此 RFC 旨在解决其中的一些...</li><li><a href="https://github.co">GitHub · 在单一协作平台上构建和交付软件</a>：加入全球应用最广泛、AI 驱动的开发者平台，数百万开发者、企业和最大的开源社区在此构建推动人类进步的软件。
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[cuda](https://discord.com/channels/1189498204333543425/1189607726595194971/1312082037125156906)** (1 条消息): 

> `cuBLAS async loads, Custom kernels performance, SASS instructions, CuTe templates, Throughput considerations` 


- **使用 SASS 剖析 cuBLAS 异步加载**：在对带有 [cuBLAS](https://developer.nvidia.com/cublas) 的自定义 Kernel 进行性能分析时，一位用户观察到异步加载的 **SASS** 使用了 `LDGSTS.E.BYPASS.LTC128B.128.CONSTANT`，而他们的代码生成的是 `LDGSTS.E.BYPASS.LTC128B.128`。
   - 他们对 **CONSTANT** 部分的含义及其对性能的潜在影响感到好奇。

- **A100 上的基准测试揭示潜在问题**：该用户正在 **A100** 上对自定义 Kernel 进行基准测试，并且不确定 **SASS** 指令的差异是否相关，因为他们目前的性能水平远未达到预期。
   - 他们正在探索各种选项，以寻求更好的吞吐量（throughput）和效率。

- **关于 SASS 和吞吐量的疑问**：用户提出了两个具体问题：**SASS** 中的 **CONSTANT** 意味着什么，以及这两类指令之间是否存在显著的**吞吐量考量**。
   - 这些疑问突显了对优化 Kernel 实现性能的深入探索。


  

---

### **GPU MODE ▷ #[torch](https://discord.com/channels/1189498204333543425/1189607750876008468/1311799587954102333)** (26 条消息🔥): 

> `Triton 性能，Fusion 策略，PyTorch 中的内存使用，Max autotune 设置，NANOgpt 集成` 


- **Triton 比 cuBLAS 慢**：一场讨论揭示了 **Triton** kernel 的性能通常不如 **cuBLAS**，特别是由于尚未采用 **TMAs** 或 **persistent** 模式的未优化模板。
   - 成员们强调了对 **fusion** 可能导致计算变慢的担忧，特别是在计算密集型场景中带有重型 **epilogues** 的情况。

- **Max Autotune 未融合 RELU Squared**：即使设置了 **TORCHINDUCTOR_MAX_AUTOTUNE_GEMM_BACKENDS=TRITON**，一位成员仍对 **RELU squared** 未被融合表示沮丧。
   - 这引发了关于 autotune 有效性的疑问，以及在保留 **cuBLAS** 以实现更快速操作的同时处理 Triton 较慢 kernel 的复杂性。

- **融合 Matmul 和 Pointwise 操作**：将 **matmul** 融合进 **pointwise** 操作缺乏收益，这被认为更多是关于确定有利可图的场景，而非技术难度。
   - 成员们指出，了解何时 fusion 会导致操作变慢对于避免对 **Inductor** 性能产生困惑至关重要。

- **Torch Snapshot 工具中的内存使用**：一位用户对使用 **torch memory snapshot tool** 时看到的显著 **'Unknown'** 内存使用提出疑问，并分享了相关截图供参考。
   - 这引发了对 PyTorch 应用中内存管理和追踪清晰度的担忧。

- **使用 Thunder Kittens 的潜力**：一位成员推测，将基于 **Thunder Kittens** 的 matmul 实现集成到 PyTorch 中可能会解决讨论中的一些性能问题。
   - 这一想法源于围绕 **BF16** 处理的复杂性以及为获得更好性能而优化 kernel 的需求。


  

---


### **GPU MODE ▷ #[algorithms](https://discord.com/channels/1189498204333543425/1189861061151690822/)** (1 条消息): 

melanimahes: https://arxiv.org/pdf/2411.17116
  

---

### **GPU MODE ▷ #[cool-links](https://discord.com/channels/1189498204333543425/1189868872887705671/1311968374615707648)** (1 条消息): 

> `Diffusion Models Overview, Classifier-free Diffusion Guidance, Perspectives on Diffusion Models, Noise Schedules in Diffusion Models` 


- **Diffusion Models 成为焦点**：Diffusion Models 已成为生成图像和声音等感知信号的**首选模型 (go-to model)**，凭借**更好的模式覆盖 (mode coverage)**和**更快的采样 (sampling)**速度超越了传统模型。其构建过程包括逐渐将数据转换为噪声，并训练神经网络来逆转这一过程。
   - 自 Song & Ermon 在 2019 年发表[开创性论文](https://arxiv.org/abs/1907.05600)以来，与 Diffusion Models 相关的研究兴趣迅速增长，引发了巨大的研究势头。

- **Classifier-free Diffusion Guidance 极大提升输出质量**：正如博客文章中所讨论的，**Classifier-free Diffusion Guidance** 的实现以极低的成本显著增强了条件 Diffusion Models 的结果。这项技术对于优化 [OpenAI 的 DALL·E 2](https://openai.com/dall-e-2/) 和 [Google 的 Imagen](https://imagen.research.google/) 中的图像生成至关重要。
   - 这种方法使 Diffusion Models 变得更加卓越，在没有复杂开销的情况下提升了样本质量。

- **多元视角推动 Diffusion 研究**：探索 Diffusion Models 的不同视角既揭示了挑战，也提供了有益的见解。Diffusion 的各种特性凸显了其**灵活性**，并激发了各类研究论文中的创新想法。
   - 该综述对比了不同研究论文的方法，使得理解它们之间的关联动态虽然*令人沮丧但富有启发性*。

- **重新评估 Noise Schedules**：Diffusion Models 中使用的 **Noise Schedule** 是一个关键但经常令人困惑的设计元素，它决定了扩散过程中的噪声强度。一篇博客文章主张重新构建关于 Noise Schedules 的讨论，以便更清晰地理解和应用。
   - 作者的主观见解旨在阐明不同的噪声水平如何影响 Diffusion Models 的性能，为这个略具争议的话题提供了一个全新的视角。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://sander.ai/2022/01/31/diffusion.html">Diffusion models are autoencoders</a>：Diffusion Models 在过去两年中变得非常流行。Diffusion Models 与 Autoencoders 之间存在一种尚未被充分认识的联系。</li><li><a href="https://sander.ai/2022/05/26/guidance.html">Guidance: a cheat code for diffusion models</a>：一篇关于 Diffusion Guidance 见解的简短文章。</li><li><a href="https://sander.ai/2023/07/20/perspectives.html">Perspectives on diffusion</a>：关于 Diffusion 的视角，或者说 Diffusion Models 如何同时是 Autoencoders、深度隐变量模型、Score Function 预测器、逆向 SDE 求解器、基于 Flow 的模型、RNN 和自回归模型等。</li><li><a href="https://sander.ai/2024/06/14/noise-schedules.html">Noise schedules considered harmful</a>：Noise Schedule 是 Diffusion Models 的关键设计参数。不幸的是，它是一个多余的抽象，将模型的几个不同方面纠缠在一起。我们真的需要它吗？
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[off-topic](https://discord.com/channels/1189498204333543425/1215328286503075953/1312019871671914497)** (2 条消息): 

> `Series A Docs Process, HR Reporting Protocols` 


- **德国公证员朗读 A 轮融资文档**：在德国，公证员会在创始人面前大声朗读 A 轮融资文档的每一个字，这被用户形容为**史前时代的疯狂**。
   - 看到这一幕，一位参与者幽默地提到，他们还有 **GDP 要增长**，强调了这种情况的荒谬性。

- **关于 HR 报告的担忧**：一位用户对公证员的流程表示担忧，建议应该将其报告给 **apaz 的 HR**。
   - 这引发了关于此类做法是否适合现代商业环境的疑问。



**提到的链接**：<a href="https://x.com/nathanbenaich/status/1862208030596636770">来自 Nathan Benaich (@nathanbenaich) 的推文</a>：已经 12 小时了——在德国，公证员正在创始人面前大声朗读 A 轮融资文档的每一个字。本人到场。伙计们，我们还得增长 GDP 呢。纯粹是史前时代的疯狂。

  

---

### **GPU MODE ▷ #[bitnet](https://discord.com/channels/1189498204333543425/1240586843292958790/1311930671626981439)** (2 条消息): 

> `BitNet b1.58, 1-bit LLMs, Open-Source Models, RedPajama Dataset, Dolma Dataset` 


- **BitNet b1.58 模型发布**：使用 [RedPajama dataset](https://github.com/togethercomputer/RedPajama-Data) 训练了 **100B tokens**，BitNet b1.58 模型在 PPL 和 zero-shot 准确率方面表现出良好的前景。
   - 训练细节记录在他们的论文 [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) 中，模型可在开源 [repo](https://huggingface.co/1bitLLM) 中获取。

- **OLMo-Bitnet-1B 作为概念验证**：[OLMo-Bitnet-1B](https://huggingface.co/NousResearch/OLMo-Bitnet-1B) 是一个 1B 参数模型，在 [Dolma dataset](https://huggingface.co/datasets/allenai/dolma) 的前 **60B tokens** 上进行了训练，强调了其研究性质。
   - 可以在 [wandb report](https://api.wandb.ai/links/emozilla/evltqiv7) 中探索与标准 fp16 权重的比较，展示了不同训练方法的有效性。

- **训练超参数详情**：模型使用特定的超参数进行训练，包括相应 [documentation](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf) 中推荐的两阶段 LR 和 weight decay。
   - 性能细节反映了报告模型和复现模型之间的不同结果，为模型有效性提供了见解。


<div class="linksMentioned">

<strong>提及的链接</strong>:

<ul>
<li>
<a href="https://huggingface.co/1bitLLM/bitnet_b1_58-3B">1bitLLM/bitnet_b1_58-3B · Hugging Face</a>: 未找到描述</li><li><a href="https://huggingface.co/NousResearch/OLMo-Bitnet-1B">NousResearch/OLMo-Bitnet-1B · Hugging Face</a>: 未找到描述
</li>
</ul>

</div>
  

---


### **GPU MODE ▷ #[self-promotion](https://discord.com/channels/1189498204333543425/1288557096404516945/1312010021395107860)** (1 条消息): 

> `Japanese LLM evaluation, Open Japanese LLM Leaderboard, Hugging Face collaboration` 


- **发布 Open Japanese LLM Leaderboard**：发布了关于 **[Open Japanese LLM Leaderboard](https://huggingface.co/spaces/llm-jp/open-japanese-llm-leaderboard)** 的激动人心的公告，旨在评估超过 **20 个数据集**和任务中的各种日语 LLM。
   - 该倡议是 **[LLM-jp](https://llm-jp.nii.ac.jp/en/)** 项目和 **Hugging Face** 的合作成果，旨在增强对日语 LLM 机制的理解。

- **专注于日语语言模型性能**：日语 LLM 的发展落后于英语，因此需要全面的性能评估。
   - 这一公告引起了日本 **HPC engineers** 的兴趣，他们热衷于母语领域的进展。



**提及的链接**: <a href="https://huggingface.co/blog/leaderboard-japanese">Introducing the Open Leaderboard for Japanese LLMs!</a>: 未找到描述

  

---

### **GPU MODE ▷ #[thunderkittens](https://discord.com/channels/1189498204333543425/1300872762163728550/1311799317014646854)** (14 条消息🔥): 

> `Non-Warp Specialized Implementations, Unification of ThunderKittens and ThunderMittens, API Contracts between TK and TM, Auto Optimizer for TK, Triton vs ThunderKittens Features` 


- **探索非 Warp 特化实现**：一位成员询问是否存在非 Warp 特化的实现，另一位成员确认目前没有预构建的 FP8 kernel，但表示可以协助创建一个。
   - 他们还分享了 TK repo 中现有的 [非 Warp 特化 kernels](https://github.com/HazyResearch/ThunderKittens/tree/main/kernels/fftconv/non_pc) 链接。

- **Tile 抽象统一了 ThunderKittens 和 ThunderMittens**：成员们讨论了 **ThunderKittens** 和 **ThunderMittens** 之间的主要统一因素，认为 **tile 抽象** 对 Tensor Core 兼容性至关重要。
   - 有人指出，这种抽象允许**直接控制寄存器使用 (register usage)**，为操作 tile 的库函数奠定了基础。

- **ThunderKittens 与 ThunderMittens 之间的 API 契约**：有人提问 **ThunderKittens** 和 **ThunderMittens** 之间是否存在 API 契约，强调了兼容性的重要性。
   - 这引发了关于框架如何看待 API 关系以及围绕 kernel 功能进行结构化的讨论。

- **对 ThunderKittens 自动优化器的渴望**：一位成员表示对 **ThunderKittens** 的**自动优化器 (auto optimizer)** 感兴趣，强调其“一次编写，多次运行”的系统特性。
   - 他们对包含此优化功能的领域特定语言 (DSLs) 表示赞赏。

- **Triton 与 ThunderKittens 的特性对比**：讨论围绕 **ThunderKittens** 如何通过显式暴露 layouts、异步操作 (async operations) 和共享内存分配 (shared memory allocations) 来区别于 **Triton**。
   - 此外，他们还提到了将这些功能直接嵌入 **CUDA/Metal** 的重要性。



**提及的链接**：<a href="https://github.com/HazyResearch/ThunderKittens/tree/main/kernels/layernorm/non_pc">ThunderKittens/kernels/layernorm/non_pc at main · HazyResearch/ThunderKittens</a>：用于快速 kernel 的 Tile 原语。通过在 GitHub 上创建账号为 HazyResearch/ThunderKittens 的开发做出贡献。

  

---


### **Nous Research AI ▷ #[general](https://discord.com/channels/1053877538025386074/1149866623109439599/1311901917487824956)** (42 条消息🔥): 

> `Hermes 3 updates, Mistral Platform issues, Truth Terminal in Crypto & AI, Job hunting in Discord, AI and Crypto community crossover` 


- **Hermes 3 的咨询引发关注**：一位成员提出了关于 **Hermes 3** 的问题，其他人暗示这可能与旧的 **O1 风格**有关。
   - 这次讨论表明了人们对 **Hermes** 进展的持续好奇。

- **Mistral 平台的新挑战**：成员们对 **Mistral AI** 平台的问题表示担忧，特别是模型选择方面，因为现在默认只有一个选项。
   - 还有关于**图像生成**功能受限的评论，这引起了用户的一些困惑。

- **Truth Terminal 的独特叙事**：一位成员分享了关于加密货币领域 **Truth Terminal** 叙事的见解，将其描述为一个半自治的 AI，正在创造自己的宗教。
   - 他们强调了其与 AI Alignment 讨论的联系，标志着 **AI 和加密社区**的一个独特交汇点。

- **对 Discord 求职有效性的质疑**：成员们讨论了在 Discord 上求职的有效性，并对在以 AI 为中心的群组中提及区块链经验的可行性表示怀疑。
   - 有人担心这种做法可能会被视为不可靠，表明了对该平台进行职业社交的复杂感受。

- **AI 社区内的不同派系**：讨论涉及 AI 爱好者中的不同**派系 (tribes)**，包括关注安全 (safety) 和加速 (acceleration) 的派系，以及一些人如何将 AI 视为加密创业的替代品。
   - 这突显了社区内多样的兴趣和观点，一些成员仅仅是为了好玩而参与。



**提及的链接**：<a href="https://www.chainofthought.xyz/p/goat-the-gospel-of-goatse">GOAT: The Gospel of Goatse</a>：为什么 Truth Terminal 是对社会日益痴迷于自主 AI Agent 的一种非对称押注。

  

---

### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1311806401323991080)** (5 条消息): 

> `Low-bit quantization 效应, Precision-aware scaling laws, Ternary quantization 对比欠训练模型, FP4 效率` 


- **Low-bit quantization 有利于欠训练的 LLM**：研究表明，与经过大量数据训练的小模型相比，**low-bit quantization** 在规模更大但训练不足的 LLM 中导致的性能下降更少。通过研究超过 1500 个 LLM checkpoint 得出的 scaling laws 有助于量化 **quantization-induced degradation** (QiD) 与模型大小及训练 token 等因素之间的关系。
   - 该研究强调，调整量化可以深入了解 LLM 的**训练水平**以及不同模型大小对训练 token 的需求。

- **引入 precision-aware scaling laws**：一种新方法提出了用于训练和推理的 **precision-aware scaling laws**，强调低精度会影响模型的**有效参数量**和整体性能。研究结果表明，虽然低精度训练看起来可能是最优的，但随着训练数据的增加，它可能会导致 loss 增加和模型有效性下降。
   - 这项工作暗示使用较低精度可能是 compute optimal 的，但警告说，随着数据输入的增加，**post-training quantization** 的影响会显著增大。

- **Ternary quantization 的效用存疑**：据观察，被称为 **BitNet** 的 **ternary quantization** 仅在模型**欠训练**时才表现出更好的结果，这引发了对其整体功效的怀疑。这表明对于现有的模型规模，可能会重新转向使用 **FP4** 作为最优的数值权重表示。
   - 此外，在 QaT 等方法中，**quantization 与较小模型**之间的关系进一步支持了反对广泛采用 ternary quantization 的论点。


<div class="linksMentioned">

<strong>提及的链接</strong>:

<ul>
<li>
<a href="https://arxiv.org/abs/2411.04330">Scaling Laws for Precision</a>: 低精度训练和推理会影响语言模型的质量和成本，但目前的 scaling laws 尚未考虑到这一点。在这项工作中，我们设计了 "precision-aware" scaling la...</li><li><a href="https://arxiv.org/abs/2411.17691">Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</a>: 我们通过观察发现，规模更大或训练 token 更少的模型经历的 quantization-induced degradation 更少，从而揭示了 low-bit quantization 有利于欠训练的 LLM...
</li>
</ul>

</div>
  

---


### **Nous Research AI ▷ #[interesting-links](https://discord.com/channels/1053877538025386074/1132352574750728192/1311894763405840415)** (3 条消息): 

> `Filter 问题, Content policy, 用户体验` 


- **过滤器导致无意的限制**：过滤器存在一些无意中过于严格的**问题**，影响了用户体验。
   - 团队正计划**回滚 (revert)** 更改以恢复正常功能。

- **对用户自由的承诺**：目标是允许用户想要的**任何内容**，同时确保禁止非法或过度不安全的内容。
   - 这体现了**用户自由**与必要的内容审核之间的平衡。

- **对造成的不便表示歉意**：团队对过滤器问题造成的不便表示歉意，并强调这并非本意。
   - 他们向用户保证，情况应该很快就会**恢复正常**。


  

---

### **Nous Research AI ▷ #[research-papers](https://discord.com/channels/1053877538025386074/1104063238934626386/1311806401323991080)** (5 messages): 

> `Low-bit quantization effects, Precision-aware scaling laws, Ternary quantization usefulness, FP4 as efficient representation, QaT for smaller models` 


- **低比特量化更倾向于训练不足的 LLM**：研究显示，在训练 Token 较少的大型 LLM 中，低比特量化导致的性能退化较小，而小型模型则面临显著困难，详见[这篇论文](https://arxiv.org/abs/2411.17691)。
   - 该研究指出，有必要探索**缩放定律 (Scaling Laws)**，以理解不同训练水平的模型在量化后产生的性能退化。

- **引入精度感知缩放定律**：一种新方法揭示了**低精度训练**会减少模型的有效参数量，并有助于预测训练期间的 Loss，如[这项研究](https://arxiv.org/abs/2411.04330)所述。
   - 研究结果表明，在使用低精度时，过量的预训练数据可能会损害模型性能，这挑战了当前的缩放假设。

- **对三值量化的怀疑**：观察表明，三值量化 (BitNet) 仅对**训练不足的网络**产生较好结果，这让人对其整体适用性产生怀疑。
   - 目前的共识是，对于主流的模型规模，我们可能不得不依赖 **FP4** 作为最高效的数值权重表示。

- **对有效性能的担忧**：讨论表明，当前的量化策略（特别是针对较小模型的策略）可能无法产生预期的性能提升。
   - 对 **QaT** (Quantization-Aware Training) 的分析也支持这一观点，即小型模型在量化有效性方面面临重大挑战。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://arxiv.org/abs/2411.04330">Scaling Laws for Precision</a>：低精度训练和推理会影响语言模型的质量和成本，但目前的缩放定律并未考虑到这一点。在这项工作中，我们设计了“精度感知”的缩放定律...</li><li><a href="https://arxiv.org/abs/2411.17691">Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</a>：我们通过观察发现，规模更大或训练 Token 较少的模型经历的量化退化更少，从而揭示了低比特量化更倾向于训练不足的大型语言模型 (LLMs)...
</li>
</ul>

</div>
  

---


### **Modular (Mojo 🔥) ▷ #[mojo](https://discord.com/channels/1087530497313357884/1151418092052815884/1311929451273125899)** (35 messages🔥): 

> `Mojo Origins, Rust Lifetimes, Compiler Behavior, Destructor Calls, Variable Naming` 


- **关于 Mojo Origins 与 Rust Lifetimes 的混淆**：一位用户对 **Mojo 的 Origins** 与 **Rust 的 Lifetimes** 的相似性表示困惑，认为两者都旨在解决内存管理问题，但在本质上有所不同。
   - *Nick.sm 澄清道*，虽然受到了 Rust 的启发，但 Mojo 的设计是有意区分的，旨在实现不同的编译器行为和目标。

- **Mojo Origins 维持内存控制**：Mojo 的 **Origin** 表示一块内存区域；当一个指针由 Origin 参数化时，表示它指向该内存内部，并根据需要延长变量的生命周期。
   - *Nick.sm 补充说*，Origins 促进了别名保证 (Aliasing Guarantees)，并且如果指针在目标失效后仍然存活，则会产生编译时错误。

- **理解 Origins 需要耐心**：从编译器的角度理解 Mojo Origins 具有挑战性，尤其是因为它们尚未最终定稿，细节可能会发生变化。
   - 一位用户表示愿意等待该主题更加明确，而不是过早地提出更多问题。

- **变量名中使用空格的命名空间挑战**：有人提出了在变量名中使用空格的可能性，例如 `var xe đạp = 'abc'`，这凸显了编程语言普遍缺乏此类支持。
   - *Darkmatter__ 解释说*，允许空格会显著增加解析器 (Parser) 实现的复杂性，使其变得不切实际。


  

---

### **Notebook LM Discord ▷ #[use-cases](https://discord.com/channels/1124402182171672732/1124403655819415592/1311799704815927407)** (6 条消息): 

> `Notebook LM Podcast 功能, 使用 NotebookLM 进行世界观构建, RAX 占领时代广场, FPD 导致德国政府解体, NotebookLM 的使用案例示例` 


- **Notebook LM Podcast 功能在音频创作方面令人印象深刻**：一位用户称赞了 **Notebook LM** 的能力，仅用 30 分钟就利用有关其**德国少棒联盟计划**（包括其历史性的世界系列赛参赛资格）的文档创建了一个音频播客。
   - 该剧集可在 [weplayball.de](https://weplayball.buzzsprout.com/1787721/episodes/16191436-episode-9-home-run-fur-deutschland-die-little-league-baseball-story) 上收听，展示了 AI 生成内容的无缝集成。

- **使用 NotebookLM 进行世界观构建**：一位用户分享了使用 **NotebookLM** 为一部史诗奇幻小说构建世界观的经验，强调了该模型提供准确且具备上下文感知能力的响应。
   - 该用户注意到 AI 独特的推理能力，基于现有规则为其魔法系统带来了新的见解和机制。

- **RAX 以大胆的信息占领时代广场**：在一场艺术性的数字表演中，赛博朋克浣熊 **RAX** 占领了时代广场的广告牌，以“不要购买你看到的一切”为口号倡导理性消费。
   - 该活动在一段 [YouTube 视频](https://youtu.be/ZAXwrUduAt0?feature=shared) 中进行了讨论，强调了质疑消费文化的必要性。

- **FPD 在德国的政治博弈**：**FDP** 正计划解散由总理 **Gerhard Schröder** 领导的联合政府，并制定了一项策略，将其退出描述为政治进步的必要之举。
   - 他们的内部文件提供了关键的叙述和时间表，以确保德国公众在即将到来的选举中拥有明确的选择。

- **展示 NotebookLM 的使用案例**：一位用户分享了一个 [YouTube 视频](https://youtu.be/po0FElaSrI4) 链接，展示了 **NotebookLM** 的个人使用案例，突出了其灵活性和功能。
   - 这展示了用户如何在各种应用中发现 **NotebookLM** 的价值。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://weplayball.buzzsprout.com/1787721/episodes/16191436-episode-9-home-run-fur-deutschland-die-little-league-baseball-story">第 9 集 | Home Run für Deutschland: Die Little League Baseball Story - weplayball.de 播客</a>: 🤖 欢迎来到新的 AI 世代 🎙️ 从危机到复苏：德国少棒联盟的惊人故事。weplayball 展示了一个关于这一非凡事件的新播客剧集...</li><li><a href="https://youtu.be/ZAXwrUduAt0?feature=shared">🌐🚨 突发：世界轰动！时代广场广告牌占领 🚨🌐</a>: 🌐🚨 突发：世界轰动！时代广场广告牌占领 🚨🌐 历史在我们这个时代最耀眼、霓虹闪烁的反叛中诞生了！认识一下 RAX，这只...</li><li><a href="https://unrelated.works/podcast/deep-dive-fpd-breaks-up-the-german-government/">深度解析：FPD 导致德国政府解体 &#8211; Unrelated Works</a>: 未找到描述
</li>
</ul>

</div>
  

---

### **Notebook LM Discord ▷ #[general](https://discord.com/channels/1124402182171672732/1124402182909857966/1311801919873880126)** (17 条消息🔥): 

> `GenFM 与 NotebookLM 的竞争，更改 NotebookLM 的语言设置，将 NotebookLM 用于游戏和世界观构建，社会心理学咨询` 


- **GenFM 进入 AI 播客领域**：一位成员分享了一个 [YouTube 视频](https://youtu.be/x6ub-9HhxGU)，标题为“GenFM，现已在 ElevenReader 上线：由生成式 AI 制作的智能播客”，强调了 AI 领域的竞争。
   - 尽管令人兴奋，但另一位成员指出，NotebookLM 仍然提供比 GenFM 更深层次的交互体验。

- **语言设置困扰**：成员们一直在讨论如何更改 NotebookLM 的语言设置，特别是对于那些使用法语等不同语言进行学习的用户。
   - 有人建议更改 Google 账号语言，而其他人则想知道是否有不影响账号设置的其他方法。

- **探索 NotebookLM 在游戏中的应用**：一位成员分享了使用 NotebookLM 进行游戏的乐趣，特别是通过规则内容的 PDF 来探索游戏机制。
   - 他们强调了它在游戏机制以及像 DnD 这种游戏的设定/世界观构建方面的实用性。

- **寻求社会心理学方面的帮助**：一位成员寻求社会心理学主题的协助，促使另一位成员询问具体需求以进一步明确。
   - 这展示了社区提供帮助的意愿，尽管并非所有问题都得到了即时回应。



**提到的链接**：<a href="https://youtu.be/x6ub-9HhxGU">GenFM, Now Playing on ElevenReader: Smart Podcasts Produced by Generative AI</a>：我们正在让 ElevenReader 应用变得更加强大。你现在可以从任何 PDF、文章、电子书、文档或导入的内容中生成智能个人播客...

---

### **Latent Space ▷ #[ai-general-chat](https://discord.com/channels/822583790773862470/1075282825051385876/1311828493046255698)** (18 条消息🔥): 

> `Perplexity 黑色星期五优惠, AI 与人类对比, 企业级生成式 AI, Freysa AI Agent 挑战, 技术采用趋势` 


- **Perplexity 巧妙的黑色星期五活动**：Perplexity 为黑色星期五推出了一项引人注目的有趣活动，链接见[此处](https://x.com/AravSrinivas/status/1861938387923701866)。这一举措符合利用 AI 能力的营销趋势。

- **人类在模式识别方面优于 AI**：人们达成共识，虽然 AI 计算速度更快，但人类在发现复杂问题中的全局模式方面表现更出色，在面对不合逻辑的结果时，通常会说 *“等一下，这不对劲”*。
   - 这种退后一步审视全局的能力与 AI 形成了对比，AI 可能会陷入特定的局部问题。

- **生成式 AI 成为企业的关键任务**：最新报告显示，2024 年 AI 支出飙升至 **138 亿美元**，反映出企业正从实验阶段转向核心业务战略。
   - 尽管投资不断增长，许多决策者仍在摸索有效的集成方式，超过三分之一的人对生成式 AI 的实施缺乏清晰的愿景。

- **成功说服 Freysa AI 释放资金**：在一项 AI 挑战中，有人通过巧妙的 Prompt 绕过了严格的转账指令，说服 Freysa Agent 转账 **47,000 美元**，这突显了用于 AI 操纵的 Prompt Engineering 的复杂性。
   - 该实验展示了 AI 在加密货币领域的独特应用，其透明且开源的设置吸引了许多参与者。

- **技术采用与投资趋势**：观察到的技术趋势类似于历史性的市场转变，将 LLM 与过去引发兴奋及随后市场修正的技术现象进行了对比。
   - 关于 AI 技术可持续性和未来盈利能力的持续讨论，呼应了早期航空等行业的模式。


<div class="linksMentioned">

<strong>提到的链接</strong>:

<ul>
<li>
<a href="https://calpaterson.com/porter.html">Building LLMs is probably not going be a brilliant business</a>: AI 领域的 Netscape</li><li><a href="https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/">2024: The State of Generative AI in the Enterprise - Menlo Ventures</a>: 企业 AI 版图正在实时重写。我们调查了 600 位美国企业 IT 决策者，以揭示新兴的赢家和输家。</li><li><a href="https://x.com/ror_fly/status/1861515830296564214?s=46">来自 Rory Flynn (@Ror_Fly) 的推文</a>: RUNWAY + MINIMAX + KLING → 史诗级。每个视频工具都有其优势。Runway → 控制力 + 清晰度；Minimax → 创造力 + 动态；Kling → 运动笔刷 + 多主体（全部使用）。MJ PROMPT 1: wide angle d...</li><li><a href="https://x.com/tonywu_71/status/1862115197608948078?s=46&t=PW8PiFwluc0tdmv2tOMdEg">来自 Tony Wu (@tonywu_71) 的推文</a>: 🚀 新 Cookbook：使用单个 ColQwen2 模型通过适配器热插拔实现完整的 RAG 流水线。可在免费层级的 Colab T4 上运行。详情请查看 https://github.com/tonywu71/colpali-cookbo...</li><li><a href="https://x.com/amgauge/status/1862310529038983668">来自 Augustinas Malinauskas (@amgauge) 的推文</a>: @jarrodWattsDev @freysa_ai 非常酷的总结 @jarrodWattsDev！不过有一点需要澄清——看交易记录，似乎 70% 进入了奖池，15% 进行了 ETH -> FAI 的兑换。所以所有玩家...</li><li><a href="https://menlovc.com/2024-the-state-of-generative-ai-">2024: The State of Generative AI in the Enterprise - Menlo Ventures</a>: 企业 AI 版图正在实时重写。我们调查了 600 位美国企业 IT 决策者，以揭示新兴的赢家和输家。</li><li><a href="https://x.com/AravSrinivas/status/1861938387923701866">来自 Aravind Srinivas (@AravSrinivas) 的推文</a>: Perplexity 黑色星期五优惠 </li><li><a href="https://x.com/jarrodwattsdev/status/1862299845710757980?s=46">来自 Jarrod Watts (@jarrodWattsDev) 的推文</a>: 有人刚刚通过说服一个 AI Agent 向其发送所有资金赢得了 50,000 美元。11 月 22 日晚上 9:00，一个 AI Agent (@freysa_ai) 发布了，它只有一个目标……不要转账。在……之下……</li><li><a href="https://steelph0enix.github.io/posts/llama-cpp-guide/">llama.cpp guide - Running LLMs locally, on any hardware, from scratch</a>: 嘿，孩子，想要一些便宜又小巧的 LLM 吗？
</li>
</ul>

</div>
  

---

### **Stability.ai (Stable Diffusion) ▷ #[general-chat](https://discord.com/channels/1002292111942635562/1002292112739549196/1311871448968859669)** (18 messages🔥): 

> `AI Model Performance, Stable Diffusion Hardware Questions, ControlNet for SD 3.5 Feedback, Content Creation Queries, LoRA Model Request` 


- **对 SD 3.5 的 ControlNet 体验褒贬不一**：一位成员对 **SD 3.5 的 ControlNet** 表示不满，指出它只有在 **1024x1024** 分辨率下才能生成没有伪影的高质量渲染图。
   - 作为回应，另一位成员建议这些问题可能源于*缺乏熟悉度*，并鼓励通过实验来更好地理解其功能。

- **寻求 Stable Diffusion 的硬件建议**：一位用户询问性能基准，透露他们达到了约 **5 IT/s**，并询问这算好还是坏。
   - 社区在分享硬件能力方面非常活跃，表明了对优化 **Stable Diffusion** 配置的浓厚兴趣。

- **AI 艺术中的 LoRA 模型请求**：一位用户询问是否有人知道 **LoRA half girl 模型**，旨在创建一个融合了两种不同女性设计的角色。
   - 这表明在 AI 生成艺术的角色开发中，实验和创意仍在持续。

- **内容创作者的感恩节祝福**：一位成员向 Stability.ai 团队和其他创作者表达了**感恩节快乐**的祝福，促进了社区归属感。
   - 这突显了 AI 领域内容创作者之间的战友情谊和协作精神。


  

---


### **tinygrad (George Hotz) ▷ #[general](https://discord.com/channels/1068976834382925865/1068976834928193609/1311900211739754526)** (14 messages🔥): 

> `TinyFPGA Memory Hierarchy, Memory Utilization Techniques, Exa Laboratories, Tenstorrent Training Algorithm, Brain-like Processing Models` 


- **TinyFPGA 的潜在内存架构**：成员们讨论了 TinyFPGA 的设计，思考如何模拟典型的 **memory hierarchy**（内存层级），同时指出 **Block RAM** 和 **DDR3** 等现有选项是不够的。
   - 有人提出了 **'first pass' memory** 的想法，将常量定位在 ALU 附近，这可能会显著提高性能。

- **传统内存模型的挑战**：随着未来设计转向更高效的内存层级，**Heuristic eviction policies**（启发式逐出策略）可能会过时。
   - 有人对**训练参数**的未来进行了推测，提到 **tensors** 可能会取代它们。

- **Exa Laboratories 与可持续芯片设计**：关于 Exa Laboratories 的讨论强调了他们的使命是创建 **reconfigurable chips**（可重构芯片），在特定 AI 需求下的**速度**和**能效**方面超越传统的 GPU/TPU。
   - 对其可行性的怀疑引发了关于小公司在芯片开发中面临挑战的评论，特别是在雄心勃勃的时间表下。

- **Tenstorrent 与生物学启发式训练**：George Hotz 提到 **Tenstorrent** 是一个严肃的参与者，押注于转向模拟生物过程的训练算法，旨在实现更高的效率。
   - 潜在的变化包括**分层内存模型**和类似于大脑功能原理的实时优化。

- **计算中的类脑处理**：一位成员描述了一种将**计算和内存**更自然地集成的计算愿景，从而提高**能效**并实现实时优化。
   - 这种方法提议建立一个计算片段模拟大脑协调的系统，从而实现内存使用的灵活性和效率。



**提到的链接**：<a href="https://exalaboratories.com/#about">Exa Laboratories</a>：未找到描述

  

---

### **tinygrad (George Hotz) ▷ #[learn-tinygrad](https://discord.com/channels/1068976834382925865/1070745817025106080/1312003457816723457)** (3 messages): 

> `VIZ tool, VIZ vs LLVM/MLIR, tinygrad tutorials` 


- **解释 VIZ 工具**：一位成员写了一篇详细的帖子来解释 **VIZ tool**，可以在[这里](https://github.com/mesozoic-egg/tinygrad-notes/blob/main/20241129_viz.md)找到。该帖子旨在增强对其在 tinygrad 中的功能和应用的理解。
   - 该帖子包含一个针对希望熟悉 **VIZ** 功能的用户的全面教程。

- **George Hotz 认可 VIZ**：George Hotz 发推文讨论了对 VIZ 工具的解释，并对帖子中提供的清晰说明表示赞赏。他表示 **VIZ=1 相比 LLVM/MLIR 是一个巨大的胜利**，强调了它的优势。
   - 这一评论表明了对 VIZ 的积极反响，以及它在特定用例中相比现有工具的潜在优越性。



**提到的链接**：<a href="https://github.com/mesozoic-egg/tinygrad-notes/blob/main/20241129_viz.md">tinygrad-notes/20241129_viz.md at main · mesozoic-egg/tinygrad-notes</a>：tinygrad 教程。通过在 GitHub 上创建账号来为 mesozoic-egg/tinygrad-notes 的开发做出贡献。

  

---


### **Cohere ▷ #[discussions](https://discord.com/channels/954421988141711382/954421988783444043/1311822642000695306)** (12 messages🔥): 

> `Thanksgiving celebrations, Aya project contributions, Healthy meal choices, Food sharing, Dungeness crab` 


- **感恩节祝福与节日餐盘**：成员们互相发送 *Happy Thanksgiving* 祝福并分享他们的餐食，其中一位成员分享了一盘令人印象深刻的食物。
   - 另一位成员评论说尝试吃得健康，并幽默地提到这并没有想象中那么好吃。

- **关于贡献 Aya 项目的指导**：一位成员寻求关于如何兼职贡献 **Cohere 的 Aya project** 的指导。
   - 另一位成员建议加入 [Aya server](https://discord.gg/8kzwCTd7) 以直接与社区取得联系。

- **食物摄影与互动**：成员们分享了他们丰盛餐点的评论和图片，其中一人开玩笑说食物的分量大得更像是甜点而不是正餐。
   - 随后出现了一个幽默的评论，说之前已经吃了一盘 **Dungeness crab**，增添了食物分享的氛围。

- **分享食物视频**：一位成员通过在频道中发布视频参与了食物分享对话。
   - 这种交流在感恩节期间营造了一种以食物为中心的社区感和庆祝氛围。


  

---


### **DSPy ▷ #[general](https://discord.com/channels/1161519468141355160/1161519469319946286/1311839683658911885)** (8 messages🔥): 

> `dspy.asyncify, dspy demo behavior, New Member Introduction` 


- **关于 dspy.asyncify 支持的咨询**：一位成员询问是否有人开始使用 `dspy.asyncify`，特别注意到它对线程的使用，并由于 celery workers 的问题质疑是否提供纯异步（pure async）支持。
   - 另一位用户对此表示赞同，表达了对 **pure async support** 的渴望。

- **dspy 中带有断言的 demo 行为**：有成员担心在激活断言（assertions）时 `dspy` 不会在最终 prompt 中使用 demo，一位用户质疑这是否为预期行为。
   - 另一位成员澄清说，在 _retry_ 模式下是否存在 demonstration 取决于编译（compilation）是在激活断言之前还是之后完成的。

- **热烈欢迎新成员 Shaun**：一位名叫 Shaun 的新成员加入了服务器，向大家打招呼，并表示很高兴看到正在进行的项目。
   - 社区热烈欢迎了 Shaun，营造了一个包容的环境。


  

---

### **Torchtune ▷ #[dev](https://discord.com/channels/1216353675241590815/1236040539409879170/1311908281857347606)** (5 messages): 

> `DPO 微调, 全参数 DPO, DPO vs LoRA-DPO, 全量微调 DPO` 


- **DPO 与 LoRA-DPO：技术相似，代码不同**：虽然来自 Hugging Face 的 [DPO Trainer](https://huggingface.co/docs/trl/en/dpo_trainer#dpo-trainer) 代码实现不同，但 **DPO 技术在不同仓库（如 LoRA-DPO）之间保持一致**。
   - *这取决于你如何定义*
- **全参数 DPO 的可能性**：实现 **全参数 DPO** 是可行的，并且与 LoRA-DPO 相比，可能提供更好的训练后对齐（alignment）效果。
   - 社区建议参考现有的 **全量 PPO** 实现作为引导。

- **创建 dpo_full_finetune_single_device**：由另一位用户发起的 PR 旨在 **为分布式设置添加全量微调 DPO**，并可作为单设备实现的良好起点。
   - 可以通过 [全量 DPO PR](https://github.com/pytorch/torchtune/pull/1966) 的链接访问更多详情。

- **向全量微调 DPO 过渡**：Torchtune 即将支持 **全量微调 DPO**，这意味着加载独立参考模型的调整将是关键。
   - 对当前设置的修改将涉及更改对参考模型的初始调用，以提升功能性。

- **FFT DPO 的内存影响**：由于需要存储梯度并维护完整的模型副本，**FFT DPO 的内存占用将显著高于** LoRA。
   - 如果 LoRA DPO 效果不佳，那么权衡利弊后引入全量微调可能是值得考虑的。


<div class="linksMentioned">

<strong>提到的链接</strong>：

<ul>
<li>
<a href="https://github.com/pytorch/torchtune/pull/1966">full dpo by jxmsML · Pull Request #1966 · pytorch/torchtune</a>：上下文：此 PR 的目的是什么？是添加新功能、修复 Bug、更新测试和/或文档还是其他（请在此处添加）。请链接此 PR 解决的任何 Issue。Changelog...</li><li><a href="https://huggingface.co/docs/trl/en/dpo_trainer#dpo-trainer)?">DPO Trainer</a>：未找到描述</li><li><a href="https://github.com/pytorch/torchtune/blob/32e265d5749fd592711a03247486eafa6c898d94/recipes/ppo_full_finetune_single_device.py#L435)).">torchtune/recipes/ppo_full_finetune_single_device.py at 32e265d5749fd592711a03247486eafa6c898d94 · pytorch/torchtune</a>：PyTorch 原生微调库。通过在 GitHub 上创建账号为 pytorch/torchtune 的开发做出贡献。</li><li><a href="https://github.com/pytorch/torchtune/blob/32e265d5749fd592711a03247486eafa6c898d94/recipes/lora_dpo_single_device.py#L534C2-L535C4)">torchtune/recipes/lora_dpo_single_device.py at 32e265d5749fd592711a03247486eafa6c898d94 · pytorch/torchtune</a>：PyTorch 原生微调库。通过在 GitHub 上创建账号为 pytorch/torchtune 的开发做出贡献。
</li>
</ul>

</div>
  

---


### **LLM Agents (Berkeley MOOC) ▷ #[mooc-questions](https://discord.com/channels/1280234300012494859/1280370030609170494/1312091243072716842)** (3 messages): 

> `Quiz 11 可用性, OpenAI 额度查询, MOOC 证书资格` 


- **Quiz 11 仍未开放？**：一位成员对 **Quiz 11** 的状态表示困惑，询问为何尚未开放。
   - *是否有预计开放的日期？*
- **OpenAI 额度查询**：一位用户询问了其 **OpenAI 额度** 的状态，提到他们上周填写了表格。
   - *他们表达了紧迫性，表示需要这些额度来支持他们的项目开发。*
- **MOOC 完成与证书**：一位成员询问现在开始学习 **MOOC** 是否仍能在完成后获得证书。
   - *他们还很好奇在剩余时间内完成所有要求是否可行。*


  

---


### **OpenInterpreter ▷ #[ai-content](https://discord.com/channels/1146610656779440188/1149229778138824765/1311957130873540639)** (2 messages): 

> `受 Open Interpreter 启发的项目, 开源仪表盘` 


- **正在开发中的 Open Interpreter 原型**：一位成员分享称，他们正在开发一个受 **Open Interpreter** 启发的项目，重点是创建一个 **实际的仪表盘**。
   - 他们计划在今年将其开源，并强调这将是一个 **有趣的个人小项目**，不带任何盈利目的。

- **社区对开发的支​​持**：另一位成员对项目创建者的努力表示祝贺，并评论道：**“干得漂亮！做得好 🚀”**。
   - 这次简短的交流突显了社区对该领域创新项目的鼓励。


  

---

### **Interconnects (Nathan Lambert) ▷ #[memes](https://discord.com/channels/1179127597926469703/1187551504995987576/1312117051279544353)** (2 条消息): 

> `OLMo 2, Weight Watcher AI, 模型性能对比` 


- **OLMo 2 模型展现出可观的性能**：**OLMo 2** 系列包含来自 Allen AI (AI2) 的 7B 和 13B 模型，在多达 **5T tokens** 上进行训练，其中 7B 的表现优于 [Llama-3.1 8B](https://weightwatcher.ai/models/Llama3.1/Llama-3.1-8B-Instruct.html)，13B 的表现优于 [Qwen 2.5 7B](https://weightwatcher.ai/models/Qwen2.5-small/Qwen2.5-7B-Instruct.html)。关键改进包括使用 **RMSNorm** 和 **QK-Norm** 的增强架构，以及全面的两阶段课程学习训练方法。

- **OLMo 2 训练中的创新技术**：OLMo 2 的显著进展包括用于最终检查点的 **model souping 技术**，以及源自 **Tülu 3** 的最先进后训练方法论。这一新方法包含三个阶段：指令微调（instruction tuning）、使用 DPO 的偏好微调，以及具有可验证奖励的 **强化学习 (reinforcement learning)**。

- **Instruct 变体与顶尖开源权重模型竞争**：据报告，OLMo 2 的 **Instruct 变体** 具有与领先开源权重模型竞争的实力，其中 **13B Instruct** 变体在指令任务中优于 [Qwen 2.5 14B](https://weightwatcher.ai/models/Qwen2.5/Qwen2.5-14B-Instruct.html) 和 **Tülu 3 8B**。其性能已通过 **OLMES suite** 得到验证。

- **Weight Watcher AI 引起关注**：一条评论强调了 **Weight Watcher AI** 网址的新颖性，称其为 AI 领域的精彩补充。有人幽默地提到，由于其趣味性，它被分享在了 **memes** 频道。



**提到的链接**：<a href="https://weightwatcher.ai/models/OLMo-summary.html">WeightWatcher: Data-Free Diagnostics for Deep Learning</a>：未找到描述内容

  

---


### **LlamaIndex ▷ #[general](https://discord.com/channels/1059199217496772688/1059201661417037995/1312125553683464222)** (1 条消息): 

> `Web 开发, JavaScript 框架, 测试工具, API 集成, 云服务` 


- **开发者技能展示**：一位成员分享了广泛的开发技能，包括 **React**、**Next.js**、**Angular** 和 **D3.js**。他们还强调了在 **UI/UX** 以及 **Protractor** 和 **TestCafe** 等各种测试框架方面的经验。

- **多样化的技术栈**：该开发者提到了广泛的技术，如 **Node**、**Nest.js**、**Solidity** 和 **Rust** 等。他们还包括了前端框架知识，以及 **Bootstrap** 和 **BEM**、**SMACSS** 等样式方法论。

- **API 集成专业知识**：他们表达了对集成多种 API 的熟悉程度，包括 **Google Maps**、**YouTube** 和 **Facebook APIs**。这些多样的知识使他们能够处理需要无缝数据交互的各种项目。

- **云部署技能**：该成员强调了 **AWS** 是其云服务能力之一。这为其开发能力增添了显著价值，因为他们可以有效地将应用程序部署到云环境中。

- **寻求合作**：他们最后发出了建立联系的邀请，旨在促进开发者社区内潜在的社交机会。这种外展活动促进了具有相似兴趣的专业人士之间的协作。


  

---


---


---


---


---


---


---


{% else %}


> 邮件中已截断完整的逐频道分析。
> 
> 如果您想查看完整分析，请访问此邮件的网页版：[{{ email.subject }}]({{ email_url }})!
>
> 如果您喜欢 AInews，请[分享给朋友](https://buttondown.email/ainews)！预谢！

{% endif %}